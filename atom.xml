<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>NYSDY</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-08-26T07:50:41.328Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>NYSDY</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Knowledge Graph Embedding via Dynamic Mapping Matrixé˜…è¯»ç¬”è®°</title>
    <link href="http://yoursite.com/post/Knowledge_Graph_Embedding_via_Dynamic_Mapping_Matrix/"/>
    <id>http://yoursite.com/post/Knowledge_Graph_Embedding_via_Dynamic_Mapping_Matrix/</id>
    <published>2019-08-24T11:58:52.000Z</published>
    <updated>2019-08-26T07:50:41.328Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>è®ºæ–‡ä¸‹è½½åœ°å€</p></blockquote><a id="more"></a><h1 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h1><ol><li>å¯¹äºç‰¹å®šçš„å…³ç³»$r$ï¼Œæ‰€æœ‰çš„å®ä½“éƒ½å…±äº«ç›¸åŒçš„æ˜ å°„çŸ©é˜µ$M_r$ã€‚ç„¶è€Œï¼Œç”±å…³ç³»é“¾æ¥çš„å®ä½“æ€»æ˜¯åŒ…å«å„ç§ç±»å‹å’Œå±æ€§ã€‚</li><li>æŠ•å½±æ˜¯å®ä½“å’Œå…³ç³»ä¹‹é—´çš„äº¤äº’è¿‡ç¨‹ï¼Œæ˜ å°„çŸ©é˜µåªèƒ½ç”±å…³ç³»å†³å®šæ˜¯ä¸åˆç†çš„ã€‚</li><li>çŸ©é˜µå‘é‡ä¹˜æ³•ä½¿å…¶å…·æœ‰å¤§é‡è®¡ç®—ï¼Œå¹¶ä¸”å½“å…³ç³»æ•°å¤§æ—¶ï¼Œå®ƒè¿˜å…·æœ‰æ¯”TransEå’ŒTransHæ›´å¤šçš„å‚æ•°ã€‚ ç”±äºå¤æ‚æ€§ï¼ŒTransR / CTransRéš¾ä»¥åº”ç”¨äºå¤§è§„æ¨¡çŸ¥è¯†å›¾</li></ol><h1 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h1><ol><li>ä½œè€…æ„å»ºäº†ä¸€ä¸ªæ–°é¢–çš„æ¨¡å‹TransDï¼Œé€šè¿‡åŒæ—¶è€ƒè™‘å®ä½“å’Œå…³ç³»çš„å¤šæ ·æ€§ï¼Œä¸ºæ¯ä¸€ä¸ªå®ä½“-å…³ç³»æ„å»ºåŠ¨æ€æ˜ å°„çŸ©é˜µã€‚å®ƒä¸ºå®ä½“è¡¨ç¤ºæ˜ å°„åˆ°å…³ç³»å‘é‡ç©ºé—´æä¾›çµæ´»çš„æ ·å¼ã€‚</li><li>ä¸TransR / CTransRç›¸æ¯”ï¼ŒTransDå…·æœ‰æ›´å°‘çš„å‚æ•°å¹¶ä¸”æ²¡æœ‰çŸ©é˜µå‘é‡ä¹˜æ³•</li><li>åœ¨å®éªŒä¸­ï¼Œä½œè€…çš„æ–¹æ³•ä¼˜äºä¹‹å‰çš„æ¨¡å‹ã€‚</li></ol><h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><p>æ¨¡å‹åœ¨TransDä¸­ï¼Œæ¯ä¸ªå‘½åçš„ç¬¦å·å¯¹è±¡ï¼ˆå®ä½“å’Œå…³ç³»ï¼‰ç”±ä¸¤ä¸ªå‘é‡è¡¨ç¤ºã€‚ ç¬¬ä¸€ä¸ªæ•è·å®ä½“ï¼ˆå…³ç³»ï¼‰çš„å«ä¹‰ï¼Œå¦ä¸€ä¸ªç”¨äºæ„é€ æ˜ å°„çŸ©é˜µã€‚</p><p><img src="http://image.nysdy.com/20190826156680488849411.png" alt="20190826156680488849411.png"></p><p>å¯¹äºä¸€ä¸ªä¸‰å…ƒç»„$(h,r,t)$,å‘é‡ä¸€å…±æœ‰$h, h_p, r, r_p, t, t_p$,å…¶ä¸­å¸¦$p$çš„ä¸ºæ˜ å°„å‘é‡ï¼Œåˆ™æœ‰</p><script type="math/tex; mode=display">\begin{aligned} \mathbf{M}_{r h} &=\mathbf{r}_{p} \mathbf{h}_{p}^{\top}+\mathbf{I}^{m \times n} \\ \mathbf{M}_{r t} &=\mathbf{r}_{p} \mathbf{t}_{p}^{\top}+\mathbf{I}^{m \times n} \end{aligned}</script><p>æ•…</p><script type="math/tex; mode=display">\mathbf{h}_{\perp}=\mathbf{M}_{r h} \mathbf{h}, \quad \mathbf{t}_{\perp}=\mathbf{M}_{r t} \mathbf{t}</script><p>å¯ä»¥ç»¼åˆä¸ºï¼š</p><script type="math/tex; mode=display">\begin{aligned} \mathbf{h}_{\perp} &=\mathbf{M}_{r h} \mathbf{h}=\mathbf{h}+\mathbf{h}_{p}^{\top} \mathbf{h} \mathbf{r}_{p} \\ \mathbf{t}_{\perp} &=\mathbf{M}_{r t} \mathbf{t}=\mathbf{t}+\mathbf{t}_{p}^{\top} \mathbf{t} \mathbf{r}_{p} \end{aligned}</script><p>è¿™æ ·å°±æ²¡æœ‰çŸ©é˜µå’Œå‘é‡é—´çš„ä¹˜æ³•è¿ç®—ï¼Œå˜æˆå‘é‡é—´è¿ç®—ï¼Œæå‡è®¡ç®—é€Ÿåº¦ã€‚</p><h1 id="Experiments-and-Results-Analysis"><a href="#Experiments-and-Results-Analysis" class="headerlink" title="Experiments and Results Analysis"></a>Experiments and Results Analysis</h1><p>å¸¸è§„å®éªŒï¼štriplets classification and link predictionä¸å†èµ˜è¿°ã€‚</p><p>ä½œè€…åœ¨å®éªŒè¿‡ç¨‹ä¸­å…³æ³¨äº†ä¸€äº›å…·æœ‰æ›´ä½accuracyçš„å…³ç³»ã€‚</p><p><img src="http://image.nysdy.com/20190826156680558013943.png" alt="20190826156680558013943.png"></p><p>åˆ†æï¼š</p><pre><code>1. å¯¹äº$similar_to$å…³ç³»ä¸»è¦å› ä¸ºè®­ç»ƒæ•°æ®ä¸å……è¶³ï¼Œåªå äº†1.5%ã€‚ 2. å¯¹äºæœ€å³ä¾§çš„å›¾è¯´æ˜äº†bernæ–¹æ³•çš„æ•ˆæœè¦å¥½äºunif</code></pre><h3 id="Properties-of-Projection-Vectors"><a href="#Properties-of-Projection-Vectors" class="headerlink" title="Properties of Projection Vectors"></a>Properties of Projection Vectors</h3><p>ä½œè€…è¿˜åšäº†case studyï¼Œé€šè¿‡ä¸åŒç±»å‹å®ä½“å’Œå…³ç³»çš„æŠ•å½±å‘é‡çš„ç›¸ä¼¼æ€§è¡¨æ˜äº†ä½œè€…æ–¹æ³•çš„åˆç†æ€§</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;è®ºæ–‡ä¸‹è½½åœ°å€&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="è®ºæ–‡é˜…è¯»ç¬”è®°" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="KGE" scheme="http://yoursite.com/tags/KGE/"/>
    
      <category term="TransD" scheme="http://yoursite.com/tags/TransD/"/>
    
  </entry>
  
  <entry>
    <title>From Knowledge Graph Embedding to Ontology Embedding  An Analysis of the Compatibility between Vector Space Representations and Rulesé˜…è¯»ç¬”è®°</title>
    <link href="http://yoursite.com/post/From_Knowledge_Graph_Embedding_to_Ontology_Embedding_An_Analysis_of_the_Compatibility_between_Vector_Space_Representations_and_Rules/"/>
    <id>http://yoursite.com/post/From_Knowledge_Graph_Embedding_to_Ontology_Embedding_An_Analysis_of_the_Compatibility_between_Vector_Space_Representations_and_Rules/</id>
    <published>2019-07-25T08:57:55.000Z</published>
    <updated>2019-07-25T09:04:31.963Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>è®ºæ–‡ä¸‹è½½åœ°å€</p></blockquote><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;è®ºæ–‡ä¸‹è½½åœ°å€&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="è®ºæ–‡é˜…è¯»ç¬”è®°" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>Knowledge graph embedding with conceptsé˜…è¯»ç¬”è®°</title>
    <link href="http://yoursite.com/post/Knowledge_graph_embedding_with_concepts/"/>
    <id>http://yoursite.com/post/Knowledge_graph_embedding_with_concepts/</id>
    <published>2019-07-25T05:19:44.000Z</published>
    <updated>2019-07-25T08:38:02.760Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>è¿™ç¯‡è®ºæ–‡ï¼Œè¿ç”¨skip-gramæ–¹æ³•ï¼Œå°†å®ä½“å¯¹åº”ç›¸å…³æ¦‚å¿µå¼•å…¥å®ä½“å‘é‡è¡¨ç¤ºï¼Œä»¥å¢å¼ºè¡¨ç¤ºæ•ˆæœã€‚å®ä½“å’Œæ¦‚å¿µåœ¨åŒä¸€ç©ºé—´ä¸­ï¼Œä½†æ˜¯æ¦‚å¿µæ˜¯ç©ºé—´ä¸­çš„ä¸€ä¸ªè¶…å¹³é¢ï¼ˆç±»ä¼¼äºtransHï¼‰ã€‚æ–‡ä¸­ä¸¾ä¾‹å¾ˆå¤šä¾‹å­æ¥è¾…åŠ©è¯´æ˜ï¼Œä½¿å¾—æ–‡ç« å¯è¯»æ€§å¤§å¹…æå‡ã€‚æ–‡ä¸­å®éªŒæœ€åä¿©ä¸ªæ¯”è¾ƒæœ‰æ„æ€ã€‚æœ¬æ–‡å€¼å¾—æ€è€ƒå€Ÿé‰´çš„ä¸œè¥¿ä¸å°‘ï¼Œå€¼å¾—å†å¥½å¥½å›é¡¾ã€‚</p><p><a href="https://www.sciencedirect.com/science/article/pii/S0950705118304945/pdfft?md5=b9bad12f2bc771990ad0feaefa7402d4&amp;pid=1-s2.0-S0950705118304945-main.pdf" target="_blank" rel="noopener">è®ºæ–‡ä¸‹è½½åœ°å€</a></p></blockquote><a id="more"></a><h1 id="problem-statement"><a href="#problem-statement" class="headerlink" title="problem statement"></a>problem statement</h1><ul><li>å·²ç»å­˜åœ¨çš„KGEæ¨¡å‹ä¸»è¦é›†ä¸­äºå®ä½“-å…³ç³»-å®ä½“ä¸‰å…ƒç»„æˆ–è€…æ–‡æœ¬è¯­æ–™äº¤äº’ã€‚<ul><li>ä¸‰å…ƒç»„æ˜¯ç¼ºå°‘ä¿¡æ¯çš„ï¼Œå¹¶ä¸”åŸŸå†…æ–‡æœ¬ä¸æ€»æ˜¯å¯ä»¥è·å¾—çš„â€”â€”å¯¼è‡´åµŒå…¥ç»“æœåç¦»å®é™…</li></ul></li><li>å¸¸è¯†æ¦‚å¿µçŸ¥è¯†å‘æŒ¥å¾ˆé‡è¦çš„ä½œç”¨ã€‚</li></ul><h1 id="background"><a href="#background" class="headerlink" title="background"></a>background</h1><blockquote><p>For example, for two triplets (Apple, Developer, IPhone) and (Apple, Developer, Samsung Mobile), it is quite difficult to distinguish which is the true triplet that contains fact triplets only, because â€˜â€˜IPhoneâ€™â€™ and â€˜â€˜Samsung Mobileâ€™â€™ both belong to mobile phones. However, in the concept graph, â€˜â€˜IPhoneâ€™â€™ has a concept â€˜â€˜apple deviceâ€™â€™, but â€˜â€˜Samsung Mobileâ€™â€™ does not. Thus, it is easy to infer the correct triplet by mapping â€˜â€˜IPhoneâ€™â€™ to the â€˜â€˜apple deviceâ€™â€™concept</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">å¾ˆå¥½çš„ä¸€ä¸ªä¸¾ä¾‹å…³äºå¦‚ä½•è¿ç”¨conceptæ¥è¾…åŠ©å…³ç³»è¯†åˆ«</span><br></pre></td></tr></table></figure><blockquote><p>Specifically, when a corpus about technology is provided, embedding methods with technical textual descriptions could easily infer the fact (Apple, Developer, IPhone), because the keywords â€˜â€˜hardware productsâ€™â€™ and â€˜â€˜iPhone smartphoneâ€™â€™ occur frequently in the textual description of â€˜â€˜Appleâ€™â€™. However, it is difficult to infer the fact (Apple, Taste, Sweet), which is irrelevant to textual descriptions of â€˜â€˜Appleâ€™â€™ about the specific topic of â€˜â€˜technology company</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">è¿™é‡Œä½œè€…ä¸¾ä¾‹è¯´æ˜ï¼šä¸å…·æœ‰æ–‡æœ¬ä¿¡æ¯çš„åµŒå…¥æ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰æ¦‚å¿µä¿¡æ¯çš„åµŒå…¥æ–¹æ³•åœ¨å…¶ä»»åŠ¡ä¸­æ›´åŠ é€šç”¨ï¼Œå¹¶ä¸”å®ƒä¸ä¾èµ–äºè¯­æ–™åº“çš„ä¸»é¢˜ã€‚</span><br></pre></td></tr></table></figure><p>ä½œè€…æŠŠKGEåˆ†æˆäº†ä¸‰ç±»ï¼Œå¦‚ä¸‹ï¼š</p><ul><li>Embedding with symbolic tripletsï¼štransç³»åˆ—éƒ½æ”¾åˆ°äº†è¿™éƒ¨åˆ†ä¸­</li><li>Embedding with textual information</li><li>Embedding with category information</li></ul><h1 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h1><h2 id="concept-graph-embedding"><a href="#concept-graph-embedding" class="headerlink" title="concept graph embedding"></a>concept graph embedding</h2><p>ä½œè€…é‡‡ç”¨skip-gramæ¥å­¦ä¹ å¯ä»¥æ•è·å…¶è¯­ä¹‰ç›¸å…³æ€§çš„æ¦‚å¿µå’Œå®ä½“çš„è¡¨ç¤ºã€‚</p><p><img src="http://image.nysdy.com/20190725156403562012591.png" alt="20190725156403562012591.png"></p><p>å…¶ä¸­ï¼Œæ¯ä¸ªå®ä½“å¯¹åº”å¤šä¸ªæ¦‚å¿µï¼Œæ¯ä¸ªæ¦‚å¿µåˆåŒ…å«å¤šä¸ªå®ä½“ï¼ˆè¿™äº›å®ä½“ä½œä¸ºå®ä½“çš„ä¸Šä¸‹æ–‡ï¼‰ã€‚</p><p>åˆ™ï¼Œskip-gramå‡½æ•°å¯ä»¥å†™ä¸ºï¼š</p><script type="math/tex; mode=display">\begin{array}{l}{P\left(e_{c} | e_{t}\right)=\frac{\exp \left(e_{c} \cdot e_{t}\right)}{\sum_{e \in E} \exp \left(e \cdot e_{t}\right)}} \\ {P\left(e_{c} | c_{i}\right)=\frac{\exp \left(e_{c} \cdot c_{i}\right)}{\sum_{e \in E} \exp \left(e \cdot c_{i}\right)}}\end{array}</script><p>æ•…æŸå¤±å‡½æ•°ä¸ºï¼š</p><script type="math/tex; mode=display">L=\frac{1}{|D|} \sum_{\left(e_{c}, e_{t}\right) \in D}\left[\log P\left(e_{c} | e_{t}\right)+\sum_{c_{i} \in C\left(e_{t}\right)} \log P\left(e_{c} | c_{i}\right)\right]</script><p>å­¦ä¹ ç‡è®¾ä¸ºï¼š</p><p>Î± = starting_alphaÃ—(1âˆ’count_actual/(real)(iter Ã— total_size+1))</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">è¿™é‡Œä½œè€…è¯´ä¸ºäº†é¿å…è¿‡æ‹Ÿåˆï¼Œå¯¹ä¼˜åŒ–ç›®æ ‡é‡‡ç”¨â€œè´ŸæŠ½æ ·â€æ–¹æ³•ã€‚&quot;è´ŸæŠ½æ ·&quot;æ–¹æ³•è¿˜å¯ä»¥é¿å…è¿‡æ‹Ÿåˆï¼Ÿ</span><br></pre></td></tr></table></figure><h2 id="knowledge-graph-embedding"><a href="#knowledge-graph-embedding" class="headerlink" title="knowledge graph embedding"></a>knowledge graph embedding</h2><p>å°†ç‰¹å®šä¸‰å…ƒç»„åµŒå…¥åˆ°æ¦‚å¿µå­ç©ºé—´ä¸­ï¼Œé¦–å…ˆæ„å»ºä¸€ä¸ªè¶…å¹³é¢ï¼Œå…¶ä¸­æ³•å‘é‡$c$ä¸ºæ¦‚å¿µå­ç©ºé—´ï¼š</p><script type="math/tex; mode=display">c=C\left(e_{h}, e_{t}\right)=\frac{e_{h}-e_{t}}{\left\|e_{h}-e_{t}\right\|_{2}^{2}}</script><p>æ ¹æ®TransEä¸‰å…ƒç»„çš„åµŒå…¥æŸå¤±ä¸ºï¼š</p><script type="math/tex; mode=display">l=h+r-t</script><p>æ‰€ä»¥ï¼Œå¯ä»¥è®¡ç®—å‡ºæ³•å‘é‡æ–¹å‘ä¸Šçš„æŸå¤±åˆ†é‡æ˜¯ï¼š</p><script type="math/tex; mode=display">\left(c^{T} l c\right)</script><p>ç„¶åï¼ŒæŠ•å½±åˆ°è¶…å¹³é¢ä¸Šçš„å¦ä¸€ä¸ªæ­£äº¤åˆ†é‡æ˜¯ï¼š</p><script type="math/tex; mode=display">\left(l-c^{T} l c\right)</script><p><img src="http://image.nysdy.com/20190725156403899071000.png" alt="20190725156403899071000.png"></p><p>å®šä¹‰æ€»æŸå¤±å‡½æ•°ï¼š</p><script type="math/tex; mode=display">f_{r}(h, t)=-\lambda\left\|l-c^{T} l c\right\|_{2}^{2}+\|l\|_{2}^{2}</script><h2 id="Model-interpretation"><a href="#Model-interpretation" class="headerlink" title="Model interpretation"></a>Model interpretation</h2><ol><li>å¯ä»¥é€šè¿‡æ¦‚å¿µæ¥è¾…åŠ©ä¸‰å…ƒç»„è¯†åˆ«ï¼Œæ–‡ä¸­ä»¥(Christopher Plummer, /people/person/nationality, Canada)ä¸¾ä¾‹</li><li>å¯ä»¥è§£å†³åœ¨å½“ä¸¤ä¸ªå€™é€‰å®ä½“åœ¨KGEï¼Œä¸­è®¡ç®—lossç›¸ç­‰æ—¶è¾¨åˆ«è¿™ä¸¤ä¸ªå“ªä¸ªæ˜¯çœŸå®çš„ã€‚æ–‡ä¸­ä»¥â€œwhich the director made the film â€˜â€˜WALL-Eâ€™â€™â€ä¸ºä¾‹æ¥è¿›è¡Œè¯´æ˜</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">éƒ½æ˜¯é€šè¿‡æŸ¥è¯¢å®ä½“å¯¹åº”æ¦‚å¿µæ¥è¿›è¡Œè¾…åŠ©</span><br></pre></td></tr></table></figure><h1 id="Objectives-and-training"><a href="#Objectives-and-training" class="headerlink" title="Objectives and training"></a>Objectives and training</h1><p>margin-based loss functionï¼š</p><script type="math/tex; mode=display">L=\sum_{(h, r, t) \in S} \sum_{\left(h^{\prime}, r, t^{\prime}\right) \in S_{(h, r, t)}^{\prime}}\left[\gamma+f_{r}(h, t)-f_{r^{\prime}}\left(h^{\prime}, t^{\prime}\right)\right]_{+}</script><h2 id="train"><a href="#train" class="headerlink" title="train"></a>train</h2><ol><li>å…ˆé¢„è®­ç»ƒæ¦‚å¿µå›¾æ¨¡å‹åµŒå…¥ï¼Œè·å¾—åœ¨æ¦‚å¿µç©ºé—´ä¸­çš„å®ä½“å‘é‡</li><li>åˆ©ç”¨1ä¸­è·å¾—çš„å®ä½“å‘é‡è¿›è¡Œæ›´æ–°ã€‚</li></ol><h1 id="datasets"><a href="#datasets" class="headerlink" title="datasets"></a>datasets</h1><ul><li><p>WN18 and FB15K</p></li><li><p>Microsoft Concept Graph<img src="http://image.nysdy.com/20190725156404326996082.png" alt="20190725156404326996082.png"></p><p>å…¶ä¸­ï¼Œrelationsè¡¨ç¤ºé¢‘ç‡</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">çœŸçš„æœ‰ç»Ÿè®¡é¢‘ç‡çš„è¿™ç§</span><br></pre></td></tr></table></figure></li></ul><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h3 id="Knowledge-graph-completion"><a href="#Knowledge-graph-completion" class="headerlink" title="Knowledge graph completion"></a>Knowledge graph completion</h3><h3 id="Entity-classification"><a href="#Entity-classification" class="headerlink" title="Entity classification"></a>Entity classification</h3><h3 id="Concept-relevance-analysis"><a href="#Concept-relevance-analysis" class="headerlink" title="Concept relevance analysis"></a>Concept relevance analysis</h3><p><img src="http://image.nysdy.com/20190725156404278694395.png" alt="20190725156404278694395.png"></p><p>è¿™ä¸ªå®éªŒæ¯”è¾ƒæœ‰æ„æ€ï¼šæ¯ä¸ªå•å…ƒæ ¼ä¸­çš„æ•°å­—è¡¨ç¤ºåœ¨TransEä¸­æ’åå¤§äºmä¸”åœ¨æˆ‘ä»¬çš„æ¨¡å‹ä¸­å°äºnçš„ä¸‰å…ƒç»„çš„æ•°é‡ã€‚</p><h3 id="Precise-semantic-expression-analysis"><a href="#Precise-semantic-expression-analysis" class="headerlink" title="Precise semantic expression analysis"></a>Precise semantic expression analysis</h3><p>æˆ‘ä»¬åœ¨é“¾æ¥é¢„æµ‹ï¼ˆæ¢å¥è¯è¯´ï¼Œè¿™äº›æ˜¯TransEçš„éš¾ä»¥è¯æ˜çš„ä¾‹å­ï¼‰ä¸­æ”¶é›†é‚£äº›å¾—åˆ†ç•¥é«˜äºçœŸå®ä¸‰å…ƒç»„ä½œä¸ºè´Ÿä¸‰å…ƒç»„</p><p>ç„¶ååœ¨KECä¸­å¯¹æ¯”ä¸¤è€…çš„åˆ†æ•°å·®å€¼ã€‚</p><p><img src="http://image.nysdy.com/20190725156404301070773.png" alt="20190725156404301070773.png"></p><p>å³è¾¹æ¡è¡¨ç¤ºKECåœ¨TransEå¤±è´¥æ—¶ä½œå‡ºæ­£ç¡®å†³å®šï¼Œå·¦è¾¹æ¡è¡¨ç¤ºKECå’ŒTransEéƒ½å¤±è´¥ã€‚</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;è¿™ç¯‡è®ºæ–‡ï¼Œè¿ç”¨skip-gramæ–¹æ³•ï¼Œå°†å®ä½“å¯¹åº”ç›¸å…³æ¦‚å¿µå¼•å…¥å®ä½“å‘é‡è¡¨ç¤ºï¼Œä»¥å¢å¼ºè¡¨ç¤ºæ•ˆæœã€‚å®ä½“å’Œæ¦‚å¿µåœ¨åŒä¸€ç©ºé—´ä¸­ï¼Œä½†æ˜¯æ¦‚å¿µæ˜¯ç©ºé—´ä¸­çš„ä¸€ä¸ªè¶…å¹³é¢ï¼ˆç±»ä¼¼äºtransHï¼‰ã€‚æ–‡ä¸­ä¸¾ä¾‹å¾ˆå¤šä¾‹å­æ¥è¾…åŠ©è¯´æ˜ï¼Œä½¿å¾—æ–‡ç« å¯è¯»æ€§å¤§å¹…æå‡ã€‚æ–‡ä¸­å®éªŒæœ€åä¿©ä¸ªæ¯”è¾ƒæœ‰æ„æ€ã€‚æœ¬æ–‡å€¼å¾—æ€è€ƒå€Ÿé‰´çš„ä¸œè¥¿ä¸å°‘ï¼Œå€¼å¾—å†å¥½å¥½å›é¡¾ã€‚&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0950705118304945/pdfft?md5=b9bad12f2bc771990ad0feaefa7402d4&amp;amp;pid=1-s2.0-S0950705118304945-main.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;è®ºæ–‡ä¸‹è½½åœ°å€&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="è®ºæ–‡é˜…è¯»ç¬”è®°" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
      <category term="KG" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/KG/"/>
    
    
      <category term="KGE" scheme="http://yoursite.com/tags/KGE/"/>
    
      <category term="ontology" scheme="http://yoursite.com/tags/ontology/"/>
    
      <category term="concept" scheme="http://yoursite.com/tags/concept/"/>
    
  </entry>
  
  <entry>
    <title>Universal Representation Learning of Knowledge Bases by Jointly Embedding Instances and Ontological Conceptsé˜…è¯»ç¬”è®°</title>
    <link href="http://yoursite.com/post/Universal_Representation_Learning_of_Knowledge_Bases_by_Jointly_Embedding_Instances_and_Ontological_Concepts/"/>
    <id>http://yoursite.com/post/Universal_Representation_Learning_of_Knowledge_Bases_by_Jointly_Embedding_Instances_and_Ontological_Concepts/</id>
    <published>2019-07-17T08:45:52.000Z</published>
    <updated>2019-07-25T08:41:32.245Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p></p><p><a href="http://web.cs.ucla.edu/~yzsun/papers/2019_KDD_JOIE.pdf" target="_blank" rel="noopener">è®ºæ–‡ä¸‹è½½åœ°å€</a></p></blockquote><a id="more"></a><h1 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h1><p>Existing KG embedding models merely focus on representing of an ontology view for abstract and commonsense concepts or an instance view for special entities that are instantiated from ontological concepts. </p><h1 id="Challenge"><a href="#Challenge" class="headerlink" title="Challenge"></a>Challenge</h1><ul><li><strong>mappings difficult</strong> :the semantic mappings from entities to concepts and from relations to meta-relations are complicated and difficult to be precisely captured by any current embedding models</li><li><strong>inadequate cross-view links:</strong> the known cross-view links inadequately cover a vast number of entities, which leads to insufficient information to align both views of the KB, and curtails discovering new cross-view links</li><li>the scales and topological structures are different in ontological views and instance views</li></ul><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p><img src="http://image.nysdy.com/20190722156378173495213.jpg" alt="20190722156378173495213.jpg"></p><p>ä»ä¸¤ç§è§†å›¾æ¥å­¦ä¹ è¡¨ç¤ºæœ‰ä»¥ä¸‹ä¸¤ç‚¹å¥½å¤„ï¼š</p><ul><li>instance embeddings provide detailed and rich information for their corresponding ontological concepts.</li><li>a concept embedding provides a high-level summary of its instances, which is extremely helpful when an instance is rarely observed.</li></ul><h1 id="contribution"><a href="#contribution" class="headerlink" title="contribution"></a>contribution</h1><ul><li>a novel KG embedding model named JOIE, which jointly encodes both the ontology and instance views of a KB<ul><li>cross-view association model : a novel KG embedding model named JOIE, which jointly encodes both the ontology and instance views of a KB<ul><li>cross-view grouping technique : assumes that the two views can be forced into the same embedding space</li><li>cross-view transformation technique : enables non-linear transformations from the instance embedding space to the ontology embedding space</li></ul></li><li>intra-view embedding model : characterizes the relational facts of ontology and instance views in two separate embedding spaces<ul><li>three state-of-the-art translational or similarity-based relational embedding techniques</li><li>hierarchy-aware embedding: based on intra-view non- linear transformations to preserve ontologies hierarchical substructures.</li></ul></li></ul></li><li>implement two experiments:<ul><li>the triple completion task : confirm the effectiveness of JOIE for populating knowledge in both ontology and instance-view KGs, which has significantly outperformed various baseline models.</li><li>the entity typing task :  show that JOIE is competent in discovering cross-view links to align the ontology-view and the instance-view KGs.</li></ul></li></ul><h2 id="Modeling"><a href="#Modeling" class="headerlink" title="Modeling"></a>Modeling</h2><p><img src="http://image.nysdy.com/2019072215637837731893.jpg" alt="2019072215637837731893.jpg"></p><h2 id="Cross-view-Association-Model"><a href="#Cross-view-Association-Model" class="headerlink" title="Cross-view Association Model"></a>Cross-view Association Model</h2><p><img src="http://image.nysdy.com/20190722156378594486796.png" alt="20190722156378594486796.png"></p><h3 id="Cross-view-Grouping-CG"><a href="#Cross-view-Grouping-CG" class="headerlink" title="Cross-view Grouping (CG)"></a>Cross-view Grouping (CG)</h3><p>è¯¥æ¨¡å‹å¯ä»¥è§†ä¸ºgrouping-based regularizationï¼Œ å‡è®¾æœ¬ä½“è§†å›¾KGå’Œå®ä¾‹è§†å›¾KGå¯ä»¥è¢«åµŒå…¥åˆ°åŒä¸€ç©ºé—´ä¸­ï¼Œå¹¶å¼ºåˆ¶ä½¿å®ä¾‹å‘é‡é è¿‘ä¸å®ƒç›¸å…³è”çš„æ¦‚å¿µå‘é‡ï¼Œå¦‚å›¾3(a)æ‰€ç¤º</p><p>å®šä¹‰æŸå¤±å‡½æ•°å¦‚ä¸‹ï¼š</p><script type="math/tex; mode=display">J_{\text { Cross }}^{\mathrm{CG}}=\frac{1}{|\mathcal{S}|} \sum_{(e, c) \in \mathcal{S}}\left[\|\mathbf{c}-\mathbf{e}\|_{2}-\gamma^{\mathrm{CG}}\right]_{+}</script><h3 id="Cross-view-Transformation-CT"><a href="#Cross-view-Transformation-CT" class="headerlink" title="Cross-view Transformation (CT)"></a>Cross-view Transformation (CT)</h3><p>è¯•å›¾åœ¨å®ä½“åµŒå…¥ç©ºé—´å’Œæ¦‚å¿µç©ºé—´ä¹‹é—´è½¬æ¢ä¿¡æ¯ï¼Œå¦‚å›¾3(b)æ‰€ç¤º</p><p>å®šä¹‰æ˜ å°„å‡½æ•°ï¼Œå°†å®ä¾‹æ˜ å°„åˆ°æœ¬ä½“è§†å›¾ç©ºé—´ï¼Œè¯¥æ˜ å°„åå‘é‡åº”è¯¥é è¿‘å®ƒçš„ç›¸å…³è”æ¦‚å¿µï¼š</p><script type="math/tex; mode=display">\mathbf{c} \leftarrow f_{\mathrm{CT}}(\mathbf{e}), \forall(e, c) \in \mathcal{S}</script><p>å…¶ä¸­ï¼Œ</p><script type="math/tex; mode=display">f_{\mathrm{CT}}(\mathbf{e})=\sigma\left(\mathbf{W}_{\mathrm{ct}} \cdot \mathbf{e}+\mathbf{b}_{\mathrm{ct}}\right)</script><p>æ•´ä¸ªCTçš„æŸå¤±å‡½æ•°ä¸ºï¼š</p><script type="math/tex; mode=display">J_{\text { Cross }}^{\mathrm{CT}}=\frac{1}{|\mathcal{S}|} \sum_{(e, c) \in \mathcal{S} \atop \wedge\left(e, c^{\prime}\right) \in \mathcal{S}}\left[\gamma^{\mathrm{CT}}+\left\|\mathbf{c}-f_{\mathrm{CT}}(\mathbf{e})\right\|_{2}-\left\|\mathbf{c}^{\prime}-f_{\mathrm{CT}}(\mathbf{e})\right\|_{2}\right]_{+}</script><h2 id="Intra-view-Model"><a href="#Intra-view-Model" class="headerlink" title="Intra-view Model"></a>Intra-view Model</h2><p>è¯¥æ¨¡å‹çš„ç›®çš„ï¼šåœ¨ä¸¤ä¸ªåµŒå…¥ç©ºé—´ä¸­åˆ†åˆ«ä¿ç•™KBçš„æ¯ä¸ªè§†å›¾ä¸­çš„åŸå§‹ç»“æ„ä¿¡æ¯ã€‚</p><h3 id="Default-Intra-view-Model"><a href="#Default-Intra-view-Model" class="headerlink" title="Default Intra-view Model"></a>Default Intra-view Model</h3><p>ä½œè€…é‡‡ç”¨ä¸‰ç§æ–¹å¼ï¼š</p><script type="math/tex; mode=display">\begin{aligned} f_{\text { TransE }}(\mathbf{h}, \mathbf{r}, \mathbf{t}) &=-\|\mathbf{h}+\mathbf{r}-\mathbf{t}\|_{2} \\ f_{\text { Mult }}(\mathbf{h}, \mathbf{r}, \mathbf{t}) &=(\mathbf{h} \circ \mathbf{t}) \cdot \mathbf{r} \\ f_{\text { HolE }}(\mathbf{h}, \mathbf{r}, \mathbf{t}) &=(\mathbf{h} \star \mathbf{t}) \cdot \mathbf{r} \end{aligned}</script><p>æŸå¤±å‡½æ•°ï¼š</p><script type="math/tex; mode=display">J_{\text { Intra }}^{G}=\frac{1}{|\mathcal{G}|} \sum_{(h, r, t) \in \mathcal{G}}\left[\gamma^{\mathcal{G}}+f\left(\mathbf{h}^{\prime}, \mathbf{r}, \mathbf{t}^{\prime}\right)-f(\mathbf{h}, \mathbf{r}, \mathbf{t})\right]_{+}</script><p>intraæŸå¤±å‡½æ•°ï¼š</p><script type="math/tex; mode=display">J_{\text { Intra }}=J_{\text { Intra }}^{\mathcal{G}_{I}}+\alpha_{1} \cdot J_{\text { Intra }}^{\mathcal{G}_{O}}</script><h3 id="Hierarchy-Aware-Intra-view-Model-for-the-Ontology"><a href="#Hierarchy-Aware-Intra-view-Model-for-the-Ontology" class="headerlink" title="Hierarchy-Aware Intra-view Model for the Ontology"></a>Hierarchy-Aware Intra-view Model for the Ontology</h3><p>è¿›ä¸€æ­¥åŒºåˆ†äº†æ„æˆæœ¬ä½“å±‚æ¬¡ç»“æ„çš„å…ƒå…³ç³»å’Œè§†å›¾å†…æ¨¡å‹ä¸­çš„è§„åˆ™è¯­ä¹‰å…³ç³»(å¦‚â€œrelated_toâ€)ã€‚</p><p>ç»™å®šæ¦‚å¿µå¯¹ï¼ˆclï¼Œchï¼‰ï¼Œæˆ‘ä»¬å°†è¿™ç§å±‚æ¬¡ç»“æ„å»ºæ¨¡ä¸ºç²—ç•¥æ¦‚å¿µå’Œç›¸å…³æ›´ç²¾ç»†æ¦‚å¿µä¹‹é—´çš„éçº¿æ€§å˜æ¢:</p><script type="math/tex; mode=display">g_{\mathrm{HA}}\left(\mathbf{c}_{h}\right)=\sigma\left(\mathbf{W}_{\mathrm{HA}} \cdot \mathbf{c}_{l}+\mathbf{b}_{\mathrm{HA}}\right)</script><p>æŸå¤±å‡½æ•°ä¸ºï¼š</p><script type="math/tex; mode=display">J_{\text { Intra }}^{\mathrm{HA}}=\frac{1}{|\mathcal{T}|} \sum_{\left(c_{l}, c_{h}\right) \in \mathcal{T}}\left[\gamma^{\mathrm{HA}}+\left\|\mathbf{c}_{h}-g\left(\mathbf{c}_{l}\right)\right\|_{2}-\left\|\mathbf{c}_{\mathrm{h}}^{\prime}-g\left(\mathbf{c}_{1}\right)\right\|_{2}\right]_{+}</script><p>æ•…ï¼Œè¯¥éƒ¨åˆ†æŸå¤±å‡½æ•°ä¸ºï¼š</p><script type="math/tex; mode=display">J_{\text { Intra }}=J_{\text { Intra }}^{G_{I}}+\alpha_{1} \cdot J_{\text { Intra }}^{\mathcal{G} o \backslash \mathcal{T}}+\alpha_{2} \cdot J_{\text { Intra }}^{\mathrm{HA}}</script><ul><li>$J_{\text { Intra }}^{\mathcal{G} o} \backslash \mathcal{T}$: é»˜è®¤çš„è§†å›¾å†…æ¨¡å‹çš„ä¸¢å¤±ï¼Œè¯¥æ¨¡å‹ä»…åœ¨å…·æœ‰è§„åˆ™è¯­ä¹‰å…³ç³»çš„ä¸‰å…ƒç»„ä¸Šè®­ç»ƒ</li><li>$J_{\text { Intra }}^{\mathrm{HA}}$æ˜ç¡®è®­ç»ƒä¸‰å…ƒç»„ä¸å½¢æˆæœ¬ä½“å±‚æ¬¡ç»“æ„çš„å…ƒå…³ç³»</li></ul><blockquote><p>æ„Ÿè§‰è¿™éƒ¨åˆ†å°±æ˜¯ä¼ é€’å…³ç³»ï¼Œç±»ä¼¼æ¨ç†æ€§è´¨çš„ã€‚</p><p>æ²¡æ˜ç™½ä¸¤ç§ontologyå…³ç³»çš„åŒºåˆ†ç‚¹</p></blockquote><h2 id="Joint-Training-on-Two-View-KBs"><a href="#Joint-Training-on-Two-View-KBs" class="headerlink" title="Joint Training on Two-View KBs"></a>Joint Training on Two-View KBs</h2><p>è”åˆæŸå¤±å‡½æ•°ï¼š</p><script type="math/tex; mode=display">J=J_{\text { Intra }}+\omega \cdot J_{\text { Cross }}</script><p>ä½œè€…å¹¶ä¸ç›´æ¥æ›´æ–°$J$ï¼Œè€Œæ˜¯äº¤æ›¿æ›´æ–°$J_{\text { Intra }}^{\mathcal{G}_{I}}, J_{\text { Intra }}^{\mathcal{G} O} \text { and } J_{\text { Cross }}$.</p><h1 id="EXPERIMENTS"><a href="#EXPERIMENTS" class="headerlink" title="EXPERIMENTS"></a>EXPERIMENTS</h1><p>å…·ä½“ç»†èŠ‚ç›´æ¥è§è®ºæ–‡</p><h2 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h2><p><img src="http://image.nysdy.com/20190722156379991044398.png" alt="20190722156379991044398.png">æ•°æ®é›†æ˜¯ä½œè€…è‡ªå·±æ„å»ºçš„ï¼Œä¿¡æ¯å¦‚ä¸Šå›¾æ‰€ç¤ºã€‚</p><h2 id="Case-Study"><a href="#Case-Study" class="headerlink" title="Case Study"></a>Case Study</h2><h3 id="Ontology-Population"><a href="#Ontology-Population" class="headerlink" title="Ontology Population"></a>Ontology Population</h3><p>ä½œè€…æƒ³é¢„æµ‹åœ¨å…ƒå…³ç³»è¯è¡¨ä¸­å¹¶ä¸å­˜åœ¨çš„å…ƒå…³ç³»ï¼Œä¾‹å¦‚ï¼šé¢„æµ‹(â€œOffice Holderâ€, ?r, â€œCountryâ€)</p><p>è¿™é‡Œï¼Œä½œè€…é‡‡å–çš„æ–¹å¼æ˜¯å°†æ¦‚å¿µé€šè¿‡ä¹‹å‰æåˆ°çš„å®ä½“ç©ºé—´åˆ°æ¦‚å¿µç©ºé—´çš„æ˜ å°„æ¥è¿›è¡Œåæ˜ å°„ã€‚ç„¶åæŒ‰ç…§$f_{\mathrm{CT}}^{\mathrm{inv}}\left(\mathbf{c}_{\text { country }}\right)-f_{\mathrm{CT}}^{\mathrm{inv}}\left(\mathbf{c}_{\text { office }}\right)$æ¥åœ¨å®ä½“åµŒå…¥ç©ºé—´è¿›è¡Œæœç´¢ä¸ä¹‹ç›¸è¿‘çš„å®ä½“é—´å…³ç³»ã€‚</p><p><img src="http://image.nysdy.com/20190722156380029962081.png" alt="20190722156380029962081.png"></p><h3 id="Long-tail-entity-typing"><a href="#Long-tail-entity-typing" class="headerlink" title="Long-tail entity typing"></a>Long-tail entity typing</h3><p><img src="http://image.nysdy.com/2019072215638007155223.png" alt="2019072215638007155223.png"></p><p>In KGs, the frequency of entities and relations often follow a long-tail distribution (Zipfâ€™s law)</p><p>ä½œè€…æŠ½å–äº†ä½é¢‘æ¬¡å®ä½“è¿›è¡Œäº†è®­ç»ƒï¼Œå‘ç°JOIEæ¨¡å‹çš„æ•ˆæœè™½ç„¶æœ‰ä¸‹é™ï¼Œä½†å°šåœ¨å¯ä»¥æ¥å—çš„ç¨‹åº¦å†…ã€‚</p><h1 id="FUTURE-WORK"><a href="#FUTURE-WORK" class="headerlink" title="FUTURE WORK"></a>FUTURE WORK</h1><ul><li>Particularly, instead of optimizing structure loss with triples (first-order neighborhood) locally, we plan to adopt more complex embedding models which leverage information from higher order neighborhood, logic paths or even global knowledge graph structures. </li><li>We also plan to explore the alignment on relations and meta-relations like entity-concept.</li><li>exploring different triple encoding techniques</li><li>Note that we are also aware of the fact that there are more comprehensive properties of relations and meta-relations in the two views such as logical rules of relations and entity types. Incorporating such properties into the learning process is left as future work.</li></ul><h1 id="æ€è€ƒ"><a href="#æ€è€ƒ" class="headerlink" title="æ€è€ƒ"></a>æ€è€ƒ</h1><p>è¿™ç¯‡è®ºæ–‡å’Œä¹‹å‰è·Ÿå¼ è€å¸ˆå®šçš„æˆ‘çš„è®ºæ–‡çš„æ€è·¯åŸºæœ¬ä¸€è‡´ï¼Œé¢ï¼Œæœ‰ç‚¹æ„Ÿè§‰æœ‰ç‚¹å—æ‰“å‡»ã€‚è¿™ç¯‡æ–‡ç« ä¹Ÿæ˜¯è¯¥ä½œè€…åšå£«æ¯•ä¸šè®ºæ–‡ä¸­çš„ä¸€éƒ¨åˆ†ï¼Œæ‰€ä»¥åº”è¯¥æ˜¯è¿™ä¸ªä½œè€…æ—©å°±æœ‰è¿™ä¸ªæ€è·¯äº†ï¼Œæ‰€ä»¥ä¹Ÿæ²¡ä»€ä¹ˆå¥½çº ç»“çš„ã€‚è¿™ç¯‡æ–‡ç« ä¹Ÿæ˜¯èµ°çš„transçš„è·¯çº¿ï¼Œå’Œåˆ˜çš„è®ºæ–‡åˆä¸ä¸€æ ·çš„æ€è·¯ï¼Œä½†æ˜¯éƒ½æ˜¯æ¦‚å¿µæœ¬ä½“è¿™ç±»çš„ã€‚å…¶ä¸­æœ‰ä¸€ç‚¹ä¸ä¸€æ ·ï¼Œå°±æ˜¯is_aå…³ç³»å¯èƒ½ä¸¤ç¯‡è®ºæ–‡ç”¨çš„ä¸ä¸€æ ·ã€‚è¿™ç¯‡è®ºæ–‡ä¸­æåˆ°äº†æ•°æ®é›†å¼€æºï¼Œå¯æ˜¯githubçš„é“¾æ¥ä¸­å¹¶æ²¡æœ‰æ•°æ®é›†ã€‚è™½ç„¶è½®æ–‡ä¸­è¯´ä»–ç»“åˆäº†æ¦‚å¿µå’Œå®ä¾‹çš„è§†å›¾ï¼Œä½†æ˜¯å…¶å®åƒåˆ˜çš„è®ºæ–‡å°±å·²ç»æå‡ºç»“åˆäº†æ¦‚å¿µå’Œå®ä¾‹çš„è§’åº¦äº†ã€‚</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://web.cs.ucla.edu/~yzsun/papers/2019_KDD_JOIE.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;è®ºæ–‡ä¸‹è½½åœ°å€&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="è®ºæ–‡é˜…è¯»ç¬”è®°" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
      <category term="KG" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/KG/"/>
    
    
      <category term="KGE" scheme="http://yoursite.com/tags/KGE/"/>
    
      <category term="ontology" scheme="http://yoursite.com/tags/ontology/"/>
    
  </entry>
  
  <entry>
    <title>DocRED A Large-Scale Document-Level Relation Extraction Dataseté˜…è¯»ç¬”è®°</title>
    <link href="http://yoursite.com/post/DocRED_A_Large-Scale_Document-Level_Relation_Extraction_Dataset/"/>
    <id>http://yoursite.com/post/DocRED_A_Large-Scale_Document-Level_Relation_Extraction_Dataset/</id>
    <published>2019-07-01T01:11:49.000Z</published>
    <updated>2019-07-01T01:49:37.278Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>è¿™æ˜¯ä¸€ä¸ªä»‹ç»æ•°æ®é›†çš„è®ºæ–‡ï¼Œä¸»è¦æ˜¯æ–‡æ¡£çº§åˆ«çš„å…³ç³»æŠ½å–æ•°æ®é›†ã€‚</p><p><a href="http://arxiv.org/abs/1906.06127" target="_blank" rel="noopener">è®ºæ–‡ä¸‹è½½åœ°å€</a></p></blockquote><h1 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h1><p>existing datasets for document-level RE </p><ul><li>either only have a small number of manually-annotated relations and entities, </li><li>or exhibit noisy annotations from distant supervision, </li><li>or serve specific domains or approaches.</li></ul><h1 id="Contribution-DocRED"><a href="#Contribution-DocRED" class="headerlink" title="Contribution (DocRED)"></a>Contribution (DocRED)</h1><ul><li>constructed from Wikipedia and Wikidata</li><li>DocRED contains 132, 375 entities and 56, 354 relational facts annotated on 5, 053 Wikipedia documents</li><li>As at least 40.7% of the relational facts in DocRED can only be extracted from multiple sentences</li><li><p>also provide large-scale distantly supervised data to support weakly supervised RE research</p></li><li><p>indicate the existing methods deal with the taks document level RE is  more difficult sentence-level RE.</p></li></ul><h1 id="data"><a href="#data" class="headerlink" title="data"></a>data</h1><p><img src="http://image.nysdy.com/20190701156194521958350.jpg" alt="20190701156194521958350.jpg"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;è¿™æ˜¯ä¸€ä¸ªä»‹ç»æ•°æ®é›†çš„è®ºæ–‡ï¼Œä¸»è¦æ˜¯æ–‡æ¡£çº§åˆ«çš„å…³ç³»æŠ½å–æ•°æ®é›†ã€‚&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://arxiv.org/abs/1906.06127&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;è®ºæ–‡ä¸‹è½½åœ°å€&lt;/a&gt;&lt;/p&gt;
      
    
    </summary>
    
      <category term="è®ºæ–‡é˜…è¯»ç¬”è®°" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="RE" scheme="http://yoursite.com/tags/RE/"/>
    
      <category term="dataset" scheme="http://yoursite.com/tags/dataset/"/>
    
  </entry>
  
  <entry>
    <title>Learning Entity and Relation Embeddings for Knowledge Graph Completioné˜…è¯»ç¬”è®°</title>
    <link href="http://yoursite.com/post/Learning_Entity_and_Relation_Embeddings_for_Knowledge_Graph_Completion/"/>
    <id>http://yoursite.com/post/Learning_Entity_and_Relation_Embeddings_for_Knowledge_Graph_Completion/</id>
    <published>2019-06-26T08:18:15.000Z</published>
    <updated>2019-06-27T07:51:44.136Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>TransR embeds entities and relations in distinct entity space and relation space, and learns embeddings via translation between projected entities.CTransR models internal complicated correlations within each relation type.</p><p>è®ºæ–‡ä¸‹è½½åœ°å€</p></blockquote><a id="more"></a><h1 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h1><p>In fact, an entity may have multiple aspects and various relaitons may focus on different aspects of entities, which makes a common space insurficient for modeling.</p><h1 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h1><ul><li>propose a TransR model which models entities and relations in distinct spaces</li><li>CTransR models internal complicated correlations within each relation type.</li><li>experiment on benchmark datasets of WordNet and Freebase and gain consistent improvements compared to state-of-the-art models</li></ul><h1 id="Future-work"><a href="#Future-work" class="headerlink" title="Future work"></a>Future work</h1><ul><li>Existing models including TransR consider each relational fact separately.<ul><li>relation transitive</li></ul></li><li>explore a unified embedding model of both text side and knowledge graph</li><li>modeling internal correlations within each relation type</li></ul><h1 id="TransR"><a href="#TransR" class="headerlink" title="TransR"></a>TransR</h1><p><img src="http://image.nysdy.com/20190627156159912894954.jpg" alt="20190627156159912894954.jpg"></p><ol><li><p>for each triple$(h, r, t)$, entities embeddings are set as $\mathbf{h}, \mathbf{t} \in \mathbb{R}^{k}$ and relation embedding is set as $\mathbf{r} \in \mathbb{R}^{d}$, $k \neq d$</p></li><li><p>for each relation $r$, set a projection matrix $\mathbf{M}_{r} \in\mathbb{R}^{k \times d}$</p><ul><li>projects entities from entity space to relation space</li></ul></li><li><p>projected vectors of entities as </p><script type="math/tex; mode=display">\mathbf{h}_{r}=\mathbf{h} \mathbf{M}_{r}, \quad \mathbf{t}_{r}=\mathbf{t} \mathbf{M}_{r}</script></li><li><p>score function:</p><script type="math/tex; mode=display">f_{r}(h, t)=\left\|\mathbf{h}_{r}+\mathbf{r}-\mathbf{t}_{r}\right\|_{2}^{2}</script></li></ol><h1 id="Cluster-based-TransR-CTransR"><a href="#Cluster-based-TransR-CTransR" class="headerlink" title="Cluster-based TransR (CTransR)"></a>Cluster-based TransR (CTransR)</h1><h3 id="why-propose-CTransR"><a href="#why-propose-CTransR" class="headerlink" title="why propose CTransR"></a>why propose CTransR</h3><p>TransE, TransH and TransR, learn a unique vector for each relation, which may be under-representative to fit all entity pairs under this relation, because these relations are usually rather diverse.</p><h3 id="basic-idea"><a href="#basic-idea" class="headerlink" title="basic idea"></a>basic idea</h3><ul><li>incorporate the idea of piecewise linear regression Ritzema and others 1994</li><li>segment input instances into several groups</li></ul><h3 id="process"><a href="#process" class="headerlink" title="process"></a>process</h3><ol><li><p>for a specific relation r, all entity pairs (h, t) in the training data are clustered into multiple groups, and entity pairs in each group are expected to exhibit similar r relation.</p><ul><li>All entity pairs (h, t) are represented with their vector offsets (h âˆ’ t) for clustering, where h and t are obtained with TransE.</li></ul></li><li><p>learn a separate relation vector $r_c$for each cluster and matrix $M_r$ for each relation, respectively</p></li><li><p>projected vectors of entities as $\mathbf{h}_{r, c}=\mathbf{h} \mathbf{M}_{r} \text { and } \mathbf{t}_{r, c}=\mathbf{t} \mathbf{M}_{r}$</p></li><li><p>sorce fuction</p><script type="math/tex; mode=display">f_{r}(h, t)=\left\|\mathbf{h}_{r, c}+\mathbf{r}_{c}-\mathbf{t}_{r, c}\right\|_{2}^{2}+\alpha\left\|\mathbf{r}_{c}-\mathbf{r}\right\|_{2}^{2}</script><p>the later item aims to ensure cluster-specific relation vector rcnot too far away from the original relation vector r</p></li></ol><h1 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h1><p>é‡‡ç”¨å’Œå‰äººæ‰€ç”¨ä¸€æ ·çš„æ•°æ®é›†</p><div class="table-container"><table><thead><tr><th>Dataset</th><th>#Rel</th><th>#Ent</th><th>#Train</th><th>#Valid</th><th># Test</th></tr></thead><tbody><tr><td>WN18</td><td>18</td><td>40,943</td><td>141,442</td><td>5,000</td><td>5,000</td></tr><tr><td>FB15K</td><td>1,345</td><td>14,951</td><td>483,142</td><td>50,000</td><td>59,071</td></tr><tr><td>WN11</td><td>11</td><td>38,696</td><td>112,581</td><td>2,609</td><td>10,544</td></tr><tr><td>FB13</td><td>13</td><td>75,043</td><td>316,232</td><td>5,908</td><td>23,733</td></tr><tr><td>FB40K</td><td>1,336</td><td>39528</td><td>370,648</td><td>67,946</td><td>96,678</td></tr></tbody></table></div><h1 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h1><p>ä½œè€…é‡‡å–å¸¸è§„å®éªŒ</p><h3 id="Link-Prediction"><a href="#Link-Prediction" class="headerlink" title="Link Prediction"></a>Link Prediction</h3><p>è¿™é‡Œä½œè€…å¯¹å…³ç³»ä¸­èšç±»è¿›è¡Œäº†å±•ç¤ºï¼š <img src="http://image.nysdy.com/2019062715616031534723.jpg" alt="2019062715616031534723.jpg"></p><blockquote><p>æˆ‘è§‰å¾—è¿™ç§æ–¹å¼æ˜¯å€¼å¾—å°è¯•çš„ã€‚</p></blockquote><h3 id="Triple-classification"><a href="#Triple-classification" class="headerlink" title="Triple classification"></a>Triple classification</h3><p>Moreover, the â€œbernâ€ sampling technique improves the performance of TransE, TransH and TransR on all three data sets.</p><blockquote><p>berné‡‡æ ·æ–¹æ³•éœ€è¦æŒæ¡ã€‚</p></blockquote><h3 id="Relation-Extraction-from-Text"><a href="#Relation-Extraction-from-Text" class="headerlink" title="Relation Extraction from Text"></a>Relation Extraction from Text</h3>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;TransR embeds entities and relations in distinct entity space and relation space, and learns embeddings via translation between projected entities.CTransR models internal complicated correlations within each relation type.&lt;/p&gt;
&lt;p&gt;è®ºæ–‡ä¸‹è½½åœ°å€&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="è®ºæ–‡é˜…è¯»ç¬”è®°" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="KGE" scheme="http://yoursite.com/tags/KGE/"/>
    
      <category term="KGR" scheme="http://yoursite.com/tags/KGR/"/>
    
      <category term="TransR" scheme="http://yoursite.com/tags/TransR/"/>
    
  </entry>
  
  <entry>
    <title>Neural Relation Extraction with Selective Attention over Instancesé˜…è¯»ç¬”è®°</title>
    <link href="http://yoursite.com/post/Neural_Relation_Extraction_with_Selective_Attention_over_Instances/"/>
    <id>http://yoursite.com/post/Neural_Relation_Extraction_with_Selective_Attention_over_Instances/</id>
    <published>2019-06-26T05:49:57.000Z</published>
    <updated>2019-06-26T08:12:44.937Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>è¿™ç¯‡æ–‡ç« ä¹‹å‰çœ‹è¿‡ğŸ˜‚ã€‚</p><p>è®ºä¸‹è½½åœ°å€</p></blockquote><a id="more"></a><h1 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h1><p>â€‹    Distant supervision inevitably accompanies with the wrong labelling problem, and thse noisy data will substantially hurt the performance of relation extraction.</p><h1 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h1><ul><li>As compared to existing neural relation extraction model, our model can make full use of all informative sentences of each entity pair.</li><li>To address the wrong labelling problem in distant supervision, we propose selective attention to de-emphasize those noisy instances.</li><li>In the experiments, we show that selective attention is beneficial to two kinds of CNN models in the task of relation extraction.</li></ul><h1 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h1><p>æ¨¡å‹æ•´ä½“æ¶æ„å¦‚ä¸‹æ‰€ç¤ºï¼š</p><p><img src="http://image.nysdy.com/20190626156153649268323.jpg" alt="20190626156153649268323.jpg"></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;è¿™ç¯‡æ–‡ç« ä¹‹å‰çœ‹è¿‡ğŸ˜‚ã€‚&lt;/p&gt;
&lt;p&gt;è®ºä¸‹è½½åœ°å€&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="è®ºæ–‡é˜…è¯»ç¬”è®°" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="RE" scheme="http://yoursite.com/tags/RE/"/>
    
  </entry>
  
  <entry>
    <title>Learning as the Unsupervised Alignment of Conceptual Systemsé˜…è¯»ç¬”è®°</title>
    <link href="http://yoursite.com/post/Learning_as_the_Unsupervised_Alignment_of_Conceptual_Systems/"/>
    <id>http://yoursite.com/post/Learning_as_the_Unsupervised_Alignment_of_Conceptual_Systems/</id>
    <published>2019-06-25T06:42:57.000Z</published>
    <updated>2019-06-26T04:02:25.369Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>è¿™ç¯‡æ–‡ç« æ²¡æ€ä¹ˆçœ‹æ‡‚ï¼Œä¸»è¦æ€æƒ³åº”è¯¥æ˜¯ä»£è¡¨åŒæ—¶æ¦‚å¿µçš„ä¸åŒå½¢å¼ï¼ˆæ–‡æœ¬ï¼Œå›¾åƒï¼Œè¯­éŸ³ç­‰ï¼‰åº”è¯¥å…·æœ‰ç›¸ä¼¼çš„åˆ†å¸ƒï¼Œä»¥æ­¤æ¥è¿›è¡Œæ— ç›‘ç£çš„æ¦‚å¿µå¯¹é½ã€‚è¿™ç§æ€è·¯æŒºä¸é”™çš„ï¼Œä¸è¿‡è¿˜æ²¡æœ‰æ·±å…¥çš„æƒ³æ³•ï¼Œç®—æ˜¯æ‹“å±•è§†é‡å§ï¼</p></blockquote><a id="more"></a><h1 id="KEY"><a href="#KEY" class="headerlink" title="KEY"></a>KEY</h1><p>The key insight is that each concept has a unique signature within one conceptual system (e.g., images) that is recapitulated in other systems (e.g., text or audio)</p><h1 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h1><ul><li>For supervised approaches, as the number of concepts grows, so does the number of required training examples</li><li>V. W. Quine argued, even supervised instruction contains a substantial amount of ambiguity (Quine, 1960).Quine suggested that meaning may derive from somethingâ€™s place within a conceptual system.</li></ul><h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><p>In order to solve Quinneâ€™s problem, we align a system of word labels and a system of visual semantics that both refer to the same underlying reality and therefore have related structure that can be discovered by unsupervised means (Figure 1ï¼‰</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;è¿™ç¯‡æ–‡ç« æ²¡æ€ä¹ˆçœ‹æ‡‚ï¼Œä¸»è¦æ€æƒ³åº”è¯¥æ˜¯ä»£è¡¨åŒæ—¶æ¦‚å¿µçš„ä¸åŒå½¢å¼ï¼ˆæ–‡æœ¬ï¼Œå›¾åƒï¼Œè¯­éŸ³ç­‰ï¼‰åº”è¯¥å…·æœ‰ç›¸ä¼¼çš„åˆ†å¸ƒï¼Œä»¥æ­¤æ¥è¿›è¡Œæ— ç›‘ç£çš„æ¦‚å¿µå¯¹é½ã€‚è¿™ç§æ€è·¯æŒºä¸é”™çš„ï¼Œä¸è¿‡è¿˜æ²¡æœ‰æ·±å…¥çš„æƒ³æ³•ï¼Œç®—æ˜¯æ‹“å±•è§†é‡å§ï¼&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="è®ºæ–‡é˜…è¯»ç¬”è®°" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ERNIE Enhanced Language Representation with Informative Entitiesé˜…è¯»ç¬”è®°</title>
    <link href="http://yoursite.com/post/ERNIE_Enhanced_Language_Representation_with_Informative_Entities/"/>
    <id>http://yoursite.com/post/ERNIE_Enhanced_Language_Representation_with_Informative_Entities/</id>
    <published>2019-06-24T03:10:26.000Z</published>
    <updated>2019-06-25T06:32:57.460Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>è¯¥ç¯‡è®ºæ–‡å€Ÿé‰´BERTï¼Œè¯•å›¾å°†å®ä½“ä¿¡æ¯ï¼ˆTransEï¼‰èå…¥token(singal word)ä¸­ï¼Œé€šè¿‡ç±»ä¼¼å®ä½“å¯¹é½çš„æ–¹æ³•å°†å®ä½“ä¸tokenå¯¹é½ï¼ˆå¹¶é‡‡å–maskæ–¹å¼è¿›è¡Œé¢„è®­ç»ƒï¼‰ï¼Œé€šè¿‡infromation fusion å°†tokenä¸å®ä½“èåˆæ˜ å°„å…¥ç›¸å…³è”çš„ä¸¤ä¸ªå‘é‡ç©ºé—´ã€‚</p><p><a href="https://arxiv.org/pdf/1905.07129" target="_blank" rel="noopener">è®ºæ–‡ä¸‹è½½åœ°å€</a></p></blockquote><a id="more"></a><h1 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h1><p>the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding.</p><h1 id="Challenge"><a href="#Challenge" class="headerlink" title="Challenge"></a>Challenge</h1><p>For incorporating external knowledge into language representation models</p><ul><li>Structured Knowledge Encoding<ul><li>regarding to the given text, how to effectively extract and encode its related informative facts in KGs for language representation models</li></ul></li><li>Heterogeneous Information Fusion<ul><li>how to design a special pre-training objective to fuse the lexical, syntactic, and knowledge information is another challenge.</li></ul></li></ul><h1 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h1><p><img src="http://image.nysdy.com/20190625156142554544546.jpg" alt="20190625156142554544546.jpg"></p><h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p>ERNIE</p><ul><li><p>the underlying textual encoder (T-Encoder)è´Ÿè´£ä»æ–‡æœ¬ä¸­æ•è·åŸºæœ¬çš„è¯æ³•å’Œè¯­æ³•ä¿¡æ¯</p><ul><li><script type="math/tex; mode=display">\left\{\boldsymbol{w}_{1}, \ldots, \boldsymbol{w}_{n}\right\}=\mathrm{T}-\operatorname{Encoder}\left(\left\{w_{1}, \ldots, w_{n}\right\}\right)</script><p>T-Encoder(Â·) is a multi-layer bidirectional Transformer encoder</p></li></ul></li><li><p>the upper knowledgeable encoder (K-Encoder)</p><ul><li>entity embeddings are pre-trained by TransEè´Ÿè´£å°†çŸ¥è¯†å›¾è°±é›†æˆåˆ°åº•å±‚çš„æ–‡æœ¬ä¿¡æ¯ä¸­</li></ul></li></ul><h2 id="Knowledgeable-Encoder"><a href="#Knowledgeable-Encoder" class="headerlink" title="Knowledgeable Encoder"></a>Knowledgeable Encoder</h2><ul><li>the knowledgeable encoder K-Encoder consists of stacked aggregators</li><li>designed for encoding both tokens and entities as well as fusing their heterogeneous features.</li></ul><p>In the i-th aggregator</p><ul><li><p>the input:</p><ul><li>token embeddings: $\left\{\boldsymbol{w}_{1}^{(i-1)}, \ldots, \boldsymbol{w}_{n}^{(i-1)}\right\}$</li><li>entity embeddings :$\left\{\boldsymbol{e}_{1}^{(i-1)}, \ldots, \boldsymbol{e}_{m}^{(i-1)}\right\}$</li></ul></li><li><p>fed into two multi-head self-attentions(MH-ATTs)</p><ul><li>$\left\{\tilde{\boldsymbol{w}}_{1}^{(i)}, \ldots, \tilde{\boldsymbol{w}}_{n}^{(i)}\right\}=\mathrm{MH}-\operatorname{ATT}\left(\left\{\boldsymbol{w}_{1}^{(i-1)}, \ldots, \boldsymbol{w}_{n}^{(i-1)}\right\}\right)$</li><li>$\left\{\tilde{\boldsymbol{e}}_{1}^{(i)}, \ldots, \tilde{\boldsymbol{e}}_{m}^{(i)}\right\}=\mathrm{MH}-\operatorname{ATT}\left(\left\{\boldsymbol{e}_{1}^{(i-1)}, \ldots, \boldsymbol{e}_{m}^{(i-1)}\right\}\right)$</li></ul></li><li><p>an information fusion layer</p><ul><li><script type="math/tex; mode=display">\begin{aligned} \boldsymbol{h}_{j} &=\sigma\left(\tilde{\boldsymbol{W}}_{t}^{(i)} \tilde{\boldsymbol{w}}_{j}^{(i)}+\tilde{\boldsymbol{W}}_{e}^{(i)} \tilde{\boldsymbol{e}}_{k}^{(i)}+\tilde{\boldsymbol{b}}^{(i)}\right) \\ \boldsymbol{w}_{j}^{(i)} &=\sigma\left(\boldsymbol{W}_{t}^{(i)} \boldsymbol{h}_{j}+\boldsymbol{b}_{t}^{(i)}\right) \\ \boldsymbol{e}_{k}^{(i)} &=\sigma\left(\boldsymbol{W}_{e}^{(i)} \boldsymbol{h}_{j}+\boldsymbol{b}_{e}^{(i)}\right) \end{aligned}</script></li></ul><p>$h_j$ is the inner hidden state</p></li></ul><p>For the tokens without corresponding entities</p><script type="math/tex; mode=display">\begin{aligned} \boldsymbol{h}_{j} &=\sigma\left(\tilde{\boldsymbol{W}}_{t}^{(i)} \tilde{\boldsymbol{w}}_{j}^{(i)}+\tilde{\boldsymbol{b}}^{(i)}\right) \\ \boldsymbol{w}_{j}^{(i)} &=\sigma\left(\boldsymbol{W}_{t}^{(i)} \boldsymbol{h}_{j}+\boldsymbol{b}_{t}^{(i)}\right) \end{aligned}</script><h2 id="Pre-training-for-Injecting-Knowledge"><a href="#Pre-training-for-Injecting-Knowledge" class="headerlink" title="Pre-training for Injecting Knowledge"></a>Pre-training for Injecting Knowledge</h2><p>In order to inject knowledge into language rep- resentation by informative entities.</p><p>Randomly masks some token-entity alignments and then requires the system to predict all corresponding entities based on aligned tokens.</p><ul><li><p>denoising entity auto-encoder (dEA)</p></li><li><p>define the aligned entity distribution for the token $w_i$ as follows:</p><script type="math/tex; mode=display">p\left(e_{j} | w_{i}\right)=\frac{\exp \left(\text { linear }\left(\boldsymbol{w}_{i}^{o}\right) \cdot \boldsymbol{e}_{j}\right)}{\sum_{k=1}^{m} \exp \left(\text { linear }\left(\boldsymbol{w}_{i}^{o}\right) \cdot \boldsymbol{e}_{k}\right)}</script></li></ul><p>  linear(Â·) is a linear layer</p><p>For dEA, perform the following operations:</p><ul><li>in 5% of the time, replace the entity with another random<ul><li>aims to train model to correct the errors that the token is aligned with a wrong entity;</li></ul></li><li>In 15% of the time, mask token-entity alignments<ul><li>aims to train model to correct the errors that entity alignment system doesnâ€™t extract all existing alignments;</li></ul></li><li>in the rest of the time, keep token-entity alignments unchanged <ul><li>aims to encourage our model to integrate the entity information into token representations for better language understanding.</li></ul></li></ul><h2 id="Fine-tuning-for-Specific-Tasks"><a href="#Fine-tuning-for-Specific-Tasks" class="headerlink" title="Fine-tuning for Specific Tasks"></a>Fine-tuning for Specific Tasks</h2><p><img src="http://image.nysdy.com/20190625156143137852366.jpg" alt="20190625156143137852366.jpg"></p><p>We can take the final output embedding for the first token, which corresponds to the special [CLS] token, as the representation of the input sequence for specific tasks.</p><p>For some knowledge-driven tasks, we design special fine-tuning procedure:</p><ul><li>relation classification<ul><li>design different tokens [HD] and [TL] for head entities and tail entities respectively</li><li>a similar role like position embeddings in the conventional relation classification models (Zeng et al., 2015)</li></ul></li><li>entity typing<ul><li>the mention mark token [ENT]</li></ul></li></ul><blockquote><ul><li>è¿™é‡Œçš„CLSä¸çŸ¥é“æœ‰ä»€ä¹ˆä½œç”¨ï¼Œæ‰€æœ‰çš„ä»»åŠ¡éƒ½æœ‰ï¼Œæ˜¯ä¸åŒçš„ä»»åŠ¡é‡CLSçš„embeddingæœ‰æ‰€ä¸åŒå—ï¼Ÿä¸ªäººç›®å‰è§‰å¾—æ˜¯è¿™æ ·çš„ã€‚</li><li>ä½œè€…è¿™é‡Œé‡‡ç”¨çš„mark tokençš„æ–¹æ³•ä»£æ›¿position embeddingï¼Œä¸çŸ¥é“ä¸¤ä¸ªå¯¹æ¯”é‚£ç§æ•ˆæœä¼šæ›´å¥½ä¸€äº›ã€‚ç›´è§‚è§‰å¾—éƒ½æ˜¯æ ‡è®°ä½ç½®ä¿¡æ¯ã€‚</li></ul></blockquote><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h3 id="Pre-training-Dataset"><a href="#Pre-training-Dataset" class="headerlink" title="Pre-training Dataset"></a>Pre-training Dataset</h3><ul><li>we use English Wikipedia as our pre-training corpus and align text to Wiki-data<ul><li>4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities</li></ul></li><li>before pre-training ERINE, entity embeddings by TransE<ul><li>sample part of Wikidata which contains 5, 040, 986 entities and 24, 267, 796 fact triples</li></ul></li></ul><h3 id="Training-Details"><a href="#Training-Details" class="headerlink" title="Training Details"></a>Training Details</h3><ul><li>We also fine-tune ERNIE on the distant supervised dataset, i.e., FIGER (Ling et al., 2015)</li><li>we use TAGME (Ferragina and Scaiella, 2010) to extract the entity mentions in the sentences and link them to their corresponding entities in KGs</li></ul><h2 id="Entity-Typing"><a href="#Entity-Typing" class="headerlink" title="Entity Typing"></a>Entity Typing</h2><h3 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h3><p>two well-established datasets FIGER (Ling et al., 2015) and Open Entity (Choi et al., 2018).</p><ul><li>The training set of FIGER is labeled with distant supervision, and its test set is annotated by human.</li><li>Open Entity is a completely manually-annotated dataset.</li></ul><p><img src="http://image.nysdy.com/20190625156143360958681.jpg" alt="20190625156143360958681.jpg"></p><h3 id="Comparble-model"><a href="#Comparble-model" class="headerlink" title="Comparble model"></a>Comparble model</h3><ul><li>NFGEC<ul><li>NFGEC is a hybrid model proposed by Shimaoka et al. (2016)</li></ul></li><li>UFET<ul><li>(Choi et al., 2018)</li></ul></li></ul><h4 id="The-results-on-FIGER"><a href="#The-results-on-FIGER" class="headerlink" title="The results on FIGER:"></a>The results on FIGER:</h4><p>However, BERT has lower accuracy than the best NFGEC model. As strict accuracy is the ratio of instances whose predictions are identical to human annotations, it illustrates <strong>some wrong labels from distant supervision are learned by BERT</strong> due to its powerful fitting ability.</p><h2 id="Relation-Classification"><a href="#Relation-Classification" class="headerlink" title="Relation Classification"></a>Relation Classification</h2><h3 id="dataset-1"><a href="#dataset-1" class="headerlink" title="dataset"></a>dataset</h3><p>two well-established datasets FewRel (Han et al., 2018b) and TACRED (Zhang et al., 2017).</p><ul><li>FewRel<ul><li>As FewRel does not have any null instance where there isnâ€™t any relation between entities, we adopt macro averaged metrics to present the model performances. Since FewRel is built by checking whether the sentences contain facts in Wiki-data, we drop the related facts in KGs before pretraining for fair comparison</li></ul></li><li>TACRED<ul><li>In TACRED, there are nearly 80% null instances so that we follow the previous work (Zhang et al., 2017) to adopt micro averaged metrics to represent the model performances instead of the macro</li></ul></li></ul><p><img src="http://image.nysdy.com/20190625156143680247028.jpg" alt="20190625156143680247028.jpg"></p><h3 id="Comparble-model-1"><a href="#Comparble-model-1" class="headerlink" title="Comparble model"></a>Comparble model</h3><ul><li>CNN:(Zeng et al., 2015).</li><li>PA-LSTM</li><li>C-GCN :Zhang et al. (2018) adopt the graph convolution operations to model dependency trees for relation classification.<graph convolution="" over="" pruned="" dependency="" trees="" improves="" relation="" extraction.=""></graph></li></ul><h2 id="GLUE"><a href="#GLUE" class="headerlink" title="GLUE"></a>GLUE</h2><p>The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is a collection of diverse natural language understanding tasks</p><h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><p>explore the effects of the informative entities and the knowledgeable pretraining task (dEA) for ERNIE using FewRel dataset</p><blockquote><p>å®éªŒéƒ¨åˆ†åšçš„å¾ˆä¸°å¯Œï¼Œæ—¢æœ‰ä¸¤ä¸ªä»»åŠ¡çš„å¯¹æ¯”å®éªŒï¼Œä¹Ÿæœ‰å¯¹è‡ªèº«æ¨¡å—çš„å¯¹æ¯”å®éªŒï¼Œå¹¶ä¸”è¿˜å¯¹æ¯”äº†bertæ¥æ£€æµ‹è‡ªå·±æ¨¡å‹æ˜¯å¦å¯¹GLUEä»»åŠ¡æ•ˆæœæœ‰é™ä½ã€‚</p></blockquote><h1 id="future-research"><a href="#future-research" class="headerlink" title="future research"></a>future research</h1><p>1) inject knowledge into feature-based pre-training models such as ELMo (Peters et al., 2018); </p><p>(2) introduce diverse structured knowledge into language representation models such as ConceptNet (Speer and Havasi, 2012) which is different from world knowledge database Wikidata; </p><p>(3) annotate more real-world corpora heuristically for larger pre-training data</p><blockquote></blockquote><h1 id="å‚è€ƒé“¾æ¥"><a href="#å‚è€ƒé“¾æ¥" class="headerlink" title="å‚è€ƒé“¾æ¥"></a>å‚è€ƒé“¾æ¥</h1><ul><li><a href="https://blog.csdn.net/summerhmh/article/details/91042273" target="_blank" rel="noopener">https://blog.csdn.net/summerhmh/article/details/91042273</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;è¯¥ç¯‡è®ºæ–‡å€Ÿé‰´BERTï¼Œè¯•å›¾å°†å®ä½“ä¿¡æ¯ï¼ˆTransEï¼‰èå…¥token(singal word)ä¸­ï¼Œé€šè¿‡ç±»ä¼¼å®ä½“å¯¹é½çš„æ–¹æ³•å°†å®ä½“ä¸tokenå¯¹é½ï¼ˆå¹¶é‡‡å–maskæ–¹å¼è¿›è¡Œé¢„è®­ç»ƒï¼‰ï¼Œé€šè¿‡infromation fusion å°†tokenä¸å®ä½“èåˆæ˜ å°„å…¥ç›¸å…³è”çš„ä¸¤ä¸ªå‘é‡ç©ºé—´ã€‚&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1905.07129&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;è®ºæ–‡ä¸‹è½½åœ°å€&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="è®ºæ–‡é˜…è¯»ç¬”è®°" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
      <category term="KG" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/KG/"/>
    
    
      <category term="KGR" scheme="http://yoursite.com/tags/KGR/"/>
    
      <category term="KG" scheme="http://yoursite.com/tags/KG/"/>
    
      <category term="BERT" scheme="http://yoursite.com/tags/BERT/"/>
    
  </entry>
  
  <entry>
    <title>Incorporating Literals into Knowledge Graph Embeddingsé˜…è¯»ç¬”è®°</title>
    <link href="http://yoursite.com/post/Incorporating_Literals_into_Knowledge_Graph_Embeddings/"/>
    <id>http://yoursite.com/post/Incorporating_Literals_into_Knowledge_Graph_Embeddings/</id>
    <published>2019-06-03T06:57:50.000Z</published>
    <updated>2019-06-03T07:33:16.832Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>è¯»å®Œäº†å‰ä¸¤ç« ï¼Œç®€å•çš„çœ‹äº†ä¸€ä¸‹ä½œè€…æå‡ºçš„æ¨¡å‹ï¼Œæ„Ÿè§‰å¹¶æ²¡æœ‰å¤ªå¤§ä»·å€¼ï¼Œå°±æ˜¯ç»™å®ä½“è¾“å…¥å¤šåŠ å…¥äº†ä¸€ä¸ªliteralçš„ä¿¡æ¯ï¼ˆåŠ å…¥æ–¹æ³•å¯ä»¥é‡‡ç”¨çº¿æ€§ã€éçº¿æ€§æˆ–è€…ç¥ç»ç½‘ç»œï¼‰ã€‚</p><p>è¯»è®ºæ–‡å‰éœ€è¦å…ˆç†Ÿæ‚‰DistMultã€ComlLExå’ŒConvEæ¨¡å‹ï¼Œæ­¤è®ºæ–‡æ–¹æ³•æ˜¯æ·»åŠ åœ¨è¿™äº›æ–¹æ³•ä¸Šçš„ã€‚</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;è¯»å®Œäº†å‰ä¸¤ç« ï¼Œç®€å•çš„çœ‹äº†ä¸€ä¸‹ä½œè€…æå‡ºçš„æ¨¡å‹ï¼Œæ„Ÿè§‰å¹¶æ²¡æœ‰å¤ªå¤§ä»·å€¼ï¼Œå°±æ˜¯ç»™å®ä½“è¾“å…¥å¤šåŠ å…¥äº†ä¸€ä¸ªliteralçš„ä¿¡æ¯ï¼ˆåŠ å…¥æ–¹æ³•å¯ä»¥é‡‡ç”¨çº¿æ€§ã€éçº¿æ€§æˆ–è€…ç¥ç»ç½‘ç»œï¼‰ã€‚&lt;/p&gt;
&lt;p&gt;è¯»è®ºæ–‡å‰éœ€è¦å…ˆç†Ÿæ‚‰DistMultã€ComlLExå’ŒConvEæ¨¡å‹ï¼Œæ­¤è®ºæ–‡æ–¹
      
    
    </summary>
    
      <category term="è®ºæ–‡é˜…è¯»ç¬”è®°" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
      <category term="KG" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/KG/"/>
    
    
      <category term="KGE" scheme="http://yoursite.com/tags/KGE/"/>
    
      <category term="link prediction" scheme="http://yoursite.com/tags/link-prediction/"/>
    
  </entry>
  
  <entry>
    <title>Learning Knowledge Embeddings by Combining Limit-based Scoring Lossé˜…è¯»ç¬”è®°</title>
    <link href="http://yoursite.com/post/Learning_Knowledge_Embeddings_by_Combining_Limit-based_Scoring_Loss/"/>
    <id>http://yoursite.com/post/Learning_Knowledge_Embeddings_by_Combining_Limit-based_Scoring_Loss/</id>
    <published>2019-06-03T02:57:20.000Z</published>
    <updated>2019-06-03T03:06:20.944Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>æ­¤ç¯‡æ–‡ç« æœ€ä¸ºé‡è¦çš„å°±æ˜¯ä½œè€…è®¾è®¡çš„ margin-based ranking loss çš„æ”¹è¿›ï¼Œå¯¹ä¸¤ä¸ªè¶…å‚æ•°$\lambda$å’Œ$\gamma$çš„å®éªŒï¼Œå¯¹äºå®éªŒç»“æœæœ‰å¾ˆå¤šå€¼å¾—åˆ†æä¸æ€è€ƒçš„åœ°æ–¹ã€‚</p><p><a href="https://dl.acm.org/ft_gateway.cfm?id=3132939&amp;ftid=1920664&amp;dwn=1&amp;CFID=135630312&amp;CFTOKEN=71536805165d7c9d-10D2074A-AD9B-A596-5CA31DB63C36A322" target="_blank" rel="noopener">è®ºæ–‡ä¸‹è½½åœ°å€</a></p></blockquote><a id="more"></a><h1 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h1><p>The margin-based ranking loss function cannot ensure the fact that the scoring of correct triplets must be low enough to fulfill the translation.</p><h1 id="research-objective"><a href="#research-objective" class="headerlink" title="research objective"></a>research objective</h1><p>reduce the scoring of correct triplets to fulfill the translation by mending the margin-based ranking loss function</p><h1 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h1><ul><li>proposing a limit-based ranking loss item combined with margin-based ranking loss </li><li>extending TransE and TransH to TransE-RS and TransH-RS</li></ul><h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><h2 id="Margin-based-Tanking-Loss"><a href="#Margin-based-Tanking-Loss" class="headerlink" title="Margin-based Tanking Loss"></a>Margin-based Tanking Loss</h2><p>formula:</p><script type="math/tex; mode=display">L_{R}=\sum_{(h, r, t) \in \Delta} \sum_{\left(h^{\prime}, r, t^{\prime}\right) \in \Delta^{\prime}}\left[\gamma_{1}+f_{r}(h, t)-f_{r}\left(h^{\prime}, t^{\prime}\right)\right) ]_{+}</script><ul><li><p>The margin-based ranking loss function aims to make the score $f_{r}\left(h^{\prime}, t^{\prime}\right)$ of corrupted triplet higher by at least $\gamma_{1}$ than  of positive triplet.</p></li><li><p>cannot be proved $f_{r}(h, t)&lt;\varepsilon$ </p></li></ul><h2 id="Limit-based-Scoring-Loss"><a href="#Limit-based-Scoring-Loss" class="headerlink" title="Limit-based Scoring Loss"></a>Limit-based Scoring Loss</h2><p>formula:</p><script type="math/tex; mode=display">L_{S}=\sum_{(h, r, t) \in \Delta}\left[f_{r}(h, t)-\gamma_{2}\right]_{+}</script><h2 id="Finally-loss"><a href="#Finally-loss" class="headerlink" title="Finally loss"></a>Finally loss</h2><p>formula:</p><script type="math/tex; mode=display">L_{R S}=L_{R}+\lambda L_{S}, \quad(\lambda>0)</script><p>detail is :</p><script type="math/tex; mode=display">\begin{array}{c}{L_{R S}=\sum_{(h, r, t) \in \Delta} \sum_{\left(h^{\prime}, r, t^{\prime}\right) \in \Delta^{\prime}}\left\{\left[\gamma_{1}+f_{r}(h, t)-f_{r}\left(h^{\prime}, t^{\prime}\right)\right]_{+}\right.} \\ {+\lambda\left[f_{r}(h, t)-\gamma_{2}\right]_{+} \}}\end{array}</script><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h2><p><img src="http://image.nysdy.com/20190603155952634332494.jpg" alt="20190603155952634332494.jpg"></p><h2 id="Link-prediction"><a href="#Link-prediction" class="headerlink" title="Link prediction"></a>Link prediction</h2><blockquote><p>æ€è€ƒ</p><p>ä½œè€…åªæ˜¯å¯¹è¡¨æ ¼çš„æ•°æ®è¿›è¡Œäº†é™ˆè¿°ï¼Œæœ‰ä¸€äº›é—®é¢˜å¹¶æ²¡æœ‰è¿›è¡Œåˆ†æè§£é‡Š</p><ul><li>å¹¶æ²¡æœ‰åˆ†ææ¯”å¦‚è¯´ä¸ºä»€ä¹ˆæ”¹è¿›lossåçš„transEä¸ºä»€ä¹ˆä¼šæ¯”TransHï¼ˆRã€Dï¼‰æ•ˆæœè¦å¥½ï¼Ÿ</li><li>ä¸ºä»€ä¹ˆåœ¨n-to-1ä¸­çš„è¡¨ç°æ•ˆæœæ²¡æœ‰è¾¾åˆ°æœ€å¥½ï¼ˆå…¶ä»–çš„éƒ½è¾¾åˆ°äº†æœ€å¥½ï¼‰ï¼Ÿ</li><li>é€šè¿‡è¿™ç§æ”¹è¿›å¯ä»¥å‘ç°ï¼ŒtransHç›¸æ¯”äºTransEå¹¶æ²¡æœ‰æ˜¾è‘—æå‡ï¼ŒåŸå› æ˜¯ä»€ä¹ˆï¼Ÿ</li></ul></blockquote><h2 id="Triple-Classification"><a href="#Triple-Classification" class="headerlink" title="Triple Classification"></a>Triple Classification</h2><ul><li>TransE-RS and TransH-RS have same parameter and operation complexities as TransE and TransH, which is lower than TransR and TransD.</li><li>Our models randomly initial the entities, not use the learned embeddings by TransE as TransR and TransD.<ul><li>It means that our models have much better ability to overcome the problem of overfitting</li></ul></li></ul><h2 id="Distributions-of-Tripletsâ€™-Scores"><a href="#Distributions-of-Tripletsâ€™-Scores" class="headerlink" title="Distributions of Tripletsâ€™ Scores"></a>Distributions of Tripletsâ€™ Scores</h2><h3 id="aim"><a href="#aim" class="headerlink" title="aim"></a>aim</h3><p>analyze the difference between $L_R$ Loss and our $L_RS$ Loss</p><h3 id="Parameters"><a href="#Parameters" class="headerlink" title="Parameters"></a>Parameters</h3><p><img src="http://image.nysdy.com/20190603155952784132260.jpg" alt="20190603155952784132260.jpg"></p><blockquote><p>æ€è€ƒï¼š</p><ul><li>å¯¹äºæˆ‘è‡ªå·±æ­£åœ¨åšçš„å®éªŒï¼šæ˜¯ä¸æ˜¯æˆ‘è‡ªå·±ç”¨çš„é—´éš”å¤ªå°äº†</li></ul></blockquote><h3 id="result"><a href="#result" class="headerlink" title="result"></a>result</h3><p><img src="http://image.nysdy.com/20190603155952821850690.jpg" alt="20190603155952821850690.jpg"></p><p><img src="http://image.nysdy.com/20190603155952848841252.jpg" alt="20190603155952848841252.jpg"></p><blockquote><p>æ€è€ƒ</p><ul><li>è¿™éƒ¨åˆ†çš„å®éªŒå€¼å¾—å€Ÿé‰´ï¼Œå®ƒå¯ä»¥ç›¸å¯¹äºç›´è§‚çš„å¯ä»¥å±•ç¤ºå‡ºä¸ºä»€ä¹ˆæ•ˆæœä¼šå¥½ã€‚</li><li>æ¯”å¦‚å¯¹äºä¸Šè¿°ä¸ºä»€ä¹ˆæ”¹è¿›åçš„transEçš„æ•ˆæœä¼šæ›´å¥½<ul><li>çœ‹åˆ°æœ€åçš„åˆ†æ•°åˆ†å¸ƒtransE-RSçš„åˆ†å¸ƒæ•ˆæœå’ŒTrans-Hçš„ååˆ†æ¥è¿‘ï¼Œ</li><li>è€ŒtransEçš„æ¨¡å‹è¾ƒä¸ºç®€å•ï¼Œå¯èƒ½æœ€ç»ˆlossæœ€å°åŒ–ä¼šä½¿å¾—æ¨¡å‹å……åˆ†è¡¨è¾¾ï¼Œè€Œå…¶ä»–æ¨¡å‹å¼•å…¥äº†æ›´å¤šçš„å‡è®¾å¯èƒ½ä¼šå¸¦æ¥æ›´å¤šçš„å™ªå£°</li><li>ä¹Ÿå¯èƒ½å½“losså¾ˆå°æ—¶ï¼Œå…¶ä»–çš„å‡è®¾æ¡ä»¶å‘æŒ¥ä½œç”¨çš„å¾ˆå°ï¼ˆè‡³å°‘ä»å®éªŒç»“æœæ¥çœ‹æ˜¯çš„ï¼Œä½†æ˜¯è¿˜æœ‰å¾…äºè¿›ä¸€æ­¥è®¾è®¡å®éªŒéªŒè¯ï¼‰</li></ul></li></ul></blockquote><h2 id="Discussion-of-Parameters"><a href="#Discussion-of-Parameters" class="headerlink" title="Discussion of Parameters"></a>Discussion of Parameters</h2><h3 id="Discussion-on-Î³1-and-Î³2"><a href="#Discussion-on-Î³1-and-Î³2" class="headerlink" title="Discussion on Î³1 and Î³2."></a>Discussion on Î³1 and Î³2.</h3><p><img src="http://image.nysdy.com/20190603155952911595962.jpg" alt="20190603155952911595962.jpg"></p><ul><li>We find that Î³2 = 3Î³1 or Î³2 = 4Î³1 is better for link prediction, but for triplet classification there are not obvious characteristics on Î³1 and Î³2.</li><li>a lower Î³2 is expected to ensure the golden condition $\mathbf{h}+\mathbf{r} \approx \mathbf{t}$ for positive triplets, but an entity needs to satisfy many golden coditions at the same time.</li></ul><blockquote><p>æ€è€ƒ</p><ul><li>æ—¢ç„¶å¦‚ä½œè€…è¯´ï¼Œé‚£ä¹ˆç†è®ºä¸ŠtransHçš„æ•ˆæœåº”è¯¥å¾ˆå¥½æ‰å¯¹ï¼Œä½†æ˜¯ç»“æœå¹¶ä¸æ˜¯è¿™æ ·çš„ï¼Œè¿™åˆäº§ç”ŸçŸ›ç›¾ã€‚</li></ul></blockquote><h3 id="Discussion-on-Î»"><a href="#Discussion-on-Î»" class="headerlink" title="Discussion on Î»"></a>Discussion on Î»</h3><p><img src="http://image.nysdy.com/20190603155953002076189.jpg" alt="20190603155953002076189.jpg"></p><blockquote><p>æ€è€ƒ</p><ul><li>çœ‹åˆ°Î»å¹¶æ²¡æœ‰å¯¹æ¨¡å‹å½±å“å¹¶æ²¡å¾ˆå¤§</li><li>Î»åœ¨1å·¦å³æ˜¯æ•ˆæœä¼šæ¯”è¾ƒå¥½</li><li>Î»å’Œmarginä¼šä¸ä¼šäº§ç”Ÿå…³è”ï¼Ÿ</li></ul></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;æ­¤ç¯‡æ–‡ç« æœ€ä¸ºé‡è¦çš„å°±æ˜¯ä½œè€…è®¾è®¡çš„ margin-based ranking loss çš„æ”¹è¿›ï¼Œå¯¹ä¸¤ä¸ªè¶…å‚æ•°$\lambda$å’Œ$\gamma$çš„å®éªŒï¼Œå¯¹äºå®éªŒç»“æœæœ‰å¾ˆå¤šå€¼å¾—åˆ†æä¸æ€è€ƒçš„åœ°æ–¹ã€‚&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://dl.acm.org/ft_gateway.cfm?id=3132939&amp;amp;ftid=1920664&amp;amp;dwn=1&amp;amp;CFID=135630312&amp;amp;CFTOKEN=71536805165d7c9d-10D2074A-AD9B-A596-5CA31DB63C36A322&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;è®ºæ–‡ä¸‹è½½åœ°å€&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="è®ºæ–‡é˜…è¯»ç¬”è®°" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
      <category term="KG" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/KG/"/>
    
    
      <category term="transH" scheme="http://yoursite.com/tags/transH/"/>
    
      <category term="margin loss" scheme="http://yoursite.com/tags/margin-loss/"/>
    
      <category term="transE" scheme="http://yoursite.com/tags/transE/"/>
    
      <category term="KG" scheme="http://yoursite.com/tags/KG/"/>
    
  </entry>
  
  <entry>
    <title>Knowledge Graph Embedding by Translating on Hyperplanesé˜…è¯»ç¬”è®°</title>
    <link href="http://yoursite.com/post/Knowledge%20Graph%20Embedding%20by%20Translating%20on%20Hyperplanes/"/>
    <id>http://yoursite.com/post/Knowledge Graph Embedding by Translating on Hyperplanes/</id>
    <published>2019-05-28T08:17:44.000Z</published>
    <updated>2019-06-01T05:38:20.042Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>ä½œä¸ºtransç³»åˆ—ç»å…¸æ–‡çŒ®ï¼Œå¿…è¯»ã€‚æ–‡ç« ä¸»è¦ç²¾ååœ¨äºè¿™ç§è¶…å¹³é¢æƒ³æ³•çš„ç”±æ¥è§£å†³äº†åŒä¸€å®ä½“çš„å¤šå…³ç³»é—®é¢˜ã€‚</p><p>Authors proposed TransH which models a relation as a hyperplane together with a translation operation on it. It solves the problem of multi-relation and makes a good trade-off between model capacity and efficiency.</p></blockquote><a id="more"></a><h1 id="æ¨æµ‹transHçš„æƒ³æ³•æ¥æº"><a href="#æ¨æµ‹transHçš„æƒ³æ³•æ¥æº" class="headerlink" title="æ¨æµ‹transHçš„æƒ³æ³•æ¥æº"></a>æ¨æµ‹transHçš„æƒ³æ³•æ¥æº</h1><blockquote><p>æ—¢ç„¶å®é™…æ˜¯è¡¨è¾¾åŒä¸€å…³ç³»ä¸åŒå®ä½“æœ€åé€šè¿‡TransEåä¼šè¶‹äºä¸€è‡´ï¼Œé‚£ä¹ˆæˆ‘ç›´æ¥é€šè¿‡ä¸€ä¸ªä¸­ä»‹æ¥è¿›è¡Œæ˜ å°„å°†åŒä¸€è¡¨ç¤ºæ˜ å°„æˆä¸åŒå‘é‡è¡¨ç¤ºï¼Œé‚£ä¹ˆè¿™äº›å‘é‡è¡¨ç¤ºå°±å¯ä»¥ä»£è¡¨ä¸åŒçš„å®ä½“ï¼Œå°±è¾¾åˆ°äº†ä¸åŒå®ä½“æ‹¥æœ‰ä¸åŒè¡¨ç¤ºçš„ç›®çš„ã€‚å› ä¸ºå…³ç³»æ˜¯ä¸å˜çš„æ‰€ä»¥æƒ³åˆ°äº†å°†å…³ç³»ä½œä¸ºæ˜ å°„å¹³é¢ï¼Œè®©å®ä½“å‘é‡å‘å…¶ä¸­æ˜ å°„ã€‚</p></blockquote><h1 id="research-objective"><a href="#research-objective" class="headerlink" title="research objective"></a>research objective</h1><ul><li>solves the problem of multi-relation </li><li>makes a good trade-off between model capacity and efficiency</li></ul><h1 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h1><ul><li>TransE canâ€™t deal with reflexive, one-to-many, many-to-many and many -to-one relations</li><li>some complex model sacrifice efficiency in the process(although can deal with transEâ€™s problem)</li></ul><h1 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h1><ul><li>proposing a method named <em>translation on hyperplanes</em>(TransH)<ul><li>interpreting a relation as a translating operation on a hyperplane</li></ul></li><li>proposing a simple trick to reduce the chance of false negative labeling</li></ul><h1 id="Embedding-by-Translating-on-Hyperplanes"><a href="#Embedding-by-Translating-on-Hyperplanes" class="headerlink" title="Embedding by Translating on Hyperplanes"></a>Embedding by Translating on Hyperplanes</h1><h2 id="Relationsâ€™-Mapping-Properties-in-Embedding"><a href="#Relationsâ€™-Mapping-Properties-in-Embedding" class="headerlink" title="Relationsâ€™ Mapping Properties in Embedding"></a>Relationsâ€™ Mapping Properties in Embedding</h2><p>transE</p><ul><li>the representation of an  entity is the same when involved in any relations, ignoring <strong>distributed representations of entities when invovled in different relaions</strong></li></ul><h2 id="Translating-on-Hyperplanes-TransH"><a href="#Translating-on-Hyperplanes-TransH" class="headerlink" title="Translating on Hyperplanes (TransH)"></a>Translating on Hyperplanes (TransH)</h2><p><strong>åŒä¸€ä¸ªå®ä½“åœ¨ä¸åŒå…³ç³»ä¸­çš„æ„ä¹‰ä¸åŒ</strong>ï¼ŒåŒæ—¶<strong>ä¸åŒå®ä½“ï¼Œåœ¨åŒä¸€å…³ç³»ä¸­çš„æ„ä¹‰ï¼Œä¹Ÿå¯ä»¥ç›¸åŒ</strong>ã€‚</p><blockquote><p>å°†æ¯ä¸ªå…³ç³»å®šä¹‰åœ¨ä¸€ä¸ªç‹¬ç‰¹çš„å¹³é¢å‘¢ï¼Œåœ¨è¯¥å¹³é¢å†…æœ‰ç¬¦åˆè¯¥å…³ç³»çš„transEçš„è¡¨ç¤ºï¼ˆh,r,t)ï¼Œå¤šåŠ å…¥çš„ä»£è¡¨è¯¥å¹³é¢çš„æ³•å‘é‡å®Œæˆäº†å°†ä¸åŒå®ä½“å‘å¹³é¢å†…å’Œhï¼Œtè½¬åŒ–çš„ä»»åŠ¡ï¼Œä½¿å¾—åŒä¸€å…³ç³»çš„ä¸åŒå®ä½“æ‹¥æœ‰ä¸åŒçš„è¡¨ç¤ºï¼Œä½†æ˜¯åœ¨å…³ç³»å¹³é¢å†…çš„æŠ•å½±ç›¸åŒï¼›åŒä¸€å®ä½“å¯ä»¥åœ¨ä¸åŒçš„å…³ç³»å¹³é¢å†…æ‹¥æœ‰ä¸åŒçš„å«ä¹‰ï¼ˆå¹³é¢å†…çš„æŠ•å½±ï¼‰</p></blockquote><p><img src="http://image.nysdy.com/20190601155935483248827.jpg" alt="20190601155935483248827.jpg"></p><p>å¦‚å›¾æ‰€ç¤ºï¼Œå¯¹äºæ­£ç¡®çš„ä¸‰å…ƒç»„æ¥è¯´$(h, r, t) \in \Delta$ï¼Œæ‰€éœ€æ»¡è¶³çš„å…³ç³»å¦‚å›¾æ‰€ç¤ºã€‚é‚£ä¹ˆå¯¹äºä¸€ä¸ªå®ä½“$hâ€™â€™$å¦‚æœæ»¡è¶³$\left(h^{\prime \prime}, r, t\right) \in \Delta    $ï¼Œåœ¨transEä¸­æ˜¯éœ€è¦$hâ€™â€™=h$ï¼Œè€Œåœ¨transHä¸­åˆ™å°†çº¦æŸæ”¾å®½åˆ°$h,hâ€™â€™$åœ¨$W_r$ä¸Šçš„æŠ•å½±ç›¸åŒå°±å¯ä»¥äº†ï¼Œä¹Ÿå¯ä»¥å®ç°å°†$h,hâ€™â€™$åŒºåˆ†å¼€å¹¶ä¸”å…·æœ‰ä¸åŒçš„è¡¨ç¤ºã€‚</p><h4 id="ç›®æ ‡å‡½æ•°"><a href="#ç›®æ ‡å‡½æ•°" class="headerlink" title="ç›®æ ‡å‡½æ•°"></a>ç›®æ ‡å‡½æ•°</h4><p>scoring functionï¼š</p><script type="math/tex; mode=display">d(h+r, t)=f_{r}(h, t)=\left\|h_{\perp}+d_{r}-t_{\perp}\right\|_{2}^{2}</script><p>As the hyperplane $W_r$, the $w_r$ is the normal vector of it, and $\left|w_{r}\right|_{2}^{2}=1$, so the projection $h$ in $w_r$ is:</p><script type="math/tex; mode=display">h_{w_{r}}=w_r^{T} h w_r</script><p>å…¶ä¸­ï¼Œ$w_r^{T} h=|w_r||h| \cos \theta$å¯ä»¥è¡¨ç¤º$h$åœ¨$w_r$ä¸Šçš„æŠ•å½±çš„é•¿åº¦å’Œ$w_r$é•¿åº¦çš„ä¹˜ç§¯ï¼Œå› ä¸º$\left|w_{r}\right|_{2}^{2}=1$,æ‰€ä»¥å¯ä»¥ä»£è¡¨æŠ•å½±çš„é•¿åº¦ï¼Œå†ä¹˜ä¸Šå•ä½å‘é‡å³å¯è¡¨ç¤ºæŠ•å½±å‘é‡ã€‚æ‰€ä»¥ï¼š</p><script type="math/tex; mode=display">\mathbf{h}_{\perp}=\mathbf{h}-\mathbf{w}_{r}^{\top} \mathbf{h w}_{r}, \quad \mathbf{t}_{\perp}=\mathbf{t}-\mathbf{w}_{r}^{\top} \mathbf{t} \mathbf{w}_{r}</script><p>å¦‚å›¾æ‰€ç¤ºï¼š<img src="http://image.nysdy.com/2019060115593616504994.jpg" alt="2019060115593616504994.jpg"></p><p>the score function is:</p><script type="math/tex; mode=display">f_{r}(\mathbf{h}, \mathbf{t})=\left\|\left(\mathbf{h}-\mathbf{w}_{r}^{\top} \mathbf{h w}_{r}\right)+\mathbf{d}_{r}-\left(\mathbf{t}-\mathbf{w}_{r}^{\top} \mathbf{t} \mathbf{w}_{r}\right)\right\|_{2}^{2}</script><h4 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h4><p>loss function consists of margin-based ranking loss and some constraints:</p><script type="math/tex; mode=display">\begin{aligned} \mathcal{L} &=\sum_{(h, r, t) \in \Delta\left(h^{\prime}, r^{\prime}, t^{\prime}\right) \in \Delta_{(h, r, t)}}\left[f_{r}(\mathbf{h}, \mathbf{t})+\gamma-f_{r^{\prime}}\left(\mathbf{h}^{\prime}, \mathbf{t}^{\prime}\right)\right]_{+} \\ &+C\left\{\sum_{e \in E}\left[\|\mathbf{e}\|_{2}^{2}-1\right]_{+}+\sum_{r \in R}\left[\frac{\left(\mathbf{w}_{r}^{\top} \mathbf{d}_{r}\right)^{2}}{\left\|\mathbf{d}_{r}\right\|_{2}^{2}}-\epsilon^{2}\right]_{+}\right\}, \text { (4) } \end{aligned}</script><p>the constraints:</p><script type="math/tex; mode=display">\forall e \in E,\|\mathrm{e}\|_{2} \leq 1, // \text { scale }\\\forall r \in R,\left|\mathbf{w}_{r}^{\top} \mathbf{d}_{r}\right| /\left\|\mathbf{d}_{r}\right\|_{2} \leq \epsilon, / / \text { orthogonal }\\\forall r \in R,\left\|\mathbf{w}_{r}\right\|_{2}=1, / / \text { unit normal vector }</script><ul><li><strong>the second grantees the translation vectot $d_r$ is in the hyperplane</strong></li><li>they project each $w_r$ to unit $l_2$-ball before visiting each mini-batch</li></ul><blockquote><p>æ—¢ç„¶transHå¯ä»¥å®Œæˆå°†åŒä¸€å®ä½“æ˜ å°„åˆ°ä¸åŒçš„å…³ç³»å¹³é¢æ¥è·å¾—ä¸åŒçš„å«ä¹‰ï¼Œé‚£ä¹ˆæˆ‘è§‰å¾—</p><ul><li>æ˜¯ä¸æ˜¯ä¸åŒä»£è¡¨åŒä¸€å«ä¹‰çš„æŠ•å½±è¡¨ç¤ºåº”è¯¥ç›¸åŒæˆ–è€…ç›¸ä¼¼</li><li>è¿™æ ·æ˜¯ä¸æ˜¯å¯ä»¥è§£å†³åŒä¸€ä¸ªå®ä½“çš„å¤šä¹‰æ€§é—®é¢˜ã€‚</li></ul></blockquote><h2 id="Reducing-Ralse-Negative-Labels"><a href="#Reducing-Ralse-Negative-Labels" class="headerlink" title="Reducing Ralse Negative Labels"></a>Reducing Ralse Negative Labels</h2><p>Authors set different probabilities for replacing the head or tail entity depending on the mapping property of the relation (one-to-many, many-to-one, many-to-many)</p><ul><li><p>give more chance to replacing the head entity if the relation is one-to-many</p><ul><li>åˆ†åˆ«ç»Ÿè®¡æ¯ä¸ªå¤´å®ä½“å¯¹åº”å°¾å®ä½“çš„æ•°é‡ï¼ˆåä¹‹äº¦ç„¶ï¼‰ï¼ŒæŒ‰å æ¯”è¿›è¡Œç”Ÿæˆè´Ÿæ ·ä¾‹</li></ul></li></ul><blockquote><ul><li>é€šè¿‡è¿™æ ·çš„æ–¹å¼ï¼Œä¾‹å¦‚one-manyå…³ç³»ï¼Œæ›¿æ¢å¤´å®ä½“æ˜¾ç„¶æ›´ä¸å®¹æ˜“å¾—åˆ°æ­£æ ·ä¾‹ï¼ˆå› ä¸ºåªæœ‰ä¸€ç§å¤´å®ä½“æ˜¯å¯¹çš„ï¼Œç„¶è€Œæ›¿æ¢å°¾å®ä½“å› ä¸ºå¯¹äºå¤´å®ä½“å¯¹åº”è¯¥å…³ç³»çš„å°¾å®ä½“æ›´å¤šï¼Œè¯´ä¸å®šå°±æœ‰å…¶ä»–ä¸åœ¨æ­¤manyä¸­çš„å°¾å®ä½“ç¬¦åˆè¿™ä¸ªå…³ç³»ã€‚</li><li>ç›¸æ¯”ä¹‹ä¸‹æˆ‘è®¤ä¸ºåœ¨ã€ŠBootstrapping-Entity-Alignment-with-Knowledge-Graph-Embeddingã€‹é‡‡ç”¨çš„å‡åŒ€æˆªæ–­è´Ÿé‡‡æ ·æ•ˆæœä¼šæ›´å¥½ä¸€äº›</li></ul></blockquote><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>the detail can be seen in the paper</p><h2 id="Link-prediction"><a href="#Link-prediction" class="headerlink" title="Link prediction"></a>Link prediction</h2><h3 id="outperform-TransE-in-one-to-one"><a href="#outperform-TransE-in-one-to-one" class="headerlink" title="outperform TransE in one-to-one"></a>outperform TransE in one-to-one</h3><p>Authors explain:</p><ul><li>entities are connected with relations so that better embeddings of some parts lead to better results on the whole.</li></ul><blockquote><p>æˆ‘æ˜¯è§‰å¾—æœ‰äº›ç‰µå¼ºï¼Œä¸è¿‡è¦æ˜¯ç¡¬ç†è§£ä¹Ÿæ˜¯å¯ä»¥ï¼Œæ¯•ç«Ÿé€šè¿‡æŠ•å½±ç›¸å½“äºæŠŠå®ä½“å’Œå…³ç³»è¿›è¡Œäº†ä¸€ä¸ªè”ç³»ï¼Œå¯èƒ½è¿™ä¸ªå¢å¼ºäº†æ•ˆæœã€‚</p></blockquote><h2 id="Triplets-Classification"><a href="#Triplets-Classification" class="headerlink" title="Triplets Classification"></a>Triplets Classification</h2><p>This means FB13 is a very dense subgraph where strong correlations exist between entities</p><h2 id="Relational-Fact-Extraction-from-Text"><a href="#Relational-Fact-Extraction-from-Text" class="headerlink" title="Relational Fact Extraction from Text"></a>Relational Fact Extraction from Text</h2><ul><li>Actually, knowledge graph embedding is able to score a candidate fact, without observing any evidence from ex- ternal text corpus</li></ul><blockquote><p>å¯ä»¥çœ‹åˆ°ä»14å¹´å¼€å§‹å°±æœ‰åˆ©ç”¨çŸ¥è¯†å›¾è°±æ¥ä»æ–‡æœ¬æŠ½å–å…³ç³»ï¼Œæœ€è¿‘è¿™ä¸ªåº”ç”¨å¥½åƒåˆæœ‰èµ·è‰²ï¼Œè¿™ä¸ªä¹Ÿå¯ä½œä¸ºè‡ªå·±å®éªŒçš„ä¸€éƒ¨åˆ†ã€‚</p></blockquote><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://blog.csdn.net/MonkeyDSummer/article/details/85273843" target="_blank" rel="noopener">https://blog.csdn.net/MonkeyDSummer/article/details/85273843</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;ä½œä¸ºtransç³»åˆ—ç»å…¸æ–‡çŒ®ï¼Œå¿…è¯»ã€‚æ–‡ç« ä¸»è¦ç²¾ååœ¨äºè¿™ç§è¶…å¹³é¢æƒ³æ³•çš„ç”±æ¥è§£å†³äº†åŒä¸€å®ä½“çš„å¤šå…³ç³»é—®é¢˜ã€‚&lt;/p&gt;
&lt;p&gt;Authors proposed TransH which models a relation as a hyperplane together with a translation operation on it. It solves the problem of multi-relation and makes a good trade-off between model capacity and efficiency.&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="è®ºæ–‡é˜…è¯»ç¬”è®°" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
      <category term="KG" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/KG/"/>
    
    
      <category term="KGE" scheme="http://yoursite.com/tags/KGE/"/>
    
      <category term="transH" scheme="http://yoursite.com/tags/transH/"/>
    
  </entry>
  
  <entry>
    <title>Attention Is All You Needé˜…è¯»ç¬”è®°</title>
    <link href="http://yoursite.com/post/Attention%20Is%20All%20You%20Need/"/>
    <id>http://yoursite.com/post/Attention Is All You Need/</id>
    <published>2019-05-25T05:57:35.000Z</published>
    <updated>2019-05-28T07:58:49.154Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>transformer æ˜¯ä¸€ä¸ªå®Œå…¨ç”±æ³¨æ„åŠ›æœºåˆ¶ç»„æˆçš„æ­å»ºçš„æ¨¡å‹ï¼Œæ¨¡å‹å¤æ‚åº¦ä½ï¼Œå¹¶å¯ä»¥è¿›è¡Œå¹¶è¡Œè®¡ç®—ï¼Œä½¿å¾—è®¡ç®—é€Ÿåº¦å¿«ã€‚åœ¨ç¿»è¯‘æ¨¡å‹ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ•ˆæœã€‚æœ¬ç¯‡è®ºæ–‡å±äºç»å…¸å¿…è¯»è®ºæ–‡ï¼Œé˜…è¯»ç¬”è®°ä¸­å¯¹ä¸€äº›ä¸æ¸…æ¥šçš„åœ°æ–¹è¿›è¡Œäº†æ±‰è¯­è§£é‡Šï¼Œè¯»å®Œè®ºæ–‡åé˜…è¯»å‚è€ƒé“¾æ¥ä»¥åŠ æ·±ç†è§£ã€‚</p><p><a href="https://arxiv.org/pdf/1706.03762" target="_blank" rel="noopener">è®ºæ–‡ä¸‹è½½åœ°å€</a></p></blockquote><a id="more"></a><h1 id="research-objective"><a href="#research-objective" class="headerlink" title="research objective"></a>research objective</h1><p>based solely on attention mechanisms, increase parallezable computation and decrease train time</p><h1 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h1><p>recurrent models hidden states depended on previous hidden state and the input for position precludes parallelization</p><h1 id="contribution"><a href="#contribution" class="headerlink" title="contribution"></a>contribution</h1><ul><li>Transformer,<ul><li>eschewing recurrence and instead relying entirely on an attention mechanism, solve the long dependency problem.</li><li>draw global dependecies between input and output </li><li>allow for significantly more parallelization</li></ul></li></ul><h1 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h1><p>The Transformer uses stacked self-attention and point-wise, fully connected layers for both the encoder and decoder.<img src="http://image.nysdy.com/2019052515587656321218.jpg" alt="2019052515587656321218.jpg"></p><h2 id="Encoder-and-Decoder-Stacks"><a href="#Encoder-and-Decoder-Stacks" class="headerlink" title="Encoder and Decoder Stacks"></a>Encoder and Decoder Stacks</h2><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><ul><li>compose of a stack of N identical layers</li><li>each layers has two sub-layers<ol><li>multi-head self-attention mechanism</li><li>position-wise fully connected feed forward network</li></ol></li><li>employ a residual connection around each of the two sub-layers, followed by layer normalization</li><li>the output of each sub-layer is $\text { LayerNorm }(x+\text { Sublayer }(x))$</li><li>encoderä¸­çš„Qï¼ŒKï¼ŒVéƒ½æ˜¯å­¦å‡ºæ¥çš„</li></ul><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><ul><li>composed of a stack of N identical layers</li><li>has the same two sub-layers as the encoder</li><li>the third sub-layer between the two sub-layers<ul><li>perform multi-head attention over the output of the encoder stack</li></ul></li><li>add a mask to modify the self-attention sub-layer to ensure that the predictions for position $i$ can depend only the known outputs at positions less than $i$</li><li>é™¤äº†ç¬¬ä¸€å­å±‚ä¸­Qï¼ŒKï¼ŒVæ˜¯è‡ªå·±å­¦å‡ºæ¥çš„ï¼Œç¬¬äºŒä¸ªå­å±‚åˆ©ç”¨äº†encoderä¸­çš„Kï¼ŒVã€‚</li></ul><h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p><img src="http://image.nysdy.com/20190526155885488441950.jpg" alt="20190526155885488441950.jpg"></p><h3 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h3><p>the calculation process as the left at the figure 2. <strong>formulaï¼š</strong></p><script type="math/tex; mode=display">\text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V</script><ul><li>where $\sqrt{d_{k}}$ is to  prevent value from getting too large, which will push the softmax function into regions where it has extremely small gradients. å› ä¸ºé‡çº§å¤ªå¤§ï¼Œsoftmaxåå°±é0å³1äº†ï¼Œä¸å¤Ÿâ€œsoftâ€äº†ã€‚ä¹Ÿä¼šå¯¼è‡´softmaxçš„æ¢¯åº¦éå¸¸å°ã€‚ä¹Ÿå°±æ˜¯è®©softmaxç»“æœ<strong>ä¸ç¨€ç–</strong>(é—®å·è„¸ï¼Œé€šå¸¸äººä»¬å¸Œæœ›å¾—åˆ°æ›´ç¨€ç–çš„attentionå§)ã€‚</li><li>$Q, K,V$ is a matrix needed to learn from input.</li></ul><h3 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h3><p><strong>helps the encoder look at other words in the input sentence as it encodes a specific word</strong></p><p>in the figure 2 right. </p><ul><li>itâ€™s beneficial to <strong>lineraly project</strong> the quries, keys and values $h$ times with different, learned projections to $d_k, d_k, d_v$ dimensions, respectively</li><li>concatenate the output </li></ul><script type="math/tex; mode=display">\begin{aligned} \text { MultiHead }(Q, K, V) &=\text { Concat (head }_{1}, \ldots, \text { head }_{h} ) W^{O} \\ \text { where head }_{i} &=\text { Attention }\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right) \end{aligned}</script><p>where $W_{i}^{Q} \in \mathbb{R}^{d_{\text { model }} \times d_{k}}, W_{i}^{K} \in \mathbb{R}^{d_{\text { model }} \times d_{k}}, W_{i}^{V} \in \mathbb{R}^{d_{\text { model }} \times d_{v}}, W^{O} \in \mathbb{R}^{h d_{v} \times d_{\mathrm{model}}}$</p><h3 id="Applications-of-Attention-in-our-Model"><a href="#Applications-of-Attention-in-our-Model" class="headerlink" title="Applications of Attention in our Model"></a>Applications of Attention in our Model</h3><ul><li>the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. mimicing the seq-to-seq</li><li>self -attention can make that each position in the encoder can attend to all positions in the previous layer of the encoder</li><li><strong>We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to âˆ’âˆ) all values in the input of the softmax which correspond to illegal connections. See Figure 2</strong>ã€‚å³æˆ‘ä»¬åªèƒ½attendåˆ°å‰é¢å·²ç»ç¿»è¯‘è¿‡çš„è¾“å‡ºçš„è¯è¯­ï¼Œå› ä¸ºç¿»è¯‘è¿‡ç¨‹æˆ‘ä»¬å½“å‰è¿˜å¹¶ä¸çŸ¥é“ä¸‹ä¸€ä¸ªè¾“å‡ºè¯è¯­ï¼Œè¿™æ˜¯æˆ‘ä»¬ä¹‹åæ‰ä¼šæ¨æµ‹åˆ°çš„ã€‚å³å°†$QK^T$ä¸­æ¯è¡Œè¯¥å•è¯ä¹‹åçš„æ•°å€¼åšå¤„ç†ï¼Œä½¿å¾—å‰é¢çš„å•è¯çœ‹ä¸åˆ°åé¢å•è¯æ‰€å çš„é‡è¦æ€§ç¨‹åº¦ã€‚</li></ul><h2 id="Position-wise-Feed-Forward-Networks"><a href="#Position-wise-Feed-Forward-Networks" class="headerlink" title="Position-wise Feed-Forward Networks"></a>Position-wise Feed-Forward Networks</h2><ul><li>applied to each position separately and identically</li><li>feed-forward network consists of tow linear transformations with a ReLU activation. formula:</li></ul><script type="math/tex; mode=display">\mathrm{FFN}(x)=\max \left(0, x W_{1}+b_{1}\right) W_{2}+b_{2}</script><blockquote><p><strong>å°ç»“</strong></p><ol><li>ä¸ºä»€ä¹ˆå«å¼ºè°ƒ<code>position-wise</code>?<ul><li>è§£é‡Šä¸€: è¿™é‡ŒFFNå±‚æ˜¯æ¯ä¸ªpositionè¿›è¡Œç›¸åŒä¸”ç‹¬ç«‹çš„æ“ä½œï¼Œæ‰€ä»¥å«position-wiseã€‚å¯¹æ¯ä¸ªpositionç‹¬ç«‹åšFFNã€‚</li><li>è§£é‡ŠäºŒï¼šä»å·ç§¯çš„è§’åº¦è§£é‡Šï¼Œè¿™é‡Œçš„FFNç­‰ä»·äºkernel_size=1çš„å·ç§¯ï¼Œè¿™æ ·æ¯ä¸ªpositionéƒ½æ˜¯ç‹¬ç«‹è¿ç®—çš„ã€‚å¦‚æœkernel_size=2ï¼Œæˆ–è€…å…¶ä»–ï¼Œpositionä¹‹é—´å°±å…·æœ‰ä¾èµ–æ€§äº†ï¼Œè²Œä¼¼å°±ä¸èƒ½å«åšposition-wiseäº†</li></ul></li><li>ä¸ºä»€ä¹ˆè¦é‡‡ç”¨å…¨è¿æ¥å±‚ï¼Ÿ<ul><li>ç›®çš„: å¢åŠ éçº¿æ€§å˜æ¢</li><li>å¦‚æœä¸é‡‡ç”¨FFNå‘¢ï¼Ÿæœ‰ä»€ä¹ˆæ›¿ä»£çš„è®¾è®¡ï¼Ÿ</li></ul></li><li>ä¸ºä»€ä¹ˆé‡‡ç”¨2å±‚å…¨è¿æ¥ï¼Œè€Œä¸”ä¸­é—´å‡ç»´ï¼Ÿ<ul><li>è¿™ä¹Ÿæ˜¯æ‰€è°“çš„bottle neckï¼Œåªä¸è¿‡ä½ç»´åœ¨IOä¸Šï¼Œä¸­é—´é‡‡ç”¨high rank</li></ul></li></ol></blockquote><h2 id="Embeddings-and-Softmax"><a href="#Embeddings-and-Softmax" class="headerlink" title="Embeddings and Softmax"></a>Embeddings and Softmax</h2><p>Sharing the same weight maatrix between the two embedding layers and the pre-softmax linear transformation</p><h2 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h2><p>Using sine and xosine functions of different frequencies:</p><script type="math/tex; mode=display">P E_{(p o s, 2 i)}=\sin \left(p o s / 10000^{2 i / d_{\text { model }}}\right)\\P E_{(p o s, 2 i+1)}=\cos \left(p o s / 10000^{2 i / d_{\mathrm{model}}}\right)</script><ul><li><p>where $pos$ is the postiiton and $i$ is the dimension</p></li><li><p><strong>Authors hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$, $PE_{pos+k}$can be represented as a linear function of $PE_{pos}$</strong></p><p>ä½†åœ¨è¯­è¨€ä¸­ï¼Œ<code>ç›¸å¯¹ä½ç½®</code>ä¹Ÿå¾ˆé‡è¦ï¼ŒGoogleé€‰æ‹©å‰è¿°çš„ä½ç½®å‘é‡å…¬å¼çš„ä¸€ä¸ªé‡è¦åŸå› æ˜¯ï¼šç”±äºæˆ‘ä»¬æœ‰$\sin (\alpha+\beta)=\sin \alpha \cos \beta+\cos \alpha \sin \beta$ä»¥åŠ$\cos (\alpha+\beta)=\cos \alpha \cos \beta-\sin \alpha \sin \beta$ï¼Œè¿™è¡¨æ˜ä½ç½®$p+k$çš„å‘é‡å¯ä»¥è¡¨ç¤ºæˆä½ç½®$p$çš„å‘é‡çš„çº¿æ€§å˜æ¢ï¼Œè¿™æä¾›äº†è¡¨è¾¾ç›¸å¯¹ä½ç½®ä¿¡æ¯çš„å¯èƒ½æ€§ã€‚</p></li><li><p>Compared with using learned positional embeddings, the sinusoidal version may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.</p></li></ul><p>æ³¨æ„ç”±äºè¯¥æ¨¡å‹æ²¡æœ‰recurrenceæˆ–convolutionæ“ä½œï¼Œæ‰€ä»¥æ²¡æœ‰æ˜ç¡®çš„å…³äºå•è¯åœ¨æºå¥å­ä¸­ä½ç½®çš„ç›¸å¯¹æˆ–ç»å¯¹çš„ä¿¡æ¯ï¼Œä¸ºäº†æ›´å¥½çš„è®©æ¨¡å‹å­¦ä¹ ä½ç½®ä¿¡æ¯ï¼Œæ‰€ä»¥æ·»åŠ äº†position encodingå¹¶å°†å…¶å åŠ åœ¨word embeddingä¸Šã€‚</p><h1 id="Why-Self-Attention"><a href="#Why-Self-Attention" class="headerlink" title="Why Self-Attention"></a>Why Self-Attention</h1><ul><li>total computational complexity per layer</li><li>the amount of computation that can be parallelized</li><li>the path between long-range dependencies in the network</li></ul><p><img src="http://image.nysdy.com/20190527155892126287777.jpg" alt="20190527155892126287777.jpg"></p><p><img src="http://image.nysdy.com/20190528155902465937774.jpg" alt="20190528155902465937774.jpg"></p><p>self-attention|ï¼š</p><ul><li>$QK^TV$ç›¸ä¹˜ï¼Œæ ¹æ®çŸ©é˜µå¤§å°ï¼ˆåˆ†åˆ«ä¸º$n<em>d, n</em>d, n<em>d$éœ€è¦çš„å¤æ‚åº¦ä¸º$O(n^2d</em>2)$ï¼ˆå¿½ç•¥softmaxï¼‰</li><li>maximum path lengthï¼šå›¾è¯´æ˜äº†ï¼Œ å¯¹äºself-attention, target node (ç”Ÿæˆçš„é‚£ä¸ªç‚¹) å®é™…ä¸Šå’Œ è¾“å…¥ä¸­çš„ä»»æ„ä¸€ç‚¹çš„è·ç¦»æ˜¯ç›¸åŒçš„</li></ul><p>convolutional:  </p><ul><li><p>æ¯å±‚æœ‰kä¸ªå·ç§¯å’Œï¼Œå¯¹äºinput matixï¼ˆ$n<em>d$)çŸ©é˜µæ‰§è¡Œå·ç§¯éœ€è¦è¿ç®—å¤æ‚åº¦æ˜¯$n</em>d*(d-m)$, mä¸ºå·ç§¯å’Œå®½åº¦æ˜¯ä¸€ä¸ªæ¯”è¾ƒå°çš„å¸¸æ•°ï¼Œæ‰€ä»¥æ€»å¤æ‚åº¦ä¸º$O\left(k \cdot n \cdot d^{2}\right)$,ä½œè€…æåˆ°å¯åˆ†ç¦»çš„å·åŸºå±‚æš‚æ—¶è¿˜ä¸äº†è§£ï¼Œå¯ä»¥ä»¥åæŸ¥é˜…ã€‚</p></li><li><p>maximum path length: æ­£å¸¸å·ç§¯å’Œçš„è·ç¦»æ˜¯$O(n/k)$, ä½†å¦‚æœæ˜¯å †å å·ç§¯å¦‚å›¾ï¼šã€€ã€€<img src="http://image.nysdy.com/2019052815590251436862.jpg" alt="2019052815590251436862.jpg"></p><p>å°±å¯ä»¥å‡å°åˆ°$O\left(\log _{k}(n)\right)$</p></li></ul><p>recurrent:</p><ul><li>è®¡ç®—æ˜¯æ¯ä¸ªè¯å‘é‡ä¹˜éšè—æƒé‡($d*d$)ï¼Œæ‰€ä»¥æ˜“å¾—è®¡ç®—å¤æ‚åº¦ï¼š$O\left(n \cdot d^{2}\right)$</li><li>maximum path length: é•¿åº¦å°±æ˜¯nã€‚</li><li>æ“ä½œæ­¥éª¤è¦ä»ç¬¬ä¸€ä¸ªåˆ°ç¬¬nä¸ªä¸ºnæ­¥ï¼Œæ˜¯æœ‰é¡ºåºçš„ã€‚å…¶ä»–çš„éƒ½æ²¡æœ‰é¡ºåºè¦æ±‚</li></ul><p>self-attentin(restricted)</p><ul><li>ç›¸å½“äºåªè¾“å…¥ré‚»è¿‘çš„å¥å­é•¿åº¦ï¼Œè‡ªç„¶å¯ä»¥å¾—åˆ°å¦‚å›¾ç»“æœ</li></ul><h1 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h1><h2 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h2><script type="math/tex; mode=display">\text { lrate }=d_{\text { model }}^{-0.5} \cdot \min \left(\text {step}_{-} n u m^{-0.5}, \text { step }_{-} n u m \cdot \text { warmup steps }^{-1.5}\right)</script><ul><li>increasing the learning rate linearly for the first warmup_steps training steps</li><li>decreasing it thereafter proportionally to the inverse square root of the step number</li></ul><h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><h3 id="Residual-Dropout"><a href="#Residual-Dropout" class="headerlink" title="Residual Dropout"></a>Residual Dropout</h3><ul><li>apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized</li><li>apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks</li></ul><h3 id="Label-Smoothing"><a href="#Label-Smoothing" class="headerlink" title="Label Smoothing"></a>Label Smoothing</h3><ul><li>This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score</li></ul><h1 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h1><h2 id="machine-Translation"><a href="#machine-Translation" class="headerlink" title="machine Translation"></a>machine Translation</h2><p>Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models</p><p><img src="http://image.nysdy.com/2019052715589406009133.jpg" alt="2019052715589406009133.jpg"></p><h2 id="Model-Variations"><a href="#Model-Variations" class="headerlink" title="Model Variations"></a>Model Variations</h2><p><img src="http://image.nysdy.com/20190527155894062794865.jpg" alt="20190527155894062794865.jpg"></p><h1 id="ç¼ºç‚¹"><a href="#ç¼ºç‚¹" class="headerlink" title="ç¼ºç‚¹"></a><strong>ç¼ºç‚¹</strong></h1><p>ç¼ºç‚¹åœ¨åŸæ–‡ä¸­æ²¡æœ‰æåˆ°ï¼Œæ˜¯åæ¥åœ¨Universal Transformersä¸­æŒ‡å‡ºçš„ï¼Œåœ¨è¿™é‡ŒåŠ ä¸€ä¸‹å§ï¼Œä¸»è¦æ˜¯ä¸¤ç‚¹ï¼š</p><ol><li>å®è·µä¸Šï¼šæœ‰äº›rnnè½»æ˜“å¯ä»¥è§£å†³çš„é—®é¢˜transformeræ²¡åšåˆ°ï¼Œæ¯”å¦‚å¤åˆ¶stringï¼Œå°¤å…¶æ˜¯ç¢°åˆ°æ¯”è®­ç»ƒæ—¶çš„sequenceæ›´é•¿çš„æ—¶</li><li>ç†è®ºä¸Šï¼štransformersécomputationally universalï¼ˆ<a href="https://www.zhihu.com/question/20115374/answer/288346717" target="_blank" rel="noopener">å›¾çµå®Œå¤‡</a>ï¼‰ï¼Œï¼ˆæˆ‘è®¤ä¸ºï¼‰å› ä¸ºæ— æ³•å®ç°â€œwhileâ€å¾ªç¯</li></ol><h1 id="æ€»ç»“"><a href="#æ€»ç»“" class="headerlink" title="æ€»ç»“"></a><strong>æ€»ç»“</strong></h1><p>Transformeræ˜¯ç¬¬ä¸€ä¸ªç”¨çº¯attentionæ­å»ºçš„æ¨¡å‹ï¼Œä¸ä»…è®¡ç®—é€Ÿåº¦æ›´å¿«ï¼Œåœ¨ç¿»è¯‘ä»»åŠ¡ä¸Šä¹Ÿè·å¾—äº†æ›´å¥½çš„ç»“æœã€‚</p><p>Googleç°åœ¨çš„ç¿»è¯‘åº”è¯¥æ˜¯åœ¨æ­¤åŸºç¡€ä¸Šåšçš„ï¼Œä½†æ˜¯æ•°æ®é‡å¤§å¯èƒ½ç”¨transformerå¥½ä¸€äº›ï¼Œå°çš„è¯è¿˜æ˜¯ç»§ç»­ç”¨rnn-based modelã€‚</p><p>èŠ±äº†ä¸å°‘æ—¶é—´ï¼Œç®—æ˜¯ç†è§£äº†attentionå’Œtransformerï¼Œå¯¹å…¶ä¸­ä¸æ˜¯å¾ˆæ¸…æ¥šçš„ç‚¹å¦‚attentionçš„å†…éƒ¨ä¸­Qï¼ŒKï¼ŒVå…·ä½“æ˜¯ä»€ä¹ˆåœ¨self-attentionå’Œmulti-head attentionä¸­å¤§å°æ˜¯ä¸åŒçš„ï¼Œå¦‚ä½•maskï¼Œå¦‚ä½•è®¡ç®—å¤æ‚ï¼Œç­‰è¿›è¡ŒæŸ¥é˜…èµ„æ–™å¼„æ‡‚äº†ã€‚æ€»ä½“æ¥è¯´è¿˜æ˜¯æ”¶è·å¾ˆå¤§çš„ã€‚å‡†å¤‡åœ¨çœ‹ä¸€äº›ä»£ç è®²è§£ã€‚</p><h1 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h1><ul><li>Attentionæœºåˆ¶è¯¦è§£ï¼ˆäºŒï¼‰â€”â€”Self-Attentionä¸Transformer - å·é™€å­¦è€…çš„æ–‡ç«  - çŸ¥ä¹<br><a href="https://zhuanlan.zhihu.com/p/47282410" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/47282410</a></li><li><strong><a href="https://jalammar.github.io/illustrated-transformer/ï¼ˆè¿™ä¸ªè®²çš„æ¯”è¾ƒè¯¦ç»†ï¼Œå»ºè®®çœ‹å®Œè®ºæ–‡åå†çœ‹ä¸€éè¿™ä¸ªä¼šåŠ æ·±ç†è§£ï¼‰" target="_blank" rel="noopener">https://jalammar.github.io/illustrated-transformer/ï¼ˆè¿™ä¸ªè®²çš„æ¯”è¾ƒè¯¦ç»†ï¼Œå»ºè®®çœ‹å®Œè®ºæ–‡åå†çœ‹ä¸€éè¿™ä¸ªä¼šåŠ æ·±ç†è§£ï¼‰</a></strong></li><li>ã€NLPã€‘Transformerè¯¦è§£ - æå¦‚çš„æ–‡ç«  - çŸ¥ä¹<br><a href="https://zhuanlan.zhihu.com/p/44121378" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/44121378</a></li><li><a href="https://blog.eson.org/pub/664e9bad/" target="_blank" rel="noopener">https://blog.eson.org/pub/664e9bad/</a></li><li><a href="https://mp.weixin.qq.com/s/J-anyCuwLd5UYjTsUFNT1g" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/J-anyCuwLd5UYjTsUFNT1g</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;transformer æ˜¯ä¸€ä¸ªå®Œå…¨ç”±æ³¨æ„åŠ›æœºåˆ¶ç»„æˆçš„æ­å»ºçš„æ¨¡å‹ï¼Œæ¨¡å‹å¤æ‚åº¦ä½ï¼Œå¹¶å¯ä»¥è¿›è¡Œå¹¶è¡Œè®¡ç®—ï¼Œä½¿å¾—è®¡ç®—é€Ÿåº¦å¿«ã€‚åœ¨ç¿»è¯‘æ¨¡å‹ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ•ˆæœã€‚æœ¬ç¯‡è®ºæ–‡å±äºç»å…¸å¿…è¯»è®ºæ–‡ï¼Œé˜…è¯»ç¬”è®°ä¸­å¯¹ä¸€äº›ä¸æ¸…æ¥šçš„åœ°æ–¹è¿›è¡Œäº†æ±‰è¯­è§£é‡Šï¼Œè¯»å®Œè®ºæ–‡åé˜…è¯»å‚è€ƒé“¾æ¥ä»¥åŠ æ·±ç†è§£ã€‚&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1706.03762&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;è®ºæ–‡ä¸‹è½½åœ°å€&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="è®ºæ–‡é˜…è¯»ç¬”è®°" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
      <category term="classical" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/classical/"/>
    
    
      <category term="attention" scheme="http://yoursite.com/tags/attention/"/>
    
      <category term="transformer" scheme="http://yoursite.com/tags/transformer/"/>
    
      <category term="translation" scheme="http://yoursite.com/tags/translation/"/>
    
      <category term="classical" scheme="http://yoursite.com/tags/classical/"/>
    
  </entry>
  
  <entry>
    <title>Graph Neural Networks with Generated Parameters for Relation Extractioné˜…è¯»ç¬”è®°</title>
    <link href="http://yoursite.com/post/Graph_Neural_Networks_with_Generated_Parameters_for_Relation/"/>
    <id>http://yoursite.com/post/Graph_Neural_Networks_with_Generated_Parameters_for_Relation/</id>
    <published>2019-05-23T02:41:51.000Z</published>
    <updated>2019-05-23T09:24:39.487Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>æœ¬æ–‡å°†GNNsåº”ç”¨åˆ°å¤„ç†éç»“æ„åŒ–æ–‡æœ¬çš„ï¼ˆå¤šè·³ï¼‰å…³ç³»æ¨ç†ä»»åŠ¡æ¥è¿›è¡Œå…³ç³»æŠ½å–ã€‚é‡‡ç”¨ä»å¥å­åºåˆ—ä¸­è·å–çš„å®ä½“æ„å»ºå…¨é“¾æ¥å›¾ï¼Œåº”ç”¨ç¼–ç ï¼ˆsequence modelï¼‰ï¼Œä¼ æ’­ï¼ˆèŠ‚ç‚¹é—´ä¿¡æ¯ï¼‰å’Œåˆ†ç±»ï¼ˆé¢„æµ‹ï¼‰ä¸‰ä¸ªæ¨¡å—æ¥å¤„ç†å…³ç³»æ¨ç†ã€‚æœ¬æ–‡æä¾›äº†ä¸‰ä¸ªæ•°æ®é›†ã€‚</p></blockquote><a id="more"></a><h1 id="problem-statement"><a href="#problem-statement" class="headerlink" title="problem statement"></a>problem statement</h1><ul><li>existing relation extraction models fail to infer the relationship without multi-hop relational reasoning.</li><li>existing GNNs canâ€™t process multi-hop relational reasoning in natural language relational reasoning </li></ul><h1 id="research-objective"><a href="#research-objective" class="headerlink" title="research objective"></a>research objective</h1><p>enable GNNs to porcess relational reasoning on unstructed text inputs</p><h1 id="contribution"><a href="#contribution" class="headerlink" title="contribution"></a>contribution</h1><ul><li>extend a GNN with generated parameters, which could be applied to process relational reasoning on unstructured inputs</li><li>verify GP-GNNs in the taks of relation extraction from text; present three datasets</li></ul><h1 id="GP-GNNs"><a href="#GP-GNNs" class="headerlink" title="GP-GNNs"></a>GP-GNNs</h1><ul><li>construct a fully-connected graph with the entities in the sequence of text</li><li>employs three models to process relational reasoning<ul><li>an encoding modul: enable edges to encode rich information from natural languages </li><li>a propagation modul: propagates realtional information among various nodes </li><li>a classification modul: make prediction with node representations </li></ul></li></ul><p>As compared to tradtional GNNs, GP-GNNs could learn edgesâ€™ parameters from natural lanuages</p><h1 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h1><h2 id="Graph-Neural-Networks-GNNs"><a href="#Graph-Neural-Networks-GNNs" class="headerlink" title="Graph Neural Networks(GNNs)"></a>Graph Neural Networks(GNNs)</h2><ul><li>existing models still perfom message-passing on predefined graphs</li><li><em>Learning Graphical State Transitions</em> is most related<ul><li>introduecs a nove lnerual architecture to generate a graph based on the textal input</li><li>dynamically update the relationship during the learning process</li></ul></li></ul><h2 id="relational-reasoning"><a href="#relational-reasoning" class="headerlink" title="relational reasoning"></a>relational reasoning</h2><ul><li>existing models could not make full use of the multi-hop inference patterns among multiple entity pair and their relaitons within the sentence </li><li><em>LEARNING GRAPHICAL STATE TRANSITIONS</em> is the most related work<ul><li>the proposed model incorporates contextual relations with attention mechanism when predicting the relation of a target entity pair</li></ul></li></ul><h1 id="Graph-Neural-Network-with-Grenerated-Parameters-GP-GNNs"><a href="#Graph-Neural-Network-with-Grenerated-Parameters-GP-GNNs" class="headerlink" title="Graph Neural Network with Grenerated Parameters(GP-GNNs)"></a>Graph Neural Network with Grenerated Parameters(GP-GNNs)</h1><p>The picture is overall architecture: encoding module, propagation module and classification module</p><p><img src="http://image.nysdy.com/20190523155858968594021.jpg" alt="20190523155858968594021.jpg"></p><h2 id="Encoding-Module"><a href="#Encoding-Module" class="headerlink" title="Encoding Module"></a>Encoding Module</h2><p>formula:</p><script type="math/tex; mode=display">\mathcal{A}_{i, j}^{(n)}=f\left(E\left(x_{0}^{i, j}\right), E\left(x_{1}^{i, j}\right), \cdots, E\left(x_{l-1}^{i, j}\right) ; \theta_{e}^{n}\right)</script><p>where $f(\cdot)$ could be any model that could sequential(such as LSTMs); $E(\cdot)$ indicates an embedding function. $x^{i, j}$ is the word in sentence labeled( $i,j$)</p><h2 id="Porpagation-Module"><a href="#Porpagation-Module" class="headerlink" title="Porpagation Module"></a>Porpagation Module</h2><p>the representations of layer n + 1 are calculated by:</p><script type="math/tex; mode=display">\mathbf{h}_{i}^{(n+1)}=\sum_{v_{j} \in \mathcal{N}\left(v_{i}\right)} \sigma\left(\mathcal{A}_{i, j}^{(n)} \mathbf{h}_{j}^{(n)}\right)</script><p>where $\mathcal{N}\left(v_{i}\right)$ denotes the neighbors of node $v_i$</p><h2 id="Classification-Module"><a href="#Classification-Module" class="headerlink" title="Classification Module"></a>Classification Module</h2><p>the loss of GP-GNNs:</p><script type="math/tex; mode=display">\mathcal{L}=g\left(\mathbf{h}_{0 :|\mathcal{V}|-1}^{0}, \mathbf{h}_{0 :|\mathcal{V}|-1}^{1}, \ldots, \mathbf{h}_{0 :|\mathcal{V}|-1}^{K}, Y ; \theta_{c}\right)</script><h1 id="Relation-Extraction-with-GP-GNNs"><a href="#Relation-Extraction-with-GP-GNNs" class="headerlink" title="Relation Extraction with GP-GNNs"></a>Relation Extraction with GP-GNNs</h1><p>Authors introduce how to apply GP-GNNs to relation extraction</p><h2 id="Encoding-Module-1"><a href="#Encoding-Module-1" class="headerlink" title="Encoding Module"></a>Encoding Module</h2><p>encoding then context of entity pairs (or edges in the graph)</p><script type="math/tex; mode=display">E\left(x_{t}^{i, j}\right)=\left[\boldsymbol{x}_{t} ; \boldsymbol{p}_{t}^{i, j}\right]</script><p>where $x_t$ denotes the word embedding; $\boldsymbol{p}_{t}^{i, j}$denotes the position embedding of word posistion t relative to the entity pairâ€™s position $i, j$.</p><h3 id="position-embedding"><a href="#position-embedding" class="headerlink" title="position embedding"></a>position embedding</h3><p>we mark each token in the sentence as either belonging to the first entity $v_i$, the second entity $v_j$ or to neither of those</p><h2 id="Propagation-Module"><a href="#Propagation-Module" class="headerlink" title="Propagation Module"></a>Propagation Module</h2><p> the formula is the same as the front</p><h3 id="The-Initial-Embeddings-of-Nodes"><a href="#The-Initial-Embeddings-of-Nodes" class="headerlink" title="The Initial Embeddings of Nodes"></a>The Initial Embeddings of Nodes</h3><ul><li>when extracting the relationship between entity $v_i$ and entity $v_j$, the initial embeddings of them are annotated as $\mathbf{h}_{v_{i}}^{(0)}=a_{\text { subject }}$, and $h_{v_{j}}^{(0)}=a_{\text { object }}$, while the intial embeddings of other entities are set to all zeros.</li><li>In our experiments, we generalize the idea of Gated Graph Neural Networks (Li et al., 2016) by setting $a_{\text { subject }}=[1 ; 0]^{\top}$and $a_{\text { object }}=[0 ; 1]^{\top}$.</li></ul><h2 id="classification-Module"><a href="#classification-Module" class="headerlink" title="classification Module"></a>classification Module</h2><p><strong>As the  target entity pair $(v_i, v_j)$:</strong></p><script type="math/tex; mode=display">\boldsymbol{r}_{v_{i}, v_{j}}=\left[\left[\boldsymbol{h}_{v_{i}}^{(1)} \odot \boldsymbol{h}_{v_{j}}^{(1)}\right]^{\top} ;\left[\boldsymbol{h}_{v_{i}}^{(2)} \odot \boldsymbol{h}_{v_{j}}^{(2)}\right]^{\top} ; \ldots ;\left[\boldsymbol{h}_{v_{i}}^{(K)} \odot \boldsymbol{h}_{v_{j}}^{(K)}\right]^{\top}\right]</script><p>where $\odot$ represents element-wise multiplication</p><p><strong>classification:</strong></p><script type="math/tex; mode=display">\mathbb{P}\left(r_{v_{i}, v_{j}} | h, t, s\right)=\operatorname{softmax}\left(M L P\left(\boldsymbol{r}_{v_{i}, v_{j}}\right)\right)</script><p><strong>loss:</strong></p><script type="math/tex; mode=display">\mathcal{L}=\sum_{s \in S} \sum_{i \neq j} \log \mathbb{P}\left(r_{v_{i}, v_{j}} | i, j, s\right)</script><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="aim"><a href="#aim" class="headerlink" title="aim"></a>aim</h2><ul><li>showing their best models could improve the performance of relation extraction under a variety of settings</li><li>illlustrating that how the number of layers affect the performance of their model</li><li>performing a qualitiative investigation to highlight the diference between their models and baseline models</li></ul><h2 id="design"><a href="#design" class="headerlink" title="design"></a>design</h2><p>as the first and second aim</p><ul><li>show that our models could improve instance-level relation extraction on a human annotated test set</li><li>we will show that our models could also help enhance the performance of bag-level relation extraction on a distantly labeled test set</li><li>split a subset of distantly labeled test set, where the number of entities and edges is large</li></ul><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><h3 id="distantly-label-set"><a href="#distantly-label-set" class="headerlink" title="distantly label set"></a>distantly label set</h3><ul><li>Sorokin and Gurevych (2017) proposed </li><li>modify their dataset<ul><li>added reversed edges</li><li>for all of the entity pairs with no relations, added â€œNAâ€ labels to them</li></ul></li></ul><h3 id="Human-annotated-test-set"><a href="#Human-annotated-test-set" class="headerlink" title="Human annotated test set"></a>Human annotated test set</h3><ul><li>Sorokin and Gurevych (2017)</li><li>select the distantly lablel pairs which all 5 annotaters are accepted.</li><li>There are 350 sentences and 1,230 triples in this test set </li></ul><h3 id="Dense-distantly-labeled-test-set"><a href="#Dense-distantly-labeled-test-set" class="headerlink" title="Dense distantly labeled test set"></a>Dense distantly labeled test set</h3><ul><li>criteria<ul><li>the number of entities should be strictly larger than 2</li><li>there must be at least one circle (with at least three entities) in the ground-truth label of the sentence</li></ul></li><li>There are 1,350 sentences and more than 17,915 triples and 7,906 relational facts in this test set.</li></ul><h2 id="Models-for-comparison"><a href="#Models-for-comparison" class="headerlink" title="Models for comparison"></a>Models for comparison</h2><ul><li>Context-aware RE</li><li>Multi-Window CNN</li><li>PCNN</li><li>LSTM or GP-GNN with K = 1 layer</li><li>GP-GNN with K = 2 or K = 3 layerss</li></ul><h2 id="Evaluation-Details"><a href="#Evaluation-Details" class="headerlink" title="Evaluation Details"></a>Evaluation Details</h2><p><strong>To evaluation models in bag-level:</strong></p><script type="math/tex; mode=display">E\left(r | v_{i}, v_{j}, S\right)=\max _{s \in S} \mathbb{P}\left(r_{v_{i}, v_{j}} | i, j, s\right)</script><p><strong>result</strong>:</p><p><img src="http://image.nysdy.com/20190523155859369893411.jpg" alt="20190523155859369893411.jpg"></p><h2 id="Effectiveness-of-Reasoning-Mechanism"><a href="#Effectiveness-of-Reasoning-Mechanism" class="headerlink" title="Effectiveness of Reasoning Mechanism"></a>Effectiveness of Reasoning Mechanism</h2><p><img src="http://image.nysdy.com/2019052315585938129506.jpg" alt="2019052315585938129506.jpg"></p><ul><li>Context-Aware RE may <strong>introduce more noise,</strong> for it may mistakenly increase the probability of a relation with the similar topic with the context relations</li><li>sentences from Wikipedia corpus are always complex, which may be hard to model for CNN and PCNN</li></ul><h2 id="The-Effectiveness-of-the-Number-of-Layers"><a href="#The-Effectiveness-of-the-Number-of-Layers" class="headerlink" title="The Effectiveness of the Number of Layers"></a>The Effectiveness of the Number of Layers</h2><p><img src="http://image.nysdy.com/20190523155859455443298.jpg" alt="20190523155859455443298.jpg"></p><ul><li>the improvement of the third layer is much smaller on the overall distantly supervised test set than the one on the dense subset<ul><li>This observation reveals that the reasoning mechanism could help us identify relations especially on sentences where there are more entities</li></ul></li><li>as the number of layers grows, the curves get higher and higher precision, <ul><li>indicating considering more hops in reasoning leads to better performance</li></ul></li></ul><h2 id="Qualitative-Results-Case-Study"><a href="#Qualitative-Results-Case-Study" class="headerlink" title="Qualitative Results: Case Study"></a>Qualitative Results: Case Study</h2><p><img src="http://image.nysdy.com/20190523155859457710223.jpg" alt="20190523155859457710223.jpg"></p><p>Context-Aware RE makes a mistake by predicting (Kentucky, share boarder with, Ohio). As we have discussed before, this is due to its mechanism to model co-occurrence of multiple relations</p><h1 id="æ€è€ƒ"><a href="#æ€è€ƒ" class="headerlink" title="æ€è€ƒ"></a>æ€è€ƒ</h1><p>æ–‡ç« æ˜¯åˆ˜çŸ¥è¿œç»„çš„è®ºæ–‡ï¼Œé’ˆå¯¹çš„æ–¹å‘æ˜¯å…³ç³»æŠ½å–ï¼Œåœ¨å…¶ä¸­ç»“åˆäº†å…³ç³»æ¨ç†ï¼Œæœ€è¿‘è®¸å¤šä»»åŠ¡éƒ½åœ¨ç»“åˆæ¨ç†çš„æ€æƒ³ã€‚æ–‡ç« æ•´ä½“çš„ç»“æ„ï¼Œé€»è¾‘ååˆ†æ¸…æ™°ï¼Œè®ºè¿°çš„ä¹Ÿæ¯”è¾ƒè¯¦ç»†ï¼Œå±äºæ ‡å‡†è®ºæ–‡ã€‚æ„Ÿè§‰æ–‡ç« ä¸­GP-GNNsç»“æ„å›¾è¿˜å¯ä»¥ç”»çš„æ›´å¥½ä¸€ç‚¹ï¼Œå±•ç°ä¸€ä¸‹encoding moduleçš„å±‚ï¼Œå¯ä»¥æ›´å¥½ç†è§£ã€‚æ–‡ç« çš„ç²¾é«“åº”è¯¥æ˜¯è¿™ä¸ªpropagation moduleçš„éƒ¨åˆ†ï¼Œè¿˜éœ€è¦æ¶ˆåŒ–ä¸€ä¸‹ï¼Œä¸è¿‡è¿™éƒ¨åˆ†å¯èƒ½æ˜¯æœ‰å…ˆå‰çš„çŸ¥è¯†æ”¯æ’‘çš„ã€‚</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;æœ¬æ–‡å°†GNNsåº”ç”¨åˆ°å¤„ç†éç»“æ„åŒ–æ–‡æœ¬çš„ï¼ˆå¤šè·³ï¼‰å…³ç³»æ¨ç†ä»»åŠ¡æ¥è¿›è¡Œå…³ç³»æŠ½å–ã€‚é‡‡ç”¨ä»å¥å­åºåˆ—ä¸­è·å–çš„å®ä½“æ„å»ºå…¨é“¾æ¥å›¾ï¼Œåº”ç”¨ç¼–ç ï¼ˆsequence modelï¼‰ï¼Œä¼ æ’­ï¼ˆèŠ‚ç‚¹é—´ä¿¡æ¯ï¼‰å’Œåˆ†ç±»ï¼ˆé¢„æµ‹ï¼‰ä¸‰ä¸ªæ¨¡å—æ¥å¤„ç†å…³ç³»æ¨ç†ã€‚æœ¬æ–‡æä¾›äº†ä¸‰ä¸ªæ•°æ®é›†ã€‚&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="è®ºæ–‡é˜…è¯»ç¬”è®°" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="GNNs" scheme="http://yoursite.com/tags/GNNs/"/>
    
      <category term="relation extraction" scheme="http://yoursite.com/tags/relation-extraction/"/>
    
      <category term="relation reasoning" scheme="http://yoursite.com/tags/relation-reasoning/"/>
    
  </entry>
  
  <entry>
    <title>allennlpå®‰è£…è¸©å‘</title>
    <link href="http://yoursite.com/post/allennlp_install/"/>
    <id>http://yoursite.com/post/allennlp_install/</id>
    <published>2019-05-22T14:09:27.000Z</published>
    <updated>2019-05-22T14:11:19.740Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>å®‰è£…allennlpçš„è¸©å‘ä¹‹è·¯ï¼Œè¸©äº†ä¸å°‘å‘æœ€åé€‰æ‹©â€™Installing from sourceâ€™çš„å®‰è£…æ–¹æ³•ï¼Œæ’å‘åä¸‹é¢æ–¹æ³•äº²æµ‹å¯ç”¨</p></blockquote><a id="more"></a><h2 id="Installing-from-source"><a href="#Installing-from-source" class="headerlink" title="Installing from source"></a>Installing from source</h2><p>å®‰è£…æ­¥éª¤ï¼š</p><h3 id="1-ä¸‹è½½GitHubæ–‡ä»¶"><a href="#1-ä¸‹è½½GitHubæ–‡ä»¶" class="headerlink" title="1.ä¸‹è½½GitHubæ–‡ä»¶"></a>1.ä¸‹è½½GitHubæ–‡ä»¶</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/allenai/allennlp.git</span><br></pre></td></tr></table></figure><h3 id="2-åˆ›å»ºcondaç¯å¢ƒ"><a href="#2-åˆ›å»ºcondaç¯å¢ƒ" class="headerlink" title="2.åˆ›å»ºcondaç¯å¢ƒ"></a>2.åˆ›å»ºcondaç¯å¢ƒ</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n allennlp python=3.6</span><br></pre></td></tr></table></figure><h3 id="3-æ¿€æ´»ç¯å¢ƒä¸‹è½½ä¾èµ–æ–‡ä»¶"><a href="#3-æ¿€æ´»ç¯å¢ƒä¸‹è½½ä¾èµ–æ–‡ä»¶" class="headerlink" title="3.æ¿€æ´»ç¯å¢ƒä¸‹è½½ä¾èµ–æ–‡ä»¶"></a>3.æ¿€æ´»ç¯å¢ƒä¸‹è½½ä¾èµ–æ–‡ä»¶</h3><ul><li><p>æ¿€æ´»ç¯å¢ƒ</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source activate allennlp</span><br></pre></td></tr></table></figure></li><li><p>è¿›å…¥githubä¸Šä¸‹è½½çš„æ–‡ä»¶å¤¹</p></li><li><p>ä¸‹è½½ä¾èµ–æ–‡ä»¶</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure><p>é‡åˆ°æŠ¥é”™é—®é¢˜ï¼Œå‚è€ƒä¸‹ä¸€å°èŠ‚ï¼Œæ‰€æ¬²é—®é¢˜è§£å†³ã€‚</p></li></ul><h3 id="4-å®‰è£…allennlp"><a href="#4-å®‰è£…allennlp" class="headerlink" title="4.å®‰è£…allennlp"></a>4.å®‰è£…allennlp</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install --editable .</span><br></pre></td></tr></table></figure><h3 id="5-æµ‹è¯•"><a href="#5-æµ‹è¯•" class="headerlink" title="5.æµ‹è¯•"></a>5.æµ‹è¯•</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">allennlp</span><br></pre></td></tr></table></figure><p>æˆåŠŸåæ•ˆæœå¦‚ä¸‹ï¼š</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> allennlp</span></span><br><span class="line">2019-05-22 21:58:42,297 - INFO - pytorch_pretrained_bert.modeling - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .</span><br><span class="line">usage: allennlp</span><br><span class="line"></span><br><span class="line">Run AllenNLP</span><br><span class="line"></span><br><span class="line">optional arguments:</span><br><span class="line">  -h, --help     show this help message and exit</span><br><span class="line">  --version      show program's version number and exit</span><br><span class="line"></span><br><span class="line">Commands:</span><br><span class="line"></span><br><span class="line">    configure    Run the configuration wizard.</span><br><span class="line">    train        Train a model.</span><br><span class="line">    evaluate     Evaluate the specified model + dataset.</span><br><span class="line">    predict      Use a trained model to make predictions.</span><br><span class="line">    make-vocab   Create a vocabulary.</span><br><span class="line">    elmo         Create word vectors using a pretrained ELMo model.</span><br><span class="line">    fine-tune    Continue training a model on a new dataset.</span><br><span class="line">    dry-run      Create a vocabulary, compute dataset statistics and other</span><br><span class="line">                 training utilities.</span><br><span class="line">    test-install</span><br><span class="line">                 Run the unit tests.</span><br><span class="line">    find-lr      Find a learning rate range.</span><br><span class="line">    print-results</span><br><span class="line">                 Print results from allennlp serialization directories to the</span><br><span class="line">                 console.</span><br></pre></td></tr></table></figure><h2 id="é‡åˆ°çš„é—®é¢˜"><a href="#é‡åˆ°çš„é—®é¢˜" class="headerlink" title="é‡åˆ°çš„é—®é¢˜"></a>é‡åˆ°çš„é—®é¢˜</h2><h3 id="é—®é¢˜1"><a href="#é—®é¢˜1" class="headerlink" title="é—®é¢˜1"></a>é—®é¢˜1</h3><h4 id="æŠ¥é”™ä¿¡æ¯ï¼š"><a href="#æŠ¥é”™ä¿¡æ¯ï¼š" class="headerlink" title="æŠ¥é”™ä¿¡æ¯ï¼š"></a>æŠ¥é”™ä¿¡æ¯ï¼š</h4><p>ERROR: Failed building wheel for jsonnet</p><p><img src="http://image.nysdy.com/20190522155853293350470.jpg" alt="20190522155853293350470.jpg"></p><h4 id="è§£å†³æ–¹æ³•ï¼š"><a href="#è§£å†³æ–¹æ³•ï¼š" class="headerlink" title="è§£å†³æ–¹æ³•ï¼š"></a>è§£å†³æ–¹æ³•ï¼š</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install -c conda-forge jsonnet</span><br></pre></td></tr></table></figure><h3 id="é—®é¢˜2"><a href="#é—®é¢˜2" class="headerlink" title="é—®é¢˜2"></a>é—®é¢˜2</h3><h4 id="æŠ¥é”™ä¿¡æ¯ï¼š-1"><a href="#æŠ¥é”™ä¿¡æ¯ï¼š-1" class="headerlink" title="æŠ¥é”™ä¿¡æ¯ï¼š"></a>æŠ¥é”™ä¿¡æ¯ï¼š</h4><p>æŠ¥çš„éƒ½æ˜¯æŸäº›åŒ…çš„ç‰ˆæœ¬é—®é¢˜</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ERROR: botocore 1.12.152 has requirement urllib3&lt;1.25,&gt;=1.20; python_version &gt;= "3.4", but you'll have urllib3 1.25.2 which is incompatible.</span><br><span class="line">ERROR: aws-sam-translator 1.11.0 has requirement jsonschema~=2.6, but you'll have jsonschema 3.0.1 which is incompatible.</span><br><span class="line">ERROR: cfn-lint 0.20.3 has requirement jsonschema~=2.6, but you'll have jsonschema 3.0.1 which is incompatible.</span><br><span class="line">ERROR: cfn-lint 0.20.3 has requirement requests&lt;=2.21.0,&gt;=2.15.0, but you'll have requests 2.22.0 which is incompatible</span><br></pre></td></tr></table></figure><h4 id="è§£å†³æ–¹æ³•"><a href="#è§£å†³æ–¹æ³•" class="headerlink" title="è§£å†³æ–¹æ³•"></a>è§£å†³æ–¹æ³•</h4><p>æ ¹æ®æŠ¥é”™ä¿¡æ¯ä¸‹è½½ç›¸åº”å®‰è£…åŒ…å³å¯</p><h2 id="é—®é¢˜3"><a href="#é—®é¢˜3" class="headerlink" title="é—®é¢˜3"></a>é—®é¢˜3</h2><h4 id="æŠ¥é”™ä¿¡æ¯ï¼š-2"><a href="#æŠ¥é”™ä¿¡æ¯ï¼š-2" class="headerlink" title="æŠ¥é”™ä¿¡æ¯ï¼š"></a>æŠ¥é”™ä¿¡æ¯ï¼š</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">ImportError: dlopen: cannot load any more object with static TLS</span><br><span class="line">___________________________________________________________________________</span><br><span class="line">Contents of /home/minelab/anaconda3/envs/allennlp/lib/python3.6/site-packages/sklearn/__check_build:</span><br><span class="line">__init__.py               setup.py                  _check_build.cpython-36m-x86_64-linux-gnu.so</span><br><span class="line">__pycache__</span><br><span class="line">___________________________________________________________________________</span><br><span class="line">It seems that scikit-learn has not been built correctly.</span><br><span class="line"></span><br><span class="line">If you have installed scikit-learn from source, please do not forget</span><br><span class="line">to build the package before using it: run `python setup.py install` or</span><br><span class="line">`make` in the source directory.</span><br><span class="line"></span><br><span class="line">If you have used an installer, please check that it is suited for your</span><br><span class="line">Python version, your operating system and your platform.</span><br></pre></td></tr></table></figure><h4 id="è§£å†³æ–¹æ³•ï¼š-1"><a href="#è§£å†³æ–¹æ³•ï¼š-1" class="headerlink" title="è§£å†³æ–¹æ³•ï¼š"></a>è§£å†³æ–¹æ³•ï¼š</h4><p>ä¸‹è½½æ›´ä½ç‰ˆæœ¬çš„scikit-learn,ä¾‹å¦‚</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install scikit-learn=0.20.3</span><br></pre></td></tr></table></figure><h2 id="å‚è€ƒé“¾æ¥"><a href="#å‚è€ƒé“¾æ¥" class="headerlink" title="å‚è€ƒé“¾æ¥"></a>å‚è€ƒé“¾æ¥</h2><ul><li><a href="https://github.com/pytorch/pytorch/issues/10443" target="_blank" rel="noopener">https://github.com/pytorch/pytorch/issues/10443</a></li><li><a href="https://github.com/pypa/pip/issues/4330" target="_blank" rel="noopener">https://github.com/pypa/pip/issues/4330</a></li></ul><h1 id="å®‰è£…çš„å¯ç¤º"><a href="#å®‰è£…çš„å¯ç¤º" class="headerlink" title="å®‰è£…çš„å¯ç¤º"></a>å®‰è£…çš„å¯ç¤º</h1><h3 id="ç¯å¢ƒé—®é¢˜"><a href="#ç¯å¢ƒé—®é¢˜" class="headerlink" title="ç¯å¢ƒé—®é¢˜"></a>ç¯å¢ƒé—®é¢˜</h3><ul><li>æœ€åŸºæœ¬çš„å°±æ˜¯å…ˆå»ç½‘ä¸ŠæŸ¥è¿™ä¸ªé”™è¯¯çš„è§£å†³æ–¹æ³•</li><li>ç½‘ä¸Šçš„è§£å†³ä¸äº†çš„ï¼Œå…ˆçŒœçŒœå¤§æ¦‚ç‡æ˜¯å“ªæ–¹é¢çš„é—®é¢˜ã€‚<ul><li>æ¯”å¦‚å¤§æ¦‚ç‡æ˜¯å„ç§ç‰ˆæœ¬äº’ç›¸ä¹‹é—´ä¸é€‚é…çš„é—®é¢˜ï¼Œé‚£å°±è°ƒè¯•ç‰ˆæœ¬ï¼Œä¸€èˆ¬éƒ½ä¼šå‘Šè¯‰ä½ å“ªä¸ªæœ‰é—®é¢˜ï¼Œæ¯”å¦‚ä¸Šé¢çš„scikit-learné—®é¢˜ã€‚</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;å®‰è£…allennlpçš„è¸©å‘ä¹‹è·¯ï¼Œè¸©äº†ä¸å°‘å‘æœ€åé€‰æ‹©â€™Installing from sourceâ€™çš„å®‰è£…æ–¹æ³•ï¼Œæ’å‘åä¸‹é¢æ–¹æ³•äº²æµ‹å¯ç”¨&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="install" scheme="http://yoursite.com/categories/install/"/>
    
    
      <category term="allennlp" scheme="http://yoursite.com/tags/allennlp/"/>
    
      <category term="åŒ…å®‰è£…" scheme="http://yoursite.com/tags/%E5%8C%85%E5%AE%89%E8%A3%85/"/>
    
  </entry>
  
  <entry>
    <title>Triple Trustworthiness Measurement for Knowledge Graphé˜…è¯»ç¬”è®°</title>
    <link href="http://yoursite.com/post/Triple%20Trustworthiness%20Measurement%20for%20Knowledge%20Graph/"/>
    <id>http://yoursite.com/post/Triple Trustworthiness Measurement for Knowledge Graph/</id>
    <published>2019-05-21T06:45:26.000Z</published>
    <updated>2019-05-22T13:33:20.088Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡è®¡ç®—triple trustworthinessæ¥è¯„ä¼°çŸ¥è¯†å›¾è°±çš„å‡†ç¡®ç¨‹åº¦çš„æ–¹æ³•ã€‚æ¨¡å‹åˆ©ç”¨ç¥ç»ç½‘ç»œç»¼åˆæ¥è‡ªå®ä½“ï¼ˆå€Ÿé‰´Resource allocationï¼‰ã€å…³ç³»ï¼ˆå€Ÿé‰´ç¿»è¯‘æ¨¡å‹çš„æ€æƒ³ï¼Œå¦‚TransEï¼‰å’ŒKGå…¨å±€ï¼ˆå€Ÿé‰´å…³ç³»è·¯å¾„ï¼ŒRNNï¼‰ä¸‰ä¸ªå±‚é¢çš„è¯­ä¹‰å’Œå…¨å±€ä¿¡æ¯ï¼Œè¾“å‡ºæœ€åçš„ trustworthinessä½œä¸ºåˆ¤æ–­ä¾æ®ã€‚</p><p><a href="http://delivery.acm.org/10.1145/3320000/3313586/p2865-jia.pdf?ip=59.64.129.22&amp;id=3313586&amp;acc=OPEN&amp;key=BF85BBA5741FDC6E%2E66A15327C2E204FC%2E4D4702B0C3E38B35%2E6D218144511F3437&amp;__acm__=1558515578_57e0bf75d529cf4656975ffe7da506b9" target="_blank" rel="noopener">ä¸‹è½½åœ°å€</a></p></blockquote><a id="more"></a><h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>This paper proposed a method for estimating the accuracy of a knowledge graph by computing triple trustworthiness. The model uses neural network to synthesize semantic and global information from three levels: entity(resource allocation), relationship(translation model ideas, such as TransE)m and KG global(relationship path, RNN) and outputting the final trustworthiness as the basis for judgment.</p><h1 id="Problem-statement"><a href="#Problem-statement" class="headerlink" title="Problem statement"></a>Problem statement</h1><p>possible noises and conflicts are inevitably intoduced in the process of constructing the KG</p><h1 id="research-objective"><a href="#research-objective" class="headerlink" title="research objective"></a>research objective</h1><p>quantify the KGâ€™s semantic correctness and the true degree of the facts expressed</p><h1 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h1><ul><li>Knowledge graph triple trustworthiness measurement<ul><li>use the  triple semantic information and globally inferring information</li><li>three levels measurement and an intergration of confidence value</li></ul></li><li>experiment result verified the model valid on large-scale KG Freebase</li><li>the KGTtm could be utilized in knowledge graph construction or improvement</li></ul><h1 id="THE-TRIPLE-TRUSTWORTHINESS-MEASUREMENT-MODEL"><a href="#THE-TRIPLE-TRUSTWORTHINESS-MEASUREMENT-MODEL" class="headerlink" title="THE TRIPLE TRUSTWORTHINESS MEASUREMENT MODEL"></a>THE TRIPLE TRUSTWORTHINESS MEASUREMENT MODEL</h1><p><img src="http://image.nysdy.com/20190521155842605796518.jpg" alt="20190521155842605796518.jpg"></p><ul><li>Longitudinally, the model can be divided into two level.<ul><li>the upper is a pool of multiple trustworthiness estimate cells(estimator)</li><li>the output of these Estimator forms the input of lower-level fusion device(Fusioner)</li></ul></li><li>Viewed laterally, three progressive levels   are be considered, as following.</li></ul><h2 id="Is-there-a-possible-relationship-between-the-entity-pairs"><a href="#Is-there-a-possible-relationship-between-the-entity-pairs" class="headerlink" title="Is there a possible relationship between the entity pairs?"></a>Is there a possible relationship between the entity pairs?</h2><p><img src="http://image.nysdy.com/20190521155842810227816.jpg" alt="20190521155842810227816.jpg"></p><p>ResourceRank:</p><ul><li>The algorithm assumes that the association between entity paires $(h,t)$ will be stronger, and more resource is passed from the  head $h$ through all associated paths to the tail $t$ in a graph</li><li>The amount of resource aggregated into $t$ ingeniously indicateds the association strength from $h$ to $t$.</li></ul><p>As pair $(e_1,e_2)$, there only one directed edge from $e_1$ to $e_2$ in the graph, where the different bandwidth of the edge indicates the number of the multiple relations.</p><p>output:</p><script type="math/tex; mode=display">\left\{\begin{array}{c}{u=\alpha\left(W_{1} V+b_{1}\right)} \\ {R R(h, t)=W_{2} u+b_{2}}\end{array}\right.</script><p>Authors constructed a $V$ vector by combining six characteristics.</p><ol><li>R (t | h); </li><li>In-degree of head node ID(h); </li><li>Out-degree of head node OD(h); </li><li>In-degree of tail node ID(t);</li><li>Out-degree of tail node OD(t);</li><li>The depth from head node to tail node Dep</li></ol><p>As for 1. the formula:</p><script type="math/tex; mode=display">R(t | h)=(1-\theta) \sum_{e_{i} \in M_{t}} \frac{R\left(e_{i} | h\right) \cdot B W_{e_{i} t}}{O D\left(e_{i}\right)}+\frac{\theta}{N}</script><ul><li>$M_t$is the set of all nodes that have outgoing links to the node $t$, $OD (e_i)$ is the out-degree of the node eiand the $BW_{e_it}$ is the bandwidth from the $e_i$ to $t$.</li><li>In order to improve the model fault-tolerance, we assume that the resource fow from each node may directly jump to a random node with the same probability Î¸</li></ul><h2 id="Can-the-determined-relationship-r-occur-between-the-entity-pair-h-t"><a href="#Can-the-determined-relationship-r-occur-between-the-entity-pair-h-t" class="headerlink" title="Can the determined relationship $r$ occur between the entity pair $(h,t)$ ?"></a>Can the determined relationship $r$ occur between the entity pair $(h,t)$ ?</h2><p><img src="http://image.nysdy.com/20190522155850919251079.jpg" alt="20190522155850919251079.jpg"></p><p>Translation-based energy function (TEF)ï¼šdepended on TransE</p><p>$E(h, r, t)=|\mathbf{h}+\mathbf{r}-\mathbf{t}|$</p><p>output:</p><script type="math/tex; mode=display">P(E(h, r, t))=\frac{1}{1+e^{-\lambda\left(\delta_{r}-E(h, r, t)\right)}}</script><h2 id="Can-the-relevant-triples-in-the-KG-infer-that-the-triple-is-trustworthy"><a href="#Can-the-relevant-triples-in-the-KG-infer-that-the-triple-is-trustworthy" class="headerlink" title="Can the relevant triples in the KG infer that the triple is trustworthy?"></a>Can the relevant triples in the KG infer that the triple is trustworthy?</h2><p><img src="http://image.nysdy.com/20190522155851013726520.jpg" alt="20190522155851013726520.jpg"></p><p>Reachable paths inference (RPI):</p><p>There two challenges to exploit the reachable paths for inferring triple trustworthiness:</p><h3 id="reachable-paths-selection"><a href="#reachable-paths-selection" class="headerlink" title="reachable paths selection"></a>reachable paths selection</h3><p>Semantic distance-based path selection<img src="http://image.nysdy.com/2019052215585105592950.jpg" alt="2019052215585105592950.jpg"></p><h3 id="Reachable-Paths-Representation"><a href="#Reachable-Paths-Representation" class="headerlink" title="Reachable Paths Representation"></a>Reachable Paths Representation</h3><p>using a RNN to deal with the embeddings of the three elements of each triple in the selected path</p><h2 id="Fusing-the-Estimators"><a href="#Fusing-the-Estimators" class="headerlink" title="Fusing the Estimators"></a>Fusing the Estimators</h2><p>a classifer based on a multi-layer perceptron </p><h1 id="EXPERIMENTS"><a href="#EXPERIMENTS" class="headerlink" title="EXPERIMENTS"></a>EXPERIMENTS</h1><h2 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h2><p>FB15K</p><h2 id="Interpreting-the-Validity-of-the-Trustworthiness"><a href="#Interpreting-the-Validity-of-the-Trustworthiness" class="headerlink" title="Interpreting the Validity of the Trustworthiness"></a>Interpreting the Validity of the Trustworthiness</h2><p><img src="http://image.nysdy.com/20190522155851139148623.jpg" alt="20190522155851139148623.jpg"></p><ul><li>The left picture shows that the positives examples are mainly concentrated in the upper region, vice versa.</li><li>As for the right picture<ul><li>only if the value of a triple is higher than the threshold can it be considered trustworthy</li><li>shows that the positive examples universally have higher confidence values</li></ul></li></ul><h2 id="Comparing-With-Other-Models-on-The-Knowledge-Graph-Error-Detection-Task"><a href="#Comparing-With-Other-Models-on-The-Knowledge-Graph-Error-Detection-Task" class="headerlink" title="Comparing With Other Models on The Knowledge Graph Error Detection Task"></a>Comparing With Other Models on The Knowledge Graph Error Detection Task</h2><p><img src="http://image.nysdy.com/20190522155851269267340.jpg" alt="20190522155851269267340.jpg"></p><p>Authorsâ€™ model has beter results in terms of accuracy and the F1-score than the other models.</p><h2 id="Analyzing-the-ability-of-models-to-tackle-the-three-type-noises"><a href="#Analyzing-the-ability-of-models-to-tackle-the-three-type-noises" class="headerlink" title="Analyzing the ability of models to tackle the three type noises."></a>Analyzing the ability of models to tackle the three type noises.</h2><p><img src="http://image.nysdy.com/20190522155851290149973.jpg" alt="20190522155851290149973.jpg"></p><ul><li>a higher recall shows that authorsâ€™ model can more accurately find the right from noisy triples</li><li>higher average trustworthiness values show that authorsâ€™ model can better identify the correct instances and with high confidence </li><li>the worst among the $(h, ?, t)$, because the various relations between a certain entity  increase the difficulty of model judgment.</li></ul><h2 id="Analyzing-the-Efects-of-Single-Estimators"><a href="#Analyzing-the-Efects-of-Single-Estimators" class="headerlink" title="Analyzing the Efects of Single Estimators"></a>Analyzing the Efects of Single Estimators</h2><p><img src="http://image.nysdy.com/20190522155851337652153.jpg" alt="20190522155851337652153.jpg"></p><p>It can be found that the accuracy obtained by each model is above 0.8, which proves the effectiveness of each Estimator</p><h1 id="æ€è€ƒ"><a href="#æ€è€ƒ" class="headerlink" title="æ€è€ƒ"></a>æ€è€ƒ</h1><p>æœ¬æ–‡åœ¨æ–¹æ³•ä¸Šå‡ ä¹æ²¡æœ‰ä»€ä¹ˆåˆ›æ–°ï¼Œæœ¬è´¨ä¸Šå°±æ˜¯ä¸€ä¸ªè€æ–¹æ³•çš„å¤šä¸ªç»„åˆã€‚æœ€å¤§äº®ç‚¹å°±æ˜¯ä½œè€…èƒ½æå‡ºtrustworthinessæ¥æŠŠè¿™ä¸ªè¯„ä»·çŸ¥è¯†å›¾è°±å‡†ç¡®åº¦çš„é—®é¢˜è¿›è¡Œäº†é‡åŒ–ã€‚è¿™ç§èƒ½åŠ›æ¯”æå‡ºæ–¹æ³•ä¸Šçš„åˆ›æ–°æ›´åŠ å‰å®³ï¼Œä¹Ÿæ˜¯éœ€è¦å­¦ä¹ çš„åœ°æ–¹ã€‚</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡è®¡ç®—triple trustworthinessæ¥è¯„ä¼°çŸ¥è¯†å›¾è°±çš„å‡†ç¡®ç¨‹åº¦çš„æ–¹æ³•ã€‚æ¨¡å‹åˆ©ç”¨ç¥ç»ç½‘ç»œç»¼åˆæ¥è‡ªå®ä½“ï¼ˆå€Ÿé‰´Resource allocationï¼‰ã€å…³ç³»ï¼ˆå€Ÿé‰´ç¿»è¯‘æ¨¡å‹çš„æ€æƒ³ï¼Œå¦‚TransEï¼‰å’ŒKGå…¨å±€ï¼ˆå€Ÿé‰´å…³ç³»è·¯å¾„ï¼ŒRNNï¼‰ä¸‰ä¸ªå±‚é¢çš„è¯­ä¹‰å’Œå…¨å±€ä¿¡æ¯ï¼Œè¾“å‡ºæœ€åçš„ trustworthinessä½œä¸ºåˆ¤æ–­ä¾æ®ã€‚&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://delivery.acm.org/10.1145/3320000/3313586/p2865-jia.pdf?ip=59.64.129.22&amp;amp;id=3313586&amp;amp;acc=OPEN&amp;amp;key=BF85BBA5741FDC6E%2E66A15327C2E204FC%2E4D4702B0C3E38B35%2E6D218144511F3437&amp;amp;__acm__=1558515578_57e0bf75d529cf4656975ffe7da506b9&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ä¸‹è½½åœ°å€&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="è®ºæ–‡é˜…è¯»ç¬”è®°" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="KG" scheme="http://yoursite.com/tags/KG/"/>
    
      <category term="Knowledge Graph" scheme="http://yoursite.com/tags/Knowledge-Graph/"/>
    
      <category term="Triple" scheme="http://yoursite.com/tags/Triple/"/>
    
  </entry>
  
  <entry>
    <title>GloVe: Global Vectors for Word Representationé˜…è¯»ç¬”è®°</title>
    <link href="http://yoursite.com/post/GloVe:Global%20Vectors%20for%20Word%20Representation/"/>
    <id>http://yoursite.com/post/GloVe:Global Vectors for Word Representation/</id>
    <published>2019-05-20T13:35:25.000Z</published>
    <updated>2019-05-21T06:43:45.987Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>è®ºæ–‡<a href="https://www.aclweb.org/anthology/D14-1162" target="_blank" rel="noopener">ä¸‹è½½åœ°å€</a>ï¼ŒGloVeæ˜¯ä¸€ä¸ªæ–°çš„å…¨çƒå¯¹æ•°åŒçº¿æ€§å›å½’æ¨¡å‹ï¼Œå±äºç»å…¸çš„è¯å‘é‡è¡¨ç¤ºæ–¹æ³•ä¹‹ä¸€ã€‚</p></blockquote><a id="more"></a><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>evaluate the intrinsic quality</p><ul><li>Most word vector methods rely on the distance or angle between pairs of word vectors </li><li>Mikolov et al. (2013c) introduced word analogies that examines word vectorâ€™s various dimensions of difference.</li></ul><p>two main model families for learning vectors:</p><ul><li>global matrix factorization methods</li><li>local context window methods</li></ul><p>Authors propose a specific weighted least squares model that trains on globla word-word co-occurrence counts and thus makes efficient use of statistics.</p><h1 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h1><h2 id="Matix-Facroization-Methods"><a href="#Matix-Facroization-Methods" class="headerlink" title="Matix Facroization Methods"></a>Matix Facroization Methods</h2><p>These methods utilize low-rank approximations to decompose large matrices that capture statistical information about a corpus.</p><h3 id="shortcoming"><a href="#shortcoming" class="headerlink" title="shortcoming"></a>shortcoming</h3><p>the most frequent words contribute a dispropoertionate amount to the similarity measure.</p><h2 id="Shallow-Window-Based-Methods"><a href="#Shallow-Window-Based-Methods" class="headerlink" title="Shallow Window-Based Methods"></a>Shallow Window-Based Methods</h2><p>Another approach is to learn word representations that aid in making predictins within local context windows.</p><h3 id="shortcoming-1"><a href="#shortcoming-1" class="headerlink" title="shortcoming"></a>shortcoming</h3><p>do not operate directly on the co-occurrence statistics of the corpus and fails to take advantage of the vast amount of repetition in the data.</p><h1 id="The-GloVe-Model"><a href="#The-GloVe-Model" class="headerlink" title="The GloVe Model"></a>The GloVe Model</h1><h3 id="GloVe-Global-Vectors"><a href="#GloVe-Global-Vectors" class="headerlink" title="GloVe: Global Vectors"></a>GloVe: Global Vectors</h3><p>the global corpus statistics are captured directly by the model</p><h3 id="the-question-about-the-model-using-the-statistics-of-word-occurrences-in-a-corpus"><a href="#the-question-about-the-model-using-the-statistics-of-word-occurrences-in-a-corpus" class="headerlink" title="the question about the model using the statistics of word occurrences in a corpus"></a>the question about the model using the statistics of word occurrences in a corpus</h3><ul><li>how meaning is generated from these statistics</li><li>how the resulting word vectors might represent that meaning</li></ul><h2 id="some-notation"><a href="#some-notation" class="headerlink" title="some notation"></a>some notation</h2><p>$X_{ij}$ : the number of times word j occurs in the context of word i</p><p>$X_i = \sum_{k} X_{i k}$ : the number of times any word appears in the context of word i</p><p>$P_{i j}=P(j | i)=X_{i j} / X_{i}$: the probability that word j appear in the context of word i</p><p><img src="http://image.nysdy.com/20190515155788329289127.jpg" alt="20190515155788329289127.jpg"></p><p>above that, werd vector learning should be with ratios of co-occurrence probabilities:</p><p><img src="http://image.nysdy.com/20190515155788352871603.jpg" alt="20190515155788352871603.jpg"></p><p>$w \in \mathbb{R}^{d}$are word vectors and $\tilde{w} \in \mathbb{R}^{d}$are separate context word vectors</p><p>For F, we should select a unique choice by enforcing a few desiderata.</p><ul><li><p>encoding the information present the ratio $P_{i k} / P_{j k}$ in the word vector space. </p><p>Since vector spaces are inherently linear structures</p><p><img src="http://image.nysdy.com/20190515155788379647745.jpg" alt="20190515155788379647745.jpg"></p></li><li><p>put F to be a compicated function parameterized, and avoiding bofuscating the linear structure<img src="http://image.nysdy.com/20190515155788397136355.jpg" alt="20190515155788397136355.jpg"></p></li><li><p>the word-word co-occurrence matrices, we can exchange a word and a context word(because a word can also be a context word)</p><ol><li><p>F should be a homomorphism<img src="http://image.nysdy.com/2019051515578842869345.jpg" alt="2019051515578842869345.jpg"></p><p>by Eqn.(3)<img src="http://image.nysdy.com/20190515155788435674239.png" alt="20190515155788435674239.png"></p><p>F = exp or <img src="http://image.nysdy.com/20190515155788448759173.jpg" alt="20190515155788448759173.jpg"></p></li><li><p>the Eqn(6) would have the exchange symmetry if not $\log \left(X_{i}\right)$ and $\log \left(X_{i}\right)$ is independent of k, so it can be absorbed into a bias $b_i$<img src="http://image.nysdy.com/20190515155788566687599.jpg" alt="20190515155788566687599.jpg"></p></li><li><p>for avoiding diverge, $\log \left(X_{i k}\right) \rightarrow \log \left(1+X_{i k}\right)$</p></li><li><p>a new weighted least squares regression model to address the problem that LSA wirhts all co-occuttences equally.</p><p>cost function:<img src="http://image.nysdy.com/20190515155788560186237.jpg" alt="20190515155788560186237.jpg"></p></li><li><p><img src="http://image.nysdy.com/20190515155788571777804.jpg" alt="20190515155788571777804.jpg"></p></li></ol></li></ul><h2 id="Relationship-to-Other-Models"><a href="#Relationship-to-Other-Models" class="headerlink" title="Relationship to Other Models"></a>Relationship to Other Models</h2><p>In this subsection authors show how these models are related to their proposed model.</p><h4 id="the-defect-of-cross-entropy"><a href="#the-defect-of-cross-entropy" class="headerlink" title="the defect of cross entropy"></a>the defect of cross entropy</h4><ul><li>it has the unfortunate property that distributions with long tails are often modeled poorly with too much wieght given to the unlikely events.</li></ul><h2 id="Complexity-of-the-model"><a href="#Complexity-of-the-model" class="headerlink" title="Complexity of the model"></a>Complexity of the model</h2><p>the computational complexity of the model depends on the number of nonzero elects in the matrix $X$</p><h4 id="some-assumptions-about-the-distribution-of-word-co-occurrences"><a href="#some-assumptions-about-the-distribution-of-word-co-occurrences" class="headerlink" title="some assumptions about the distribution of word co-occurrences"></a>some assumptions about the distribution of word co-occurrences</h4><ul><li><p>the number of co-occurrences of word $i$ with word $j$, $X_{ij}$, can be modeled as a power-law function of the frequency rank of that word pair, $r_{ij}$:</p><p>$X_{i j}=\frac{k}{\left(r_{i j}\right)^{\alpha}}$</p></li></ul><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Evaluation-methods"><a href="#Evaluation-methods" class="headerlink" title="Evaluation methods"></a>Evaluation methods</h2><p>authors conduct experiments on the word analogy taks of Mikolov et al. (2013a)</p><h3 id="Word-analogies"><a href="#Word-analogies" class="headerlink" title="Word analogies"></a>Word analogies</h3><p>The word analogy task consists of questions like, â€œa is to b as c is to ?â€</p><h3 id="Word-similarity"><a href="#Word-similarity" class="headerlink" title="Word similarity"></a>Word similarity</h3><p><img src="http://image.nysdy.com/20190520155833277478435.jpg" alt="20190520155833277478435.jpg"></p><h3 id="Named-entity-recognition"><a href="#Named-entity-recognition" class="headerlink" title="Named entity recognition"></a>Named entity recognition</h3><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>Table 2 shows the CloVe model performs significantly better than the other baslines, often with smaller vector sizes and smaller corpora.</p><p><img src="http://image.nysdy.com/20190520155833353468570.jpg" alt="20190520155833353468570.jpg"></p><p>Table 3 shows results on five different word similarity datasets.</p><p>Table 4 shows results on the NER task with the CRF-based model.</p><p><img src="http://image.nysdy.com/20190520155833377540169.jpg" alt="20190520155833377540169.jpg"></p><h2 id="Model-Analysis-Vector-Length-and-Context-Size"><a href="#Model-Analysis-Vector-Length-and-Context-Size" class="headerlink" title="Model Analysis: Vector Length and Context Size"></a>Model Analysis: Vector Length and Context Size</h2><p><img src="http://image.nysdy.com/2019052015583339399087.jpg" alt="2019052015583339399087.jpg"></p><h3 id="Model-Analysis-Corpus-Size"><a href="#Model-Analysis-Corpus-Size" class="headerlink" title="Model Analysis: Corpus Size"></a>Model Analysis: Corpus Size</h3><p><img src="http://image.nysdy.com/20190520155833403240640.jpg" alt="20190520155833403240640.jpg"></p><ul><li>On the syntactic subtask, larger corpora typically produce better statistics so that there is a monotonic increase in performance as the cor- pus size increases.</li><li>But the same trend is not true for the semantic subtask, which is probably because of analogy dataset</li></ul><h2 id="Model-Analysis-Run-time"><a href="#Model-Analysis-Run-time" class="headerlink" title="Model Analysis: Run-time"></a>Model Analysis: Run-time</h2><p><img src="http://image.nysdy.com/20190520155833432462881.jpg" alt="20190520155833432462881.jpg"></p><h2 id="Model-Analysis-Comparison-with-word2vec"><a href="#Model-Analysis-Comparison-with-word2vec" class="headerlink" title="Model Analysis: Comparison with word2vec"></a>Model Analysis: Comparison with word2vec</h2><p>For the same corpus, vocabulary, window size, and training time, GloVe consistently outperforms word2vec</p><h1 id="å‚è€ƒé“¾æ¥"><a href="#å‚è€ƒé“¾æ¥" class="headerlink" title="å‚è€ƒé“¾æ¥"></a>å‚è€ƒé“¾æ¥</h1><ul><li><a href="https://blog.csdn.net/coderTC/article/details/73864097" target="_blank" rel="noopener">https://blog.csdn.net/coderTC/article/details/73864097</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;è®ºæ–‡&lt;a href=&quot;https://www.aclweb.org/anthology/D14-1162&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ä¸‹è½½åœ°å€&lt;/a&gt;ï¼ŒGloVeæ˜¯ä¸€ä¸ªæ–°çš„å…¨çƒå¯¹æ•°åŒçº¿æ€§å›å½’æ¨¡å‹ï¼Œå±äºç»å…¸çš„è¯å‘é‡è¡¨ç¤ºæ–¹æ³•ä¹‹ä¸€ã€‚&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="è®ºæ–‡é˜…è¯»ç¬”è®°" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="word vector" scheme="http://yoursite.com/tags/word-vector/"/>
    
      <category term="GloVe" scheme="http://yoursite.com/tags/GloVe/"/>
    
  </entry>
  
  <entry>
    <title>Deep contextualized word representations é˜…è¯»ç¬”è®°</title>
    <link href="http://yoursite.com/post/Deep%20contextualized%20word%20representations/"/>
    <id>http://yoursite.com/post/Deep contextualized word representations/</id>
    <published>2019-05-10T07:28:06.000Z</published>
    <updated>2019-05-13T06:50:58.808Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://arxiv.org/pdf/1802.05365.pdf" target="_blank" rel="noopener">è®ºæ–‡ä¸‹è½½åœ°å€</a>ï¼ŒELMoäº‹å…ˆç”¨è¯­è¨€æ¨¡å‹å­¦å¥½ä¸€ä¸ªå•è¯çš„ Word Embeddingï¼Œæ­¤æ—¶å¤šä¹‰è¯æ— æ³•åŒºåˆ†ï¼Œä¸è¿‡è¿™æ²¡å…³ç³»ã€‚åœ¨æˆ‘å®é™…ä½¿ç”¨ Word Embedding çš„æ—¶å€™ï¼Œå•è¯å·²ç»å…·å¤‡äº†ç‰¹å®šçš„ä¸Šä¸‹æ–‡äº†ï¼Œè¿™ä¸ªæ—¶å€™æˆ‘å¯ä»¥æ ¹æ®ä¸Šä¸‹æ–‡å•è¯çš„è¯­ä¹‰å»è°ƒæ•´å•è¯çš„ Word Embedding è¡¨ç¤ºï¼Œè¿™æ ·ç»è¿‡è°ƒæ•´åçš„ Word Embedding æ›´èƒ½è¡¨è¾¾åœ¨è¿™ä¸ªä¸Šä¸‹æ–‡ä¸­çš„å…·ä½“å«ä¹‰ï¼Œè‡ªç„¶ä¹Ÿå°±è§£å†³äº†å¤šä¹‰è¯çš„é—®é¢˜äº†ã€‚<strong>æ‰€ä»¥ ELMO æœ¬èº«æ˜¯ä¸ªæ ¹æ®å½“å‰ä¸Šä¸‹æ–‡å¯¹ Word Embedding åŠ¨æ€è°ƒæ•´çš„æ€è·¯ã€‚</strong></p></blockquote><a id="more"></a><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><h2 id="ELMo-Embedddings-from-Language-Models"><a href="#ELMo-Embedddings-from-Language-Models" class="headerlink" title="ELMo(Embedddings from Language Models):"></a>ELMo(Embedddings from Language Models):</h2><h3 id="why-call-ELMo"><a href="#why-call-ELMo" class="headerlink" title="why call ELMo:"></a>why call ELMo:</h3><p>Using vectors derived from a bidirectional LSTM that is trained with a coupled language model(LM) objective on a large text corups.</p><h3 id="characteristics"><a href="#characteristics" class="headerlink" title="characteristics"></a>characteristics</h3><ul><li><p>ELMo representations are a function of all of the internal layers of the biLM.</p></li><li><p>learn a linear combination of the vectors stacked above each input word for each end task</p></li><li><p>the higher-level LSTM states capture context-dependent aspects of word meaning</p><p>the lower-level states model aspects of syntax</p></li></ul><h3 id="Extensive-experiments"><a href="#Extensive-experiments" class="headerlink" title="Extensive experiments"></a>Extensive experiments</h3><ul><li>EMLo representations can be easily added to existing models</li><li>improve the state of art in every case</li><li>ELMo outperform those derived from just the top layer of a LSTM</li></ul><h1 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h1><ul><li><p>Some approaches for learning word vectors only allow a single context-independent representation for each word.</p></li><li><p>to overcome some shortcomings of traditional word vectors:</p><ul><li>enriching them with subword information</li><li>learning separate vectors for each word sense</li></ul><p>Authors uses subword units through the use of character convolutions, seamlessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes.</p></li><li><p>context-depends representations</p><p> Authors take full advantage of access to plentiful monolingual data</p></li><li><p>Previous work also shown that different layers of deep biRNNs encode different types of information</p><ul><li>introducing multi-task syntactic supervision at the lower levels of a deep LSTM can improve overall performance of higher level tasks</li><li>the top layer of an LSTM for encoding word context (Melamud et al., 2016) has been shown to learn representations of word sense.</li></ul><p>ELMo representations can also induce similar signals.</p></li></ul><h1 id="ELMo-Embeddings-from-Language-Models"><a href="#ELMo-Embeddings-from-Language-Models" class="headerlink" title="ELMo: Embeddings from Language Models"></a>ELMo: Embeddings from Language Models</h1><h2 id="Bidirectional-language-models"><a href="#Bidirectional-language-models" class="headerlink" title="Bidirectional language models"></a>Bidirectional language models</h2><ul><li><p>model the probability of token $t_k$ given the history($t_1, â€¦ , t_{k-1}$):</p><p><img src="http://image.nysdy.com/20190512155766627486478.png" alt="20190512155766627486478.png"></p></li><li><p>a backward LM:<img src="http://image.nysdy.com/2019051215576663543534.png" alt="2019051215576663543534.png"></p></li></ul><p>Authorsâ€™ formulation jointly maximizes the log likelihood of the forward and backward directions:</p><p><img src="http://image.nysdy.com/20190512155766643539454.png" alt="20190512155766643539454.png"></p><h2 id="ELMo"><a href="#ELMo" class="headerlink" title="ELMo"></a>ELMo</h2><ul><li><p>For each token $t_k$, a L-layer biLM computes a set of 2L + 1 representations:<img src="http://image.nysdy.com/20190512155767113638451.png" alt="20190512155767113638451.png"></p></li><li><p>For a downstream model, ELMo collapses all layers in R into a single vector.</p><p>In the simplest case, ELMo just selects the top layer.</p></li><li><p>For a task specific weighting of all biLM layers:<img src="http://image.nysdy.com/20190512155767132777658.png" alt="20190512155767132777658.png"></p><p>$s^{task}$ are softmax-normalized weithts and the scalar parameter $Î³^{task}$ allows the task model to scale the entire ELMo vector</p></li></ul><h2 id="Using-biLMs-for-supervised-NLP-tasks"><a href="#Using-biLMs-for-supervised-NLP-tasks" class="headerlink" title="Using biLMs for supervised NLP tasks"></a>Using biLMs for supervised NLP tasks</h2><ul><li>Given a pre-trained biLM and a supervised architecture for a target NLP task</li><li>let the end task model learn a linear combination of these representations<ol><li>consider the lowest layers of th supervised model without the biLM</li><li>add ELMo to the supervised model<ul><li>freeze the weights of the biLM</li><li>concatenate the ELMo vector $ELMo^{task}_k$ with $x_k$ and pass the ELMo enhanced representation $[x_k,;ELMo^{task}_k ]$ into the task RNN.</li><li>for some tasks, authors also include ELMo ar the output of task RNN by introducing another set of out put specific linear weights and replacing $h_k$ with $[h_k,;ELMo^{task}_k ]$</li><li>add a moderate amount of dropout to ELMo and in some case to regularize the ELMo weights</li></ul></li></ol></li></ul><h2 id="Pre-trained-bidirectional-language-model-architecture"><a href="#Pre-trained-bidirectional-language-model-architecture" class="headerlink" title="Pre-trained bidirectional language model architecture"></a>Pre-trained bidirectional language model architecture</h2><ul><li>the biLM provides three layers of representations for each input token, both directions and a residual connection between LSTM layers </li><li>fine tuning the biLM on domain specific data</li></ul><h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><p>the following picture shows the performance of ELMo in Question answering, Textual entailment, Semantic role labeling, Corefrence resolution, Named entity extraction, Sentiment analysis.</p><p><img src="http://image.nysdy.com/2019051315577106394943.png" alt="2019051315577106394943.png"></p><p>In every task considered, simply adding ELMo establishes a new state-of-the-art result.</p><h1 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h1><h2 id="Alternate-layer-weighting-schemes"><a href="#Alternate-layer-weighting-schemes" class="headerlink" title="Alternate layer weighting schemes"></a>Alternate layer weighting schemes</h2><p><img src="http://image.nysdy.com/20190512155767132777658.png" alt="20190512155767132777658.png"></p><p>the following picture compares these alternatives.</p><p><img src="http://image.nysdy.com/20190513155771140936480.png" alt="20190513155771140936480.png"></p><p>Including representations from all layers improves overall performance over just using the last layer, and including contextual representations from the last layer improves performace over the baseline.</p><p>Also shows the $\lambda$ is important.</p><h2 id="Where-to-include-ELMo"><a href="#Where-to-include-ELMo" class="headerlink" title="Where to include ELMo?"></a>Where to include ELMo?</h2><p>The ELMo can be included in both the input and output.</p><p><img src="http://image.nysdy.com/20190513155771190646517.png" alt="20190513155771190646517.png"></p><p>the results show including the ELMo in both input and output can preform better.</p><h2 id="What-information-is-captured-by-the-biLMâ€™s-representations"><a href="#What-information-is-captured-by-the-biLMâ€™s-representations" class="headerlink" title="What information is captured by the biLMâ€™s representations?"></a>What information is captured by the biLMâ€™s representations?</h2><p>Intuitively, the biLM must be disambiguating the meaning of words using their context.<img src="http://image.nysdy.com/20190513155771262634271.png" alt="20190513155771262634271.png"></p><p>The GloVe can only capure the speech. but the biLM is able to disambiguate both the part of speech and word sense in the source sentence.</p><h3 id="Word-sense-disambiguation"><a href="#Word-sense-disambiguation" class="headerlink" title="Word sense disambiguation"></a>Word sense disambiguation</h3><p>given a sentence, predicting  the sense of a target word using a simple 1-nearst negihbor approach</p><p><img src="http://image.nysdy.com/20190513155771312514655.png" alt="20190513155771312514655.png"></p><h3 id="POS-tagging"><a href="#POS-tagging" class="headerlink" title="POS tagging"></a>POS tagging</h3><p>to examine whether the biLM captures basic syntax.</p><p><img src="http://image.nysdy.com/20190513155771328944169.png" alt="20190513155771328944169.png"></p><h2 id="Sample-efficiency"><a href="#Sample-efficiency" class="headerlink" title="Sample efficiency"></a>Sample efficiency</h2><p>Adding ELMo to a model increases the sample efficiency considerably, both in terms of number of parameter updates to reach state-of-the-art performance and the overall training set size.<img src="http://image.nysdy.com/20190513155771349825964.png" alt="20190513155771349825964.png"></p><h2 id="Visualization-of-learned-weights"><a href="#Visualization-of-learned-weights" class="headerlink" title="Visualization of learned weights"></a>Visualization of learned weights</h2><p><img src="http://image.nysdy.com/20190513155771355211483.png" alt="20190513155771355211483.png"></p><h1 id="å‚è€ƒé“¾æ¥"><a href="#å‚è€ƒé“¾æ¥" class="headerlink" title="å‚è€ƒé“¾æ¥"></a>å‚è€ƒé“¾æ¥</h1><ul><li><a href="https://zhuanlan.zhihu.com/p/63115885" target="_blank" rel="noopener">NAACL2018:é«˜çº§è¯å‘é‡(ELMo)è¯¦è§£(è¶…è¯¦ç»†) ç»å…¸</a>ï¼Œè¿™ç¯‡æ–‡ç« ä¸­é˜è¿°äº†ä¸€äº›ä½¿ç”¨çš„ç»†èŠ‚ï¼Œå¹¶ç”¨å›¾æ¥è¡¨ç¤ºï¼Œæ›´åŠ æ¸…æ™°ã€‚</li><li><a href="https://blog.csdn.net/triplemeng/article/details/82380202" target="_blank" rel="noopener">ELMoç®—æ³•ä»‹ç»</a>ï¼Œè¿™ç¯‡åšå®¢ä¸­è‡ªå·±å¯¹æ•´ä¸ªè®ºæ–‡çš„æ¦‚è¿°å’Œæ€»ç»“å’Œå¥½ï¼Œéœ€è¦å­¦ä¹ ã€‚</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1802.05365.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;è®ºæ–‡ä¸‹è½½åœ°å€&lt;/a&gt;ï¼ŒELMoäº‹å…ˆç”¨è¯­è¨€æ¨¡å‹å­¦å¥½ä¸€ä¸ªå•è¯çš„ Word Embeddingï¼Œæ­¤æ—¶å¤šä¹‰è¯æ— æ³•åŒºåˆ†ï¼Œä¸è¿‡è¿™æ²¡å…³ç³»ã€‚åœ¨æˆ‘å®é™…ä½¿ç”¨ Word Embedding çš„æ—¶å€™ï¼Œå•è¯å·²ç»å…·å¤‡äº†ç‰¹å®šçš„ä¸Šä¸‹æ–‡äº†ï¼Œè¿™ä¸ªæ—¶å€™æˆ‘å¯ä»¥æ ¹æ®ä¸Šä¸‹æ–‡å•è¯çš„è¯­ä¹‰å»è°ƒæ•´å•è¯çš„ Word Embedding è¡¨ç¤ºï¼Œè¿™æ ·ç»è¿‡è°ƒæ•´åçš„ Word Embedding æ›´èƒ½è¡¨è¾¾åœ¨è¿™ä¸ªä¸Šä¸‹æ–‡ä¸­çš„å…·ä½“å«ä¹‰ï¼Œè‡ªç„¶ä¹Ÿå°±è§£å†³äº†å¤šä¹‰è¯çš„é—®é¢˜äº†ã€‚&lt;strong&gt;æ‰€ä»¥ ELMO æœ¬èº«æ˜¯ä¸ªæ ¹æ®å½“å‰ä¸Šä¸‹æ–‡å¯¹ Word Embedding åŠ¨æ€è°ƒæ•´çš„æ€è·¯ã€‚&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="è®ºæ–‡é˜…è¯»ç¬”è®°" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="embedding" scheme="http://yoursite.com/tags/embedding/"/>
    
  </entry>
  
  <entry>
    <title>ã€ŠEfficient Estimation of Word Representations in Vector Spaceã€‹é˜…è¯»ç¬”è®°</title>
    <link href="http://yoursite.com/post/Efficient%20Estimation%20of%20Word%20Representations%20in%20Vector%20Space/"/>
    <id>http://yoursite.com/post/Efficient Estimation of Word Representations in Vector Space/</id>
    <published>2019-04-30T02:37:23.000Z</published>
    <updated>2019-05-06T09:00:22.589Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://arxiv.org/pdf/1301.3781.pdf" target="_blank" rel="noopener">è®ºæ–‡ä¸‹è½½åœ°å€</a>ï¼Œè¯¥ç¯‡è®ºæ–‡çš„å¤§ç¯‡å¹…éƒ½åœ¨è®¨è®ºå®éªŒç»“æœçš„åˆ†æï¼Œæ¨¡å‹çš„éƒ¨åˆ†æ¯”è¾ƒç®€å•ï¼Œæ²¡æœ‰è¯¦ç»†åˆ†æï¼Œæœ¬æ¥æ˜¯æƒ³è¯»ä¸€ä¸‹CBOWå’Œskip-gramçš„åŸå§‹è®ºæ–‡ï¼Œå‘ç°å¹¶æ²¡æœ‰æƒ³è±¡ä¸­çš„é‚£ä¹ˆå¤§çš„ç”¨å¤„ã€‚</p></blockquote><a id="more"></a><h2 id="Goals-of-paper"><a href="#Goals-of-paper" class="headerlink" title="Goals of paper"></a>Goals of paper</h2><ul><li>å¼€å‘äº†ä¸¤ç§æ–°æ¨¡å‹ï¼Œå¹¶ä¿ç•™äº†å•è¯ä¹‹é—´çš„çº¿æ€§è§„å¾‹</li><li>è®¾è®¡äº†ä¸€ä¸ªæ–°çš„ç»¼åˆæµ‹è¯•é›†ï¼Œç”¨äºæµ‹é‡å¥æ³•å’Œè¯­ä¹‰è§„å¾‹</li><li>è®¨è®ºäº†è®­ç»ƒæ—¶é—´å’Œå‡†ç¡®æ€§å¦‚ä½•å–å†³äºå•è¯å‘é‡çš„ç»´åº¦å’Œè®­ç»ƒæ•°æ®çš„æ•°é‡</li></ul><h2 id="Model-Architectures"><a href="#Model-Architectures" class="headerlink" title="Model Architectures"></a>Model Architectures</h2><p>è®­ç»ƒå¤æ‚åº¦ï¼š</p><p><img src="http://image.nysdy.com/20190506155712909488421.png" alt="20190506155712909488421.png"></p><p>å…¶ä¸­ï¼ŒEæ˜¯è®­ç»ƒæ¬¡æ•°ï¼ŒTæ˜¯è®­ç»ƒé›†å•è¯æ•°é‡ï¼ŒQæ˜¯æ¨¡å‹ç»“æ„ã€‚</p><h3 id="Feedforward-Neural-Net-Language-Model-NNLM"><a href="#Feedforward-Neural-Net-Language-Model-NNLM" class="headerlink" title="Feedforward Neural Net Language Model (NNLM)"></a>Feedforward Neural Net Language Model (NNLM)</h3><p>å®ƒç”±è¾“å…¥ï¼Œæ˜ å°„ï¼Œéšè—å’Œè¾“å‡ºå±‚ç»„æˆã€‚é€šè¿‡ç®€åŒ–æ–¹æ³•ï¼ŒQ= N x D x H</p><h3 id="Recurrent-Neural-Net-Language-Model-RNNLM"><a href="#Recurrent-Neural-Net-Language-Model-RNNLM" class="headerlink" title="Recurrent Neural Net Language Model (RNNLM)"></a>Recurrent Neural Net Language Model (RNNLM)</h3><p>å…‹æœäº†æ¨¡å‹éœ€è¦å›ºå®šçš„ä¸Šä¸‹æ–‡é•¿åº¦çš„é—®é¢˜ï¼Œå¹¶ä¸”åªæœ‰è¾“å…¥ï¼Œéšè—å’Œè¾“å‡ºå±‚ã€‚</p><p>Q= H x H + H x Vï¼Œå…¶ä¸­H = Dï¼ˆå•è¯è¡¨ç¤ºï¼‰ï¼ŒH x V å¯ä»¥é€šè¿‡åˆ†çº§softmaxè¢«ç®€åŒ–ä¸ºH x log_2(V)ã€‚æ‰€ä»¥ä¸»è¦çš„å¤æ‚åº¦æ¥è‡ªäºH x Hã€‚</p><h3 id="Parallel-Training-of-Neural-Networks"><a href="#Parallel-Training-of-Neural-Networks" class="headerlink" title="Parallel Training of Neural Networks"></a>Parallel Training of Neural Networks</h3><p>æ¨¡å‹ä½¿ç”¨çš„DistBeliefæ¡†æ¶å…è®¸æˆ‘ä»¬å¹¶è¡Œè¿è¡ŒåŒä¸€æ¨¡å‹çš„å¤šä¸ªå‰¯æœ¬ï¼Œæ¯ä¸ªå‰¯æœ¬é€šè¿‡é›†ä¸­çš„æœåŠ¡å™¨åŒæ­¥å…¶æ¢¯åº¦æ›´æ–°ï¼Œè¯¥æœåŠ¡å™¨ä¿ç•™æ‰€æœ‰å‚æ•°</p><h2 id="New-Log-linear-Models"><a href="#New-Log-linear-Models" class="headerlink" title="New Log-linear Models"></a>New Log-linear Models</h2><p>å¤§å¤šæ•°å¤æ‚æ€§æ˜¯ç”±äºæ¨¡å‹ä¸­çš„éçº¿æ€§éšè—å±‚å¼•èµ·çš„ã€‚æ¨¡å‹ç»“æ„å¦‚ä¸‹ï¼š<img src="http://image.nysdy.com/20190506155713050638684.png" alt="20190506155713050638684.png"></p><h3 id="Continuous-Bag-of-Words-Model-CBOW"><a href="#Continuous-Bag-of-Words-Model-CBOW" class="headerlink" title="Continuous Bag-of-Words Model(CBOW)"></a>Continuous Bag-of-Words Model(CBOW)</h3><p>ç¬¬ä¸€ä¸ªæå‡ºçš„ä½“ç³»ç»“æ„ç±»ä¼¼äºå‰é¦ˆNNLMï¼Œå…¶ä¸­å»é™¤äº†éçº¿æ€§éšè—å±‚ï¼Œå¹¶ä¸”æ‰€æœ‰å•è¯ï¼ˆä¸ä»…ä»…æ˜¯æŠ•å½±çŸ©é˜µï¼‰å…±äº«æŠ•å½±å±‚ã€‚ å› æ­¤ï¼Œæ‰€æœ‰å•è¯éƒ½è¢«æŠ•å°„åˆ°ç›¸åŒçš„ä½ç½®ï¼ˆå®ƒä»¬çš„å‘é‡è¢«å¹³å‡ï¼‰ã€‚ å°†è¿™ä¸ªæ¶æ„ç§°ä¸ºè¯è¢‹æ¨¡å‹ï¼Œå› ä¸ºå†å²ä¸­çš„å•è¯é¡ºåºä¸ä¼šå½±å“æŠ•å½±ã€‚</p><p>æ¨¡å‹çš„å¤æ‚åº¦ï¼šQ = N Ã— D + D Ã— log_2(V )</p><h3 id="Continuous-Skip-gram-Model"><a href="#Continuous-Skip-gram-Model" class="headerlink" title="Continuous Skip-gram Model"></a>Continuous Skip-gram Model</h3><p>åŸºäºåŒä¸€å¥å­ä¸­çš„å¦ä¸€ä¸ªå•è¯æœ€å¤§åŒ–å•è¯çš„åˆ†ç±»ã€‚ æ›´å‡†ç¡®åœ°è¯´ï¼Œä½¿ç”¨æ¯ä¸ªå½“å‰å•è¯ä½œä¸ºå…·æœ‰è¿ç»­æŠ•å½±å±‚çš„å¯¹æ•°çº¿æ€§åˆ†ç±»å™¨çš„è¾“å…¥ï¼Œå¹¶é¢„æµ‹å½“å‰å•è¯ä¹‹å‰å’Œä¹‹åçš„ç‰¹å®šèŒƒå›´å†…çš„å•è¯ã€‚</p><p>æ¨¡å‹çš„å¤æ‚åº¦ï¼šQ = C Ã— (D + D Ã— log2(V ))ï¼Œå…¶ä¸­Cæ˜¯å•è¯çš„æœ€å¤§è·ç¦»ã€‚</p><h2 id="å®éªŒ"><a href="#å®éªŒ" class="headerlink" title="å®éªŒ"></a>å®éªŒ</h2><h3 id="ä»»åŠ¡æè¿°"><a href="#ä»»åŠ¡æè¿°" class="headerlink" title="ä»»åŠ¡æè¿°"></a>ä»»åŠ¡æè¿°</h3><p>ä¸ºäº†åº¦é‡è¯å‘é‡çš„è´¨é‡ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªå¤æ‚çš„æµ‹è¯•é›†ï¼Œå®ƒåŒ…æ‹¬äº†äº”ç§ç±»å‹çš„è¯­ä¹‰é—®é¢˜ã€‚ä¹ä¸ªç±»å‹çš„å¥æ³•é—®é¢˜ã€‚åŒ…æ‹¬æ¯ä¸ªç±»åˆ«çš„ä¸¤ä¸ªæ ·æœ¬é›†åœ¨ä¸Šè¡¨å±•ç¤ºï¼›æ€»ä¹‹ï¼Œå…±æ‹¥æœ‰8869ä¸ªè¯­ä¹‰é—®é¢˜å’Œ10675ä¸ªå¥æ³•é—®é¢˜</p><p>ä½œè€…é€šè¿‡ï¼šæœ€å¤§åŒ–ç²¾ç¡®åº¦ ï¼Œæ¨¡å‹ä½“ç³»ç»“æ„çš„æ¯”è¾ƒï¼Œæ¨¡å‹çš„å¤§è§„æ¨¡å¹¶è¡Œè®­ç»ƒæ¥è¯æ˜æå‡ºæ¨¡å‹çš„è¿é€Ÿåº¦å’Œç²¾ç¡®çš„ä¼˜åŠ¿ã€‚</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1301.3781.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;è®ºæ–‡ä¸‹è½½åœ°å€&lt;/a&gt;ï¼Œè¯¥ç¯‡è®ºæ–‡çš„å¤§ç¯‡å¹…éƒ½åœ¨è®¨è®ºå®éªŒç»“æœçš„åˆ†æï¼Œæ¨¡å‹çš„éƒ¨åˆ†æ¯”è¾ƒç®€å•ï¼Œæ²¡æœ‰è¯¦ç»†åˆ†æï¼Œæœ¬æ¥æ˜¯æƒ³è¯»ä¸€ä¸‹CBOWå’Œskip-gramçš„åŸå§‹è®ºæ–‡ï¼Œå‘ç°å¹¶æ²¡æœ‰æƒ³è±¡ä¸­çš„é‚£ä¹ˆå¤§çš„ç”¨å¤„ã€‚&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="è®ºæ–‡é˜…è¯»ç¬”è®°" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="word2vec" scheme="http://yoursite.com/tags/word2vec/"/>
    
  </entry>
  
  <entry>
    <title>Shared Embedding Based Neural Networks for Knowledge Graph Completioné˜…è¯»ç¬”è®°</title>
    <link href="http://yoursite.com/post/Shared%20Embedding%20Based%20Neural%20Networks%20for%20Knowledge%20Graph%20Completion/"/>
    <id>http://yoursite.com/post/Shared Embedding Based Neural Networks for Knowledge Graph Completion/</id>
    <published>2019-04-19T06:52:38.000Z</published>
    <updated>2019-04-19T08:09:35.184Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="http://delivery.acm.org/10.1145/3280000/3271704/p247-guan.pdf?ip=59.64.129.243&amp;id=3271704&amp;acc=ACTIVE%20SERVICE&amp;key=BF85BBA5741FDC6E%2E66A15327C2E204FC%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1555657159_db1582f1a6ea923a16011064e5cc7955" target="_blank" rel="noopener">åŸæ–‡ä¸‹è½½é“¾æ¥</a>ï¼ŒçŸ¥è¯†å›¾è°±è¡¥å…¨ï¼ˆKGCï¼ŒKnowledge Graph Completion)æ˜¯ä¸€ç§è‡ªåŠ¨å»ºç«‹å›¾è°±å†…éƒ¨çŸ¥è¯†å…³è”çš„å·¥ä½œã€‚ç›®æ ‡æ˜¯è¡¥å…¨çŸ¥è¯†å›¾è°±ä¸­ä¸‰å…ƒç»„çš„ç¼ºå¤±éƒ¨åˆ†ã€‚ä¸»è¦æ–¹æ³•ä¸ºåŸºäºå¼ é‡ï¼ˆæˆ–è€…çŸ©é˜µï¼‰å’ŒåŸºäºç¿»è¯‘ä¸¤ç±»ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºå…±äº«åµŒå…¥çš„ç¥ç»ç½‘ç»œçš„æ¨¡å‹ï¼ˆSENNï¼‰æ¥å¤„ç†KGCã€‚</p></blockquote><a id="more"></a><h2 id="Contribulation"><a href="#Contribulation" class="headerlink" title="Contribulation"></a>Contribulation</h2><ul><li>æå‡ºäº†SENNæ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ˜ç¡®åŒºåˆ†å¤´å®ä½“ã€å…³ç³»å’Œä¸ºå®ä½“é¢„æµ‹ä»»åŠ¡ï¼Œå¹¶æŠŠå®ƒä»¬æ•´åˆåˆ°ä¸€ä¸ªåŸºäºå…¨è¿æ¥ç¥ç»ç½‘ç»œæ¡†æ¶ä¸­ï¼Œè¯¥æ¡†æ¶å…±äº«çš„å®ä½“å’Œå…³ç³»åµŒå…¥ã€‚</li><li>SENNæå‡ºäº†ä¸€ä¸ªè‡ªé€‚åº”å…¨ä¸­æŸå¤±æœºåˆ¶ï¼Œè¯¥æ–¹æ³•å¯ä»¥å¾ˆå¥½çš„å¤„ç†å…·æœ‰ä¸åŒæ˜ å°„å±æ€§çš„ä¸‰å…ƒç»„ï¼Œå¹¶å¤„ç†ä¸åŒçš„é¢„æµ‹ä»»åŠ¡ã€‚</li><li>ç”±äºå…³ç³»é¢„æµ‹é€šå¸¸æ¯”å¤´å°¾å®ä½“é¢„æµ‹å…·æœ‰æ›´å¥½çš„æ€§èƒ½ï¼Œæˆ‘ä»¬æŠŠSENNåº”ç”¨åˆ°å¤´å°¾å®ä½“é¢„æµ‹ï¼Œä»è€Œå°†SENNæ‰©å±•åˆ°SENN+ã€‚</li></ul><h2 id="Related-works"><a href="#Related-works" class="headerlink" title="Related works"></a>Related works</h2><h3 id="Tensor-Matrix-Based-Methods"><a href="#Tensor-Matrix-Based-Methods" class="headerlink" title="Tensor/Matrix Based Methods"></a>Tensor/Matrix Based Methods</h3><p>RESCALæ˜¯ä¸€ä¸ªå…¸å‹çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŸºäºä¸‰å‘å¼ é‡å› å­åˆ†è§£çš„æ–¹æ³•ã€‚</p><p>ç›®æ ‡å‡½æ•°ä¸ºï¼š<img src="http://image.nysdy.com/20190419155565817094503.png" alt="20190419155565817094503.png"></p><p>$M_r$æ˜¯rçš„å…³ç³»çŸ©é˜µï¼Œå¤§å°ä¸ºk x kã€‚</p><p>ComlExæ˜¯æœ€è¿‘æå‡ºçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŸºäºçŸ©é˜µåˆ†è§£ï¼Œå¹¶ä¸”å®ƒä½¿ç”¨å¤æ•°å€¼æ¥å®šä¹‰å®ä½“å’Œå…³ç³»çš„åµŒå…¥ã€‚</p><p>ç›®æ ‡å‡½æ•°ä¸ºï¼š<img src="http://image.nysdy.com/20190419155565837140789.png" alt="20190419155565837140789.png"></p><p>Re(x)è¿”å›xçš„å®éƒ¨ã€‚</p><h3 id="Translation-Based-Methods"><a href="#Translation-Based-Methods" class="headerlink" title="Translation Based Methods"></a>Translation Based Methods</h3><p>ä»£è¡¨æ¨¡å‹ä¸ºç»å…¸çš„TransEæ¨¡å‹ï¼ˆè¿™é‡Œä¸å†èµ˜è¿°ï¼‰</p><h3 id="Translation-Based-Methods-1"><a href="#Translation-Based-Methods-1" class="headerlink" title="Translation Based Methods"></a>Translation Based Methods</h3><p>ER-MLPä½¿ç”¨å¤šå±‚æ„ŸçŸ¥å™¨æ¥æ•è·å¤´å®ä½“ï¼Œå…³ç³»å’Œå°¾å®ä½“ä¹‹é—´çš„éšå¼äº¤äº’ã€‚</p><p>ç›®æ ‡å‡½æ•°ä¸ºï¼š<img src="http://image.nysdy.com/2019041915556586361053.png" alt="2019041915556586361053.png"></p><p>ProjEä½¿ç”¨å…·æœ‰ç»„åˆå±‚å’ŒæŠ•å½±å±‚çš„ç¥ç»ç½‘ç»œæ¥å¯¹å¤´å°¾å®ä½“é¢„æµ‹å»ºæ¨¡ã€‚</p><h2 id="THE-SENN-METHOD"><a href="#THE-SENN-METHOD" class="headerlink" title="THE SENN METHOD"></a>THE SENN METHOD</h2><p>æ¨¡å‹ç»“æ„å¦‚å›¾æ‰€ç¤ºï¼š<img src="http://image.nysdy.com/20190419155565880062506.png" alt="20190419155565880062506.png"></p><p>ä½œè€…å°†æ¡†æ¶åˆ’åˆ†ä¸ºä»¥ä¸‹å››ä¸ªéƒ¨åˆ†ï¼š</p><ol><li>ä¸‰å…ƒç»„çš„æ‰¹é‡é¢„å¤„ç†</li><li>çŸ¥è¯†å›¾è°±çš„Shared embeddingsè¡¨ç¤ºå­¦ä¹ </li><li>ç‹¬ç«‹çš„å¤´å°¾å®ä½“åŠå…³ç³»é¢„æµ‹å­æ¨¡å‹è®­ç»ƒä¸èåˆ</li><li>è”åˆæŸå¤±å‡½æ•°æ„æˆ</li></ol><p>æ•´ä¸ªKGCçš„æµç¨‹å¯ä»¥æè¿°å¦‚ä¸‹ï¼š</p><ol><li>å°†è®­ç»ƒæ•°æ®ä¸­çš„å®Œæ•´ä¸‰å…ƒç»„ï¼ˆçŸ¥è¯†å›¾è°±ï¼‰åˆ’åˆ†æ‰¹é‡åä½œä¸ºæ¨¡å‹çš„è¾“å…¥</li><li>å¯¹äºè¾“å…¥çš„ä¸‰å…ƒç»„ï¼Œåˆ†åˆ«è®­ç»ƒå¾—åˆ°å®ä½“ï¼ˆåŒ…æ‹¬å¤´å°¾å®ä½“ï¼‰åµŒå…¥çŸ©é˜µä¸å…³ç³»åµŒå…¥çŸ©é˜µï¼ˆembeddingsï¼‰</li><li>å°†å¤´å°¾å®ä½“åŠå…³ç³»embeddingsåˆ†åˆ«è¾“å…¥åˆ°ä¸‰ä¸ªé¢„æµ‹æ¨¡å‹ä¸­ï¼ˆå¤´å®ä½“é¢„æµ‹ï¼ˆ?, r, tï¼‰ï¼Œå…³ç³»é¢„æµ‹(h, ?, t)ï¼Œå°¾å®ä½“é¢„æµ‹(h, r, ?)ï¼‰</li></ol><h3 id="The-Three-Substructures"><a href="#The-Three-Substructures" class="headerlink" title="The Three Substructures"></a>The Three Substructures</h3><p>é¢„æµ‹å­æ¨¡å‹å…·æœ‰ç›¸ä¼¼çš„ç»“æ„å¦‚ä¸‹å›¾ï¼Œæ¨¡å‹è¾“å…¥å…³ç³»å‘é‡ä¸å®ä½“å‘é‡åï¼Œè¿›å…¥nå±‚å…¨è¿æ¥å±‚ï¼Œå¾—åˆ°é¢„æµ‹å‘é‡ï¼Œå†ç»è¿‡ä¸€ä¸ªsigmoidï¼ˆæˆ–è€…softmaxï¼‰å±‚ï¼Œè¾“å‡ºé¢„æµ‹æ ‡ç­¾å‘é‡ã€‚<img src="http://image.nysdy.com/20190419155565925349707.png" alt="20190419155565925349707.png"></p><p>å¤´å®ä½“é¢„æµ‹ç›®æ ‡å‡½æ•°ï¼š<img src="http://image.nysdy.com/20190419155565929066802.png" alt="20190419155565929066802.png"></p><p>f(x)= max(0,x).</p><p>é¢„æµ‹æ ‡ç­¾ï¼š<img src="http://image.nysdy.com/20190419155565936981620.png" alt="20190419155565936981620.png"></p><p>å…¶å®ƒä¸¤ç§ä¸æ­¤å¤´å®ä½“ç±»ä¼¼ã€‚</p><h3 id="Model-Training"><a href="#Model-Training" class="headerlink" title="Model Training"></a>Model Training</h3><h4 id="The-General-Loss-Function"><a href="#The-General-Loss-Function" class="headerlink" title="The General Loss Function"></a>The General Loss Function</h4><p>æ¨¡å‹ç›®æ ‡æ ‡ç­¾å‘é‡è¡¨ç¤ºä¸ºï¼š<img src="http://image.nysdy.com/20190419155565949172586.png" alt="20190419155565949172586.png"></p><p>$I_h$æ˜¯åœ¨è®­ç»ƒé›†ä¸­ç»™å®šrå’Œtçš„æ‰€æœ‰æœ‰æ•ˆå¤´å®ä½“é›†ã€‚</p><p>ä¸‰è€…çš„å¹³æ»‘å‘é‡è¡¨ç¤ºä¸ºï¼š<img src="http://image.nysdy.com/20190419155565967448577.png" alt="20190419155565967448577.png"></p><p>ä¸‰ä¸ªé¢„æµ‹ä»»åŠ¡çš„æŸå¤±å‡½æ•°ä¸ºï¼š<img src="http://image.nysdy.com/20190419155565972110097.png" alt="20190419155565972110097.png"></p><p>æ€»æŸå¤±å‡½æ•°ä¸ºï¼š<img src="http://image.nysdy.com/20190419155565974814392.png" alt="20190419155565974814392.png"></p><h4 id="The-Adaptively-Weighted-Loss-Mechanism"><a href="#The-Adaptively-Weighted-Loss-Mechanism" class="headerlink" title="The Adaptively Weighted Loss Mechanism."></a>The Adaptively Weighted Loss Mechanism.</h4><p>è¯¥æ–¹æ³•çš„åŠ¨æœºï¼š</p><ul><li>åœ¨çŸ¥è¯†å›¾è°±ä¸­çš„ä¸‰å…ƒç»„æœ‰4ç§ç±»å‹ï¼š1-TO-1, 1-TO-M, M-TO-1 and M-TO-Mã€‚æ‰€ä»¥é¢„æµ‹åœ¨è®­ç»ƒé›†ä¸­å…·æœ‰çš„æœ‰æ•ˆå®ä½“/å…³ç³»è¶Šå¤šï¼Œå®ƒå°±è¶Šä¸ç¡®å®šã€‚æ‰€ä»¥ä½œè€…å°†å¯¹åº”äºå¤´éƒ¨å®ä½“é¢„æµ‹ï¼Œå…³ç³»é¢„æµ‹å’Œå°¾éƒ¨å®ä½“é¢„æµ‹çš„æŸå¤±çš„æƒé‡ä¸æœ‰æ•ˆå®ä½“çš„æ•°é‡ç›¸å…³è”ã€‚</li><li>å› ä¸ºå…³ç³»é¢„æµ‹æ¯”å®ä½“é¢„æµ‹æ›´åŠ å®¹æ˜“ã€‚æ‰€ä»¥ä½œè€…åŠ å¤§å¯¹å¤´å°¾å®ä½“çš„é”™è¯¯é¢„æµ‹çš„æƒ©ç½šã€‚</li></ul><p>æ‰€ä»¥ä½œè€…å¾—åˆ°æ–°çš„æŸå¤±å‡½æ•°ï¼š<img src="http://image.nysdy.com/20190419155566013038361.png" alt="20190419155566013038361.png"></p><p>æ€»æŸå¤±å‡½æ•°å˜ä¸ºï¼š<img src="http://image.nysdy.com/20190419155566016395908.png" alt="20190419155566016395908.png"></p><h2 id="THE-SENN-METHOD-1"><a href="#THE-SENN-METHOD-1" class="headerlink" title="THE SENN+METHOD"></a>THE SENN+METHOD</h2><p>ä½œè€…ç›¸ä¿¡å¯ä»¥è¿›ä¸€æ­¥åˆ©ç”¨å…³ç³»é¢„æµ‹çš„ç›¸å½“å¥½çš„æ€§èƒ½æ¥è¾…åŠ©æµ‹è¯•è¿‡ç¨‹ä¸­çš„å¤´éƒ¨å’Œå°¾éƒ¨å®ä½“é¢„æµ‹ã€‚</p><p>ç»™å®šå¤´éƒ¨é¢„æµ‹ä»»åŠ¡ï¼ˆï¼Ÿï¼Œrï¼Œtï¼‰å¹¶å‡è®¾hæ˜¯æœ‰æ•ˆçš„å¤´éƒ¨å®ä½“ã€‚ å¦‚æœæˆ‘ä»¬é‡‡ç”¨SENNæ–¹æ³•æ¥é¢„æµ‹hå’Œtä¹‹é—´çš„å…³ç³»ï¼Œå³æ‰§è¡Œå…³ç³»é¢„æµ‹ä»»åŠ¡ï¼ˆhï¼Œï¼Ÿï¼Œtï¼‰ï¼Œåˆ™å…³ç³»ræœ€æœ‰å¯èƒ½å…·æœ‰ é¢„æµ‹æ ‡ç­¾é«˜äºå…¶ä»–å…³ç³»ï¼Œå› æ­¤åº”æ’åé«˜äºå…¶ä»–å…³ç³»ã€‚</p><p><img src="http://image.nysdy.com/20190419155566073220975.png" alt="20190419155566073220975.png"></p><p>å…¶ä¸­Valueï¼ˆxï¼Œrï¼‰è¿”å›å¯¹åº”äºå…³ç³»rçš„å‘é‡xçš„æ¡ç›®; Rankï¼ˆxï¼Œrï¼‰ä»¥é™åºè¿”å›å¯¹åº”äºå…³ç³»rçš„å‘é‡xçš„æ¡ç›®çš„ç­‰çº§ã€‚</p><p>æœ€åSENN+ç§é¢„æµ‹æ ‡ç­¾ä¸ºï¼š<img src="http://image.nysdy.com/20190419155566089424123.png" alt="20190419155566089424123.png"></p><p>å…¶ä¸­<img src="http://image.nysdy.com/20190419155566090898589.png" alt="20190419155566090898589.png"></p><h2 id="EXPERIMENTS"><a href="#EXPERIMENTS" class="headerlink" title="EXPERIMENTS"></a>EXPERIMENTS</h2><h3 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h3><p><img src="http://image.nysdy.com/20190419155566095994824.png" alt="20190419155566095994824.png"></p><h3 id="Entity-Prediction"><a href="#Entity-Prediction" class="headerlink" title="Entity Prediction"></a>Entity Prediction</h3><p><img src="http://image.nysdy.com/20190419155566101939979.png" alt="20190419155566101939979.png"></p><p><img src="http://image.nysdy.com/20190419155566104394441.png" alt="20190419155566104394441.png"></p><h3 id="Relation-Prediction"><a href="#Relation-Prediction" class="headerlink" title="Relation Prediction"></a>Relation Prediction</h3><p><img src="http://image.nysdy.com/20190419155566106385514.png" alt="20190419155566106385514.png"></p><p>è®ºæ–‡è¿˜è¿›è¡Œäº†å…±äº«åµŒå…¥å’Œè‡ªé€‚åº”æƒé‡æŸå¤±æœºåˆ¶æœ‰æ•ˆæ€§çš„éªŒè¯ã€‚</p><h2 id="å‚è€ƒé“¾æ¥"><a href="#å‚è€ƒé“¾æ¥" class="headerlink" title="å‚è€ƒé“¾æ¥"></a>å‚è€ƒé“¾æ¥</h2><ul><li><a href="http://blog.openkg.cn/%E8%AE%BA%E6%96%87%E6%B5%85%E5%B0%9D-%E9%9D%A2%E5%90%91%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E8%A1%A5%E5%85%A8%E7%9A%84%E5%85%B1%E4%BA%AB%E5%B5%8C%E5%85%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">http://blog.openkg.cn/%E8%AE%BA%E6%96%87%E6%B5%85%E5%B0%9D-%E9%9D%A2%E5%90%91%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E8%A1%A5%E5%85%A8%E7%9A%84%E5%85%B1%E4%BA%AB%E5%B5%8C%E5%85%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;http://delivery.acm.org/10.1145/3280000/3271704/p247-guan.pdf?ip=59.64.129.243&amp;amp;id=3271704&amp;amp;acc=ACTIVE%20SERVICE&amp;amp;key=BF85BBA5741FDC6E%2E66A15327C2E204FC%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;amp;__acm__=1555657159_db1582f1a6ea923a16011064e5cc7955&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;åŸæ–‡ä¸‹è½½é“¾æ¥&lt;/a&gt;ï¼ŒçŸ¥è¯†å›¾è°±è¡¥å…¨ï¼ˆKGCï¼ŒKnowledge Graph Completion)æ˜¯ä¸€ç§è‡ªåŠ¨å»ºç«‹å›¾è°±å†…éƒ¨çŸ¥è¯†å…³è”çš„å·¥ä½œã€‚ç›®æ ‡æ˜¯è¡¥å…¨çŸ¥è¯†å›¾è°±ä¸­ä¸‰å…ƒç»„çš„ç¼ºå¤±éƒ¨åˆ†ã€‚ä¸»è¦æ–¹æ³•ä¸ºåŸºäºå¼ é‡ï¼ˆæˆ–è€…çŸ©é˜µï¼‰å’ŒåŸºäºç¿»è¯‘ä¸¤ç±»ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºå…±äº«åµŒå…¥çš„ç¥ç»ç½‘ç»œçš„æ¨¡å‹ï¼ˆSENNï¼‰æ¥å¤„ç†KGCã€‚&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="è®ºæ–‡é˜…è¯»ç¬”è®°" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="çŸ¥è¯†å›¾è°±" scheme="http://yoursite.com/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    
      <category term="ç¥ç»ç½‘ç»œ" scheme="http://yoursite.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="çŸ¥è¯†å›¾è°±è¡¥å…¨" scheme="http://yoursite.com/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E8%A1%A5%E5%85%A8/"/>
    
  </entry>
  
</feed>
