<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>NYSDY</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-01-05T03:21:52.999Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>NYSDY</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>《Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation》阅读笔记</title>
    <link href="http://yoursite.com/post/Learning_Phrase_Representations_using_RNN_Encoder%E2%80%93Decoder_for_Statistical_Machine_Translation/"/>
    <id>http://yoursite.com/post/Learning_Phrase_Representations_using_RNN_Encoder–Decoder_for_Statistical_Machine_Translation/</id>
    <published>2019-01-05T02:23:27.000Z</published>
    <updated>2019-01-05T03:21:52.999Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://www.aclweb.org/anthology/D14-1179" target="_blank" rel="noopener">原文链接</a>。该论文是Sequence to Sequence学习的最早原型，论文中提出一种崭新的RNN Encoder-Decoder算法，虽然文章属于比较旧的文章，但作为seq2seq的基础原型，还是需要阅读了解一下的。文章写的比较详细，各部分细节都有讲解。</p></blockquote><h2 id="文章的主要结构"><a href="#文章的主要结构" class="headerlink" title="文章的主要结构"></a>文章的主要结构</h2><p><img src="https://i.loli.net/2019/01/05/5c3019fd55653.jpg" alt=""></p><h2 id="contribution"><a href="#contribution" class="headerlink" title="contribution"></a>contribution</h2><ul><li><p>a novel RNN Encoder–Decoder</p><p>能够处理变长序列</p></li><li><p>a novel hidden unit</p><ul><li>reset gate</li><li>update gate</li></ul></li></ul><h2 id="RNN-Encoder–Decoder"><a href="#RNN-Encoder–Decoder" class="headerlink" title="RNN Encoder–Decoder"></a>RNN Encoder–Decoder</h2><p>模型结构图如下：</p><p><img src="https://i.loli.net/2019/01/05/5c301b9717fd8.jpg" alt=""></p><p>文中作者对齐进行总体概述为：</p><blockquote><p>From a probabilistic perspective, this new model is a general method to learn the conditional distribution over a variable-length sequence conditioned on yet another variable-length sequence</p></blockquote><p>从概率的角度来看，这个新模型是学习在另一个可变长度序列条件下的可变长度序列上的条件分布的一般方法</p><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>这部分是一个RNN单元。每个时间步，我们向Encoder中输入一个字/词（一般为向量形式），直到我们输入这个句子的最后一个字/词$X_T$，然后输入整个句子的语义向量c。由于RNN的特带你就是把前面每一步的输入信息都考虑进来，所以理论上这个c就包含了整个句子的所有信息。我们可以把当成这个句子的一个语义表示。</p><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>Decoder是另一个RNN，其被训练出来以通过预测隐藏状态$h_t$的下一个符号$y_t$来生成输出序列。计算公式如下</p><p>$$h_t = f(h_{t-1},y_{t-1},c)$$</p><p>下一个序列的计算公式如下：</p><p>$$P(y_t|y_{t-1},y_{t-2},\dots,y_1,c)=g(h_t,y_{t-1},c)$$</p><h2 id="Hidden-Unit"><a href="#Hidden-Unit" class="headerlink" title="Hidden Unit"></a>Hidden Unit</h2><p>该部分是对各部分具体的公式讲解，实际是GRU的具体公式算法，不再此详细叙述了。</p><h3 id="reset-gate"><a href="#reset-gate" class="headerlink" title="reset gate"></a>reset gate</h3><blockquote><p>In this formulation, when the reset gate is close to 0, the hidden state is forced to ignore the pre- vious hidden state and reset with the current input only. This effectively allows the hidden state to drop any information that is found to be irrelevant later in the future, thus, allowing a more compact representation.</p></blockquote><p>这段原文主要讲解了复位门的作用：有效地允许隐藏状态丢弃在将来稍后发现不相关的任何信息，从而允许更紧凑的表示。</p><p><strong>当捕获短期依赖时，复位门活跃</strong></p><h3 id="update-gate"><a href="#update-gate" class="headerlink" title="update gate"></a>update gate</h3><blockquote><p>the update gate controls how much information from the previous hidden state will carry over to the current hidden state.</p></blockquote><p>更新门控制来自先前隐藏状态的多少信息将转移到当前隐藏状态。</p><p><strong>当捕获长期依赖时，更新门活跃</strong></p><h2 id="Statistical-Machine-Translation-SMT"><a href="#Statistical-Machine-Translation-SMT" class="headerlink" title="Statistical Machine Translation(SMT)"></a>Statistical Machine Translation(SMT)</h2><p><img src="https://i.loli.net/2019/01/05/5c30217d42b8e.jpg" alt=""></p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>这部分作者主要做了量化分析和性质分析，主要就是说他的模型怎么厉害。。。（没有具体的数值指标，翻译的还不是中英翻译，想看的话可以去看一下，就不贴实验结果了）。</p><h2 id="future"><a href="#future" class="headerlink" title="future"></a>future</h2><p>这里作者提出了可以用decoder生成的目标短语来替换原句中短语的思路，如果没记错的话，这个想法好像对后面的机器翻译有很大的指导作用。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://www.aclweb.org/anthology/D14-1179&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;原文链接&lt;/a&gt;。该论文是Sequence to Sequence学习的最早原型，
      
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="RNN" scheme="http://yoursite.com/tags/RNN/"/>
    
      <category term="seq2seq" scheme="http://yoursite.com/tags/seq2seq/"/>
    
  </entry>
  
  <entry>
    <title>《Differentiating Concepts and Instances for Knowledge Graph Embedding》阅读笔记</title>
    <link href="http://yoursite.com/post/read_Differentiating_Concepts_and_Instances_for_Knowledge_Graph_Embedding/"/>
    <id>http://yoursite.com/post/read_Differentiating_Concepts_and_Instances_for_Knowledge_Graph_Embedding/</id>
    <published>2018-12-24T07:15:30.000Z</published>
    <updated>2018-12-24T07:18:49.884Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>这篇文章最大的亮点就是把concept映射为一个球面，然后把instance映射为一个向量，通过这种空间关系来进行embedding。如果instance和concept满足InstanceOf的关系，则instance应该在球内；如果两个concept满足SubClassOf的关系，则一个球会在另一个球面内。</p></blockquote><h3 id="concept"><a href="#concept" class="headerlink" title="concept"></a>concept</h3><p>A concept is a fundamental category of existence (Rosch, 1973) and can be reified by all of its actual or potential instances.Concepts, which <strong>represent a group of different instances sharing common properties</strong>, are essential information in knowledge representation. </p><p><img src="https://i.loli.net/2018/12/16/5c1634f6c26c3.jpg" alt=""></p><h2 id="drawback-of-the-previous-method"><a href="#drawback-of-the-previous-method" class="headerlink" title="drawback of the previous method"></a>drawback of the previous method</h2><p> ignore to distinguish between concepts and instances will lead to two drawbacks:</p><ul><li><p><strong>Insufficient concept representation</strong>：</p><p>cannot explicitly represent the difference between concepts and instances</p></li><li><p><strong>Lack transitivity of both isA relations</strong>:</p><p><em>instanceOf</em> and <em>subClassOf</em> (generally known as isA)isA relations exhibit transitivity</p></li></ul><h2 id="contributions"><a href="#contributions" class="headerlink" title="contributions"></a>contributions</h2><ul><li><strong>the first to propose</strong> and formalize the problem of knowledge graph embedding which <strong>differentiates</strong> between concepts and instances</li><li>a novel knowledge embedding method named <strong>TransC</strong></li><li><strong>state-of-the-art</strong> on link prediction and triple classification</li></ul><h2 id="Translation-based-Models"><a href="#Translation-based-Models" class="headerlink" title="Translation-based Models"></a>Translation-based Models</h2><h3 id="TransE"><a href="#TransE" class="headerlink" title="TransE"></a>TransE</h3><ul><li>triple (h, r, t) should satisfy <strong>h + r ≈ t</strong></li><li>loss function:$f_r(h,t) = ||h + r - t||^2_2$</li><li>suitable for 1-to-1 relations</li></ul><h3 id="TransH"><a href="#TransH" class="headerlink" title="TransH"></a>TransH</h3><ul><li>It regards a relation vector r as a translation on a hyperplane with $w_r$ as the normal vector. </li><li>loss function:$f_r(h,t) = ||h_{\bot} + r - t_{\bot}||^2_2$，其中$h_{\bot}=h-w^{\top}_r h w_r$，$t_{\bot}=t-w^{\top}_r t w_r$</li><li>suitable for 1-to-N, N-to-1, and N-to-N relations</li></ul><h3 id="TransR-CTransR"><a href="#TransR-CTransR" class="headerlink" title="TransR/CTransR"></a>TransR/CTransR</h3><ul><li>addresses the issue in TransE and TransH that some entities are <strong>similar in the entity space</strong> but comparably <strong>different in other specific aspects</strong>.</li><li>loss function:$f_r(h,t) = ||M_rh +r -M_rt||^2_2$，$M_r$ for each relation r</li></ul><h3 id="TransD"><a href="#TransD" class="headerlink" title="TransD"></a>TransD</h3><ul><li>considers the different types of entities and relations at the same time</li><li>loss function:$f_r(h,t) = ||h_{\bot} + r - t_{\bot}||^2_2$，$h_{\bot} = M_{rh}h$和$t_{\bot} = M_{rt}t$，$M_{r,e}$ for each relation-entity pair (r, e)</li></ul><h2 id="Bilinear-Models"><a href="#Bilinear-Models" class="headerlink" title="Bilinear Models"></a>Bilinear Models</h2><h3 id="RESCAL"><a href="#RESCAL" class="headerlink" title="RESCAL"></a>RESCAL</h3><ul><li>the <strong>first</strong> bilinear model</li><li>It associates each entity with a vector to capture its <strong>latent semantics</strong>. Each relation is represented as a <strong>matrix</strong> which models pairwise interactions between latent factors.</li></ul><h2 id="External-Information-Learning-Models"><a href="#External-Information-Learning-Models" class="headerlink" title="External Information Learning Models"></a>External Information Learning Models</h2><ul><li>textual information</li><li>entity descriptions</li></ul><h2 id="TranC"><a href="#TranC" class="headerlink" title="TranC"></a>TranC</h2><p>Specifically, TransC encodes each concept in knowledge graph as a sphere and each instance as a vector in the same semantic space. </p><h3 id="InstanceOf-Triple-Representation"><a href="#InstanceOf-Triple-Representation" class="headerlink" title="InstanceOf Triple Representation"></a>InstanceOf Triple Representation</h3><h3 id="SubClassOf-Triple-Representation"><a href="#SubClassOf-Triple-Representation" class="headerlink" title="SubClassOf Triple Representation"></a>SubClassOf Triple Representation</h3><p><img src="/Users/dy/Library/Application%20Support/typora-user-images/image-20181216173058465.png" alt="image-20181216173058465"></p><h3 id="SubClassOf-Triple-Representation-1"><a href="#SubClassOf-Triple-Representation-1" class="headerlink" title="SubClassOf Triple Representation"></a>SubClassOf Triple Representation</h3><h2 id="train-model"><a href="#train-model" class="headerlink" title="train model"></a>train model</h2><h3 id="margin-based-loss详解"><a href="#margin-based-loss详解" class="headerlink" title="margin based loss详解"></a><a href="https://zhuanlan.zhihu.com/p/27748177" target="_blank" rel="noopener">margin based loss</a>详解</h3><h3 id="unit-and-bern"><a href="#unit-and-bern" class="headerlink" title="unit and bern"></a><a href="https://pdfs.semanticscholar.org/2a3f/862199883ceff5e3c74126f0c80770653e05.pdf" target="_blank" rel="noopener">unit and bern</a></h3><h2 id="the-following-research-directions"><a href="#the-following-research-directions" class="headerlink" title="the following research directions"></a>the following research directions</h2><ul><li><p>find a more expressive model instead of spheres to represent concepts</p></li><li><p>A concept may have different meanings in different triples. </p><p>use several typical vectors of instances as a concept’s centers to represent different meanings of a concept. </p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;这篇文章最大的亮点就是把concept映射为一个球面，然后把instance映射为一个向量，通过这种空间关系来进行embedding。如果instance和concept满足InstanceOf的关系，则instance应该在球内；如果两个conc
      
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="知识图谱" scheme="http://yoursite.com/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    
  </entry>
  
  <entry>
    <title>A Discourse-Level Named Entity Recognition and Relation Extraction Dataset for Chinese Literature Text 阅读笔记</title>
    <link href="http://yoursite.com/post/READ_A_Discourse-Level_Named_Entity_Recognition_and_Relation_Extraction_Dataset_for_Chinese_Literature_Text/"/>
    <id>http://yoursite.com/post/READ_A_Discourse-Level_Named_Entity_Recognition_and_Relation_Extraction_Dataset_for_Chinese_Literature_Text/</id>
    <published>2018-12-10T03:07:23.000Z</published>
    <updated>2018-12-10T03:19:22.276Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>该论文最主要的贡献就是这个数据，<a href="https://github.com/lancopku/Chinese-Literature-NER-RE-Dataset" target="_blank" rel="noopener">数据集地址</a>。论文中提到的标标签过程也是一个创新点，运用了启发式和机器辅助标标签，这样可以提高准确度并减少标注人员工作。</p></blockquote><a id="more"></a><h2 id="contribution"><a href="#contribution" class="headerlink" title="contribution"></a>contribution</h2><ul><li>provide a new dataset for joint learning of NER and RE for <strong>Chinese literature text</strong></li><li>the proposed dataset is based on the <strong>discourse level</strong> which provides additional context information</li><li>introduce some widely used models to conduct experiments </li></ul><h2 id="tagging-process"><a href="#tagging-process" class="headerlink" title="tagging process"></a>tagging process</h2><p><strong>two methods:</strong>one is a <strong>heuristic tagging</strong> method and another is a <strong>machine auxiliary tagging</strong> method. </p><h3 id="Step-1-First-Tagging-Process"><a href="#Step-1-First-Tagging-Process" class="headerlink" title="Step 1: First Tagging Process"></a>Step 1: First Tagging Process</h3><p>find a problem of data inconsistency.</p><h3 id="Step-2-Heuristic-Tagging-Based-on-Generic-disambiguating-Rules"><a href="#Step-2-Heuristic-Tagging-Based-on-Generic-disambiguating-Rules" class="headerlink" title="Step 2: Heuristic Tagging Based on Generic disambiguating Rules"></a>Step 2: Heuristic Tagging Based on Generic disambiguating Rules</h3><ul><li>For example, remove all adjective words and only tag “entity header” .</li><li><strong>re-annotate</strong> all articles and <strong>correct</strong> all inconsistency entities and relations based on the heuristic rules.</li></ul><h3 id="Step-3-Machine-Auxiliary-Tagging"><a href="#Step-3-Machine-Auxiliary-Tagging" class="headerlink" title="Step 3: Machine Auxiliary Tagging"></a>Step 3: Machine Auxiliary Tagging</h3><ul><li>The core idea is to train a model to <strong>learn annotation guidelines</strong> on the subset of the corpus and produce predicted tags on the rest data. </li><li>CRF</li></ul><h2 id="tagging-set"><a href="#tagging-set" class="headerlink" title="tagging set"></a><img src="https://i.loli.net/2018/12/10/5c0dd578c91a0.jpg" alt="">tagging set</h2><p><img src="https://i.loli.net/2018/12/10/5c0dd6230f1c3.jpg" alt=""></p><h2 id="Annotation-Format"><a href="#Annotation-Format" class="headerlink" title="Annotation Format"></a>Annotation Format</h2><h3 id="Entity"><a href="#Entity" class="headerlink" title="Entity"></a>Entity</h3><p>Each entity is identified by T tag, which takes several attributes.</p><ul><li>Id: a unique number identifying the entity within the document. It starts at 0, and is incremented every time a new entity is identified within the same document.</li><li>Type: one of the entity tags.</li><li>Begin Index: the begin index of an entity. It starts at 0, and is incremented every character.</li><li>End Index: the end index of an entity. It starts at 0, and is incremented every character.</li><li>Value: words being referred to an identifiable object.</li></ul><h3 id="Relation"><a href="#Relation" class="headerlink" title="Relation"></a>Relation</h3><p>Each relation is identified by R tag, which can take several attributes:</p><ul><li>Id: a unique number identifying the relation within the document. It starts at 0, and is incremented every time a new relation is identified within the same document.</li><li>Arg1 and Arg2: two entities associated with a relation.</li><li>Type: one of the relation tags.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;该论文最主要的贡献就是这个数据，&lt;a href=&quot;https://github.com/lancopku/Chinese-Literature-NER-RE-Dataset&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;数据集地址&lt;/a&gt;。论文中提到的标标签过程也是一个创新点，运用了启发式和机器辅助标标签，这样可以提高准确度并减少标注人员工作。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="NER" scheme="http://yoursite.com/tags/NER/"/>
    
      <category term="RE" scheme="http://yoursite.com/tags/RE/"/>
    
  </entry>
  
  <entry>
    <title>pandas的数据类型操作</title>
    <link href="http://yoursite.com/post/Pandas_data_type_manipulation/"/>
    <id>http://yoursite.com/post/Pandas_data_type_manipulation/</id>
    <published>2018-12-01T05:11:30.000Z</published>
    <updated>2018-12-01T05:18:49.278Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>在<a href="https://juejin.im/post/5acc36e66fb9a028d043c2a5" target="_blank" rel="noopener">原文链接</a>中摘抄出部分信息作为记录形成本文。</p></blockquote><a id="more"></a><h3 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h3><table><thead><tr><th>Pandas dtype</th><th>Python 类型</th><th>NumPy 类型</th><th>用途</th></tr></thead><tbody><tr><td>object</td><td>str</td><td>string_, unicode_</td><td>文本</td></tr><tr><td>int64</td><td>int</td><td>int_, int8, int16, int32, int64, uint8, uint16, uint32, uint64</td><td>整数</td></tr><tr><td>float64</td><td>float</td><td>float_, float16, float32, float64</td><td>浮点数</td></tr><tr><td>bool</td><td>bool</td><td>bool_</td><td>布尔值</td></tr><tr><td>datetime64</td><td>NA</td><td>NA</td><td>日期时间</td></tr><tr><td>timedelta[ns]</td><td>NA</td><td>NA</td><td>时间差</td></tr><tr><td>category</td><td>NA</td><td>NA</td><td>有限长度的文本值列表</td></tr></tbody></table><h3 id="数据类型操作"><a href="#数据类型操作" class="headerlink" title="数据类型操作"></a>数据类型操作</h3><ul><li><p>使用<code>df.dtypes</code>可以显示数据所有列的类型</p></li><li><p><code>df.info（）</code> 函数可以显示更有用的信息</p></li></ul><h2 id="使用-astype-函数"><a href="#使用-astype-函数" class="headerlink" title="使用 astype() 函数"></a>使用 <code>astype()</code> 函数</h2><h3 id="使用条件"><a href="#使用条件" class="headerlink" title="使用条件"></a>使用条件</h3><ul><li>数据是干净的，可以简单地解释为一个数字</li><li>你想要将一个数值转换为一个字符串对象</li></ul><p>如果数据具有非数字字符或它们间不同质（homogeneous），那么 <code>astype()</code> 并不是类型转换的好选择。你需要进行额外的变换才能完成正确的类型转换。</p><h3 id="使用方式"><a href="#使用方式" class="headerlink" title="使用方式"></a>使用方式</h3><p>为了真正修改原始 dataframe 中数据类型，记得把 <code>astype()</code> 函数的返回值重新赋值给 dataframe，因为 <code>astype()</code> 仅返回数据的副本而不原地修改。</p><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ul><li><a href="https://juejin.im/post/5acc36e66fb9a028d043c2a5" target="_blank" rel="noopener">https://juejin.im/post/5acc36e66fb9a028d043c2a5</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;在&lt;a href=&quot;https://juejin.im/post/5acc36e66fb9a028d043c2a5&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;原文链接&lt;/a&gt;中摘抄出部分信息作为记录形成本文。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="pandas" scheme="http://yoursite.com/categories/pandas/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="pandas" scheme="http://yoursite.com/tags/pandas/"/>
    
      <category term="数据分析" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>python爬取中文网页时中文字符变英文的解决方法</title>
    <link href="http://yoursite.com/post/solution_of_python_for_Chinese_characters_to_become_English_when_crawling_Chinese_web_pages/"/>
    <id>http://yoursite.com/post/solution_of_python_for_Chinese_characters_to_become_English_when_crawling_Chinese_web_pages/</id>
    <published>2018-10-25T07:01:56.000Z</published>
    <updated>2018-10-25T07:03:47.014Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>使用python的scrapy爬取网页时，源代码中的中文字符在爬取下来后变成了英文字符。</p></blockquote><a id="more"></a><h3 id="问题举例"><a href="#问题举例" class="headerlink" title="问题举例"></a>问题举例</h3><p>例如，原网页为：</p><p><img src="https://i.loli.net/2018/10/25/5bd16544a16a3.jpg" alt=""></p><p>爬取结果为：</p><p><img src="https://i.loli.net/2018/10/25/5bd165a84336a.jpg" alt=""></p><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p><strong>修改请求头</strong>：在<code>settings.py</code>文件中找到下属代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Override the default request headers:</span></span><br><span class="line"><span class="comment">#DEFAULT_REQUEST_HEADERS = &#123;</span></span><br><span class="line"><span class="comment">#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',</span></span><br><span class="line"><span class="comment">#   'Accept-Language': 'en',</span></span><br><span class="line"><span class="comment">#&#125;</span></span><br></pre></td></tr></table></figure><p>改为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Override the default request headers:</span></span><br><span class="line">DEFAULT_REQUEST_HEADERS = &#123;</span><br><span class="line">   <span class="string">'Accept'</span>: <span class="string">'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'</span>,</span><br><span class="line">   <span class="string">'Accept-Language'</span>: <span class="string">'zh-CN'</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>修改结果展示：</p><p><img src="https://i.loli.net/2018/10/25/5bd168d7293ce.jpg" alt=""></p><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ul><li><a href="https://blog.csdn.net/wuqili_1025/article/details/79690103" target="_blank" rel="noopener">https://blog.csdn.net/wuqili_1025/article/details/79690103</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;使用python的scrapy爬取网页时，源代码中的中文字符在爬取下来后变成了英文字符。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="scrapy" scheme="http://yoursite.com/tags/scrapy/"/>
    
  </entry>
  
  <entry>
    <title>python动态网页爬取之安装docker和splash</title>
    <link href="http://yoursite.com/post/Python_dynamic_web_crawler_installed_docker_and_splash/"/>
    <id>http://yoursite.com/post/Python_dynamic_web_crawler_installed_docker_and_splash/</id>
    <published>2018-10-25T02:19:22.000Z</published>
    <updated>2018-10-25T02:28:18.214Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>利用python进行动态网页爬取时，在安装docker和splash时踩过的坑，记录了一下自己的安装过程。用的系统是mac os。</p></blockquote><a id="more"></a><h3 id="安装scrapy-splash"><a href="#安装scrapy-splash" class="headerlink" title="安装scrapy-splash"></a>安装scrapy-splash</h3><ul><li>利用pip安装scrapy-splash库：<br><code>$ pip install scrapy-splash</code></li></ul><h3 id="安装Docker"><a href="#安装Docker" class="headerlink" title="安装Docker"></a>安装Docker</h3><p>==下面👇这样安不下去了==</p><ul><li><p>如果是Mac的话需要使用brew安装，如下：<code>brew install docker</code>  </p><p>报错：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Error: Failure while executing; `git config --local --replace-all homebrew.private true` exited with 1.</span><br></pre></td></tr></table></figure><p>解决方法：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xcode-select --install</span><br></pre></td></tr></table></figure><p>然后在执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install docker</span><br></pre></td></tr></table></figure><p>再继续：</p><p><code>service docker start</code></p><p>报错：</p><p><code>-bash: service: command not found</code>上网上查一堆乱七八糟的解决方式，该路径啥的，真的不想改路径，怕把其他的改崩了。最后放弃这种方式，如果有兴趣也可以尝试解决。</p></li></ul><p>==尝试如下安装DOCKER方法==</p><ol><li><p><a href="https://www.docker.com/get-started" target="_blank" rel="noopener">去官网下载</a><img src="https://i.loli.net/2018/10/23/5bcf2ef4c4994.jpg" alt="">这种方法下载docker客户端需要从服务器下载，自己电脑下载12k/s，简直慢死了。</p></li><li><p>拉取镜像(pull the image)：<code>docker pull scrapinghub/splash</code></p><p><img src="https://i.loli.net/2018/10/23/5bcf34a4265a2.jpg" alt=""></p></li><li><p>用docker运行scrapinghub/splash：</p><p><code>docker run -p 8050:8050 scrapinghub/splash</code></p><p><img src="https://i.loli.net/2018/10/23/5bcf3578aa04a.jpg" alt=""></p><p>在浏览器中输入<code>localhost:8050</code><img src="https://i.loli.net/2018/10/23/5bcf363bc08d8.jpg" alt=""></p><p>==安装成功==</p></li></ol><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ul><li><a href="http://www.morecoder.com/article/1001249.html" target="_blank" rel="noopener">http://www.morecoder.com/article/1001249.html</a></li><li><a href="https://www.jianshu.com/p/e54a407c8a0a" target="_blank" rel="noopener">https://www.jianshu.com/p/e54a407c8a0a</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;利用python进行动态网页爬取时，在安装docker和splash时踩过的坑，记录了一下自己的安装过程。用的系统是mac os。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="splash" scheme="http://yoursite.com/tags/splash/"/>
    
      <category term="docker" scheme="http://yoursite.com/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>用python3读写csv文档</title>
    <link href="http://yoursite.com/post/read%20the%20CSV%20document%20using%20python3/"/>
    <id>http://yoursite.com/post/read the CSV document using python3/</id>
    <published>2018-10-16T13:36:26.000Z</published>
    <updated>2018-10-16T13:42:42.671Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>对于大多数的CSV格式的数据读写问题，都可以使用 <code>csv</code> 库。</p></blockquote><a id="more"></a><p> 例如：假设你在一个名叫stocks.csv文件中有一些股票市场数据，就像这样：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Symbol,Price,Date,Time,Change,Volume</span><br><span class="line">&quot;AA&quot;,39.48,&quot;6/11/2007&quot;,&quot;9:36am&quot;,-0.18,181800</span><br><span class="line">&quot;AIG&quot;,71.38,&quot;6/11/2007&quot;,&quot;9:36am&quot;,-0.15,195500</span><br><span class="line">&quot;AXP&quot;,62.58,&quot;6/11/2007&quot;,&quot;9:36am&quot;,-0.46,935000</span><br><span class="line">&quot;BA&quot;,98.31,&quot;6/11/2007&quot;,&quot;9:36am&quot;,+0.12,104800</span><br><span class="line">&quot;C&quot;,53.08,&quot;6/11/2007&quot;,&quot;9:36am&quot;,-0.25,360900</span><br><span class="line">&quot;CAT&quot;,78.29,&quot;6/11/2007&quot;,&quot;9:36am&quot;,-0.23,225400</span><br></pre></td></tr></table></figure><h1 id="csv文档的读取"><a href="#csv文档的读取" class="headerlink" title="csv文档的读取"></a>csv文档的读取</h1><h2 id="1-常规读取"><a href="#1-常规读取" class="headerlink" title="1. 常规读取"></a>1. 常规读取</h2><p>下面向你展示如何将这些数据读取为一个元组的序列：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'stocks.csv'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f_csv = csv.reader(f)</span><br><span class="line">    headers = next(f_csv)</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> f_csv:</span><br><span class="line">        <span class="comment"># Process row</span></span><br><span class="line">        ...</span><br></pre></td></tr></table></figure><p>在上面的代码中， <code>row</code> 会是一个列表。因此，为了访问某个字段，你需要使用下标，如 <code>row[0]</code>访问Symbol， <code>row[4]</code> 访问Change。==这样可以通过外建字典来存储读出的csv数据。==</p><h2 id="2-命名元组"><a href="#2-命名元组" class="headerlink" title="2. 命名元组"></a>2. 命名元组</h2><p>由于这种下标访问通常会引起混淆，你可以考虑使用==命名元组==。例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'stock.csv'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f_csv = csv.reader(f)</span><br><span class="line">    headings = next(f_csv)</span><br><span class="line">    Row = namedtuple(<span class="string">'Row'</span>, headings)</span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> f_csv:</span><br><span class="line">        row = Row(*r)</span><br><span class="line">        <span class="comment"># Process row</span></span><br><span class="line">        ...</span><br></pre></td></tr></table></figure><p>它允许你使用列名如 <code>row.Symbol</code> 和 <code>row.Change</code> 代替下标访问。 需要注意的是这个只有在列名是合法的Python标识符的时候才生效。如果不是的话， 你可能需要修改下原始的列名(如将非标识符字符替换成下划线之类的)。</p><h2 id="3-字典"><a href="#3-字典" class="headerlink" title="3. 字典"></a>3. 字典</h2><p>另外一个选择就是将数据读取到一个字典序列中去。可以这样做：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'stocks.csv'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f_csv = csv.DictReader(f)</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> f_csv:</span><br><span class="line">        <span class="comment"># process row</span></span><br><span class="line">        ...</span><br></pre></td></tr></table></figure><p>在这个版本中，你可以使用列名去访问每一行的数据了。比如，<code>row[&#39;Symbol&#39;]</code> 或者 <code>row[&#39;Change&#39;]</code>。</p><p><code>fieldnames</code> 是dict_reader的一个属性，表示CSV文档的数据名称。可以通过<code>f_csv.fieldnames</code>来访问数据名称那一行。</p><h1 id="CSV文件写入"><a href="#CSV文件写入" class="headerlink" title="CSV文件写入"></a>CSV文件写入</h1><p>为了写入CSV数据，你仍然可以使用csv模块，不过这时候先创建一个 <code>writer</code> 对象。例如:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">headers = [<span class="string">'Symbol'</span>,<span class="string">'Price'</span>,<span class="string">'Date'</span>,<span class="string">'Time'</span>,<span class="string">'Change'</span>,<span class="string">'Volume'</span>]</span><br><span class="line">rows = [(<span class="string">'AA'</span>, <span class="number">39.48</span>, <span class="string">'6/11/2007'</span>, <span class="string">'9:36am'</span>, <span class="number">-0.18</span>, <span class="number">181800</span>),</span><br><span class="line">         (<span class="string">'AIG'</span>, <span class="number">71.38</span>, <span class="string">'6/11/2007'</span>, <span class="string">'9:36am'</span>, <span class="number">-0.15</span>, <span class="number">195500</span>),</span><br><span class="line">         (<span class="string">'AXP'</span>, <span class="number">62.58</span>, <span class="string">'6/11/2007'</span>, <span class="string">'9:36am'</span>, <span class="number">-0.46</span>, <span class="number">935000</span>),</span><br><span class="line">       ]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'stocks.csv'</span>,<span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f_csv = csv.writer(f)</span><br><span class="line">    f_csv.writerow(headers)</span><br><span class="line">    f_csv.writerows(rows)</span><br></pre></td></tr></table></figure><p>如果你有一个字典序列的数据，可以像这样做：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">headers = [<span class="string">'Symbol'</span>, <span class="string">'Price'</span>, <span class="string">'Date'</span>, <span class="string">'Time'</span>, <span class="string">'Change'</span>, <span class="string">'Volume'</span>]</span><br><span class="line">rows = [&#123;<span class="string">'Symbol'</span>:<span class="string">'AA'</span>, <span class="string">'Price'</span>:<span class="number">39.48</span>, <span class="string">'Date'</span>:<span class="string">'6/11/2007'</span>,</span><br><span class="line">        <span class="string">'Time'</span>:<span class="string">'9:36am'</span>, <span class="string">'Change'</span>:<span class="number">-0.18</span>, <span class="string">'Volume'</span>:<span class="number">181800</span>&#125;,</span><br><span class="line">        &#123;<span class="string">'Symbol'</span>:<span class="string">'AIG'</span>, <span class="string">'Price'</span>: <span class="number">71.38</span>, <span class="string">'Date'</span>:<span class="string">'6/11/2007'</span>,</span><br><span class="line">        <span class="string">'Time'</span>:<span class="string">'9:36am'</span>, <span class="string">'Change'</span>:<span class="number">-0.15</span>, <span class="string">'Volume'</span>: <span class="number">195500</span>&#125;,</span><br><span class="line">        &#123;<span class="string">'Symbol'</span>:<span class="string">'AXP'</span>, <span class="string">'Price'</span>: <span class="number">62.58</span>, <span class="string">'Date'</span>:<span class="string">'6/11/2007'</span>,</span><br><span class="line">        <span class="string">'Time'</span>:<span class="string">'9:36am'</span>, <span class="string">'Change'</span>:<span class="number">-0.46</span>, <span class="string">'Volume'</span>: <span class="number">935000</span>&#125;,</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'stocks.csv'</span>,<span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f_csv = csv.DictWriter(f, headers)</span><br><span class="line">    f_csv.writeheader()</span><br><span class="line">    f_csv.writerows(rows)</span><br></pre></td></tr></table></figure><p>其中<code>f_csv.writeheader()</code>也可以替换成<code>f_csv.writerow(dict(zip(headers, headers)))</code></p><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul><li><a href="https://python3-cookbook.readthedocs.io/zh_CN/latest/c06/p01_read_write_csv_data.html" target="_blank" rel="noopener">https://python3-cookbook.readthedocs.io/zh_CN/latest/c06/p01_read_write_csv_data.html</a></li><li><a href="https://blog.csdn.net/guoziqing506/article/details/52014506" target="_blank" rel="noopener">https://blog.csdn.net/guoziqing506/article/details/52014506</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;对于大多数的CSV格式的数据读写问题，都可以使用 &lt;code&gt;csv&lt;/code&gt; 库。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="文件读取" scheme="http://yoursite.com/tags/%E6%96%87%E4%BB%B6%E8%AF%BB%E5%8F%96/"/>
    
      <category term="csv" scheme="http://yoursite.com/tags/csv/"/>
    
  </entry>
  
  <entry>
    <title>Neo4j初始化节点显示设置</title>
    <link href="http://yoursite.com/post/Neo4j_initializes_the_node_display_Settings/"/>
    <id>http://yoursite.com/post/Neo4j_initializes_the_node_display_Settings/</id>
    <published>2018-10-10T08:13:05.000Z</published>
    <updated>2018-10-10T08:21:08.779Z</updated>
    
    <content type="html"><![CDATA[<h3 id="问题描述："><a href="#问题描述：" class="headerlink" title="问题描述："></a>问题描述：</h3><p>neo4j中有默认的初始化节点显示设置为300个节点，如果想要显示的节点多于300个，则会只显示300个，并给予以下提示语句：</p><p><code>Not all return nodes are being displayed due to Initial Node Display setting. Only 300 of 300 nodes are being displayed.</code></p><h3 id="解决方法："><a href="#解决方法：" class="headerlink" title="解决方法："></a>解决方法：</h3><p>在如图所示<code>initial Node Display</code>处可以修改，在此处修改为300000.<img src="https://i.loli.net/2018/10/10/5bbdb388a368b.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;问题描述：&quot;&gt;&lt;a href=&quot;#问题描述：&quot; class=&quot;headerlink&quot; title=&quot;问题描述：&quot;&gt;&lt;/a&gt;问题描述：&lt;/h3&gt;&lt;p&gt;neo4j中有默认的初始化节点显示设置为300个节点，如果想要显示的节点多于300个，则会只显示300个，并给予以下
      
    
    </summary>
    
      <category term="Neo4j" scheme="http://yoursite.com/categories/Neo4j/"/>
    
    
      <category term="Neo4j" scheme="http://yoursite.com/tags/Neo4j/"/>
    
      <category term="数据库" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>《Bidirectional LSTM-CRF Models for Sequence Tagging》阅读笔记</title>
    <link href="http://yoursite.com/post/read_Bidirectional_LSTM-CRF_Models_for_Sequence_Tagging/"/>
    <id>http://yoursite.com/post/read_Bidirectional_LSTM-CRF_Models_for_Sequence_Tagging/</id>
    <published>2018-10-05T05:05:12.000Z</published>
    <updated>2018-10-25T02:26:46.123Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>这篇论文可以作为一个RNN和LSTM学习的一个例子来看，有利于新手对LSTM的理解。对于NER的处理主要是作为一个序列标注问题。但是作为经典文章还是可以读一读了解一下的。</p></blockquote><a id="more"></a><p>在本篇论文中，作者提出了4种模型：LSTM、BI-LSTM、LSTM-CRF和BI-LSTM-CRF。</p><h2 id="contribution-贡献"><a href="#contribution-贡献" class="headerlink" title="contribution(贡献)"></a>contribution(贡献)</h2><ol><li>作者在NLP标注数据集上系统的对比了以上四个模型；</li><li>作者是首先提出把BI-LSTM-CRF模型用于NLP序列标注，并且达到了state-of-the-art的水平；</li><li>作者展示了BI-LSTM-CRF是robust，并且极少依赖于词向量。</li></ol><h2 id="model-模型"><a href="#model-模型" class="headerlink" title="model(模型)"></a>model(模型)</h2><h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p>首先，作者先介绍了RNN的结构和工作原理，如图：<img src="https://i.loli.net/2018/10/04/5bb5a3e22e140.jpg" alt=""></p><p>其中输入为句子：EU rejects German call to boycott British lamb。输出为标签：B-ORG O B-MISC O O O B-MISC O O，其中B-，I-表示实体开始和中间位置。标签种类为：other (O)和四种实体标签：Person (PER), Location (LOC), Organization (ORG), and Miscellaneous (MISC).</p><p>输入层表示在时间步 t 的特征。它们可以是 one-hot-encoding 的词特征，稠密或者稀疏的向量特征。输入层与特征有相同大小的维度。输出层表示在时间步 t 的标签上的概率分布，维度与标注数量相同。相比前馈神经网络，RNN 引入前一个隐藏状态和当前隐藏状态的结合，因此可以储存历史信息。</p><p>涉及公式为：<img src="https://i.loli.net/2018/10/04/5bb5a556ad0ab.jpg" alt=""></p><p><img src="https://i.loli.net/2018/10/04/5bb5a57379fd7.jpg" alt=""></p><p>其中，U，W，V都是权重，函数f，g分别为sigmoid和softmax函数。  </p><p>接下来，作者展示了LSTM的结构和原理，如图：<img src="https://i.loli.net/2018/10/04/5bb5a61bdb483.jpg" alt=""></p><p>公式：<img src="https://i.loli.net/2018/10/04/5bb5a670e3b88.jpg" alt=""></p><p>其中，σ是逻辑sigmoid函数，i, f, o 和 c分别是输入门，忘记门，输出门和细胞向量，所有的大小都和向量h一样。w权重的含义如其下表所示。</p><p>LSTM序列标注模型如图所示：<img src="https://i.loli.net/2018/10/04/5bb5a84cd95d5.jpg" alt=""></p><p>其中，中间的画斜线的格子即为图2中所示部分。</p><h3 id="Bidirectional-LSTM-双向LSTM"><a href="#Bidirectional-LSTM-双向LSTM" class="headerlink" title="Bidirectional LSTM(双向LSTM)"></a>Bidirectional LSTM(双向LSTM)</h3><p>作者展示了双向LSTM的结构，如图所示：<img src="https://i.loli.net/2018/10/04/5bb5a8f44f16e.jpg" alt=""></p><p>双向LSTM网络可以有效利用过去特征和未来特征。在作者的实现中，对于整个句子的前向和后向操作，作者只需要在每个句子开始时将隐藏状态重置为0。作者采用批处理，使得可以同时处理多个句子。</p><h3 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h3><p>使用邻居标记信息预测当前标记有两种不同的方法：</p><ol><li>预测每个时间步长的标签分布，然后使用波束式解码来找到最优的标签序列，代表方法：MEMMs</li><li>注重句子层次而不是个体位置，代表方法：CRF，输入和输出是直接相连的；如图：<img src="https://i.loli.net/2018/10/04/5bb5ac191d2dd.jpg" alt=""></li></ol><p>研究表明，CRFs一般能够产生更高的标签精度。</p><h3 id="LSTM-CRF"><a href="#LSTM-CRF" class="headerlink" title="LSTM-CRF"></a>LSTM-CRF</h3><p>作者展示了LSTM-CRF的结构，如图：<img src="https://i.loli.net/2018/10/04/5bb5ad05ebec0.jpg" alt=""></p><p>这网络可以有效地通过 LSTM 利用过去的输入特征和通过 CRF 利用句子级的标注信息。图中CRF层由连接连续输出层的线表示。CRF层有一个状态转移矩阵作为参数。</p><p>公式为：<img src="https://i.loli.net/2018/10/04/5bb5b6ea7f87e.jpg" alt=""></p><p>函数f为网络的输出分数，[x]为输入， [fθ]i,t 为带有参数θ（句子x，第i 个标签，第t个单词）的网络输出；</p><p>[A]i,j为转移分数，从连续的时间步i状态到j状态的转移分数。注意，该转换矩阵是位置无关的。</p><h3 id="BI-LSTM-CRF"><a href="#BI-LSTM-CRF" class="headerlink" title="BI-LSTM-CRF"></a>BI-LSTM-CRF</h3><p>作者展示了BI-LSTM-CRF的结构，如图所示：<img src="https://i.loli.net/2018/10/04/5bb5b87ca950f.jpg" alt=""></p><p>作者在实验中展示了额外的未来特征可以提高标签的准确率。</p><h2 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h2><p>本文使用的所有模型都共享一个通用SGD前向和后向训练过程。作者展示了BI-LSTM-CRF的算法，如图<img src="https://i.loli.net/2018/10/04/5bb5ba7ce18c7.jpg" alt=""></p><p>作者设置了批次大小为100。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="data"><a href="#data" class="headerlink" title="data"></a>data</h3><p>作者在以下三个数据集上测试自己的模型：Penn TreeBank (PTB) POS tagging, CoNLL 2000 chunking, and CoNLL 2003 named entity tagging.数据集信息展示如下：<img src="https://i.loli.net/2018/10/04/5bb5bc263ec9f.jpg" alt=""></p><h3 id="Features"><a href="#Features" class="headerlink" title="Features"></a>Features</h3><p>作者从三个数据集中提取出其公共特征。特征可以分为拼写特征和上下文特征。最终，作者对于POS（词性标注）、chunking（组块）和NER（命名实体识别）分别提取401K，76K和341K个特征。</p><h3 id="spelling-features（拼写特征）"><a href="#spelling-features（拼写特征）" class="headerlink" title="spelling features（拼写特征）"></a>spelling features（拼写特征）</h3><p>除了小写字母特征之外，我们提取给定单词的以下特征。</p><p><img src="https://i.loli.net/2018/10/04/5bb5bdde879c9.jpg" alt=""></p><h3 id="context-featurs（上下文特征）"><a href="#context-featurs（上下文特征）" class="headerlink" title="context featurs（上下文特征）"></a>context featurs（上下文特征）</h3><p>对于单词特征，作者使用unigram和bi-grams特征。对于在CoNLL2000数据集中的POS特征和在CoNLL2003数据集中的 POS &amp; CHUNK特征，作者使用了unigram，bi-gram和tri-gram特征。</p><h3 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h3><p>词向量在改进序列标注任务的表现方面起着至关重要的作用，我们使用 130K 词汇并且每个词汇的词向量维度是 50 维。我们将 one-hot-encoding词表示替换每个词对应的词向量。</p><h3 id="Features-connection-tricks"><a href="#Features-connection-tricks" class="headerlink" title="Features connection tricks"></a>Features connection tricks</h3><p>我们可以将拼写和上下文特征与单词特征一样对待。这样网络的输入包括单词，单词的拼写和上下文特征。然而，==我们发现将拼写和上下文特征与输出直接连接可以加速训练过程，同时也能保持标注的准确率，==如下图所示：<img src="https://i.loli.net/2018/10/04/5bb5bfc63f961.jpg" alt=""></p><p>我们注意到，这种特征的使用与使用的最大熵特征类似。区别在于采用特征三列技术可能会发生特征冲突。由于序列标注数据集中的输出标签小于语言模型（通常为数十万），所以我们可以在特征和输出之间建立完整的连接，以避免潜在的特征冲突。</p><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>在相同的数据集上分别训练LSTM，BI-LSTM，CRF，LSTM-CRF和BI-LSTM-CRF模型，并且采用两种方式初始化word embedding：随机和Senna方式。模型的训练速率为0.1，隐藏层数量为300.不同模型在不同word embedding下的结果如表2所示，同时列出了之前最好模型Cov-CRF。</p><p><img src="https://i.loli.net/2018/10/04/5bb5c1da95e97.jpg" alt=""></p><ul><li><p>与Cov-CRF比较</p><p>实验中设置了3个基准模型，分别为LSTM、BI-LSTM和CRF，结果中LSTM在三个数据集中效果最差，BI-LSTM跟CRF在POS和chunking中效果接近，但是在NER中后者要优于前者。有趣的是表现最好的模型BI-LSTM-CRF相对于Cov-CRF来说对Senna embedding的依赖程度更小。</p></li><li><p>(robustness)模型鲁棒性  </p><p>为验证模型的鲁棒性，对不同模型只采用word feature特征进行训练，训练结果如表3，括号中数字表示相比于全部特征，模型的结果下降数值。<img src="https://i.loli.net/2018/10/05/5bb6ee3958635.jpg" alt=""></p></li><li><p>与其他系统的比较</p><p>这里就不贴图了，总之就是阐述作者自己模型好。</p></li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>总之作者的模型是基于之前模型的一些改进，主要运用了IBI-LSTM和CRF的结合。</p><h3 id="论文下载地址"><a href="#论文下载地址" class="headerlink" title="论文下载地址"></a><a href="https://arxiv.org/pdf/1508.01991.pdf" target="_blank" rel="noopener">论文下载地址</a></h3>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;这篇论文可以作为一个RNN和LSTM学习的一个例子来看，有利于新手对LSTM的理解。对于NER的处理主要是作为一个序列标注问题。但是作为经典文章还是可以读一读了解一下的。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="NER" scheme="http://yoursite.com/tags/NER/"/>
    
      <category term="LSTM" scheme="http://yoursite.com/tags/LSTM/"/>
    
      <category term="BI-LSTM" scheme="http://yoursite.com/tags/BI-LSTM/"/>
    
  </entry>
  
  <entry>
    <title>CN-DBpedia A Never-Ending Chinese Knowledge Extraction SystemCN-DBpedia System</title>
    <link href="http://yoursite.com/post/essay/"/>
    <id>http://yoursite.com/post/essay/</id>
    <published>2018-10-01T14:27:25.000Z</published>
    <updated>2018-10-02T07:14:02.447Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><blockquote><p>本篇论文为2016年的一篇论文，主要介绍了作者构建中文知识图谱所遇到的一些问题和解决方法。</p></blockquote><a id="more"></a><h3 id="challenge"><a href="#challenge" class="headerlink" title="challenge"></a>challenge</h3><ol><li>如何降低人力成本？</li><li>如何保持知识库的新鲜度？</li></ol><h3 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h3><ol><li>在构建中文知识库中降低了人力成本：<ul><li>重复利用已经存在的本体论</li><li>提出了一个不用人工监督的端到端的深度学习模型</li></ul></li><li>提出了一个智能主动更新策略</li></ol><h3 id="系统结构"><a href="#系统结构" class="headerlink" title="系统结构"></a>系统结构</h3><p><img src="https://i.loli.net/2018/09/26/5baaf081b2644.jpg" alt=""></p><p>提高知识库质量：</p><ol><li><strong>Normalization</strong>： normalize the attributes and values</li><li><strong>Enrichment</strong>：reuse the ontology</li><li><strong>Correction</strong>：two steps<ol><li>error detection:<ul><li>rule-based detection</li><li>based on user feedbacks</li></ul></li><li>error correction<ul><li>crowd-sourcing</li></ul></li></ol></li></ol><h3 id="降低人力成本"><a href="#降低人力成本" class="headerlink" title="降低人力成本"></a>降低人力成本</h3><p>这部分作者采用了两种方法：</p><ol><li>重复利用已经存在在知识库的本体论和类型化的中文实体</li><li>构建一个端到端提取器</li></ol><h4 id="Cross-Lingual-Entity-Typing（跨语言的实体类型）"><a href="#Cross-Lingual-Entity-Typing（跨语言的实体类型）" class="headerlink" title="Cross-Lingual Entity Typing（跨语言的实体类型）"></a>Cross-Lingual Entity Typing（跨语言的实体类型）</h4><ul><li>第一步是通过用英文DBpedia类型来类型化中文实体。为了达到这个目的，作者提出了如下系统：<img src="https://i.loli.net/2018/10/01/5bb2273b35ab0.jpg" alt="">系统建立了监督层次分类模型，系统输入为没有标记类型的中文实体，输出为在DB中所有有效的英文类型。作者将中文实体与共享相同中文标签名称的英语实体配对，这样中文实体以及配对英语实体的类型自然是标记样本。</li><li>用上述方法得到的训练集可能出现下面一些问题：<ul><li>英文DBpedia实体类型在许多情况下可能不完全；</li><li>英文DBpedia实体类型在许多情况下可能是错误的；</li><li>中英文链接可能出错；</li><li>中文实体的特征通常不完整。</li></ul></li><li>为了解决以上问题，作者提出了两种方法：<ul><li>完善英文DBpedia实体类型；</li><li>设计一个过滤步骤来剔除错误样本。</li></ul></li></ul><h4 id="infobox-completion"><a href="#infobox-completion" class="headerlink" title="infobox completion"></a>infobox completion</h4><blockquote><p>Infobox completion is a task to extract object for a given pair of entity and predicate from encyclopedia articles.</p></blockquote><p>作者建模了一个seq2seq模型，输入为包含tokens的自然语言句子，输出为每个token的标签。对于标签为0或1。</p><p>对于建立一个有效的提取器有以下两个关键：</p><ol><li>如何构建训练集：作者采用远程监督方法（利用Wikipedia）</li><li>如何选取期望的提取模型：LSTM-RNN，如图所示<img src="https://i.loli.net/2018/10/01/5bb22b72af582.jpg" alt=""></li></ol><h3 id="知识库更新"><a href="#知识库更新" class="headerlink" title="知识库更新"></a>知识库更新</h3><p>作者采用动态更新：识别新实体或可能包含新事实的旧实体</p><p>作者根据以下两方面来辨别这些实体：</p><ul><li>近期热点新闻中提及的实体</li><li>在搜索引擎的流行搜索关键字或其他流行网页中提到的实体</li></ul><p>对于如何从新闻标题和搜素指令中提取实体名字，作者采用简单的词分割方法，从百科全书中判断其是否为实体，并提出IDF值低的分割子串。</p><h3 id="统计数据"><a href="#统计数据" class="headerlink" title="统计数据"></a>统计数据</h3><p><img src="https://i.loli.net/2018/10/01/5bb22d57ab3ec.jpg" alt=""></p><h3 id="论文下载链接"><a href="#论文下载链接" class="headerlink" title="论文下载链接"></a><a href="https://www.researchgate.net/publication/318144300_CN-DBpedia_A_Never-Ending_Chinese_Knowledge_Extraction_System" target="_blank" rel="noopener">论文下载链接</a></h3>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;本篇论文为2016年的一篇论文，主要介绍了作者构建中文知识图谱所遇到的一些问题和解决方法。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="知识图谱" scheme="http://yoursite.com/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="RNN" scheme="http://yoursite.com/tags/RNN/"/>
    
  </entry>
  
  <entry>
    <title>Ontology reasoning with deep neural networks</title>
    <link href="http://yoursite.com/post/essay/"/>
    <id>http://yoursite.com/post/essay/</id>
    <published>2018-09-29T08:29:49.000Z</published>
    <updated>2018-10-02T07:13:58.996Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Ontology-reasoning-with-deep-neural-networks（基于深度神经网络的本体推理）"><a href="#Ontology-reasoning-with-deep-neural-networks（基于深度神经网络的本体推理）" class="headerlink" title="Ontology reasoning with deep neural networks（基于深度神经网络的本体推理）"></a>Ontology reasoning with deep neural networks（基于深度神经网络的本体推理）</h1><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><blockquote><p>本论文属于知识图谱的下游任务，也就是知识图谱的应用的一个例子。这篇论文的方法根据作者描述RRN是第一个基于深度学习的全面本体推理方法。</p></blockquote><a id="more"></a><h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><p>获得一个可以在不同的场景进行有效推理的模</p><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>基于机器学习的推理文章通常假设了一个特定的应用案例：自然语言或视觉输入的推理。<br>作者采用一个不同的方法：将正式的推理问题作为起点。<br>对于特定的问题选择：选择一种在表现力与另一方面复杂性之间取得适当平衡的方法通常是明智的。<br>OWL RL<br>本体推理是指一种常见的场景，在这种场景中，用于推理的推理规则（在此上下文中称为本体）与我们寻求推理的事实信息一起指定。  </p><p>本体推理是一种非常灵活的工具，它允许对大量不同的场景进行建模，因此满足了我们对适用于各种应用的系统的需求。<br>==首先引出了什么是本质推理，然后进一步阐述为什么要用机器学习==</p><p>今天用于推理的大多数KRR形式都植根于符号逻辑,这些方法在实践中会遇到许多问题：例如处理不完整，冲突或不确定数据的困难<br>机器学习模型通常具有高度可扩展性，更能抵抗数据中的干扰，并且即使所提供的形式是错误的也能够提供预测。</p><p><strong>作者的目标是通过采用尖端的深度学习技术，目标是在近似于形式方法的高度期望（理论）属性和另一方面利用机器学习的稳健性之间管理平衡行为。</strong>  </p><p>对于用于推理的知识图谱：作者采用的是由个体、类和二元关系组成的信息构成，其中个体对应于顶点，关系对应于被标记的有向边缘，类对应于二进制顶点标签。关系是主体和客体之间的关系或者个人和类之间的关系。这与关系学习不同：在关系学习的背景下，知识图通常通过将类视为个人以及将成员视为普通关系来简化。然而，就作者的目的而言，明确区分类和关系是很重要的，因为在用于推理的知识图谱中类和关系可能不同。  </p><h3 id="模型总览"><a href="#模型总览" class="headerlink" title="模型总览"></a>模型总览</h3><p><img src="https://i.loli.net/2018/09/04/5b8e474079995.jpg" alt=""><br>整个模型是以RRN为基础进行构建的，每个RRN都针对特定的本体进行训练。当训练模型应用于一组特定的事实时，它分为如下两个步骤：</p><ol><li>它为所有的步骤生成矢量表示，也就是嵌入在所考虑数据中出现的个体。</li><li>它仅基于这些生成向量计算查询预测</li></ol><p>在图中，</p><ul><li>a中它考虑一个事实三元组，并根据数据集重复多次。</li><li>b中它每读取一个事实就获取三元组中的个体潜入，并将他们的反馈送入更新层，该层产生已提供的嵌入的更新版本，然后将其存储在前一个版本的位置。 </li><li>c中从随机生成的向量开始，逐步更新嵌入，以便对关于它们所代表的个体的事实和推论进行编码。</li></ul><h3 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h3><p>作者在四个不同的数据集上训练和评估了RRN，其中两个是人工生成的玩具数据集，两个是从现实世界的数据库中提取的。这样做的原因：</p><ol><li>玩具问题具有很大的优势，即很明显某些推论是多么困难，从而为我们提供了对模型能力的相当好的印象。</li><li>在现实环境中评估方法当然是性能不可或缺的衡量标准</li></ol><p>作者为了评估真实世界数据的RRN模型，还从从两个著名的知识库DBpedia和Claros中提取了数据集。</p><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p><img src="https://i.loli.net/2018/09/04/5b8e6d1edbfdc.jpg" alt=""></p><ol><li>RRN能够有效地编码提供的关于类和关系的事实</li><li>对于关系的推理，可以看到DBpedia的准确度略低于98.9％，而其他数据集中的可导出关系在所有情况中至少99.6％被正确预测。</li><li>可以预测该模型在预测可推断类别方面比在关系方面表现更好，因为大多数这些都是仅依赖于单个三元组的推论。  </li></ol><p>为了评估作者提出的KRR方法常常遇到的问题，作者进行了如下实验：</p><ol><li>对于缺少信息的问题，作者随机删除了一个无法通过每个样本的符号推理推断出的事实，并检查模型是否能够正确地重建它。结果：对于DBpedia来说，33.8％的失踪三元组就是这种情况，而对于Claros来说，38.4％被正确预测</li><li>对于冲突的问题，作者通过在每个测试样本中随机选择一个事实来测试模型解决冲突的能力，并添加相同的否定版本作为另一个事实。对于DBpedia，RRN正确解决了88.4％的引入冲突，而对于Claros，它甚至达到了96.2％。然而，最重要的是，对于任何一个损坏的数据集，之前报告的总精度都没有下降超过0.9。</li></ol><p>所有RRN的查询预测都完全基于它为各个数据集中的个体生成的嵌入，这就是为什么仔细研究这样一组嵌入向量是有益的。</p><h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><p>本论文属于知识图谱的下游任务，也就是知识图谱的应用的一个例子。这篇论文的方法根据作者描述RRN是第一个基于深度学习的全面本体推理方法。但是具体的操作方法论文中写的比较清晰，感觉自己是理解了。重点就是对于个体的嵌入表示，如果类比的话就是词向量，作者通过不断的处理更新这个词向量，最后通过所获的词向量进行推理。并且从这篇文章中可以看到作者使用的知识图谱和我之前在弄的关系三元组有所区别。</p><h3 id="论文下载链接"><a href="#论文下载链接" class="headerlink" title="论文下载链接"></a><a href="https://arxiv.org/abs/1808.07980" target="_blank" rel="noopener">论文下载链接</a></h3>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Ontology-reasoning-with-deep-neural-networks（基于深度神经网络的本体推理）&quot;&gt;&lt;a href=&quot;#Ontology-reasoning-with-deep-neural-networks（基于深度神经网络的本体推理）&quot; class=&quot;headerlink&quot; title=&quot;Ontology reasoning with deep neural networks（基于深度神经网络的本体推理）&quot;&gt;&lt;/a&gt;Ontology reasoning with deep neural networks（基于深度神经网络的本体推理）&lt;/h1&gt;&lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;本论文属于知识图谱的下游任务，也就是知识图谱的应用的一个例子。这篇论文的方法根据作者描述RRN是第一个基于深度学习的全面本体推理方法。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="知识图谱" scheme="http://yoursite.com/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Ontology" scheme="http://yoursite.com/tags/Ontology/"/>
    
  </entry>
  
  <entry>
    <title>初次见面，你好NYSDY！</title>
    <link href="http://yoursite.com/post/essay/"/>
    <id>http://yoursite.com/post/essay/</id>
    <published>2018-09-21T02:35:47.000Z</published>
    <updated>2018-09-24T16:03:43.310Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
</feed>
