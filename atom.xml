<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>NYSDY</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-06-27T07:51:44.136Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>NYSDY</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Learning Entity and Relation Embeddings for Knowledge Graph Completioné˜…è¯»ç¬”è®°</title>
    <link href="http://yoursite.com/post/Learning_Entity_and_Relation_Embeddings_for_Knowledge_Graph_Completion/"/>
    <id>http://yoursite.com/post/Learning_Entity_and_Relation_Embeddings_for_Knowledge_Graph_Completion/</id>
    <published>2019-06-26T08:18:15.000Z</published>
    <updated>2019-06-27T07:51:44.136Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>TransR embeds entities and relations in distinct entity space and relation space, and learns embeddings via translation between projected entities.CTransR models internal complicated correlations within each relation type.</p><p>è®ºæ–‡ä¸‹è½½åœ°å€</p></blockquote><a id="more"></a><h1 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h1><p>In fact, an entity may have multiple aspects and various relaitons may focus on different aspects of entities, which makes a common space insurficient for modeling.</p><h1 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h1><ul><li>propose a TransR model which models entities and relations in distinct spaces</li><li>CTransR models internal complicated correlations within each relation type.</li><li>experiment on benchmark datasets of WordNet and Freebase and gain consistent improvements compared to state-of-the-art models</li></ul><h1 id="Future-work"><a href="#Future-work" class="headerlink" title="Future work"></a>Future work</h1><ul><li>Existing models including TransR consider each relational fact separately.<ul><li>relation transitive</li></ul></li><li>explore a unified embedding model of both text side and knowledge graph</li><li>modeling internal correlations within each relation type</li></ul><h1 id="TransR"><a href="#TransR" class="headerlink" title="TransR"></a>TransR</h1><p><img src="http://image.nysdy.com/20190627156159912894954.jpg" alt="20190627156159912894954.jpg"></p><ol><li><p>for each triple$(h, r, t)$, entities embeddings are set as $\mathbf{h}, \mathbf{t} \in \mathbb{R}^{k}$ and relation embedding is set as $\mathbf{r} \in \mathbb{R}^{d}$, $k \neq d$</p></li><li><p>for each relation $r$, set a projection matrix $\mathbf{M}_{r} \in\mathbb{R}^{k \times d}$</p><ul><li>projects entities from entity space to relation space</li></ul></li><li><p>projected vectors of entities as </p><script type="math/tex; mode=display">\mathbf{h}_{r}=\mathbf{h} \mathbf{M}_{r}, \quad \mathbf{t}_{r}=\mathbf{t} \mathbf{M}_{r}</script></li><li><p>score function:</p><script type="math/tex; mode=display">f_{r}(h, t)=\left\|\mathbf{h}_{r}+\mathbf{r}-\mathbf{t}_{r}\right\|_{2}^{2}</script></li></ol><h1 id="Cluster-based-TransR-CTransR"><a href="#Cluster-based-TransR-CTransR" class="headerlink" title="Cluster-based TransR (CTransR)"></a>Cluster-based TransR (CTransR)</h1><h3 id="why-propose-CTransR"><a href="#why-propose-CTransR" class="headerlink" title="why propose CTransR"></a>why propose CTransR</h3><p>TransE, TransH and TransR, learn a unique vector for each relation, which may be under-representative to fit all entity pairs under this relation, because these relations are usually rather diverse.</p><h3 id="basic-idea"><a href="#basic-idea" class="headerlink" title="basic idea"></a>basic idea</h3><ul><li>incorporate the idea of piecewise linear regression Ritzema and others 1994</li><li>segment input instances into several groups</li></ul><h3 id="process"><a href="#process" class="headerlink" title="process"></a>process</h3><ol><li><p>for a specific relation r, all entity pairs (h, t) in the training data are clustered into multiple groups, and entity pairs in each group are expected to exhibit similar r relation.</p><ul><li>All entity pairs (h, t) are represented with their vector offsets (h âˆ’ t) for clustering, where h and t are obtained with TransE.</li></ul></li><li><p>learn a separate relation vector $r_c$for each cluster and matrix $M_r$ for each relation, respectively</p></li><li><p>projected vectors of entities as $\mathbf{h}_{r, c}=\mathbf{h} \mathbf{M}_{r} \text { and } \mathbf{t}_{r, c}=\mathbf{t} \mathbf{M}_{r}$</p></li><li><p>sorce fuction</p><script type="math/tex; mode=display">f_{r}(h, t)=\left\|\mathbf{h}_{r, c}+\mathbf{r}_{c}-\mathbf{t}_{r, c}\right\|_{2}^{2}+\alpha\left\|\mathbf{r}_{c}-\mathbf{r}\right\|_{2}^{2}</script><p>the later item aims to ensure cluster-specific relation vector rcnot too far away from the original relation vector r</p></li></ol><h1 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h1><p>é‡‡ç”¨å’Œå‰äººæ‰€ç”¨ä¸€æ ·çš„æ•°æ®é›†</p><div class="table-container"><table><thead><tr><th>Dataset</th><th>#Rel</th><th>#Ent</th><th>#Train</th><th>#Valid</th><th># Test</th></tr></thead><tbody><tr><td>WN18</td><td>18</td><td>40,943</td><td>141,442</td><td>5,000</td><td>5,000</td></tr><tr><td>FB15K</td><td>1,345</td><td>14,951</td><td>483,142</td><td>50,000</td><td>59,071</td></tr><tr><td>WN11</td><td>11</td><td>38,696</td><td>112,581</td><td>2,609</td><td>10,544</td></tr><tr><td>FB13</td><td>13</td><td>75,043</td><td>316,232</td><td>5,908</td><td>23,733</td></tr><tr><td>FB40K</td><td>1,336</td><td>39528</td><td>370,648</td><td>67,946</td><td>96,678</td></tr></tbody></table></div><h1 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h1><p>ä½œè€…é‡‡å–å¸¸è§„å®éªŒ</p><h3 id="Link-Prediction"><a href="#Link-Prediction" class="headerlink" title="Link Prediction"></a>Link Prediction</h3><p>è¿™é‡Œä½œè€…å¯¹å…³ç³»ä¸­èšç±»è¿›è¡Œäº†å±•ç¤ºï¼š <img src="http://image.nysdy.com/2019062715616031534723.jpg" alt="2019062715616031534723.jpg"></p><blockquote><p>æˆ‘è§‰å¾—è¿™ç§æ–¹å¼æ˜¯å€¼å¾—å°è¯•çš„ã€‚</p></blockquote><h3 id="Triple-classification"><a href="#Triple-classification" class="headerlink" title="Triple classification"></a>Triple classification</h3><p>Moreover, the â€œbernâ€ sampling technique improves the performance of TransE, TransH and TransR on all three data sets.</p><blockquote><p>berné‡‡æ ·æ–¹æ³•éœ€è¦æŒæ¡ã€‚</p></blockquote><h3 id="Relation-Extraction-from-Text"><a href="#Relation-Extraction-from-Text" class="headerlink" title="Relation Extraction from Text"></a>Relation Extraction from Text</h3>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;TransR embeds entities and relations in distinct entity space and relation space, and learns embeddings via translation between projected entities.CTransR models internal complicated correlations within each relation type.&lt;/p&gt;
&lt;p&gt;è®ºæ–‡ä¸‹è½½åœ°å€&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="è®ºæ–‡é˜…è¯»ç¬”è®°" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="KGE" scheme="http://yoursite.com/tags/KGE/"/>
    
      <category term="KGR" scheme="http://yoursite.com/tags/KGR/"/>
    
      <category term="TransR" scheme="http://yoursite.com/tags/TransR/"/>
    
  </entry>
  
  <entry>
    <title>Neural Relation Extraction with Selective Attention over Instancesé˜…è¯»ç¬”è®°</title>
    <link href="http://yoursite.com/post/Neural_Relation_Extraction_with_Selective_Attention_over_Instances/"/>
    <id>http://yoursite.com/post/Neural_Relation_Extraction_with_Selective_Attention_over_Instances/</id>
    <published>2019-06-26T05:49:57.000Z</published>
    <updated>2019-06-26T08:12:44.937Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>è¿™ç¯‡æ–‡ç« ä¹‹å‰çœ‹è¿‡ğŸ˜‚ã€‚</p><p>è®ºä¸‹è½½åœ°å€</p></blockquote><a id="more"></a><h1 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h1><p>â€‹    Distant supervision inevitably accompanies with the wrong labelling problem, and thse noisy data will substantially hurt the performance of relation extraction.</p><h1 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h1><ul><li>As compared to existing neural relation extraction model, our model can make full use of all informative sentences of each entity pair.</li><li>To address the wrong labelling problem in distant supervision, we propose selective attention to de-emphasize those noisy instances.</li><li>In the experiments, we show that selective attention is beneficial to two kinds of CNN models in the task of relation extraction.</li></ul><h1 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h1><p>æ¨¡å‹æ•´ä½“æ¶æ„å¦‚ä¸‹æ‰€ç¤ºï¼š</p><p><img src="http://image.nysdy.com/20190626156153649268323.jpg" alt="20190626156153649268323.jpg"></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;è¿™ç¯‡æ–‡ç« ä¹‹å‰çœ‹è¿‡ğŸ˜‚ã€‚&lt;/p&gt;
&lt;p&gt;è®ºä¸‹è½½åœ°å€&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="è®ºæ–‡é˜…è¯»ç¬”è®°" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="RE" scheme="http://yoursite.com/tags/RE/"/>
    
  </entry>
  
  <entry>
    <title>Learning as the Unsupervised Alignment of Conceptual Systemsé˜…è¯»ç¬”è®°</title>
    <link href="http://yoursite.com/post/Learning_as_the_Unsupervised_Alignment_of_Conceptual_Systems/"/>
    <id>http://yoursite.com/post/Learning_as_the_Unsupervised_Alignment_of_Conceptual_Systems/</id>
    <published>2019-06-25T06:42:57.000Z</published>
    <updated>2019-06-26T04:02:25.369Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>è¿™ç¯‡æ–‡ç« æ²¡æ€ä¹ˆçœ‹æ‡‚ï¼Œä¸»è¦æ€æƒ³åº”è¯¥æ˜¯ä»£è¡¨åŒæ—¶æ¦‚å¿µçš„ä¸åŒå½¢å¼ï¼ˆæ–‡æœ¬ï¼Œå›¾åƒï¼Œè¯­éŸ³ç­‰ï¼‰åº”è¯¥å…·æœ‰ç›¸ä¼¼çš„åˆ†å¸ƒï¼Œä»¥æ­¤æ¥è¿›è¡Œæ— ç›‘ç£çš„æ¦‚å¿µå¯¹é½ã€‚è¿™ç§æ€è·¯æŒºä¸é”™çš„ï¼Œä¸è¿‡è¿˜æ²¡æœ‰æ·±å…¥çš„æƒ³æ³•ï¼Œç®—æ˜¯æ‹“å±•è§†é‡å§ï¼</p></blockquote><a id="more"></a><h1 id="KEY"><a href="#KEY" class="headerlink" title="KEY"></a>KEY</h1><p>The key insight is that each concept has a unique signature within one conceptual system (e.g., images) that is recapitulated in other systems (e.g., text or audio)</p><h1 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h1><ul><li>For supervised approaches, as the number of concepts grows, so does the number of required training examples</li><li>V. W. Quine argued, even supervised instruction contains a substantial amount of ambiguity (Quine, 1960).Quine suggested that meaning may derive from somethingâ€™s place within a conceptual system.</li></ul><h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><p>In order to solve Quinneâ€™s problem, we align a system of word labels and a system of visual semantics that both refer to the same underlying reality and therefore have related structure that can be discovered by unsupervised means (Figure 1ï¼‰</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;è¿™ç¯‡æ–‡ç« æ²¡æ€ä¹ˆçœ‹æ‡‚ï¼Œä¸»è¦æ€æƒ³åº”è¯¥æ˜¯ä»£è¡¨åŒæ—¶æ¦‚å¿µçš„ä¸åŒå½¢å¼ï¼ˆæ–‡æœ¬ï¼Œå›¾åƒï¼Œè¯­éŸ³ç­‰ï¼‰åº”è¯¥å…·æœ‰ç›¸ä¼¼çš„åˆ†å¸ƒï¼Œä»¥æ­¤æ¥è¿›è¡Œæ— ç›‘ç£çš„æ¦‚å¿µå¯¹é½ã€‚è¿™ç§æ€è·¯æŒºä¸é”™çš„ï¼Œä¸è¿‡è¿˜æ²¡æœ‰æ·±å…¥çš„æƒ³æ³•ï¼Œç®—æ˜¯æ‹“å±•è§†é‡å§ï¼&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="è®ºæ–‡é˜…è¯»ç¬”è®°" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ERNIE Enhanced Language Representation with Informative Entitiesé˜…è¯»ç¬”è®°</title>
    <link href="http://yoursite.com/post/ERNIE_Enhanced_Language_Representation_with_Informative_Entities/"/>
    <id>http://yoursite.com/post/ERNIE_Enhanced_Language_Representation_with_Informative_Entities/</id>
    <published>2019-06-24T03:10:26.000Z</published>
    <updated>2019-06-25T06:32:57.460Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>è¯¥ç¯‡è®ºæ–‡å€Ÿé‰´BERTï¼Œè¯•å›¾å°†å®ä½“ä¿¡æ¯ï¼ˆTransEï¼‰èå…¥token(singal word)ä¸­ï¼Œé€šè¿‡ç±»ä¼¼å®ä½“å¯¹é½çš„æ–¹æ³•å°†å®ä½“ä¸tokenå¯¹é½ï¼ˆå¹¶é‡‡å–maskæ–¹å¼è¿›è¡Œé¢„è®­ç»ƒï¼‰ï¼Œé€šè¿‡infromation fusion å°†tokenä¸å®ä½“èåˆæ˜ å°„å…¥ç›¸å…³è”çš„ä¸¤ä¸ªå‘é‡ç©ºé—´ã€‚</p><p><a href="https://arxiv.org/pdf/1905.07129" target="_blank" rel="noopener">è®ºæ–‡ä¸‹è½½åœ°å€</a></p></blockquote><a id="more"></a><h1 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h1><p>the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding.</p><h1 id="Challenge"><a href="#Challenge" class="headerlink" title="Challenge"></a>Challenge</h1><p>For incorporating external knowledge into language representation models</p><ul><li>Structured Knowledge Encoding<ul><li>regarding to the given text, how to effectively extract and encode its related informative facts in KGs for language representation models</li></ul></li><li>Heterogeneous Information Fusion<ul><li>how to design a special pre-training objective to fuse the lexical, syntactic, and knowledge information is another challenge.</li></ul></li></ul><h1 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h1><p><img src="http://image.nysdy.com/20190625156142554544546.jpg" alt="20190625156142554544546.jpg"></p><h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p>ERNIE</p><ul><li><p>the underlying textual encoder (T-Encoder)è´Ÿè´£ä»æ–‡æœ¬ä¸­æ•è·åŸºæœ¬çš„è¯æ³•å’Œè¯­æ³•ä¿¡æ¯</p><ul><li><script type="math/tex; mode=display">\left\{\boldsymbol{w}_{1}, \ldots, \boldsymbol{w}_{n}\right\}=\mathrm{T}-\operatorname{Encoder}\left(\left\{w_{1}, \ldots, w_{n}\right\}\right)</script><p>T-Encoder(Â·) is a multi-layer bidirectional Transformer encoder</p></li></ul></li><li><p>the upper knowledgeable encoder (K-Encoder)</p><ul><li>entity embeddings are pre-trained by TransEè´Ÿè´£å°†çŸ¥è¯†å›¾è°±é›†æˆåˆ°åº•å±‚çš„æ–‡æœ¬ä¿¡æ¯ä¸­</li></ul></li></ul><h2 id="Knowledgeable-Encoder"><a href="#Knowledgeable-Encoder" class="headerlink" title="Knowledgeable Encoder"></a>Knowledgeable Encoder</h2><ul><li>the knowledgeable encoder K-Encoder consists of stacked aggregators</li><li>designed for encoding both tokens and entities as well as fusing their heterogeneous features.</li></ul><p>In the i-th aggregator</p><ul><li><p>the input:</p><ul><li>token embeddings: $\left\{\boldsymbol{w}_{1}^{(i-1)}, \ldots, \boldsymbol{w}_{n}^{(i-1)}\right\}$</li><li>entity embeddings :$\left\{\boldsymbol{e}_{1}^{(i-1)}, \ldots, \boldsymbol{e}_{m}^{(i-1)}\right\}$</li></ul></li><li><p>fed into two multi-head self-attentions(MH-ATTs)</p><ul><li>$\left\{\tilde{\boldsymbol{w}}_{1}^{(i)}, \ldots, \tilde{\boldsymbol{w}}_{n}^{(i)}\right\}=\mathrm{MH}-\operatorname{ATT}\left(\left\{\boldsymbol{w}_{1}^{(i-1)}, \ldots, \boldsymbol{w}_{n}^{(i-1)}\right\}\right)$</li><li>$\left\{\tilde{\boldsymbol{e}}_{1}^{(i)}, \ldots, \tilde{\boldsymbol{e}}_{m}^{(i)}\right\}=\mathrm{MH}-\operatorname{ATT}\left(\left\{\boldsymbol{e}_{1}^{(i-1)}, \ldots, \boldsymbol{e}_{m}^{(i-1)}\right\}\right)$</li></ul></li><li><p>an information fusion layer</p><ul><li><script type="math/tex; mode=display">\begin{aligned} \boldsymbol{h}_{j} &=\sigma\left(\tilde{\boldsymbol{W}}_{t}^{(i)} \tilde{\boldsymbol{w}}_{j}^{(i)}+\tilde{\boldsymbol{W}}_{e}^{(i)} \tilde{\boldsymbol{e}}_{k}^{(i)}+\tilde{\boldsymbol{b}}^{(i)}\right) \\ \boldsymbol{w}_{j}^{(i)} &=\sigma\left(\boldsymbol{W}_{t}^{(i)} \boldsymbol{h}_{j}+\boldsymbol{b}_{t}^{(i)}\right) \\ \boldsymbol{e}_{k}^{(i)} &=\sigma\left(\boldsymbol{W}_{e}^{(i)} \boldsymbol{h}_{j}+\boldsymbol{b}_{e}^{(i)}\right) \end{aligned}</script></li></ul><p>$h_j$ is the inner hidden state</p></li></ul><p>For the tokens without corresponding entities</p><script type="math/tex; mode=display">\begin{aligned} \boldsymbol{h}_{j} &=\sigma\left(\tilde{\boldsymbol{W}}_{t}^{(i)} \tilde{\boldsymbol{w}}_{j}^{(i)}+\tilde{\boldsymbol{b}}^{(i)}\right) \\ \boldsymbol{w}_{j}^{(i)} &=\sigma\left(\boldsymbol{W}_{t}^{(i)} \boldsymbol{h}_{j}+\boldsymbol{b}_{t}^{(i)}\right) \end{aligned}</script><h2 id="Pre-training-for-Injecting-Knowledge"><a href="#Pre-training-for-Injecting-Knowledge" class="headerlink" title="Pre-training for Injecting Knowledge"></a>Pre-training for Injecting Knowledge</h2><p>In order to inject knowledge into language rep- resentation by informative entities.</p><p>Randomly masks some token-entity alignments and then requires the system to predict all corresponding entities based on aligned tokens.</p><ul><li><p>denoising entity auto-encoder (dEA)</p></li><li><p>define the aligned entity distribution for the token $w_i$ as follows:</p><script type="math/tex; mode=display">p\left(e_{j} | w_{i}\right)=\frac{\exp \left(\text { linear }\left(\boldsymbol{w}_{i}^{o}\right) \cdot \boldsymbol{e}_{j}\right)}{\sum_{k=1}^{m} \exp \left(\text { linear }\left(\boldsymbol{w}_{i}^{o}\right) \cdot \boldsymbol{e}_{k}\right)}</script></li></ul><p>  linear(Â·) is a linear layer</p><p>For dEA, perform the following operations:</p><ul><li>in 5% of the time, replace the entity with another random<ul><li>aims to train model to correct the errors that the token is aligned with a wrong entity;</li></ul></li><li>In 15% of the time, mask token-entity alignments<ul><li>aims to train model to correct the errors that entity alignment system doesnâ€™t extract all existing alignments;</li></ul></li><li>in the rest of the time, keep token-entity alignments unchanged <ul><li>aims to encourage our model to integrate the entity information into token representations for better language understanding.</li></ul></li></ul><h2 id="Fine-tuning-for-Specific-Tasks"><a href="#Fine-tuning-for-Specific-Tasks" class="headerlink" title="Fine-tuning for Specific Tasks"></a>Fine-tuning for Specific Tasks</h2><p><img src="http://image.nysdy.com/20190625156143137852366.jpg" alt="20190625156143137852366.jpg"></p><p>We can take the final output embedding for the first token, which corresponds to the special [CLS] token, as the representation of the input sequence for specific tasks.</p><p>For some knowledge-driven tasks, we design special fine-tuning procedure:</p><ul><li>relation classification<ul><li>design different tokens [HD] and [TL] for head entities and tail entities respectively</li><li>a similar role like position embeddings in the conventional relation classification models (Zeng et al., 2015)</li></ul></li><li>entity typing<ul><li>the mention mark token [ENT]</li></ul></li></ul><blockquote><ul><li>è¿™é‡Œçš„CLSä¸çŸ¥é“æœ‰ä»€ä¹ˆä½œç”¨ï¼Œæ‰€æœ‰çš„ä»»åŠ¡éƒ½æœ‰ï¼Œæ˜¯ä¸åŒçš„ä»»åŠ¡é‡CLSçš„embeddingæœ‰æ‰€ä¸åŒå—ï¼Ÿä¸ªäººç›®å‰è§‰å¾—æ˜¯è¿™æ ·çš„ã€‚</li><li>ä½œè€…è¿™é‡Œé‡‡ç”¨çš„mark tokençš„æ–¹æ³•ä»£æ›¿position embeddingï¼Œä¸çŸ¥é“ä¸¤ä¸ªå¯¹æ¯”é‚£ç§æ•ˆæœä¼šæ›´å¥½ä¸€äº›ã€‚ç›´è§‚è§‰å¾—éƒ½æ˜¯æ ‡è®°ä½ç½®ä¿¡æ¯ã€‚</li></ul></blockquote><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h3 id="Pre-training-Dataset"><a href="#Pre-training-Dataset" class="headerlink" title="Pre-training Dataset"></a>Pre-training Dataset</h3><ul><li>we use English Wikipedia as our pre-training corpus and align text to Wiki-data<ul><li>4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities</li></ul></li><li>before pre-training ERINE, entity embeddings by TransE<ul><li>sample part of Wikidata which contains 5, 040, 986 entities and 24, 267, 796 fact triples</li></ul></li></ul><h3 id="Training-Details"><a href="#Training-Details" class="headerlink" title="Training Details"></a>Training Details</h3><ul><li>We also fine-tune ERNIE on the distant supervised dataset, i.e., FIGER (Ling et al., 2015)</li><li>we use TAGME (Ferragina and Scaiella, 2010) to extract the entity mentions in the sentences and link them to their corresponding entities in KGs</li></ul><h2 id="Entity-Typing"><a href="#Entity-Typing" class="headerlink" title="Entity Typing"></a>Entity Typing</h2><h3 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h3><p>two well-established datasets FIGER (Ling et al., 2015) and Open Entity (Choi et al., 2018).</p><ul><li>The training set of FIGER is labeled with distant supervision, and its test set is annotated by human.</li><li>Open Entity is a completely manually-annotated dataset.</li></ul><p><img src="http://image.nysdy.com/20190625156143360958681.jpg" alt="20190625156143360958681.jpg"></p><h3 id="Comparble-model"><a href="#Comparble-model" class="headerlink" title="Comparble model"></a>Comparble model</h3><ul><li>NFGEC<ul><li>NFGEC is a hybrid model proposed by Shimaoka et al. (2016)</li></ul></li><li>UFET<ul><li>(Choi et al., 2018)</li></ul></li></ul><h4 id="The-results-on-FIGER"><a href="#The-results-on-FIGER" class="headerlink" title="The results on FIGER:"></a>The results on FIGER:</h4><p>However, BERT has lower accuracy than the best NFGEC model. As strict accuracy is the ratio of instances whose predictions are identical to human annotations, it illustrates <strong>some wrong labels from distant supervision are learned by BERT</strong> due to its powerful fitting ability.</p><h2 id="Relation-Classification"><a href="#Relation-Classification" class="headerlink" title="Relation Classification"></a>Relation Classification</h2><h3 id="dataset-1"><a href="#dataset-1" class="headerlink" title="dataset"></a>dataset</h3><p>two well-established datasets FewRel (Han et al., 2018b) and TACRED (Zhang et al., 2017).</p><ul><li>FewRel<ul><li>As FewRel does not have any null instance where there isnâ€™t any relation between entities, we adopt macro averaged metrics to present the model performances. Since FewRel is built by checking whether the sentences contain facts in Wiki-data, we drop the related facts in KGs before pretraining for fair comparison</li></ul></li><li>TACRED<ul><li>In TACRED, there are nearly 80% null instances so that we follow the previous work (Zhang et al., 2017) to adopt micro averaged metrics to represent the model performances instead of the macro</li></ul></li></ul><p><img src="http://image.nysdy.com/20190625156143680247028.jpg" alt="20190625156143680247028.jpg"></p><h3 id="Comparble-model-1"><a href="#Comparble-model-1" class="headerlink" title="Comparble model"></a>Comparble model</h3><ul><li>CNN:(Zeng et al., 2015).</li><li>PA-LSTM</li><li>C-GCN :Zhang et al. (2018) adopt the graph convolution operations to model dependency trees for relation classification.<graph convolution="" over="" pruned="" dependency="" trees="" improves="" relation="" extraction.=""></graph></li></ul><h2 id="GLUE"><a href="#GLUE" class="headerlink" title="GLUE"></a>GLUE</h2><p>The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is a collection of diverse natural language understanding tasks</p><h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><p>explore the effects of the informative entities and the knowledgeable pretraining task (dEA) for ERNIE using FewRel dataset</p><blockquote><p>å®éªŒéƒ¨åˆ†åšçš„å¾ˆä¸°å¯Œï¼Œæ—¢æœ‰ä¸¤ä¸ªä»»åŠ¡çš„å¯¹æ¯”å®éªŒï¼Œä¹Ÿæœ‰å¯¹è‡ªèº«æ¨¡å—çš„å¯¹æ¯”å®éªŒï¼Œå¹¶ä¸”è¿˜å¯¹æ¯”äº†bertæ¥æ£€æµ‹è‡ªå·±æ¨¡å‹æ˜¯å¦å¯¹GLUEä»»åŠ¡æ•ˆæœæœ‰é™ä½ã€‚</p></blockquote><h1 id="future-research"><a href="#future-research" class="headerlink" title="future research"></a>future research</h1><p>1) inject knowledge into feature-based pre-training models such as ELMo (Peters et al., 2018); </p><p>(2) introduce diverse structured knowledge into language representation models such as ConceptNet (Speer and Havasi, 2012) which is different from world knowledge database Wikidata; </p><p>(3) annotate more real-world corpora heuristically for larger pre-training data</p><blockquote></blockquote><h1 id="å‚è€ƒé“¾æ¥"><a href="#å‚è€ƒé“¾æ¥" class="headerlink" title="å‚è€ƒé“¾æ¥"></a>å‚è€ƒé“¾æ¥</h1><ul><li><a href="https://blog.csdn.net/summerhmh/article/details/91042273" target="_blank" rel="noopener">https://blog.csdn.net/summerhmh/article/details/91042273</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;è¯¥ç¯‡è®ºæ–‡å€Ÿé‰´BERTï¼Œè¯•å›¾å°†å®ä½“ä¿¡æ¯ï¼ˆTransEï¼‰èå…¥token(singal word)ä¸­ï¼Œé€šè¿‡ç±»ä¼¼å®ä½“å¯¹é½çš„æ–¹æ³•å°†å®ä½“ä¸tokenå¯¹é½ï¼ˆå¹¶é‡‡å–maskæ–¹å¼è¿›è¡Œé¢„è®­ç»ƒï¼‰ï¼Œé€šè¿‡infromation fusion å°†tokenä¸å®ä½“èåˆæ˜ å°„å…¥ç›¸å…³è”çš„ä¸¤ä¸ªå‘é‡ç©ºé—´ã€‚&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1905.07129&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;è®ºæ–‡ä¸‹è½½åœ°å€&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="è®ºæ–‡é˜…è¯»ç¬”è®°" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
      <category term="KG" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/KG/"/>
    
    
      <category term="KG" scheme="http://yoursite.com/tags/KG/"/>
    
      <category term="KGR" scheme="http://yoursite.com/tags/KGR/"/>
    
      <category term="BERT" scheme="http://yoursite.com/tags/BERT/"/>
    
  </entry>
  
  <entry>
    <title>Incorporating Literals into Knowledge Graph Embeddingsé˜…è¯»ç¬”è®°</title>
    <link href="http://yoursite.com/post/Incorporating_Literals_into_Knowledge_Graph_Embeddings/"/>
    <id>http://yoursite.com/post/Incorporating_Literals_into_Knowledge_Graph_Embeddings/</id>
    <published>2019-06-03T06:57:50.000Z</published>
    <updated>2019-06-03T07:33:16.832Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>è¯»å®Œäº†å‰ä¸¤ç« ï¼Œç®€å•çš„çœ‹äº†ä¸€ä¸‹ä½œè€…æå‡ºçš„æ¨¡å‹ï¼Œæ„Ÿè§‰å¹¶æ²¡æœ‰å¤ªå¤§ä»·å€¼ï¼Œå°±æ˜¯ç»™å®ä½“è¾“å…¥å¤šåŠ å…¥äº†ä¸€ä¸ªliteralçš„ä¿¡æ¯ï¼ˆåŠ å…¥æ–¹æ³•å¯ä»¥é‡‡ç”¨çº¿æ€§ã€éçº¿æ€§æˆ–è€…ç¥ç»ç½‘ç»œï¼‰ã€‚</p><p>è¯»è®ºæ–‡å‰éœ€è¦å…ˆç†Ÿæ‚‰DistMultã€ComlLExå’ŒConvEæ¨¡å‹ï¼Œæ­¤è®ºæ–‡æ–¹æ³•æ˜¯æ·»åŠ åœ¨è¿™äº›æ–¹æ³•ä¸Šçš„ã€‚</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;è¯»å®Œäº†å‰ä¸¤ç« ï¼Œç®€å•çš„çœ‹äº†ä¸€ä¸‹ä½œè€…æå‡ºçš„æ¨¡å‹ï¼Œæ„Ÿè§‰å¹¶æ²¡æœ‰å¤ªå¤§ä»·å€¼ï¼Œå°±æ˜¯ç»™å®ä½“è¾“å…¥å¤šåŠ å…¥äº†ä¸€ä¸ªliteralçš„ä¿¡æ¯ï¼ˆåŠ å…¥æ–¹æ³•å¯ä»¥é‡‡ç”¨çº¿æ€§ã€éçº¿æ€§æˆ–è€…ç¥ç»ç½‘ç»œï¼‰ã€‚&lt;/p&gt;
&lt;p&gt;è¯»è®ºæ–‡å‰éœ€è¦å…ˆç†Ÿæ‚‰DistMultã€ComlLExå’ŒConvEæ¨¡å‹ï¼Œæ­¤è®ºæ–‡æ–¹
      
    
    </summary>
    
      <category term="è®ºæ–‡é˜…è¯»ç¬”è®°" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
      <category term="KG" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/KG/"/>
    
    
      <category term="KGE" scheme="http://yoursite.com/tags/KGE/"/>
    
      <category term="link prediction" scheme="http://yoursite.com/tags/link-prediction/"/>
    
  </entry>
  
  <entry>
    <title>Learning Knowledge Embeddings by Combining Limit-based Scoring Lossé˜…è¯»ç¬”è®°</title>
    <link href="http://yoursite.com/post/Learning_Knowledge_Embeddings_by_Combining_Limit-based_Scoring_Loss/"/>
    <id>http://yoursite.com/post/Learning_Knowledge_Embeddings_by_Combining_Limit-based_Scoring_Loss/</id>
    <published>2019-06-03T02:57:20.000Z</published>
    <updated>2019-06-03T03:06:20.944Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>æ­¤ç¯‡æ–‡ç« æœ€ä¸ºé‡è¦çš„å°±æ˜¯ä½œè€…è®¾è®¡çš„ margin-based ranking loss çš„æ”¹è¿›ï¼Œå¯¹ä¸¤ä¸ªè¶…å‚æ•°$\lambda$å’Œ$\gamma$çš„å®éªŒï¼Œå¯¹äºå®éªŒç»“æœæœ‰å¾ˆå¤šå€¼å¾—åˆ†æä¸æ€è€ƒçš„åœ°æ–¹ã€‚</p><p><a href="https://dl.acm.org/ft_gateway.cfm?id=3132939&amp;ftid=1920664&amp;dwn=1&amp;CFID=135630312&amp;CFTOKEN=71536805165d7c9d-10D2074A-AD9B-A596-5CA31DB63C36A322" target="_blank" rel="noopener">è®ºæ–‡ä¸‹è½½åœ°å€</a></p></blockquote><a id="more"></a><h1 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h1><p>The margin-based ranking loss function cannot ensure the fact that the scoring of correct triplets must be low enough to fulfill the translation.</p><h1 id="research-objective"><a href="#research-objective" class="headerlink" title="research objective"></a>research objective</h1><p>reduce the scoring of correct triplets to fulfill the translation by mending the margin-based ranking loss function</p><h1 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h1><ul><li>proposing a limit-based ranking loss item combined with margin-based ranking loss </li><li>extending TransE and TransH to TransE-RS and TransH-RS</li></ul><h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><h2 id="Margin-based-Tanking-Loss"><a href="#Margin-based-Tanking-Loss" class="headerlink" title="Margin-based Tanking Loss"></a>Margin-based Tanking Loss</h2><p>formula:</p><script type="math/tex; mode=display">L_{R}=\sum_{(h, r, t) \in \Delta} \sum_{\left(h^{\prime}, r, t^{\prime}\right) \in \Delta^{\prime}}\left[\gamma_{1}+f_{r}(h, t)-f_{r}\left(h^{\prime}, t^{\prime}\right)\right) ]_{+}</script><ul><li><p>The margin-based ranking loss function aims to make the score $f_{r}\left(h^{\prime}, t^{\prime}\right)$ of corrupted triplet higher by at least $\gamma_{1}$ than  of positive triplet.</p></li><li><p>cannot be proved $f_{r}(h, t)&lt;\varepsilon$ </p></li></ul><h2 id="Limit-based-Scoring-Loss"><a href="#Limit-based-Scoring-Loss" class="headerlink" title="Limit-based Scoring Loss"></a>Limit-based Scoring Loss</h2><p>formula:</p><script type="math/tex; mode=display">L_{S}=\sum_{(h, r, t) \in \Delta}\left[f_{r}(h, t)-\gamma_{2}\right]_{+}</script><h2 id="Finally-loss"><a href="#Finally-loss" class="headerlink" title="Finally loss"></a>Finally loss</h2><p>formula:</p><script type="math/tex; mode=display">L_{R S}=L_{R}+\lambda L_{S}, \quad(\lambda>0)</script><p>detail is :</p><script type="math/tex; mode=display">\begin{array}{c}{L_{R S}=\sum_{(h, r, t) \in \Delta} \sum_{\left(h^{\prime}, r, t^{\prime}\right) \in \Delta^{\prime}}\left\{\left[\gamma_{1}+f_{r}(h, t)-f_{r}\left(h^{\prime}, t^{\prime}\right)\right]_{+}\right.} \\ {+\lambda\left[f_{r}(h, t)-\gamma_{2}\right]_{+} \}}\end{array}</script><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h2><p><img src="http://image.nysdy.com/20190603155952634332494.jpg" alt="20190603155952634332494.jpg"></p><h2 id="Link-prediction"><a href="#Link-prediction" class="headerlink" title="Link prediction"></a>Link prediction</h2><blockquote><p>æ€è€ƒ</p><p>ä½œè€…åªæ˜¯å¯¹è¡¨æ ¼çš„æ•°æ®è¿›è¡Œäº†é™ˆè¿°ï¼Œæœ‰ä¸€äº›é—®é¢˜å¹¶æ²¡æœ‰è¿›è¡Œåˆ†æè§£é‡Š</p><ul><li>å¹¶æ²¡æœ‰åˆ†ææ¯”å¦‚è¯´ä¸ºä»€ä¹ˆæ”¹è¿›lossåçš„transEä¸ºä»€ä¹ˆä¼šæ¯”TransHï¼ˆRã€Dï¼‰æ•ˆæœè¦å¥½ï¼Ÿ</li><li>ä¸ºä»€ä¹ˆåœ¨n-to-1ä¸­çš„è¡¨ç°æ•ˆæœæ²¡æœ‰è¾¾åˆ°æœ€å¥½ï¼ˆå…¶ä»–çš„éƒ½è¾¾åˆ°äº†æœ€å¥½ï¼‰ï¼Ÿ</li><li>é€šè¿‡è¿™ç§æ”¹è¿›å¯ä»¥å‘ç°ï¼ŒtransHç›¸æ¯”äºTransEå¹¶æ²¡æœ‰æ˜¾è‘—æå‡ï¼ŒåŸå› æ˜¯ä»€ä¹ˆï¼Ÿ</li></ul></blockquote><h2 id="Triple-Classification"><a href="#Triple-Classification" class="headerlink" title="Triple Classification"></a>Triple Classification</h2><ul><li>TransE-RS and TransH-RS have same parameter and operation complexities as TransE and TransH, which is lower than TransR and TransD.</li><li>Our models randomly initial the entities, not use the learned embeddings by TransE as TransR and TransD.<ul><li>It means that our models have much better ability to overcome the problem of overfitting</li></ul></li></ul><h2 id="Distributions-of-Tripletsâ€™-Scores"><a href="#Distributions-of-Tripletsâ€™-Scores" class="headerlink" title="Distributions of Tripletsâ€™ Scores"></a>Distributions of Tripletsâ€™ Scores</h2><h3 id="aim"><a href="#aim" class="headerlink" title="aim"></a>aim</h3><p>analyze the difference between $L_R$ Loss and our $L_RS$ Loss</p><h3 id="Parameters"><a href="#Parameters" class="headerlink" title="Parameters"></a>Parameters</h3><p><img src="http://image.nysdy.com/20190603155952784132260.jpg" alt="20190603155952784132260.jpg"></p><blockquote><p>æ€è€ƒï¼š</p><ul><li>å¯¹äºæˆ‘è‡ªå·±æ­£åœ¨åšçš„å®éªŒï¼šæ˜¯ä¸æ˜¯æˆ‘è‡ªå·±ç”¨çš„é—´éš”å¤ªå°äº†</li></ul></blockquote><h3 id="result"><a href="#result" class="headerlink" title="result"></a>result</h3><p><img src="http://image.nysdy.com/20190603155952821850690.jpg" alt="20190603155952821850690.jpg"></p><p><img src="http://image.nysdy.com/20190603155952848841252.jpg" alt="20190603155952848841252.jpg"></p><blockquote><p>æ€è€ƒ</p><ul><li>è¿™éƒ¨åˆ†çš„å®éªŒå€¼å¾—å€Ÿé‰´ï¼Œå®ƒå¯ä»¥ç›¸å¯¹äºç›´è§‚çš„å¯ä»¥å±•ç¤ºå‡ºä¸ºä»€ä¹ˆæ•ˆæœä¼šå¥½ã€‚</li><li>æ¯”å¦‚å¯¹äºä¸Šè¿°ä¸ºä»€ä¹ˆæ”¹è¿›åçš„transEçš„æ•ˆæœä¼šæ›´å¥½<ul><li>çœ‹åˆ°æœ€åçš„åˆ†æ•°åˆ†å¸ƒtransE-RSçš„åˆ†å¸ƒæ•ˆæœå’ŒTrans-Hçš„ååˆ†æ¥è¿‘ï¼Œ</li><li>è€ŒtransEçš„æ¨¡å‹è¾ƒä¸ºç®€å•ï¼Œå¯èƒ½æœ€ç»ˆlossæœ€å°åŒ–ä¼šä½¿å¾—æ¨¡å‹å……åˆ†è¡¨è¾¾ï¼Œè€Œå…¶ä»–æ¨¡å‹å¼•å…¥äº†æ›´å¤šçš„å‡è®¾å¯èƒ½ä¼šå¸¦æ¥æ›´å¤šçš„å™ªå£°</li><li>ä¹Ÿå¯èƒ½å½“losså¾ˆå°æ—¶ï¼Œå…¶ä»–çš„å‡è®¾æ¡ä»¶å‘æŒ¥ä½œç”¨çš„å¾ˆå°ï¼ˆè‡³å°‘ä»å®éªŒç»“æœæ¥çœ‹æ˜¯çš„ï¼Œä½†æ˜¯è¿˜æœ‰å¾…äºè¿›ä¸€æ­¥è®¾è®¡å®éªŒéªŒè¯ï¼‰</li></ul></li></ul></blockquote><h2 id="Discussion-of-Parameters"><a href="#Discussion-of-Parameters" class="headerlink" title="Discussion of Parameters"></a>Discussion of Parameters</h2><h3 id="Discussion-on-Î³1-and-Î³2"><a href="#Discussion-on-Î³1-and-Î³2" class="headerlink" title="Discussion on Î³1 and Î³2."></a>Discussion on Î³1 and Î³2.</h3><p><img src="http://image.nysdy.com/20190603155952911595962.jpg" alt="20190603155952911595962.jpg"></p><ul><li>We find that Î³2 = 3Î³1 or Î³2 = 4Î³1 is better for link prediction, but for triplet classification there are not obvious characteristics on Î³1 and Î³2.</li><li>a lower Î³2 is expected to ensure the golden condition $\mathbf{h}+\mathbf{r} \approx \mathbf{t}$ for positive triplets, but an entity needs to satisfy many golden coditions at the same time.</li></ul><blockquote><p>æ€è€ƒ</p><ul><li>æ—¢ç„¶å¦‚ä½œè€…è¯´ï¼Œé‚£ä¹ˆç†è®ºä¸ŠtransHçš„æ•ˆæœåº”è¯¥å¾ˆå¥½æ‰å¯¹ï¼Œä½†æ˜¯ç»“æœå¹¶ä¸æ˜¯è¿™æ ·çš„ï¼Œè¿™åˆäº§ç”ŸçŸ›ç›¾ã€‚</li></ul></blockquote><h3 id="Discussion-on-Î»"><a href="#Discussion-on-Î»" class="headerlink" title="Discussion on Î»"></a>Discussion on Î»</h3><p><img src="http://image.nysdy.com/20190603155953002076189.jpg" alt="20190603155953002076189.jpg"></p><blockquote><p>æ€è€ƒ</p><ul><li>çœ‹åˆ°Î»å¹¶æ²¡æœ‰å¯¹æ¨¡å‹å½±å“å¹¶æ²¡å¾ˆå¤§</li><li>Î»åœ¨1å·¦å³æ˜¯æ•ˆæœä¼šæ¯”è¾ƒå¥½</li><li>Î»å’Œmarginä¼šä¸ä¼šäº§ç”Ÿå…³è”ï¼Ÿ</li></ul></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;æ­¤ç¯‡æ–‡ç« æœ€ä¸ºé‡è¦çš„å°±æ˜¯ä½œè€…è®¾è®¡çš„ margin-based ranking loss çš„æ”¹è¿›ï¼Œå¯¹ä¸¤ä¸ªè¶…å‚æ•°$\lambda$å’Œ$\gamma$çš„å®éªŒï¼Œå¯¹äºå®éªŒç»“æœæœ‰å¾ˆå¤šå€¼å¾—åˆ†æä¸æ€è€ƒçš„åœ°æ–¹ã€‚&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://dl.acm.org/ft_gateway.cfm?id=3132939&amp;amp;ftid=1920664&amp;amp;dwn=1&amp;amp;CFID=135630312&amp;amp;CFTOKEN=71536805165d7c9d-10D2074A-AD9B-A596-5CA31DB63C36A322&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;è®ºæ–‡ä¸‹è½½åœ°å€&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="è®ºæ–‡é˜…è¯»ç¬”è®°" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
      <category term="KG" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/KG/"/>
    
    
      <category term="margin loss" scheme="http://yoursite.com/tags/margin-loss/"/>
    
      <category term="transE" scheme="http://yoursite.com/tags/transE/"/>
    
      <category term="transH" scheme="http://yoursite.com/tags/transH/"/>
    
      <category term="KG" scheme="http://yoursite.com/tags/KG/"/>
    
  </entry>
  
  <entry>
    <title>Knowledge Graph Embedding by Translating on Hyperplanesé˜…è¯»ç¬”è®°</title>
    <link href="http://yoursite.com/post/Knowledge%20Graph%20Embedding%20by%20Translating%20on%20Hyperplanes/"/>
    <id>http://yoursite.com/post/Knowledge Graph Embedding by Translating on Hyperplanes/</id>
    <published>2019-05-28T08:17:44.000Z</published>
    <updated>2019-06-01T05:38:20.042Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>ä½œä¸ºtransç³»åˆ—ç»å…¸æ–‡çŒ®ï¼Œå¿…è¯»ã€‚æ–‡ç« ä¸»è¦ç²¾ååœ¨äºè¿™ç§è¶…å¹³é¢æƒ³æ³•çš„ç”±æ¥è§£å†³äº†åŒä¸€å®ä½“çš„å¤šå…³ç³»é—®é¢˜ã€‚</p><p>Authors proposed TransH which models a relation as a hyperplane together with a translation operation on it. It solves the problem of multi-relation and makes a good trade-off between model capacity and efficiency.</p></blockquote><a id="more"></a><h1 id="æ¨æµ‹transHçš„æƒ³æ³•æ¥æº"><a href="#æ¨æµ‹transHçš„æƒ³æ³•æ¥æº" class="headerlink" title="æ¨æµ‹transHçš„æƒ³æ³•æ¥æº"></a>æ¨æµ‹transHçš„æƒ³æ³•æ¥æº</h1><blockquote><p>æ—¢ç„¶å®é™…æ˜¯è¡¨è¾¾åŒä¸€å…³ç³»ä¸åŒå®ä½“æœ€åé€šè¿‡TransEåä¼šè¶‹äºä¸€è‡´ï¼Œé‚£ä¹ˆæˆ‘ç›´æ¥é€šè¿‡ä¸€ä¸ªä¸­ä»‹æ¥è¿›è¡Œæ˜ å°„å°†åŒä¸€è¡¨ç¤ºæ˜ å°„æˆä¸åŒå‘é‡è¡¨ç¤ºï¼Œé‚£ä¹ˆè¿™äº›å‘é‡è¡¨ç¤ºå°±å¯ä»¥ä»£è¡¨ä¸åŒçš„å®ä½“ï¼Œå°±è¾¾åˆ°äº†ä¸åŒå®ä½“æ‹¥æœ‰ä¸åŒè¡¨ç¤ºçš„ç›®çš„ã€‚å› ä¸ºå…³ç³»æ˜¯ä¸å˜çš„æ‰€ä»¥æƒ³åˆ°äº†å°†å…³ç³»ä½œä¸ºæ˜ å°„å¹³é¢ï¼Œè®©å®ä½“å‘é‡å‘å…¶ä¸­æ˜ å°„ã€‚</p></blockquote><h1 id="research-objective"><a href="#research-objective" class="headerlink" title="research objective"></a>research objective</h1><ul><li>solves the problem of multi-relation </li><li>makes a good trade-off between model capacity and efficiency</li></ul><h1 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h1><ul><li>TransE canâ€™t deal with reflexive, one-to-many, many-to-many and many -to-one relations</li><li>some complex model sacrifice efficiency in the process(although can deal with transEâ€™s problem)</li></ul><h1 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h1><ul><li>proposing a method named <em>translation on hyperplanes</em>(TransH)<ul><li>interpreting a relation as a translating operation on a hyperplane</li></ul></li><li>proposing a simple trick to reduce the chance of false negative labeling</li></ul><h1 id="Embedding-by-Translating-on-Hyperplanes"><a href="#Embedding-by-Translating-on-Hyperplanes" class="headerlink" title="Embedding by Translating on Hyperplanes"></a>Embedding by Translating on Hyperplanes</h1><h2 id="Relationsâ€™-Mapping-Properties-in-Embedding"><a href="#Relationsâ€™-Mapping-Properties-in-Embedding" class="headerlink" title="Relationsâ€™ Mapping Properties in Embedding"></a>Relationsâ€™ Mapping Properties in Embedding</h2><p>transE</p><ul><li>the representation of an  entity is the same when involved in any relations, ignoring <strong>distributed representations of entities when invovled in different relaions</strong></li></ul><h2 id="Translating-on-Hyperplanes-TransH"><a href="#Translating-on-Hyperplanes-TransH" class="headerlink" title="Translating on Hyperplanes (TransH)"></a>Translating on Hyperplanes (TransH)</h2><p><strong>åŒä¸€ä¸ªå®ä½“åœ¨ä¸åŒå…³ç³»ä¸­çš„æ„ä¹‰ä¸åŒ</strong>ï¼ŒåŒæ—¶<strong>ä¸åŒå®ä½“ï¼Œåœ¨åŒä¸€å…³ç³»ä¸­çš„æ„ä¹‰ï¼Œä¹Ÿå¯ä»¥ç›¸åŒ</strong>ã€‚</p><blockquote><p>å°†æ¯ä¸ªå…³ç³»å®šä¹‰åœ¨ä¸€ä¸ªç‹¬ç‰¹çš„å¹³é¢å‘¢ï¼Œåœ¨è¯¥å¹³é¢å†…æœ‰ç¬¦åˆè¯¥å…³ç³»çš„transEçš„è¡¨ç¤ºï¼ˆh,r,t)ï¼Œå¤šåŠ å…¥çš„ä»£è¡¨è¯¥å¹³é¢çš„æ³•å‘é‡å®Œæˆäº†å°†ä¸åŒå®ä½“å‘å¹³é¢å†…å’Œhï¼Œtè½¬åŒ–çš„ä»»åŠ¡ï¼Œä½¿å¾—åŒä¸€å…³ç³»çš„ä¸åŒå®ä½“æ‹¥æœ‰ä¸åŒçš„è¡¨ç¤ºï¼Œä½†æ˜¯åœ¨å…³ç³»å¹³é¢å†…çš„æŠ•å½±ç›¸åŒï¼›åŒä¸€å®ä½“å¯ä»¥åœ¨ä¸åŒçš„å…³ç³»å¹³é¢å†…æ‹¥æœ‰ä¸åŒçš„å«ä¹‰ï¼ˆå¹³é¢å†…çš„æŠ•å½±ï¼‰</p></blockquote><p><img src="http://image.nysdy.com/20190601155935483248827.jpg" alt="20190601155935483248827.jpg"></p><p>å¦‚å›¾æ‰€ç¤ºï¼Œå¯¹äºæ­£ç¡®çš„ä¸‰å…ƒç»„æ¥è¯´$(h, r, t) \in \Delta$ï¼Œæ‰€éœ€æ»¡è¶³çš„å…³ç³»å¦‚å›¾æ‰€ç¤ºã€‚é‚£ä¹ˆå¯¹äºä¸€ä¸ªå®ä½“$hâ€™â€™$å¦‚æœæ»¡è¶³$\left(h^{\prime \prime}, r, t\right) \in \Delta    $ï¼Œåœ¨transEä¸­æ˜¯éœ€è¦$hâ€™â€™=h$ï¼Œè€Œåœ¨transHä¸­åˆ™å°†çº¦æŸæ”¾å®½åˆ°$h,hâ€™â€™$åœ¨$W_r$ä¸Šçš„æŠ•å½±ç›¸åŒå°±å¯ä»¥äº†ï¼Œä¹Ÿå¯ä»¥å®ç°å°†$h,hâ€™â€™$åŒºåˆ†å¼€å¹¶ä¸”å…·æœ‰ä¸åŒçš„è¡¨ç¤ºã€‚</p><h4 id="ç›®æ ‡å‡½æ•°"><a href="#ç›®æ ‡å‡½æ•°" class="headerlink" title="ç›®æ ‡å‡½æ•°"></a>ç›®æ ‡å‡½æ•°</h4><p>scoring functionï¼š</p><script type="math/tex; mode=display">d(h+r, t)=f_{r}(h, t)=\left\|h_{\perp}+d_{r}-t_{\perp}\right\|_{2}^{2}</script><p>As the hyperplane $W_r$, the $w_r$ is the normal vector of it, and $\left|w_{r}\right|_{2}^{2}=1$, so the projection $h$ in $w_r$ is:</p><script type="math/tex; mode=display">h_{w_{r}}=w_r^{T} h w_r</script><p>å…¶ä¸­ï¼Œ$w_r^{T} h=|w_r||h| \cos \theta$å¯ä»¥è¡¨ç¤º$h$åœ¨$w_r$ä¸Šçš„æŠ•å½±çš„é•¿åº¦å’Œ$w_r$é•¿åº¦çš„ä¹˜ç§¯ï¼Œå› ä¸º$\left|w_{r}\right|_{2}^{2}=1$,æ‰€ä»¥å¯ä»¥ä»£è¡¨æŠ•å½±çš„é•¿åº¦ï¼Œå†ä¹˜ä¸Šå•ä½å‘é‡å³å¯è¡¨ç¤ºæŠ•å½±å‘é‡ã€‚æ‰€ä»¥ï¼š</p><script type="math/tex; mode=display">\mathbf{h}_{\perp}=\mathbf{h}-\mathbf{w}_{r}^{\top} \mathbf{h w}_{r}, \quad \mathbf{t}_{\perp}=\mathbf{t}-\mathbf{w}_{r}^{\top} \mathbf{t} \mathbf{w}_{r}</script><p>å¦‚å›¾æ‰€ç¤ºï¼š<img src="http://image.nysdy.com/2019060115593616504994.jpg" alt="2019060115593616504994.jpg"></p><p>the score function is:</p><script type="math/tex; mode=display">f_{r}(\mathbf{h}, \mathbf{t})=\left\|\left(\mathbf{h}-\mathbf{w}_{r}^{\top} \mathbf{h w}_{r}\right)+\mathbf{d}_{r}-\left(\mathbf{t}-\mathbf{w}_{r}^{\top} \mathbf{t} \mathbf{w}_{r}\right)\right\|_{2}^{2}</script><h4 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h4><p>loss function consists of margin-based ranking loss and some constraints:</p><script type="math/tex; mode=display">\begin{aligned} \mathcal{L} &=\sum_{(h, r, t) \in \Delta\left(h^{\prime}, r^{\prime}, t^{\prime}\right) \in \Delta_{(h, r, t)}}\left[f_{r}(\mathbf{h}, \mathbf{t})+\gamma-f_{r^{\prime}}\left(\mathbf{h}^{\prime}, \mathbf{t}^{\prime}\right)\right]_{+} \\ &+C\left\{\sum_{e \in E}\left[\|\mathbf{e}\|_{2}^{2}-1\right]_{+}+\sum_{r \in R}\left[\frac{\left(\mathbf{w}_{r}^{\top} \mathbf{d}_{r}\right)^{2}}{\left\|\mathbf{d}_{r}\right\|_{2}^{2}}-\epsilon^{2}\right]_{+}\right\}, \text { (4) } \end{aligned}</script><p>the constraints:</p><script type="math/tex; mode=display">\forall e \in E,\|\mathrm{e}\|_{2} \leq 1, // \text { scale }\\\forall r \in R,\left|\mathbf{w}_{r}^{\top} \mathbf{d}_{r}\right| /\left\|\mathbf{d}_{r}\right\|_{2} \leq \epsilon, / / \text { orthogonal }\\\forall r \in R,\left\|\mathbf{w}_{r}\right\|_{2}=1, / / \text { unit normal vector }</script><ul><li><strong>the second grantees the translation vectot $d_r$ is in the hyperplane</strong></li><li>they project each $w_r$ to unit $l_2$-ball before visiting each mini-batch</li></ul><blockquote><p>æ—¢ç„¶transHå¯ä»¥å®Œæˆå°†åŒä¸€å®ä½“æ˜ å°„åˆ°ä¸åŒçš„å…³ç³»å¹³é¢æ¥è·å¾—ä¸åŒçš„å«ä¹‰ï¼Œé‚£ä¹ˆæˆ‘è§‰å¾—</p><ul><li>æ˜¯ä¸æ˜¯ä¸åŒä»£è¡¨åŒä¸€å«ä¹‰çš„æŠ•å½±è¡¨ç¤ºåº”è¯¥ç›¸åŒæˆ–è€…ç›¸ä¼¼</li><li>è¿™æ ·æ˜¯ä¸æ˜¯å¯ä»¥è§£å†³åŒä¸€ä¸ªå®ä½“çš„å¤šä¹‰æ€§é—®é¢˜ã€‚</li></ul></blockquote><h2 id="Reducing-Ralse-Negative-Labels"><a href="#Reducing-Ralse-Negative-Labels" class="headerlink" title="Reducing Ralse Negative Labels"></a>Reducing Ralse Negative Labels</h2><p>Authors set different probabilities for replacing the head or tail entity depending on the mapping property of the relation (one-to-many, many-to-one, many-to-many)</p><ul><li><p>give more chance to replacing the head entity if the relation is one-to-many</p><ul><li>åˆ†åˆ«ç»Ÿè®¡æ¯ä¸ªå¤´å®ä½“å¯¹åº”å°¾å®ä½“çš„æ•°é‡ï¼ˆåä¹‹äº¦ç„¶ï¼‰ï¼ŒæŒ‰å æ¯”è¿›è¡Œç”Ÿæˆè´Ÿæ ·ä¾‹</li></ul></li></ul><blockquote><ul><li>é€šè¿‡è¿™æ ·çš„æ–¹å¼ï¼Œä¾‹å¦‚one-manyå…³ç³»ï¼Œæ›¿æ¢å¤´å®ä½“æ˜¾ç„¶æ›´ä¸å®¹æ˜“å¾—åˆ°æ­£æ ·ä¾‹ï¼ˆå› ä¸ºåªæœ‰ä¸€ç§å¤´å®ä½“æ˜¯å¯¹çš„ï¼Œç„¶è€Œæ›¿æ¢å°¾å®ä½“å› ä¸ºå¯¹äºå¤´å®ä½“å¯¹åº”è¯¥å…³ç³»çš„å°¾å®ä½“æ›´å¤šï¼Œè¯´ä¸å®šå°±æœ‰å…¶ä»–ä¸åœ¨æ­¤manyä¸­çš„å°¾å®ä½“ç¬¦åˆè¿™ä¸ªå…³ç³»ã€‚</li><li>ç›¸æ¯”ä¹‹ä¸‹æˆ‘è®¤ä¸ºåœ¨ã€ŠBootstrapping-Entity-Alignment-with-Knowledge-Graph-Embeddingã€‹é‡‡ç”¨çš„å‡åŒ€æˆªæ–­è´Ÿé‡‡æ ·æ•ˆæœä¼šæ›´å¥½ä¸€äº›</li></ul></blockquote><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>the detail can be seen in the paper</p><h2 id="Link-prediction"><a href="#Link-prediction" class="headerlink" title="Link prediction"></a>Link prediction</h2><h3 id="outperform-TransE-in-one-to-one"><a href="#outperform-TransE-in-one-to-one" class="headerlink" title="outperform TransE in one-to-one"></a>outperform TransE in one-to-one</h3><p>Authors explain:</p><ul><li>entities are connected with relations so that better embeddings of some parts lead to better results on the whole.</li></ul><blockquote><p>æˆ‘æ˜¯è§‰å¾—æœ‰äº›ç‰µå¼ºï¼Œä¸è¿‡è¦æ˜¯ç¡¬ç†è§£ä¹Ÿæ˜¯å¯ä»¥ï¼Œæ¯•ç«Ÿé€šè¿‡æŠ•å½±ç›¸å½“äºæŠŠå®ä½“å’Œå…³ç³»è¿›è¡Œäº†ä¸€ä¸ªè”ç³»ï¼Œå¯èƒ½è¿™ä¸ªå¢å¼ºäº†æ•ˆæœã€‚</p></blockquote><h2 id="Triplets-Classification"><a href="#Triplets-Classification" class="headerlink" title="Triplets Classification"></a>Triplets Classification</h2><p>This means FB13 is a very dense subgraph where strong correlations exist between entities</p><h2 id="Relational-Fact-Extraction-from-Text"><a href="#Relational-Fact-Extraction-from-Text" class="headerlink" title="Relational Fact Extraction from Text"></a>Relational Fact Extraction from Text</h2><ul><li>Actually, knowledge graph embedding is able to score a candidate fact, without observing any evidence from ex- ternal text corpus</li></ul><blockquote><p>å¯ä»¥çœ‹åˆ°ä»14å¹´å¼€å§‹å°±æœ‰åˆ©ç”¨çŸ¥è¯†å›¾è°±æ¥ä»æ–‡æœ¬æŠ½å–å…³ç³»ï¼Œæœ€è¿‘è¿™ä¸ªåº”ç”¨å¥½åƒåˆæœ‰èµ·è‰²ï¼Œè¿™ä¸ªä¹Ÿå¯ä½œä¸ºè‡ªå·±å®éªŒçš„ä¸€éƒ¨åˆ†ã€‚</p></blockquote><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://blog.csdn.net/MonkeyDSummer/article/details/85273843" target="_blank" rel="noopener">https://blog.csdn.net/MonkeyDSummer/article/details/85273843</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;ä½œä¸ºtransç³»åˆ—ç»å…¸æ–‡çŒ®ï¼Œå¿…è¯»ã€‚æ–‡ç« ä¸»è¦ç²¾ååœ¨äºè¿™ç§è¶…å¹³é¢æƒ³æ³•çš„ç”±æ¥è§£å†³äº†åŒä¸€å®ä½“çš„å¤šå…³ç³»é—®é¢˜ã€‚&lt;/p&gt;
&lt;p&gt;Authors proposed TransH which models a relation as a hyperplane together with a translation operation on it. It solves the problem of multi-relation and makes a good trade-off between model capacity and efficiency.&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="è®ºæ–‡é˜…è¯»ç¬”è®°" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
      <category term="KG" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/KG/"/>
    
    
      <category term="KGE" scheme="http://yoursite.com/tags/KGE/"/>
    
      <category term="transH" scheme="http://yoursite.com/tags/transH/"/>
    
  </entry>
  
  <entry>
    <title>Attention Is All You Needé˜…è¯»ç¬”è®°</title>
    <link href="http://yoursite.com/post/Attention%20Is%20All%20You%20Need/"/>
    <id>http://yoursite.com/post/Attention Is All You Need/</id>
    <published>2019-05-25T05:57:35.000Z</published>
    <updated>2019-05-28T07:58:49.154Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>transformer æ˜¯ä¸€ä¸ªå®Œå…¨ç”±æ³¨æ„åŠ›æœºåˆ¶ç»„æˆçš„æ­å»ºçš„æ¨¡å‹ï¼Œæ¨¡å‹å¤æ‚åº¦ä½ï¼Œå¹¶å¯ä»¥è¿›è¡Œå¹¶è¡Œè®¡ç®—ï¼Œä½¿å¾—è®¡ç®—é€Ÿåº¦å¿«ã€‚åœ¨ç¿»è¯‘æ¨¡å‹ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ•ˆæœã€‚æœ¬ç¯‡è®ºæ–‡å±äºç»å…¸å¿…è¯»è®ºæ–‡ï¼Œé˜…è¯»ç¬”è®°ä¸­å¯¹ä¸€äº›ä¸æ¸…æ¥šçš„åœ°æ–¹è¿›è¡Œäº†æ±‰è¯­è§£é‡Šï¼Œè¯»å®Œè®ºæ–‡åé˜…è¯»å‚è€ƒé“¾æ¥ä»¥åŠ æ·±ç†è§£ã€‚</p><p><a href="https://arxiv.org/pdf/1706.03762" target="_blank" rel="noopener">è®ºæ–‡ä¸‹è½½åœ°å€</a></p></blockquote><a id="more"></a><h1 id="research-objective"><a href="#research-objective" class="headerlink" title="research objective"></a>research objective</h1><p>based solely on attention mechanisms, increase parallezable computation and decrease train time</p><h1 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h1><p>recurrent models hidden states depended on previous hidden state and the input for position precludes parallelization</p><h1 id="contribution"><a href="#contribution" class="headerlink" title="contribution"></a>contribution</h1><ul><li>Transformer,<ul><li>eschewing recurrence and instead relying entirely on an attention mechanism, solve the long dependency problem.</li><li>draw global dependecies between input and output </li><li>allow for significantly more parallelization</li></ul></li></ul><h1 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h1><p>The Transformer uses stacked self-attention and point-wise, fully connected layers for both the encoder and decoder.<img src="http://image.nysdy.com/2019052515587656321218.jpg" alt="2019052515587656321218.jpg"></p><h2 id="Encoder-and-Decoder-Stacks"><a href="#Encoder-and-Decoder-Stacks" class="headerlink" title="Encoder and Decoder Stacks"></a>Encoder and Decoder Stacks</h2><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><ul><li>compose of a stack of N identical layers</li><li>each layers has two sub-layers<ol><li>multi-head self-attention mechanism</li><li>position-wise fully connected feed forward network</li></ol></li><li>employ a residual connection around each of the two sub-layers, followed by layer normalization</li><li>the output of each sub-layer is $\text { LayerNorm }(x+\text { Sublayer }(x))$</li><li>encoderä¸­çš„Qï¼ŒKï¼ŒVéƒ½æ˜¯å­¦å‡ºæ¥çš„</li></ul><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><ul><li>composed of a stack of N identical layers</li><li>has the same two sub-layers as the encoder</li><li>the third sub-layer between the two sub-layers<ul><li>perform multi-head attention over the output of the encoder stack</li></ul></li><li>add a mask to modify the self-attention sub-layer to ensure that the predictions for position $i$ can depend only the known outputs at positions less than $i$</li><li>é™¤äº†ç¬¬ä¸€å­å±‚ä¸­Qï¼ŒKï¼ŒVæ˜¯è‡ªå·±å­¦å‡ºæ¥çš„ï¼Œç¬¬äºŒä¸ªå­å±‚åˆ©ç”¨äº†encoderä¸­çš„Kï¼ŒVã€‚</li></ul><h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p><img src="http://image.nysdy.com/20190526155885488441950.jpg" alt="20190526155885488441950.jpg"></p><h3 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h3><p>the calculation process as the left at the figure 2. <strong>formulaï¼š</strong></p><script type="math/tex; mode=display">\text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V</script><ul><li>where $\sqrt{d_{k}}$ is to  prevent value from getting too large, which will push the softmax function into regions where it has extremely small gradients. å› ä¸ºé‡çº§å¤ªå¤§ï¼Œsoftmaxåå°±é0å³1äº†ï¼Œä¸å¤Ÿâ€œsoftâ€äº†ã€‚ä¹Ÿä¼šå¯¼è‡´softmaxçš„æ¢¯åº¦éå¸¸å°ã€‚ä¹Ÿå°±æ˜¯è®©softmaxç»“æœ<strong>ä¸ç¨€ç–</strong>(é—®å·è„¸ï¼Œé€šå¸¸äººä»¬å¸Œæœ›å¾—åˆ°æ›´ç¨€ç–çš„attentionå§)ã€‚</li><li>$Q, K,V$ is a matrix needed to learn from input.</li></ul><h3 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h3><p><strong>helps the encoder look at other words in the input sentence as it encodes a specific word</strong></p><p>in the figure 2 right. </p><ul><li>itâ€™s beneficial to <strong>lineraly project</strong> the quries, keys and values $h$ times with different, learned projections to $d_k, d_k, d_v$ dimensions, respectively</li><li>concatenate the output </li></ul><script type="math/tex; mode=display">\begin{aligned} \text { MultiHead }(Q, K, V) &=\text { Concat (head }_{1}, \ldots, \text { head }_{h} ) W^{O} \\ \text { where head }_{i} &=\text { Attention }\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right) \end{aligned}</script><p>where $W_{i}^{Q} \in \mathbb{R}^{d_{\text { model }} \times d_{k}}, W_{i}^{K} \in \mathbb{R}^{d_{\text { model }} \times d_{k}}, W_{i}^{V} \in \mathbb{R}^{d_{\text { model }} \times d_{v}}, W^{O} \in \mathbb{R}^{h d_{v} \times d_{\mathrm{model}}}$</p><h3 id="Applications-of-Attention-in-our-Model"><a href="#Applications-of-Attention-in-our-Model" class="headerlink" title="Applications of Attention in our Model"></a>Applications of Attention in our Model</h3><ul><li>the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. mimicing the seq-to-seq</li><li>self -attention can make that each position in the encoder can attend to all positions in the previous layer of the encoder</li><li><strong>We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to âˆ’âˆ) all values in the input of the softmax which correspond to illegal connections. See Figure 2</strong>ã€‚å³æˆ‘ä»¬åªèƒ½attendåˆ°å‰é¢å·²ç»ç¿»è¯‘è¿‡çš„è¾“å‡ºçš„è¯è¯­ï¼Œå› ä¸ºç¿»è¯‘è¿‡ç¨‹æˆ‘ä»¬å½“å‰è¿˜å¹¶ä¸çŸ¥é“ä¸‹ä¸€ä¸ªè¾“å‡ºè¯è¯­ï¼Œè¿™æ˜¯æˆ‘ä»¬ä¹‹åæ‰ä¼šæ¨æµ‹åˆ°çš„ã€‚å³å°†$QK^T$ä¸­æ¯è¡Œè¯¥å•è¯ä¹‹åçš„æ•°å€¼åšå¤„ç†ï¼Œä½¿å¾—å‰é¢çš„å•è¯çœ‹ä¸åˆ°åé¢å•è¯æ‰€å çš„é‡è¦æ€§ç¨‹åº¦ã€‚</li></ul><h2 id="Position-wise-Feed-Forward-Networks"><a href="#Position-wise-Feed-Forward-Networks" class="headerlink" title="Position-wise Feed-Forward Networks"></a>Position-wise Feed-Forward Networks</h2><ul><li>applied to each position separately and identically</li><li>feed-forward network consists of tow linear transformations with a ReLU activation. formula:</li></ul><script type="math/tex; mode=display">\mathrm{FFN}(x)=\max \left(0, x W_{1}+b_{1}\right) W_{2}+b_{2}</script><blockquote><p><strong>å°ç»“</strong></p><ol><li>ä¸ºä»€ä¹ˆå«å¼ºè°ƒ<code>position-wise</code>?<ul><li>è§£é‡Šä¸€: è¿™é‡ŒFFNå±‚æ˜¯æ¯ä¸ªpositionè¿›è¡Œç›¸åŒä¸”ç‹¬ç«‹çš„æ“ä½œï¼Œæ‰€ä»¥å«position-wiseã€‚å¯¹æ¯ä¸ªpositionç‹¬ç«‹åšFFNã€‚</li><li>è§£é‡ŠäºŒï¼šä»å·ç§¯çš„è§’åº¦è§£é‡Šï¼Œè¿™é‡Œçš„FFNç­‰ä»·äºkernel_size=1çš„å·ç§¯ï¼Œè¿™æ ·æ¯ä¸ªpositionéƒ½æ˜¯ç‹¬ç«‹è¿ç®—çš„ã€‚å¦‚æœkernel_size=2ï¼Œæˆ–è€…å…¶ä»–ï¼Œpositionä¹‹é—´å°±å…·æœ‰ä¾èµ–æ€§äº†ï¼Œè²Œä¼¼å°±ä¸èƒ½å«åšposition-wiseäº†</li></ul></li><li>ä¸ºä»€ä¹ˆè¦é‡‡ç”¨å…¨è¿æ¥å±‚ï¼Ÿ<ul><li>ç›®çš„: å¢åŠ éçº¿æ€§å˜æ¢</li><li>å¦‚æœä¸é‡‡ç”¨FFNå‘¢ï¼Ÿæœ‰ä»€ä¹ˆæ›¿ä»£çš„è®¾è®¡ï¼Ÿ</li></ul></li><li>ä¸ºä»€ä¹ˆé‡‡ç”¨2å±‚å…¨è¿æ¥ï¼Œè€Œä¸”ä¸­é—´å‡ç»´ï¼Ÿ<ul><li>è¿™ä¹Ÿæ˜¯æ‰€è°“çš„bottle neckï¼Œåªä¸è¿‡ä½ç»´åœ¨IOä¸Šï¼Œä¸­é—´é‡‡ç”¨high rank</li></ul></li></ol></blockquote><h2 id="Embeddings-and-Softmax"><a href="#Embeddings-and-Softmax" class="headerlink" title="Embeddings and Softmax"></a>Embeddings and Softmax</h2><p>Sharing the same weight maatrix between the two embedding layers and the pre-softmax linear transformation</p><h2 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h2><p>Using sine and xosine functions of different frequencies:</p><script type="math/tex; mode=display">P E_{(p o s, 2 i)}=\sin \left(p o s / 10000^{2 i / d_{\text { model }}}\right)\\P E_{(p o s, 2 i+1)}=\cos \left(p o s / 10000^{2 i / d_{\mathrm{model}}}\right)</script><ul><li><p>where $pos$ is the postiiton and $i$ is the dimension</p></li><li><p><strong>Authors hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$, $PE_{pos+k}$can be represented as a linear function of $PE_{pos}$</strong></p><p>ä½†åœ¨è¯­è¨€ä¸­ï¼Œ<code>ç›¸å¯¹ä½ç½®</code>ä¹Ÿå¾ˆé‡è¦ï¼ŒGoogleé€‰æ‹©å‰è¿°çš„ä½ç½®å‘é‡å…¬å¼çš„ä¸€ä¸ªé‡è¦åŸå› æ˜¯ï¼šç”±äºæˆ‘ä»¬æœ‰$\sin (\alpha+\beta)=\sin \alpha \cos \beta+\cos \alpha \sin \beta$ä»¥åŠ$\cos (\alpha+\beta)=\cos \alpha \cos \beta-\sin \alpha \sin \beta$ï¼Œè¿™è¡¨æ˜ä½ç½®$p+k$çš„å‘é‡å¯ä»¥è¡¨ç¤ºæˆä½ç½®$p$çš„å‘é‡çš„çº¿æ€§å˜æ¢ï¼Œè¿™æä¾›äº†è¡¨è¾¾ç›¸å¯¹ä½ç½®ä¿¡æ¯çš„å¯èƒ½æ€§ã€‚</p></li><li><p>Compared with using learned positional embeddings, the sinusoidal version may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.</p></li></ul><p>æ³¨æ„ç”±äºè¯¥æ¨¡å‹æ²¡æœ‰recurrenceæˆ–convolutionæ“ä½œï¼Œæ‰€ä»¥æ²¡æœ‰æ˜ç¡®çš„å…³äºå•è¯åœ¨æºå¥å­ä¸­ä½ç½®çš„ç›¸å¯¹æˆ–ç»å¯¹çš„ä¿¡æ¯ï¼Œä¸ºäº†æ›´å¥½çš„è®©æ¨¡å‹å­¦ä¹ ä½ç½®ä¿¡æ¯ï¼Œæ‰€ä»¥æ·»åŠ äº†position encodingå¹¶å°†å…¶å åŠ åœ¨word embeddingä¸Šã€‚</p><h1 id="Why-Self-Attention"><a href="#Why-Self-Attention" class="headerlink" title="Why Self-Attention"></a>Why Self-Attention</h1><ul><li>total computational complexity per layer</li><li>the amount of computation that can be parallelized</li><li>the path between long-range dependencies in the network</li></ul><p><img src="http://image.nysdy.com/20190527155892126287777.jpg" alt="20190527155892126287777.jpg"></p><p><img src="http://image.nysdy.com/20190528155902465937774.jpg" alt="20190528155902465937774.jpg"></p><p>self-attention|ï¼š</p><ul><li>$QK^TV$ç›¸ä¹˜ï¼Œæ ¹æ®çŸ©é˜µå¤§å°ï¼ˆåˆ†åˆ«ä¸º$n<em>d, n</em>d, n<em>d$éœ€è¦çš„å¤æ‚åº¦ä¸º$O(n^2d</em>2)$ï¼ˆå¿½ç•¥softmaxï¼‰</li><li>maximum path lengthï¼šå›¾è¯´æ˜äº†ï¼Œ å¯¹äºself-attention, target node (ç”Ÿæˆçš„é‚£ä¸ªç‚¹) å®é™…ä¸Šå’Œ è¾“å…¥ä¸­çš„ä»»æ„ä¸€ç‚¹çš„è·ç¦»æ˜¯ç›¸åŒçš„</li></ul><p>convolutional:  </p><ul><li><p>æ¯å±‚æœ‰kä¸ªå·ç§¯å’Œï¼Œå¯¹äºinput matixï¼ˆ$n<em>d$)çŸ©é˜µæ‰§è¡Œå·ç§¯éœ€è¦è¿ç®—å¤æ‚åº¦æ˜¯$n</em>d*(d-m)$, mä¸ºå·ç§¯å’Œå®½åº¦æ˜¯ä¸€ä¸ªæ¯”è¾ƒå°çš„å¸¸æ•°ï¼Œæ‰€ä»¥æ€»å¤æ‚åº¦ä¸º$O\left(k \cdot n \cdot d^{2}\right)$,ä½œè€…æåˆ°å¯åˆ†ç¦»çš„å·åŸºå±‚æš‚æ—¶è¿˜ä¸äº†è§£ï¼Œå¯ä»¥ä»¥åæŸ¥é˜…ã€‚</p></li><li><p>maximum path length: æ­£å¸¸å·ç§¯å’Œçš„è·ç¦»æ˜¯$O(n/k)$, ä½†å¦‚æœæ˜¯å †å å·ç§¯å¦‚å›¾ï¼šã€€ã€€<img src="http://image.nysdy.com/2019052815590251436862.jpg" alt="2019052815590251436862.jpg"></p><p>å°±å¯ä»¥å‡å°åˆ°$O\left(\log _{k}(n)\right)$</p></li></ul><p>recurrent:</p><ul><li>è®¡ç®—æ˜¯æ¯ä¸ªè¯å‘é‡ä¹˜éšè—æƒé‡($d*d$)ï¼Œæ‰€ä»¥æ˜“å¾—è®¡ç®—å¤æ‚åº¦ï¼š$O\left(n \cdot d^{2}\right)$</li><li>maximum path length: é•¿åº¦å°±æ˜¯nã€‚</li><li>æ“ä½œæ­¥éª¤è¦ä»ç¬¬ä¸€ä¸ªåˆ°ç¬¬nä¸ªä¸ºnæ­¥ï¼Œæ˜¯æœ‰é¡ºåºçš„ã€‚å…¶ä»–çš„éƒ½æ²¡æœ‰é¡ºåºè¦æ±‚</li></ul><p>self-attentin(restricted)</p><ul><li>ç›¸å½“äºåªè¾“å…¥ré‚»è¿‘çš„å¥å­é•¿åº¦ï¼Œè‡ªç„¶å¯ä»¥å¾—åˆ°å¦‚å›¾ç»“æœ</li></ul><h1 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h1><h2 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h2><script type="math/tex; mode=display">\text { lrate }=d_{\text { model }}^{-0.5} \cdot \min \left(\text {step}_{-} n u m^{-0.5}, \text { step }_{-} n u m \cdot \text { warmup steps }^{-1.5}\right)</script><ul><li>increasing the learning rate linearly for the first warmup_steps training steps</li><li>decreasing it thereafter proportionally to the inverse square root of the step number</li></ul><h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><h3 id="Residual-Dropout"><a href="#Residual-Dropout" class="headerlink" title="Residual Dropout"></a>Residual Dropout</h3><ul><li>apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized</li><li>apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks</li></ul><h3 id="Label-Smoothing"><a href="#Label-Smoothing" class="headerlink" title="Label Smoothing"></a>Label Smoothing</h3><ul><li>This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score</li></ul><h1 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h1><h2 id="machine-Translation"><a href="#machine-Translation" class="headerlink" title="machine Translation"></a>machine Translation</h2><p>Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models</p><p><img src="http://image.nysdy.com/2019052715589406009133.jpg" alt="2019052715589406009133.jpg"></p><h2 id="Model-Variations"><a href="#Model-Variations" class="headerlink" title="Model Variations"></a>Model Variations</h2><p><img src="http://image.nysdy.com/20190527155894062794865.jpg" alt="20190527155894062794865.jpg"></p><h1 id="ç¼ºç‚¹"><a href="#ç¼ºç‚¹" class="headerlink" title="ç¼ºç‚¹"></a><strong>ç¼ºç‚¹</strong></h1><p>ç¼ºç‚¹åœ¨åŸæ–‡ä¸­æ²¡æœ‰æåˆ°ï¼Œæ˜¯åæ¥åœ¨Universal Transformersä¸­æŒ‡å‡ºçš„ï¼Œåœ¨è¿™é‡ŒåŠ ä¸€ä¸‹å§ï¼Œä¸»è¦æ˜¯ä¸¤ç‚¹ï¼š</p><ol><li>å®è·µä¸Šï¼šæœ‰äº›rnnè½»æ˜“å¯ä»¥è§£å†³çš„é—®é¢˜transformeræ²¡åšåˆ°ï¼Œæ¯”å¦‚å¤åˆ¶stringï¼Œå°¤å…¶æ˜¯ç¢°åˆ°æ¯”è®­ç»ƒæ—¶çš„sequenceæ›´é•¿çš„æ—¶</li><li>ç†è®ºä¸Šï¼štransformersécomputationally universalï¼ˆ<a href="https://www.zhihu.com/question/20115374/answer/288346717" target="_blank" rel="noopener">å›¾çµå®Œå¤‡</a>ï¼‰ï¼Œï¼ˆæˆ‘è®¤ä¸ºï¼‰å› ä¸ºæ— æ³•å®ç°â€œwhileâ€å¾ªç¯</li></ol><h1 id="æ€»ç»“"><a href="#æ€»ç»“" class="headerlink" title="æ€»ç»“"></a><strong>æ€»ç»“</strong></h1><p>Transformeræ˜¯ç¬¬ä¸€ä¸ªç”¨çº¯attentionæ­å»ºçš„æ¨¡å‹ï¼Œä¸ä»…è®¡ç®—é€Ÿåº¦æ›´å¿«ï¼Œåœ¨ç¿»è¯‘ä»»åŠ¡ä¸Šä¹Ÿè·å¾—äº†æ›´å¥½çš„ç»“æœã€‚</p><p>Googleç°åœ¨çš„ç¿»è¯‘åº”è¯¥æ˜¯åœ¨æ­¤åŸºç¡€ä¸Šåšçš„ï¼Œä½†æ˜¯æ•°æ®é‡å¤§å¯èƒ½ç”¨transformerå¥½ä¸€äº›ï¼Œå°çš„è¯è¿˜æ˜¯ç»§ç»­ç”¨rnn-based modelã€‚</p><p>èŠ±äº†ä¸å°‘æ—¶é—´ï¼Œç®—æ˜¯ç†è§£äº†attentionå’Œtransformerï¼Œå¯¹å…¶ä¸­ä¸æ˜¯å¾ˆæ¸…æ¥šçš„ç‚¹å¦‚attentionçš„å†…éƒ¨ä¸­Qï¼ŒKï¼ŒVå…·ä½“æ˜¯ä»€ä¹ˆåœ¨self-attentionå’Œmulti-head attentionä¸­å¤§å°æ˜¯ä¸åŒçš„ï¼Œå¦‚ä½•maskï¼Œå¦‚ä½•è®¡ç®—å¤æ‚ï¼Œç­‰è¿›è¡ŒæŸ¥é˜…èµ„æ–™å¼„æ‡‚äº†ã€‚æ€»ä½“æ¥è¯´è¿˜æ˜¯æ”¶è·å¾ˆå¤§çš„ã€‚å‡†å¤‡åœ¨çœ‹ä¸€äº›ä»£ç è®²è§£ã€‚</p><h1 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h1><ul><li>Attentionæœºåˆ¶è¯¦è§£ï¼ˆäºŒï¼‰â€”â€”Self-Attentionä¸Transformer - å·é™€å­¦è€…çš„æ–‡ç«  - çŸ¥ä¹<br><a href="https://zhuanlan.zhihu.com/p/47282410" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/47282410</a></li><li><strong><a href="https://jalammar.github.io/illustrated-transformer/ï¼ˆè¿™ä¸ªè®²çš„æ¯”è¾ƒè¯¦ç»†ï¼Œå»ºè®®çœ‹å®Œè®ºæ–‡åå†çœ‹ä¸€éè¿™ä¸ªä¼šåŠ æ·±ç†è§£ï¼‰" target="_blank" rel="noopener">https://jalammar.github.io/illustrated-transformer/ï¼ˆè¿™ä¸ªè®²çš„æ¯”è¾ƒè¯¦ç»†ï¼Œå»ºè®®çœ‹å®Œè®ºæ–‡åå†çœ‹ä¸€éè¿™ä¸ªä¼šåŠ æ·±ç†è§£ï¼‰</a></strong></li><li>ã€NLPã€‘Transformerè¯¦è§£ - æå¦‚çš„æ–‡ç«  - çŸ¥ä¹<br><a href="https://zhuanlan.zhihu.com/p/44121378" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/44121378</a></li><li><a href="https://blog.eson.org/pub/664e9bad/" target="_blank" rel="noopener">https://blog.eson.org/pub/664e9bad/</a></li><li><a href="https://mp.weixin.qq.com/s/J-anyCuwLd5UYjTsUFNT1g" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/J-anyCuwLd5UYjTsUFNT1g</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;transformer æ˜¯ä¸€ä¸ªå®Œå…¨ç”±æ³¨æ„åŠ›æœºåˆ¶ç»„æˆçš„æ­å»ºçš„æ¨¡å‹ï¼Œæ¨¡å‹å¤æ‚åº¦ä½ï¼Œå¹¶å¯ä»¥è¿›è¡Œå¹¶è¡Œè®¡ç®—ï¼Œä½¿å¾—è®¡ç®—é€Ÿåº¦å¿«ã€‚åœ¨ç¿»è¯‘æ¨¡å‹ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ•ˆæœã€‚æœ¬ç¯‡è®ºæ–‡å±äºç»å…¸å¿…è¯»è®ºæ–‡ï¼Œé˜…è¯»ç¬”è®°ä¸­å¯¹ä¸€äº›ä¸æ¸…æ¥šçš„åœ°æ–¹è¿›è¡Œäº†æ±‰è¯­è§£é‡Šï¼Œè¯»å®Œè®ºæ–‡åé˜…è¯»å‚è€ƒé“¾æ¥ä»¥åŠ æ·±ç†è§£ã€‚&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1706.03762&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;è®ºæ–‡ä¸‹è½½åœ°å€&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="è®ºæ–‡é˜…è¯»ç¬”è®°" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
      <category term="classical" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/classical/"/>
    
    
      <category term="attention" scheme="http://yoursite.com/tags/attention/"/>
    
      <category term="transformer" scheme="http://yoursite.com/tags/transformer/"/>
    
      <category term="translation" scheme="http://yoursite.com/tags/translation/"/>
    
      <category term="classical" scheme="http://yoursite.com/tags/classical/"/>
    
  </entry>
  
  <entry>
    <title>Graph Neural Networks with Generated Parameters for Relation Extractioné˜…è¯»ç¬”è®°</title>
    <link href="http://yoursite.com/post/Graph_Neural_Networks_with_Generated_Parameters_for_Relation/"/>
    <id>http://yoursite.com/post/Graph_Neural_Networks_with_Generated_Parameters_for_Relation/</id>
    <published>2019-05-23T02:41:51.000Z</published>
    <updated>2019-05-23T09:24:39.487Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>æœ¬æ–‡å°†GNNsåº”ç”¨åˆ°å¤„ç†éç»“æ„åŒ–æ–‡æœ¬çš„ï¼ˆå¤šè·³ï¼‰å…³ç³»æ¨ç†ä»»åŠ¡æ¥è¿›è¡Œå…³ç³»æŠ½å–ã€‚é‡‡ç”¨ä»å¥å­åºåˆ—ä¸­è·å–çš„å®ä½“æ„å»ºå…¨é“¾æ¥å›¾ï¼Œåº”ç”¨ç¼–ç ï¼ˆsequence modelï¼‰ï¼Œä¼ æ’­ï¼ˆèŠ‚ç‚¹é—´ä¿¡æ¯ï¼‰å’Œåˆ†ç±»ï¼ˆé¢„æµ‹ï¼‰ä¸‰ä¸ªæ¨¡å—æ¥å¤„ç†å…³ç³»æ¨ç†ã€‚æœ¬æ–‡æä¾›äº†ä¸‰ä¸ªæ•°æ®é›†ã€‚</p></blockquote><a id="more"></a><h1 id="problem-statement"><a href="#problem-statement" class="headerlink" title="problem statement"></a>problem statement</h1><ul><li>existing relation extraction models fail to infer the relationship without multi-hop relational reasoning.</li><li>existing GNNs canâ€™t process multi-hop relational reasoning in natural language relational reasoning </li></ul><h1 id="research-objective"><a href="#research-objective" class="headerlink" title="research objective"></a>research objective</h1><p>enable GNNs to porcess relational reasoning on unstructed text inputs</p><h1 id="contribution"><a href="#contribution" class="headerlink" title="contribution"></a>contribution</h1><ul><li>extend a GNN with generated parameters, which could be applied to process relational reasoning on unstructured inputs</li><li>verify GP-GNNs in the taks of relation extraction from text; present three datasets</li></ul><h1 id="GP-GNNs"><a href="#GP-GNNs" class="headerlink" title="GP-GNNs"></a>GP-GNNs</h1><ul><li>construct a fully-connected graph with the entities in the sequence of text</li><li>employs three models to process relational reasoning<ul><li>an encoding modul: enable edges to encode rich information from natural languages </li><li>a propagation modul: propagates realtional information among various nodes </li><li>a classification modul: make prediction with node representations </li></ul></li></ul><p>As compared to tradtional GNNs, GP-GNNs could learn edgesâ€™ parameters from natural lanuages</p><h1 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h1><h2 id="Graph-Neural-Networks-GNNs"><a href="#Graph-Neural-Networks-GNNs" class="headerlink" title="Graph Neural Networks(GNNs)"></a>Graph Neural Networks(GNNs)</h2><ul><li>existing models still perfom message-passing on predefined graphs</li><li><em>Learning Graphical State Transitions</em> is most related<ul><li>introduecs a nove lnerual architecture to generate a graph based on the textal input</li><li>dynamically update the relationship during the learning process</li></ul></li></ul><h2 id="relational-reasoning"><a href="#relational-reasoning" class="headerlink" title="relational reasoning"></a>relational reasoning</h2><ul><li>existing models could not make full use of the multi-hop inference patterns among multiple entity pair and their relaitons within the sentence </li><li><em>LEARNING GRAPHICAL STATE TRANSITIONS</em> is the most related work<ul><li>the proposed model incorporates contextual relations with attention mechanism when predicting the relation of a target entity pair</li></ul></li></ul><h1 id="Graph-Neural-Network-with-Grenerated-Parameters-GP-GNNs"><a href="#Graph-Neural-Network-with-Grenerated-Parameters-GP-GNNs" class="headerlink" title="Graph Neural Network with Grenerated Parameters(GP-GNNs)"></a>Graph Neural Network with Grenerated Parameters(GP-GNNs)</h1><p>The picture is overall architecture: encoding module, propagation module and classification module</p><p><img src="http://image.nysdy.com/20190523155858968594021.jpg" alt="20190523155858968594021.jpg"></p><h2 id="Encoding-Module"><a href="#Encoding-Module" class="headerlink" title="Encoding Module"></a>Encoding Module</h2><p>formula:</p><script type="math/tex; mode=display">\mathcal{A}_{i, j}^{(n)}=f\left(E\left(x_{0}^{i, j}\right), E\left(x_{1}^{i, j}\right), \cdots, E\left(x_{l-1}^{i, j}\right) ; \theta_{e}^{n}\right)</script><p>where $f(\cdot)$ could be any model that could sequential(such as LSTMs); $E(\cdot)$ indicates an embedding function. $x^{i, j}$ is the word in sentence labeled( $i,j$)</p><h2 id="Porpagation-Module"><a href="#Porpagation-Module" class="headerlink" title="Porpagation Module"></a>Porpagation Module</h2><p>the representations of layer n + 1 are calculated by:</p><script type="math/tex; mode=display">\mathbf{h}_{i}^{(n+1)}=\sum_{v_{j} \in \mathcal{N}\left(v_{i}\right)} \sigma\left(\mathcal{A}_{i, j}^{(n)} \mathbf{h}_{j}^{(n)}\right)</script><p>where $\mathcal{N}\left(v_{i}\right)$ denotes the neighbors of node $v_i$</p><h2 id="Classification-Module"><a href="#Classification-Module" class="headerlink" title="Classification Module"></a>Classification Module</h2><p>the loss of GP-GNNs:</p><script type="math/tex; mode=display">\mathcal{L}=g\left(\mathbf{h}_{0 :|\mathcal{V}|-1}^{0}, \mathbf{h}_{0 :|\mathcal{V}|-1}^{1}, \ldots, \mathbf{h}_{0 :|\mathcal{V}|-1}^{K}, Y ; \theta_{c}\right)</script><h1 id="Relation-Extraction-with-GP-GNNs"><a href="#Relation-Extraction-with-GP-GNNs" class="headerlink" title="Relation Extraction with GP-GNNs"></a>Relation Extraction with GP-GNNs</h1><p>Authors introduce how to apply GP-GNNs to relation extraction</p><h2 id="Encoding-Module-1"><a href="#Encoding-Module-1" class="headerlink" title="Encoding Module"></a>Encoding Module</h2><p>encoding then context of entity pairs (or edges in the graph)</p><script type="math/tex; mode=display">E\left(x_{t}^{i, j}\right)=\left[\boldsymbol{x}_{t} ; \boldsymbol{p}_{t}^{i, j}\right]</script><p>where $x_t$ denotes the word embedding; $\boldsymbol{p}_{t}^{i, j}$denotes the position embedding of word posistion t relative to the entity pairâ€™s position $i, j$.</p><h3 id="position-embedding"><a href="#position-embedding" class="headerlink" title="position embedding"></a>position embedding</h3><p>we mark each token in the sentence as either belonging to the first entity $v_i$, the second entity $v_j$ or to neither of those</p><h2 id="Propagation-Module"><a href="#Propagation-Module" class="headerlink" title="Propagation Module"></a>Propagation Module</h2><p> the formula is the same as the front</p><h3 id="The-Initial-Embeddings-of-Nodes"><a href="#The-Initial-Embeddings-of-Nodes" class="headerlink" title="The Initial Embeddings of Nodes"></a>The Initial Embeddings of Nodes</h3><ul><li>when extracting the relationship between entity $v_i$ and entity $v_j$, the initial embeddings of them are annotated as $\mathbf{h}_{v_{i}}^{(0)}=a_{\text { subject }}$, and $h_{v_{j}}^{(0)}=a_{\text { object }}$, while the intial embeddings of other entities are set to all zeros.</li><li>In our experiments, we generalize the idea of Gated Graph Neural Networks (Li et al., 2016) by setting $a_{\text { subject }}=[1 ; 0]^{\top}$and $a_{\text { object }}=[0 ; 1]^{\top}$.</li></ul><h2 id="classification-Module"><a href="#classification-Module" class="headerlink" title="classification Module"></a>classification Module</h2><p><strong>As the  target entity pair $(v_i, v_j)$:</strong></p><script type="math/tex; mode=display">\boldsymbol{r}_{v_{i}, v_{j}}=\left[\left[\boldsymbol{h}_{v_{i}}^{(1)} \odot \boldsymbol{h}_{v_{j}}^{(1)}\right]^{\top} ;\left[\boldsymbol{h}_{v_{i}}^{(2)} \odot \boldsymbol{h}_{v_{j}}^{(2)}\right]^{\top} ; \ldots ;\left[\boldsymbol{h}_{v_{i}}^{(K)} \odot \boldsymbol{h}_{v_{j}}^{(K)}\right]^{\top}\right]</script><p>where $\odot$ represents element-wise multiplication</p><p><strong>classification:</strong></p><script type="math/tex; mode=display">\mathbb{P}\left(r_{v_{i}, v_{j}} | h, t, s\right)=\operatorname{softmax}\left(M L P\left(\boldsymbol{r}_{v_{i}, v_{j}}\right)\right)</script><p><strong>loss:</strong></p><script type="math/tex; mode=display">\mathcal{L}=\sum_{s \in S} \sum_{i \neq j} \log \mathbb{P}\left(r_{v_{i}, v_{j}} | i, j, s\right)</script><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="aim"><a href="#aim" class="headerlink" title="aim"></a>aim</h2><ul><li>showing their best models could improve the performance of relation extraction under a variety of settings</li><li>illlustrating that how the number of layers affect the performance of their model</li><li>performing a qualitiative investigation to highlight the diference between their models and baseline models</li></ul><h2 id="design"><a href="#design" class="headerlink" title="design"></a>design</h2><p>as the first and second aim</p><ul><li>show that our models could improve instance-level relation extraction on a human annotated test set</li><li>we will show that our models could also help enhance the performance of bag-level relation extraction on a distantly labeled test set</li><li>split a subset of distantly labeled test set, where the number of entities and edges is large</li></ul><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><h3 id="distantly-label-set"><a href="#distantly-label-set" class="headerlink" title="distantly label set"></a>distantly label set</h3><ul><li>Sorokin and Gurevych (2017) proposed </li><li>modify their dataset<ul><li>added reversed edges</li><li>for all of the entity pairs with no relations, added â€œNAâ€ labels to them</li></ul></li></ul><h3 id="Human-annotated-test-set"><a href="#Human-annotated-test-set" class="headerlink" title="Human annotated test set"></a>Human annotated test set</h3><ul><li>Sorokin and Gurevych (2017)</li><li>select the distantly lablel pairs which all 5 annotaters are accepted.</li><li>There are 350 sentences and 1,230 triples in this test set </li></ul><h3 id="Dense-distantly-labeled-test-set"><a href="#Dense-distantly-labeled-test-set" class="headerlink" title="Dense distantly labeled test set"></a>Dense distantly labeled test set</h3><ul><li>criteria<ul><li>the number of entities should be strictly larger than 2</li><li>there must be at least one circle (with at least three entities) in the ground-truth label of the sentence</li></ul></li><li>There are 1,350 sentences and more than 17,915 triples and 7,906 relational facts in this test set.</li></ul><h2 id="Models-for-comparison"><a href="#Models-for-comparison" class="headerlink" title="Models for comparison"></a>Models for comparison</h2><ul><li>Context-aware RE</li><li>Multi-Window CNN</li><li>PCNN</li><li>LSTM or GP-GNN with K = 1 layer</li><li>GP-GNN with K = 2 or K = 3 layerss</li></ul><h2 id="Evaluation-Details"><a href="#Evaluation-Details" class="headerlink" title="Evaluation Details"></a>Evaluation Details</h2><p><strong>To evaluation models in bag-level:</strong></p><script type="math/tex; mode=display">E\left(r | v_{i}, v_{j}, S\right)=\max _{s \in S} \mathbb{P}\left(r_{v_{i}, v_{j}} | i, j, s\right)</script><p><strong>result</strong>:</p><p><img src="http://image.nysdy.com/20190523155859369893411.jpg" alt="20190523155859369893411.jpg"></p><h2 id="Effectiveness-of-Reasoning-Mechanism"><a href="#Effectiveness-of-Reasoning-Mechanism" class="headerlink" title="Effectiveness of Reasoning Mechanism"></a>Effectiveness of Reasoning Mechanism</h2><p><img src="http://image.nysdy.com/2019052315585938129506.jpg" alt="2019052315585938129506.jpg"></p><ul><li>Context-Aware RE may <strong>introduce more noise,</strong> for it may mistakenly increase the probability of a relation with the similar topic with the context relations</li><li>sentences from Wikipedia corpus are always complex, which may be hard to model for CNN and PCNN</li></ul><h2 id="The-Effectiveness-of-the-Number-of-Layers"><a href="#The-Effectiveness-of-the-Number-of-Layers" class="headerlink" title="The Effectiveness of the Number of Layers"></a>The Effectiveness of the Number of Layers</h2><p><img src="http://image.nysdy.com/20190523155859455443298.jpg" alt="20190523155859455443298.jpg"></p><ul><li>the improvement of the third layer is much smaller on the overall distantly supervised test set than the one on the dense subset<ul><li>This observation reveals that the reasoning mechanism could help us identify relations especially on sentences where there are more entities</li></ul></li><li>as the number of layers grows, the curves get higher and higher precision, <ul><li>indicating considering more hops in reasoning leads to better performance</li></ul></li></ul><h2 id="Qualitative-Results-Case-Study"><a href="#Qualitative-Results-Case-Study" class="headerlink" title="Qualitative Results: Case Study"></a>Qualitative Results: Case Study</h2><p><img src="http://image.nysdy.com/20190523155859457710223.jpg" alt="20190523155859457710223.jpg"></p><p>Context-Aware RE makes a mistake by predicting (Kentucky, share boarder with, Ohio). As we have discussed before, this is due to its mechanism to model co-occurrence of multiple relations</p><h1 id="æ€è€ƒ"><a href="#æ€è€ƒ" class="headerlink" title="æ€è€ƒ"></a>æ€è€ƒ</h1><p>æ–‡ç« æ˜¯åˆ˜çŸ¥è¿œç»„çš„è®ºæ–‡ï¼Œé’ˆå¯¹çš„æ–¹å‘æ˜¯å…³ç³»æŠ½å–ï¼Œåœ¨å…¶ä¸­ç»“åˆäº†å…³ç³»æ¨ç†ï¼Œæœ€è¿‘è®¸å¤šä»»åŠ¡éƒ½åœ¨ç»“åˆæ¨ç†çš„æ€æƒ³ã€‚æ–‡ç« æ•´ä½“çš„ç»“æ„ï¼Œé€»è¾‘ååˆ†æ¸…æ™°ï¼Œè®ºè¿°çš„ä¹Ÿæ¯”è¾ƒè¯¦ç»†ï¼Œå±äºæ ‡å‡†è®ºæ–‡ã€‚æ„Ÿè§‰æ–‡ç« ä¸­GP-GNNsç»“æ„å›¾è¿˜å¯ä»¥ç”»çš„æ›´å¥½ä¸€ç‚¹ï¼Œå±•ç°ä¸€ä¸‹encoding moduleçš„å±‚ï¼Œå¯ä»¥æ›´å¥½ç†è§£ã€‚æ–‡ç« çš„ç²¾é«“åº”è¯¥æ˜¯è¿™ä¸ªpropagation moduleçš„éƒ¨åˆ†ï¼Œè¿˜éœ€è¦æ¶ˆåŒ–ä¸€ä¸‹ï¼Œä¸è¿‡è¿™éƒ¨åˆ†å¯èƒ½æ˜¯æœ‰å…ˆå‰çš„çŸ¥è¯†æ”¯æ’‘çš„ã€‚</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;æœ¬æ–‡å°†GNNsåº”ç”¨åˆ°å¤„ç†éç»“æ„åŒ–æ–‡æœ¬çš„ï¼ˆå¤šè·³ï¼‰å…³ç³»æ¨ç†ä»»åŠ¡æ¥è¿›è¡Œå…³ç³»æŠ½å–ã€‚é‡‡ç”¨ä»å¥å­åºåˆ—ä¸­è·å–çš„å®ä½“æ„å»ºå…¨é“¾æ¥å›¾ï¼Œåº”ç”¨ç¼–ç ï¼ˆsequence modelï¼‰ï¼Œä¼ æ’­ï¼ˆèŠ‚ç‚¹é—´ä¿¡æ¯ï¼‰å’Œåˆ†ç±»ï¼ˆé¢„æµ‹ï¼‰ä¸‰ä¸ªæ¨¡å—æ¥å¤„ç†å…³ç³»æ¨ç†ã€‚æœ¬æ–‡æä¾›äº†ä¸‰ä¸ªæ•°æ®é›†ã€‚&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="è®ºæ–‡é˜…è¯»ç¬”è®°" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="GNNs" scheme="http://yoursite.com/tags/GNNs/"/>
    
      <category term="relation extraction" scheme="http://yoursite.com/tags/relation-extraction/"/>
    
      <category term="relation reasoning" scheme="http://yoursite.com/tags/relation-reasoning/"/>
    
  </entry>
  
  <entry>
    <title>allennlpå®‰è£…è¸©å‘</title>
    <link href="http://yoursite.com/post/allennlp_install/"/>
    <id>http://yoursite.com/post/allennlp_install/</id>
    <published>2019-05-22T14:09:27.000Z</published>
    <updated>2019-05-22T14:11:19.740Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>å®‰è£…allennlpçš„è¸©å‘ä¹‹è·¯ï¼Œè¸©äº†ä¸å°‘å‘æœ€åé€‰æ‹©â€™Installing from sourceâ€™çš„å®‰è£…æ–¹æ³•ï¼Œæ’å‘åä¸‹é¢æ–¹æ³•äº²æµ‹å¯ç”¨</p></blockquote><a id="more"></a><h2 id="Installing-from-source"><a href="#Installing-from-source" class="headerlink" title="Installing from source"></a>Installing from source</h2><p>å®‰è£…æ­¥éª¤ï¼š</p><h3 id="1-ä¸‹è½½GitHubæ–‡ä»¶"><a href="#1-ä¸‹è½½GitHubæ–‡ä»¶" class="headerlink" title="1.ä¸‹è½½GitHubæ–‡ä»¶"></a>1.ä¸‹è½½GitHubæ–‡ä»¶</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/allenai/allennlp.git</span><br></pre></td></tr></table></figure><h3 id="2-åˆ›å»ºcondaç¯å¢ƒ"><a href="#2-åˆ›å»ºcondaç¯å¢ƒ" class="headerlink" title="2.åˆ›å»ºcondaç¯å¢ƒ"></a>2.åˆ›å»ºcondaç¯å¢ƒ</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n allennlp python=3.6</span><br></pre></td></tr></table></figure><h3 id="3-æ¿€æ´»ç¯å¢ƒä¸‹è½½ä¾èµ–æ–‡ä»¶"><a href="#3-æ¿€æ´»ç¯å¢ƒä¸‹è½½ä¾èµ–æ–‡ä»¶" class="headerlink" title="3.æ¿€æ´»ç¯å¢ƒä¸‹è½½ä¾èµ–æ–‡ä»¶"></a>3.æ¿€æ´»ç¯å¢ƒä¸‹è½½ä¾èµ–æ–‡ä»¶</h3><ul><li><p>æ¿€æ´»ç¯å¢ƒ</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source activate allennlp</span><br></pre></td></tr></table></figure></li><li><p>è¿›å…¥githubä¸Šä¸‹è½½çš„æ–‡ä»¶å¤¹</p></li><li><p>ä¸‹è½½ä¾èµ–æ–‡ä»¶</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure><p>é‡åˆ°æŠ¥é”™é—®é¢˜ï¼Œå‚è€ƒä¸‹ä¸€å°èŠ‚ï¼Œæ‰€æ¬²é—®é¢˜è§£å†³ã€‚</p></li></ul><h3 id="4-å®‰è£…allennlp"><a href="#4-å®‰è£…allennlp" class="headerlink" title="4.å®‰è£…allennlp"></a>4.å®‰è£…allennlp</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install --editable .</span><br></pre></td></tr></table></figure><h3 id="5-æµ‹è¯•"><a href="#5-æµ‹è¯•" class="headerlink" title="5.æµ‹è¯•"></a>5.æµ‹è¯•</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">allennlp</span><br></pre></td></tr></table></figure><p>æˆåŠŸåæ•ˆæœå¦‚ä¸‹ï¼š</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> allennlp</span><br><span class="line">2019-05-22 21:58:42,297 - INFO - pytorch_pretrained_bert.modeling - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .</span><br><span class="line">usage: allennlp</span><br><span class="line"></span><br><span class="line">Run AllenNLP</span><br><span class="line"></span><br><span class="line">optional arguments:</span><br><span class="line">  -h, --help     show this help message and exit</span><br><span class="line">  --version      show program's version number and exit</span><br><span class="line"></span><br><span class="line">Commands:</span><br><span class="line"></span><br><span class="line">    configure    Run the configuration wizard.</span><br><span class="line">    train        Train a model.</span><br><span class="line">    evaluate     Evaluate the specified model + dataset.</span><br><span class="line">    predict      Use a trained model to make predictions.</span><br><span class="line">    make-vocab   Create a vocabulary.</span><br><span class="line">    elmo         Create word vectors using a pretrained ELMo model.</span><br><span class="line">    fine-tune    Continue training a model on a new dataset.</span><br><span class="line">    dry-run      Create a vocabulary, compute dataset statistics and other</span><br><span class="line">                 training utilities.</span><br><span class="line">    test-install</span><br><span class="line">                 Run the unit tests.</span><br><span class="line">    find-lr      Find a learning rate range.</span><br><span class="line">    print-results</span><br><span class="line">                 Print results from allennlp serialization directories to the</span><br><span class="line">                 console.</span><br></pre></td></tr></table></figure><h2 id="é‡åˆ°çš„é—®é¢˜"><a href="#é‡åˆ°çš„é—®é¢˜" class="headerlink" title="é‡åˆ°çš„é—®é¢˜"></a>é‡åˆ°çš„é—®é¢˜</h2><h3 id="é—®é¢˜1"><a href="#é—®é¢˜1" class="headerlink" title="é—®é¢˜1"></a>é—®é¢˜1</h3><h4 id="æŠ¥é”™ä¿¡æ¯ï¼š"><a href="#æŠ¥é”™ä¿¡æ¯ï¼š" class="headerlink" title="æŠ¥é”™ä¿¡æ¯ï¼š"></a>æŠ¥é”™ä¿¡æ¯ï¼š</h4><p>ERROR: Failed building wheel for jsonnet</p><p><img src="http://image.nysdy.com/20190522155853293350470.jpg" alt="20190522155853293350470.jpg"></p><h4 id="è§£å†³æ–¹æ³•ï¼š"><a href="#è§£å†³æ–¹æ³•ï¼š" class="headerlink" title="è§£å†³æ–¹æ³•ï¼š"></a>è§£å†³æ–¹æ³•ï¼š</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install -c conda-forge jsonnet</span><br></pre></td></tr></table></figure><h3 id="é—®é¢˜2"><a href="#é—®é¢˜2" class="headerlink" title="é—®é¢˜2"></a>é—®é¢˜2</h3><h4 id="æŠ¥é”™ä¿¡æ¯ï¼š-1"><a href="#æŠ¥é”™ä¿¡æ¯ï¼š-1" class="headerlink" title="æŠ¥é”™ä¿¡æ¯ï¼š"></a>æŠ¥é”™ä¿¡æ¯ï¼š</h4><p>æŠ¥çš„éƒ½æ˜¯æŸäº›åŒ…çš„ç‰ˆæœ¬é—®é¢˜</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ERROR: botocore 1.12.152 has requirement urllib3&lt;1.25,&gt;=1.20; python_version &gt;= "3.4", but you'll have urllib3 1.25.2 which is incompatible.</span><br><span class="line">ERROR: aws-sam-translator 1.11.0 has requirement jsonschema~=2.6, but you'll have jsonschema 3.0.1 which is incompatible.</span><br><span class="line">ERROR: cfn-lint 0.20.3 has requirement jsonschema~=2.6, but you'll have jsonschema 3.0.1 which is incompatible.</span><br><span class="line">ERROR: cfn-lint 0.20.3 has requirement requests&lt;=2.21.0,&gt;=2.15.0, but you'll have requests 2.22.0 which is incompatible</span><br></pre></td></tr></table></figure><h4 id="è§£å†³æ–¹æ³•"><a href="#è§£å†³æ–¹æ³•" class="headerlink" title="è§£å†³æ–¹æ³•"></a>è§£å†³æ–¹æ³•</h4><p>æ ¹æ®æŠ¥é”™ä¿¡æ¯ä¸‹è½½ç›¸åº”å®‰è£…åŒ…å³å¯</p><h2 id="é—®é¢˜3"><a href="#é—®é¢˜3" class="headerlink" title="é—®é¢˜3"></a>é—®é¢˜3</h2><h4 id="æŠ¥é”™ä¿¡æ¯ï¼š-2"><a href="#æŠ¥é”™ä¿¡æ¯ï¼š-2" class="headerlink" title="æŠ¥é”™ä¿¡æ¯ï¼š"></a>æŠ¥é”™ä¿¡æ¯ï¼š</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">ImportError: dlopen: cannot load any more object with static TLS</span><br><span class="line">___________________________________________________________________________</span><br><span class="line">Contents of /home/minelab/anaconda3/envs/allennlp/lib/python3.6/site-packages/sklearn/__check_build:</span><br><span class="line">__init__.py               setup.py                  _check_build.cpython-36m-x86_64-linux-gnu.so</span><br><span class="line">__pycache__</span><br><span class="line">___________________________________________________________________________</span><br><span class="line">It seems that scikit-learn has not been built correctly.</span><br><span class="line"></span><br><span class="line">If you have installed scikit-learn from source, please do not forget</span><br><span class="line">to build the package before using it: run `python setup.py install` or</span><br><span class="line">`make` in the source directory.</span><br><span class="line"></span><br><span class="line">If you have used an installer, please check that it is suited for your</span><br><span class="line">Python version, your operating system and your platform.</span><br></pre></td></tr></table></figure><h4 id="è§£å†³æ–¹æ³•ï¼š-1"><a href="#è§£å†³æ–¹æ³•ï¼š-1" class="headerlink" title="è§£å†³æ–¹æ³•ï¼š"></a>è§£å†³æ–¹æ³•ï¼š</h4><p>ä¸‹è½½æ›´ä½ç‰ˆæœ¬çš„scikit-learn,ä¾‹å¦‚</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install scikit-learn=0.20.3</span><br></pre></td></tr></table></figure><h2 id="å‚è€ƒé“¾æ¥"><a href="#å‚è€ƒé“¾æ¥" class="headerlink" title="å‚è€ƒé“¾æ¥"></a>å‚è€ƒé“¾æ¥</h2><ul><li><a href="https://github.com/pytorch/pytorch/issues/10443" target="_blank" rel="noopener">https://github.com/pytorch/pytorch/issues/10443</a></li><li><a href="https://github.com/pypa/pip/issues/4330" target="_blank" rel="noopener">https://github.com/pypa/pip/issues/4330</a></li></ul><h1 id="å®‰è£…çš„å¯ç¤º"><a href="#å®‰è£…çš„å¯ç¤º" class="headerlink" title="å®‰è£…çš„å¯ç¤º"></a>å®‰è£…çš„å¯ç¤º</h1><h3 id="ç¯å¢ƒé—®é¢˜"><a href="#ç¯å¢ƒé—®é¢˜" class="headerlink" title="ç¯å¢ƒé—®é¢˜"></a>ç¯å¢ƒé—®é¢˜</h3><ul><li>æœ€åŸºæœ¬çš„å°±æ˜¯å…ˆå»ç½‘ä¸ŠæŸ¥è¿™ä¸ªé”™è¯¯çš„è§£å†³æ–¹æ³•</li><li>ç½‘ä¸Šçš„è§£å†³ä¸äº†çš„ï¼Œå…ˆçŒœçŒœå¤§æ¦‚ç‡æ˜¯å“ªæ–¹é¢çš„é—®é¢˜ã€‚<ul><li>æ¯”å¦‚å¤§æ¦‚ç‡æ˜¯å„ç§ç‰ˆæœ¬äº’ç›¸ä¹‹é—´ä¸é€‚é…çš„é—®é¢˜ï¼Œé‚£å°±è°ƒè¯•ç‰ˆæœ¬ï¼Œä¸€èˆ¬éƒ½ä¼šå‘Šè¯‰ä½ å“ªä¸ªæœ‰é—®é¢˜ï¼Œæ¯”å¦‚ä¸Šé¢çš„scikit-learné—®é¢˜ã€‚</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;å®‰è£…allennlpçš„è¸©å‘ä¹‹è·¯ï¼Œè¸©äº†ä¸å°‘å‘æœ€åé€‰æ‹©â€™Installing from sourceâ€™çš„å®‰è£…æ–¹æ³•ï¼Œæ’å‘åä¸‹é¢æ–¹æ³•äº²æµ‹å¯ç”¨&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="install" scheme="http://yoursite.com/categories/install/"/>
    
    
      <category term="allennlp" scheme="http://yoursite.com/tags/allennlp/"/>
    
      <category term="åŒ…å®‰è£…" scheme="http://yoursite.com/tags/%E5%8C%85%E5%AE%89%E8%A3%85/"/>
    
  </entry>
  
  <entry>
    <title>Triple Trustworthiness Measurement for Knowledge Graphé˜…è¯»ç¬”è®°</title>
    <link href="http://yoursite.com/post/Triple%20Trustworthiness%20Measurement%20for%20Knowledge%20Graph/"/>
    <id>http://yoursite.com/post/Triple Trustworthiness Measurement for Knowledge Graph/</id>
    <published>2019-05-21T06:45:26.000Z</published>
    <updated>2019-05-22T13:33:20.088Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡è®¡ç®—triple trustworthinessæ¥è¯„ä¼°çŸ¥è¯†å›¾è°±çš„å‡†ç¡®ç¨‹åº¦çš„æ–¹æ³•ã€‚æ¨¡å‹åˆ©ç”¨ç¥ç»ç½‘ç»œç»¼åˆæ¥è‡ªå®ä½“ï¼ˆå€Ÿé‰´Resource allocationï¼‰ã€å…³ç³»ï¼ˆå€Ÿé‰´ç¿»è¯‘æ¨¡å‹çš„æ€æƒ³ï¼Œå¦‚TransEï¼‰å’ŒKGå…¨å±€ï¼ˆå€Ÿé‰´å…³ç³»è·¯å¾„ï¼ŒRNNï¼‰ä¸‰ä¸ªå±‚é¢çš„è¯­ä¹‰å’Œå…¨å±€ä¿¡æ¯ï¼Œè¾“å‡ºæœ€åçš„ trustworthinessä½œä¸ºåˆ¤æ–­ä¾æ®ã€‚</p><p><a href="http://delivery.acm.org/10.1145/3320000/3313586/p2865-jia.pdf?ip=59.64.129.22&amp;id=3313586&amp;acc=OPEN&amp;key=BF85BBA5741FDC6E%2E66A15327C2E204FC%2E4D4702B0C3E38B35%2E6D218144511F3437&amp;__acm__=1558515578_57e0bf75d529cf4656975ffe7da506b9" target="_blank" rel="noopener">ä¸‹è½½åœ°å€</a></p></blockquote><a id="more"></a><h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>This paper proposed a method for estimating the accuracy of a knowledge graph by computing triple trustworthiness. The model uses neural network to synthesize semantic and global information from three levels: entity(resource allocation), relationship(translation model ideas, such as TransE)m and KG global(relationship path, RNN) and outputting the final trustworthiness as the basis for judgment.</p><h1 id="Problem-statement"><a href="#Problem-statement" class="headerlink" title="Problem statement"></a>Problem statement</h1><p>possible noises and conflicts are inevitably intoduced in the process of constructing the KG</p><h1 id="research-objective"><a href="#research-objective" class="headerlink" title="research objective"></a>research objective</h1><p>quantify the KGâ€™s semantic correctness and the true degree of the facts expressed</p><h1 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h1><ul><li>Knowledge graph triple trustworthiness measurement<ul><li>use the  triple semantic information and globally inferring information</li><li>three levels measurement and an intergration of confidence value</li></ul></li><li>experiment result verified the model valid on large-scale KG Freebase</li><li>the KGTtm could be utilized in knowledge graph construction or improvement</li></ul><h1 id="THE-TRIPLE-TRUSTWORTHINESS-MEASUREMENT-MODEL"><a href="#THE-TRIPLE-TRUSTWORTHINESS-MEASUREMENT-MODEL" class="headerlink" title="THE TRIPLE TRUSTWORTHINESS MEASUREMENT MODEL"></a>THE TRIPLE TRUSTWORTHINESS MEASUREMENT MODEL</h1><p><img src="http://image.nysdy.com/20190521155842605796518.jpg" alt="20190521155842605796518.jpg"></p><ul><li>Longitudinally, the model can be divided into two level.<ul><li>the upper is a pool of multiple trustworthiness estimate cells(estimator)</li><li>the output of these Estimator forms the input of lower-level fusion device(Fusioner)</li></ul></li><li>Viewed laterally, three progressive levels   are be considered, as following.</li></ul><h2 id="Is-there-a-possible-relationship-between-the-entity-pairs"><a href="#Is-there-a-possible-relationship-between-the-entity-pairs" class="headerlink" title="Is there a possible relationship between the entity pairs?"></a>Is there a possible relationship between the entity pairs?</h2><p><img src="http://image.nysdy.com/20190521155842810227816.jpg" alt="20190521155842810227816.jpg"></p><p>ResourceRank:</p><ul><li>The algorithm assumes that the association between entity paires $(h,t)$ will be stronger, and more resource is passed from the  head $h$ through all associated paths to the tail $t$ in a graph</li><li>The amount of resource aggregated into $t$ ingeniously indicateds the association strength from $h$ to $t$.</li></ul><p>As pair $(e_1,e_2)$, there only one directed edge from $e_1$ to $e_2$ in the graph, where the different bandwidth of the edge indicates the number of the multiple relations.</p><p>output:</p><script type="math/tex; mode=display">\left\{\begin{array}{c}{u=\alpha\left(W_{1} V+b_{1}\right)} \\ {R R(h, t)=W_{2} u+b_{2}}\end{array}\right.</script><p>Authors constructed a $V$ vector by combining six characteristics.</p><ol><li>R (t | h); </li><li>In-degree of head node ID(h); </li><li>Out-degree of head node OD(h); </li><li>In-degree of tail node ID(t);</li><li>Out-degree of tail node OD(t);</li><li>The depth from head node to tail node Dep</li></ol><p>As for 1. the formula:</p><script type="math/tex; mode=display">R(t | h)=(1-\theta) \sum_{e_{i} \in M_{t}} \frac{R\left(e_{i} | h\right) \cdot B W_{e_{i} t}}{O D\left(e_{i}\right)}+\frac{\theta}{N}</script><ul><li>$M_t$is the set of all nodes that have outgoing links to the node $t$, $OD (e_i)$ is the out-degree of the node eiand the $BW_{e_it}$ is the bandwidth from the $e_i$ to $t$.</li><li>In order to improve the model fault-tolerance, we assume that the resource fow from each node may directly jump to a random node with the same probability Î¸</li></ul><h2 id="Can-the-determined-relationship-r-occur-between-the-entity-pair-h-t"><a href="#Can-the-determined-relationship-r-occur-between-the-entity-pair-h-t" class="headerlink" title="Can the determined relationship $r$ occur between the entity pair $(h,t)$ ?"></a>Can the determined relationship $r$ occur between the entity pair $(h,t)$ ?</h2><p><img src="http://image.nysdy.com/20190522155850919251079.jpg" alt="20190522155850919251079.jpg"></p><p>Translation-based energy function (TEF)ï¼šdepended on TransE</p><p>$E(h, r, t)=|\mathbf{h}+\mathbf{r}-\mathbf{t}|$</p><p>output:</p><script type="math/tex; mode=display">P(E(h, r, t))=\frac{1}{1+e^{-\lambda\left(\delta_{r}-E(h, r, t)\right)}}</script><h2 id="Can-the-relevant-triples-in-the-KG-infer-that-the-triple-is-trustworthy"><a href="#Can-the-relevant-triples-in-the-KG-infer-that-the-triple-is-trustworthy" class="headerlink" title="Can the relevant triples in the KG infer that the triple is trustworthy?"></a>Can the relevant triples in the KG infer that the triple is trustworthy?</h2><p><img src="http://image.nysdy.com/20190522155851013726520.jpg" alt="20190522155851013726520.jpg"></p><p>Reachable paths inference (RPI):</p><p>There two challenges to exploit the reachable paths for inferring triple trustworthiness:</p><h3 id="reachable-paths-selection"><a href="#reachable-paths-selection" class="headerlink" title="reachable paths selection"></a>reachable paths selection</h3><p>Semantic distance-based path selection<img src="http://image.nysdy.com/2019052215585105592950.jpg" alt="2019052215585105592950.jpg"></p><h3 id="Reachable-Paths-Representation"><a href="#Reachable-Paths-Representation" class="headerlink" title="Reachable Paths Representation"></a>Reachable Paths Representation</h3><p>using a RNN to deal with the embeddings of the three elements of each triple in the selected path</p><h2 id="Fusing-the-Estimators"><a href="#Fusing-the-Estimators" class="headerlink" title="Fusing the Estimators"></a>Fusing the Estimators</h2><p>a classifer based on a multi-layer perceptron </p><h1 id="EXPERIMENTS"><a href="#EXPERIMENTS" class="headerlink" title="EXPERIMENTS"></a>EXPERIMENTS</h1><h2 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h2><p>FB15K</p><h2 id="Interpreting-the-Validity-of-the-Trustworthiness"><a href="#Interpreting-the-Validity-of-the-Trustworthiness" class="headerlink" title="Interpreting the Validity of the Trustworthiness"></a>Interpreting the Validity of the Trustworthiness</h2><p><img src="http://image.nysdy.com/20190522155851139148623.jpg" alt="20190522155851139148623.jpg"></p><ul><li>The left picture shows that the positives examples are mainly concentrated in the upper region, vice versa.</li><li>As for the right picture<ul><li>only if the value of a triple is higher than the threshold can it be considered trustworthy</li><li>shows that the positive examples universally have higher confidence values</li></ul></li></ul><h2 id="Comparing-With-Other-Models-on-The-Knowledge-Graph-Error-Detection-Task"><a href="#Comparing-With-Other-Models-on-The-Knowledge-Graph-Error-Detection-Task" class="headerlink" title="Comparing With Other Models on The Knowledge Graph Error Detection Task"></a>Comparing With Other Models on The Knowledge Graph Error Detection Task</h2><p><img src="http://image.nysdy.com/20190522155851269267340.jpg" alt="20190522155851269267340.jpg"></p><p>Authorsâ€™ model has beter results in terms of accuracy and the F1-score than the other models.</p><h2 id="Analyzing-the-ability-of-models-to-tackle-the-three-type-noises"><a href="#Analyzing-the-ability-of-models-to-tackle-the-three-type-noises" class="headerlink" title="Analyzing the ability of models to tackle the three type noises."></a>Analyzing the ability of models to tackle the three type noises.</h2><p><img src="http://image.nysdy.com/20190522155851290149973.jpg" alt="20190522155851290149973.jpg"></p><ul><li>a higher recall shows that authorsâ€™ model can more accurately find the right from noisy triples</li><li>higher average trustworthiness values show that authorsâ€™ model can better identify the correct instances and with high confidence </li><li>the worst among the $(h, ?, t)$, because the various relations between a certain entity  increase the difficulty of model judgment.</li></ul><h2 id="Analyzing-the-Efects-of-Single-Estimators"><a href="#Analyzing-the-Efects-of-Single-Estimators" class="headerlink" title="Analyzing the Efects of Single Estimators"></a>Analyzing the Efects of Single Estimators</h2><p><img src="http://image.nysdy.com/20190522155851337652153.jpg" alt="20190522155851337652153.jpg"></p><p>It can be found that the accuracy obtained by each model is above 0.8, which proves the effectiveness of each Estimator</p><h1 id="æ€è€ƒ"><a href="#æ€è€ƒ" class="headerlink" title="æ€è€ƒ"></a>æ€è€ƒ</h1><p>æœ¬æ–‡åœ¨æ–¹æ³•ä¸Šå‡ ä¹æ²¡æœ‰ä»€ä¹ˆåˆ›æ–°ï¼Œæœ¬è´¨ä¸Šå°±æ˜¯ä¸€ä¸ªè€æ–¹æ³•çš„å¤šä¸ªç»„åˆã€‚æœ€å¤§äº®ç‚¹å°±æ˜¯ä½œè€…èƒ½æå‡ºtrustworthinessæ¥æŠŠè¿™ä¸ªè¯„ä»·çŸ¥è¯†å›¾è°±å‡†ç¡®åº¦çš„é—®é¢˜è¿›è¡Œäº†é‡åŒ–ã€‚è¿™ç§èƒ½åŠ›æ¯”æå‡ºæ–¹æ³•ä¸Šçš„åˆ›æ–°æ›´åŠ å‰å®³ï¼Œä¹Ÿæ˜¯éœ€è¦å­¦ä¹ çš„åœ°æ–¹ã€‚</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡è®¡ç®—triple trustworthinessæ¥è¯„ä¼°çŸ¥è¯†å›¾è°±çš„å‡†ç¡®ç¨‹åº¦çš„æ–¹æ³•ã€‚æ¨¡å‹åˆ©ç”¨ç¥ç»ç½‘ç»œç»¼åˆæ¥è‡ªå®ä½“ï¼ˆå€Ÿé‰´Resource allocationï¼‰ã€å…³ç³»ï¼ˆå€Ÿé‰´ç¿»è¯‘æ¨¡å‹çš„æ€æƒ³ï¼Œå¦‚TransEï¼‰å’ŒKGå…¨å±€ï¼ˆå€Ÿé‰´å…³ç³»è·¯å¾„ï¼ŒRNNï¼‰ä¸‰ä¸ªå±‚é¢çš„è¯­ä¹‰å’Œå…¨å±€ä¿¡æ¯ï¼Œè¾“å‡ºæœ€åçš„ trustworthinessä½œä¸ºåˆ¤æ–­ä¾æ®ã€‚&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://delivery.acm.org/10.1145/3320000/3313586/p2865-jia.pdf?ip=59.64.129.22&amp;amp;id=3313586&amp;amp;acc=OPEN&amp;amp;key=BF85BBA5741FDC6E%2E66A15327C2E204FC%2E4D4702B0C3E38B35%2E6D218144511F3437&amp;amp;__acm__=1558515578_57e0bf75d529cf4656975ffe7da506b9&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ä¸‹è½½åœ°å€&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="è®ºæ–‡é˜…è¯»ç¬”è®°" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="KG" scheme="http://yoursite.com/tags/KG/"/>
    
      <category term="Knowledge Graph" scheme="http://yoursite.com/tags/Knowledge-Graph/"/>
    
      <category term="Triple" scheme="http://yoursite.com/tags/Triple/"/>
    
  </entry>
  
  <entry>
    <title>GloVe: Global Vectors for Word Representationé˜…è¯»ç¬”è®°</title>
    <link href="http://yoursite.com/post/GloVe:Global%20Vectors%20for%20Word%20Representation/"/>
    <id>http://yoursite.com/post/GloVe:Global Vectors for Word Representation/</id>
    <published>2019-05-20T13:35:25.000Z</published>
    <updated>2019-05-21T06:43:45.987Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>è®ºæ–‡<a href="https://www.aclweb.org/anthology/D14-1162" target="_blank" rel="noopener">ä¸‹è½½åœ°å€</a>ï¼ŒGloVeæ˜¯ä¸€ä¸ªæ–°çš„å…¨çƒå¯¹æ•°åŒçº¿æ€§å›å½’æ¨¡å‹ï¼Œå±äºç»å…¸çš„è¯å‘é‡è¡¨ç¤ºæ–¹æ³•ä¹‹ä¸€ã€‚</p></blockquote><a id="more"></a><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>evaluate the intrinsic quality</p><ul><li>Most word vector methods rely on the distance or angle between pairs of word vectors </li><li>Mikolov et al. (2013c) introduced word analogies that examines word vectorâ€™s various dimensions of difference.</li></ul><p>two main model families for learning vectors:</p><ul><li>global matrix factorization methods</li><li>local context window methods</li></ul><p>Authors propose a specific weighted least squares model that trains on globla word-word co-occurrence counts and thus makes efficient use of statistics.</p><h1 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h1><h2 id="Matix-Facroization-Methods"><a href="#Matix-Facroization-Methods" class="headerlink" title="Matix Facroization Methods"></a>Matix Facroization Methods</h2><p>These methods utilize low-rank approximations to decompose large matrices that capture statistical information about a corpus.</p><h3 id="shortcoming"><a href="#shortcoming" class="headerlink" title="shortcoming"></a>shortcoming</h3><p>the most frequent words contribute a dispropoertionate amount to the similarity measure.</p><h2 id="Shallow-Window-Based-Methods"><a href="#Shallow-Window-Based-Methods" class="headerlink" title="Shallow Window-Based Methods"></a>Shallow Window-Based Methods</h2><p>Another approach is to learn word representations that aid in making predictins within local context windows.</p><h3 id="shortcoming-1"><a href="#shortcoming-1" class="headerlink" title="shortcoming"></a>shortcoming</h3><p>do not operate directly on the co-occurrence statistics of the corpus and fails to take advantage of the vast amount of repetition in the data.</p><h1 id="The-GloVe-Model"><a href="#The-GloVe-Model" class="headerlink" title="The GloVe Model"></a>The GloVe Model</h1><h3 id="GloVe-Global-Vectors"><a href="#GloVe-Global-Vectors" class="headerlink" title="GloVe: Global Vectors"></a>GloVe: Global Vectors</h3><p>the global corpus statistics are captured directly by the model</p><h3 id="the-question-about-the-model-using-the-statistics-of-word-occurrences-in-a-corpus"><a href="#the-question-about-the-model-using-the-statistics-of-word-occurrences-in-a-corpus" class="headerlink" title="the question about the model using the statistics of word occurrences in a corpus"></a>the question about the model using the statistics of word occurrences in a corpus</h3><ul><li>how meaning is generated from these statistics</li><li>how the resulting word vectors might represent that meaning</li></ul><h2 id="some-notation"><a href="#some-notation" class="headerlink" title="some notation"></a>some notation</h2><p>$X_{ij}$ : the number of times word j occurs in the context of word i</p><p>$X_i = \sum_{k} X_{i k}$ : the number of times any word appears in the context of word i</p><p>$P_{i j}=P(j | i)=X_{i j} / X_{i}$: the probability that word j appear in the context of word i</p><p><img src="http://image.nysdy.com/20190515155788329289127.jpg" alt="20190515155788329289127.jpg"></p><p>above that, werd vector learning should be with ratios of co-occurrence probabilities:</p><p><img src="http://image.nysdy.com/20190515155788352871603.jpg" alt="20190515155788352871603.jpg"></p><p>$w \in \mathbb{R}^{d}$are word vectors and $\tilde{w} \in \mathbb{R}^{d}$are separate context word vectors</p><p>For F, we should select a unique choice by enforcing a few desiderata.</p><ul><li><p>encoding the information present the ratio $P_{i k} / P_{j k}$ in the word vector space. </p><p>Since vector spaces are inherently linear structures</p><p><img src="http://image.nysdy.com/20190515155788379647745.jpg" alt="20190515155788379647745.jpg"></p></li><li><p>put F to be a compicated function parameterized, and avoiding bofuscating the linear structure<img src="http://image.nysdy.com/20190515155788397136355.jpg" alt="20190515155788397136355.jpg"></p></li><li><p>the word-word co-occurrence matrices, we can exchange a word and a context word(because a word can also be a context word)</p><ol><li><p>F should be a homomorphism<img src="http://image.nysdy.com/2019051515578842869345.jpg" alt="2019051515578842869345.jpg"></p><p>by Eqn.(3)<img src="http://image.nysdy.com/20190515155788435674239.png" alt="20190515155788435674239.png"></p><p>F = exp or <img src="http://image.nysdy.com/20190515155788448759173.jpg" alt="20190515155788448759173.jpg"></p></li><li><p>the Eqn(6) would have the exchange symmetry if not $\log \left(X_{i}\right)$ and $\log \left(X_{i}\right)$ is independent of k, so it can be absorbed into a bias $b_i$<img src="http://image.nysdy.com/20190515155788566687599.jpg" alt="20190515155788566687599.jpg"></p></li><li><p>for avoiding diverge, $\log \left(X_{i k}\right) \rightarrow \log \left(1+X_{i k}\right)$</p></li><li><p>a new weighted least squares regression model to address the problem that LSA wirhts all co-occuttences equally.</p><p>cost function:<img src="http://image.nysdy.com/20190515155788560186237.jpg" alt="20190515155788560186237.jpg"></p></li><li><p><img src="http://image.nysdy.com/20190515155788571777804.jpg" alt="20190515155788571777804.jpg"></p></li></ol></li></ul><h2 id="Relationship-to-Other-Models"><a href="#Relationship-to-Other-Models" class="headerlink" title="Relationship to Other Models"></a>Relationship to Other Models</h2><p>In this subsection authors show how these models are related to their proposed model.</p><h4 id="the-defect-of-cross-entropy"><a href="#the-defect-of-cross-entropy" class="headerlink" title="the defect of cross entropy"></a>the defect of cross entropy</h4><ul><li>it has the unfortunate property that distributions with long tails are often modeled poorly with too much wieght given to the unlikely events.</li></ul><h2 id="Complexity-of-the-model"><a href="#Complexity-of-the-model" class="headerlink" title="Complexity of the model"></a>Complexity of the model</h2><p>the computational complexity of the model depends on the number of nonzero elects in the matrix $X$</p><h4 id="some-assumptions-about-the-distribution-of-word-co-occurrences"><a href="#some-assumptions-about-the-distribution-of-word-co-occurrences" class="headerlink" title="some assumptions about the distribution of word co-occurrences"></a>some assumptions about the distribution of word co-occurrences</h4><ul><li><p>the number of co-occurrences of word $i$ with word $j$, $X_{ij}$, can be modeled as a power-law function of the frequency rank of that word pair, $r_{ij}$:</p><p>$X_{i j}=\frac{k}{\left(r_{i j}\right)^{\alpha}}$</p></li></ul><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Evaluation-methods"><a href="#Evaluation-methods" class="headerlink" title="Evaluation methods"></a>Evaluation methods</h2><p>authors conduct experiments on the word analogy taks of Mikolov et al. (2013a)</p><h3 id="Word-analogies"><a href="#Word-analogies" class="headerlink" title="Word analogies"></a>Word analogies</h3><p>The word analogy task consists of questions like, â€œa is to b as c is to ?â€</p><h3 id="Word-similarity"><a href="#Word-similarity" class="headerlink" title="Word similarity"></a>Word similarity</h3><p><img src="http://image.nysdy.com/20190520155833277478435.jpg" alt="20190520155833277478435.jpg"></p><h3 id="Named-entity-recognition"><a href="#Named-entity-recognition" class="headerlink" title="Named entity recognition"></a>Named entity recognition</h3><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>Table 2 shows the CloVe model performs significantly better than the other baslines, often with smaller vector sizes and smaller corpora.</p><p><img src="http://image.nysdy.com/20190520155833353468570.jpg" alt="20190520155833353468570.jpg"></p><p>Table 3 shows results on five different word similarity datasets.</p><p>Table 4 shows results on the NER task with the CRF-based model.</p><p><img src="http://image.nysdy.com/20190520155833377540169.jpg" alt="20190520155833377540169.jpg"></p><h2 id="Model-Analysis-Vector-Length-and-Context-Size"><a href="#Model-Analysis-Vector-Length-and-Context-Size" class="headerlink" title="Model Analysis: Vector Length and Context Size"></a>Model Analysis: Vector Length and Context Size</h2><p><img src="http://image.nysdy.com/2019052015583339399087.jpg" alt="2019052015583339399087.jpg"></p><h3 id="Model-Analysis-Corpus-Size"><a href="#Model-Analysis-Corpus-Size" class="headerlink" title="Model Analysis: Corpus Size"></a>Model Analysis: Corpus Size</h3><p><img src="http://image.nysdy.com/20190520155833403240640.jpg" alt="20190520155833403240640.jpg"></p><ul><li>On the syntactic subtask, larger corpora typically produce better statistics so that there is a monotonic increase in performance as the cor- pus size increases.</li><li>But the same trend is not true for the semantic subtask, which is probably because of analogy dataset</li></ul><h2 id="Model-Analysis-Run-time"><a href="#Model-Analysis-Run-time" class="headerlink" title="Model Analysis: Run-time"></a>Model Analysis: Run-time</h2><p><img src="http://image.nysdy.com/20190520155833432462881.jpg" alt="20190520155833432462881.jpg"></p><h2 id="Model-Analysis-Comparison-with-word2vec"><a href="#Model-Analysis-Comparison-with-word2vec" class="headerlink" title="Model Analysis: Comparison with word2vec"></a>Model Analysis: Comparison with word2vec</h2><p>For the same corpus, vocabulary, window size, and training time, GloVe consistently outperforms word2vec</p><h1 id="å‚è€ƒé“¾æ¥"><a href="#å‚è€ƒé“¾æ¥" class="headerlink" title="å‚è€ƒé“¾æ¥"></a>å‚è€ƒé“¾æ¥</h1><ul><li><a href="https://blog.csdn.net/coderTC/article/details/73864097" target="_blank" rel="noopener">https://blog.csdn.net/coderTC/article/details/73864097</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;è®ºæ–‡&lt;a href=&quot;https://www.aclweb.org/anthology/D14-1162&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ä¸‹è½½åœ°å€&lt;/a&gt;ï¼ŒGloVeæ˜¯ä¸€ä¸ªæ–°çš„å…¨çƒå¯¹æ•°åŒçº¿æ€§å›å½’æ¨¡å‹ï¼Œå±äºç»å…¸çš„è¯å‘é‡è¡¨ç¤ºæ–¹æ³•ä¹‹ä¸€ã€‚&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="è®ºæ–‡é˜…è¯»ç¬”è®°" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="word vector" scheme="http://yoursite.com/tags/word-vector/"/>
    
      <category term="GloVe" scheme="http://yoursite.com/tags/GloVe/"/>
    
  </entry>
  
  <entry>
    <title>Deep contextualized word representations é˜…è¯»ç¬”è®°</title>
    <link href="http://yoursite.com/post/Deep%20contextualized%20word%20representations/"/>
    <id>http://yoursite.com/post/Deep contextualized word representations/</id>
    <published>2019-05-10T07:28:06.000Z</published>
    <updated>2019-05-13T06:50:58.808Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://arxiv.org/pdf/1802.05365.pdf" target="_blank" rel="noopener">è®ºæ–‡ä¸‹è½½åœ°å€</a>ï¼ŒELMoäº‹å…ˆç”¨è¯­è¨€æ¨¡å‹å­¦å¥½ä¸€ä¸ªå•è¯çš„ Word Embeddingï¼Œæ­¤æ—¶å¤šä¹‰è¯æ— æ³•åŒºåˆ†ï¼Œä¸è¿‡è¿™æ²¡å…³ç³»ã€‚åœ¨æˆ‘å®é™…ä½¿ç”¨ Word Embedding çš„æ—¶å€™ï¼Œå•è¯å·²ç»å…·å¤‡äº†ç‰¹å®šçš„ä¸Šä¸‹æ–‡äº†ï¼Œè¿™ä¸ªæ—¶å€™æˆ‘å¯ä»¥æ ¹æ®ä¸Šä¸‹æ–‡å•è¯çš„è¯­ä¹‰å»è°ƒæ•´å•è¯çš„ Word Embedding è¡¨ç¤ºï¼Œè¿™æ ·ç»è¿‡è°ƒæ•´åçš„ Word Embedding æ›´èƒ½è¡¨è¾¾åœ¨è¿™ä¸ªä¸Šä¸‹æ–‡ä¸­çš„å…·ä½“å«ä¹‰ï¼Œè‡ªç„¶ä¹Ÿå°±è§£å†³äº†å¤šä¹‰è¯çš„é—®é¢˜äº†ã€‚<strong>æ‰€ä»¥ ELMO æœ¬èº«æ˜¯ä¸ªæ ¹æ®å½“å‰ä¸Šä¸‹æ–‡å¯¹ Word Embedding åŠ¨æ€è°ƒæ•´çš„æ€è·¯ã€‚</strong></p></blockquote><a id="more"></a><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><h2 id="ELMo-Embedddings-from-Language-Models"><a href="#ELMo-Embedddings-from-Language-Models" class="headerlink" title="ELMo(Embedddings from Language Models):"></a>ELMo(Embedddings from Language Models):</h2><h3 id="why-call-ELMo"><a href="#why-call-ELMo" class="headerlink" title="why call ELMo:"></a>why call ELMo:</h3><p>Using vectors derived from a bidirectional LSTM that is trained with a coupled language model(LM) objective on a large text corups.</p><h3 id="characteristics"><a href="#characteristics" class="headerlink" title="characteristics"></a>characteristics</h3><ul><li><p>ELMo representations are a function of all of the internal layers of the biLM.</p></li><li><p>learn a linear combination of the vectors stacked above each input word for each end task</p></li><li><p>the higher-level LSTM states capture context-dependent aspects of word meaning</p><p>the lower-level states model aspects of syntax</p></li></ul><h3 id="Extensive-experiments"><a href="#Extensive-experiments" class="headerlink" title="Extensive experiments"></a>Extensive experiments</h3><ul><li>EMLo representations can be easily added to existing models</li><li>improve the state of art in every case</li><li>ELMo outperform those derived from just the top layer of a LSTM</li></ul><h1 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h1><ul><li><p>Some approaches for learning word vectors only allow a single context-independent representation for each word.</p></li><li><p>to overcome some shortcomings of traditional word vectors:</p><ul><li>enriching them with subword information</li><li>learning separate vectors for each word sense</li></ul><p>Authors uses subword units through the use of character convolutions, seamlessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes.</p></li><li><p>context-depends representations</p><p> Authors take full advantage of access to plentiful monolingual data</p></li><li><p>Previous work also shown that different layers of deep biRNNs encode different types of information</p><ul><li>introducing multi-task syntactic supervision at the lower levels of a deep LSTM can improve overall performance of higher level tasks</li><li>the top layer of an LSTM for encoding word context (Melamud et al., 2016) has been shown to learn representations of word sense.</li></ul><p>ELMo representations can also induce similar signals.</p></li></ul><h1 id="ELMo-Embeddings-from-Language-Models"><a href="#ELMo-Embeddings-from-Language-Models" class="headerlink" title="ELMo: Embeddings from Language Models"></a>ELMo: Embeddings from Language Models</h1><h2 id="Bidirectional-language-models"><a href="#Bidirectional-language-models" class="headerlink" title="Bidirectional language models"></a>Bidirectional language models</h2><ul><li><p>model the probability of token $t_k$ given the history($t_1, â€¦ , t_{k-1}$):</p><p><img src="http://image.nysdy.com/20190512155766627486478.png" alt="20190512155766627486478.png"></p></li><li><p>a backward LM:<img src="http://image.nysdy.com/2019051215576663543534.png" alt="2019051215576663543534.png"></p></li></ul><p>Authorsâ€™ formulation jointly maximizes the log likelihood of the forward and backward directions:</p><p><img src="http://image.nysdy.com/20190512155766643539454.png" alt="20190512155766643539454.png"></p><h2 id="ELMo"><a href="#ELMo" class="headerlink" title="ELMo"></a>ELMo</h2><ul><li><p>For each token $t_k$, a L-layer biLM computes a set of 2L + 1 representations:<img src="http://image.nysdy.com/20190512155767113638451.png" alt="20190512155767113638451.png"></p></li><li><p>For a downstream model, ELMo collapses all layers in R into a single vector.</p><p>In the simplest case, ELMo just selects the top layer.</p></li><li><p>For a task specific weighting of all biLM layers:<img src="http://image.nysdy.com/20190512155767132777658.png" alt="20190512155767132777658.png"></p><p>$s^{task}$ are softmax-normalized weithts and the scalar parameter $Î³^{task}$ allows the task model to scale the entire ELMo vector</p></li></ul><h2 id="Using-biLMs-for-supervised-NLP-tasks"><a href="#Using-biLMs-for-supervised-NLP-tasks" class="headerlink" title="Using biLMs for supervised NLP tasks"></a>Using biLMs for supervised NLP tasks</h2><ul><li>Given a pre-trained biLM and a supervised architecture for a target NLP task</li><li>let the end task model learn a linear combination of these representations<ol><li>consider the lowest layers of th supervised model without the biLM</li><li>add ELMo to the supervised model<ul><li>freeze the weights of the biLM</li><li>concatenate the ELMo vector $ELMo^{task}_k$ with $x_k$ and pass the ELMo enhanced representation $[x_k,;ELMo^{task}_k ]$ into the task RNN.</li><li>for some tasks, authors also include ELMo ar the output of task RNN by introducing another set of out put specific linear weights and replacing $h_k$ with $[h_k,;ELMo^{task}_k ]$</li><li>add a moderate amount of dropout to ELMo and in some case to regularize the ELMo weights</li></ul></li></ol></li></ul><h2 id="Pre-trained-bidirectional-language-model-architecture"><a href="#Pre-trained-bidirectional-language-model-architecture" class="headerlink" title="Pre-trained bidirectional language model architecture"></a>Pre-trained bidirectional language model architecture</h2><ul><li>the biLM provides three layers of representations for each input token, both directions and a residual connection between LSTM layers </li><li>fine tuning the biLM on domain specific data</li></ul><h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><p>the following picture shows the performance of ELMo in Question answering, Textual entailment, Semantic role labeling, Corefrence resolution, Named entity extraction, Sentiment analysis.</p><p><img src="http://image.nysdy.com/2019051315577106394943.png" alt="2019051315577106394943.png"></p><p>In every task considered, simply adding ELMo establishes a new state-of-the-art result.</p><h1 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h1><h2 id="Alternate-layer-weighting-schemes"><a href="#Alternate-layer-weighting-schemes" class="headerlink" title="Alternate layer weighting schemes"></a>Alternate layer weighting schemes</h2><p><img src="http://image.nysdy.com/20190512155767132777658.png" alt="20190512155767132777658.png"></p><p>the following picture compares these alternatives.</p><p><img src="http://image.nysdy.com/20190513155771140936480.png" alt="20190513155771140936480.png"></p><p>Including representations from all layers improves overall performance over just using the last layer, and including contextual representations from the last layer improves performace over the baseline.</p><p>Also shows the $\lambda$ is important.</p><h2 id="Where-to-include-ELMo"><a href="#Where-to-include-ELMo" class="headerlink" title="Where to include ELMo?"></a>Where to include ELMo?</h2><p>The ELMo can be included in both the input and output.</p><p><img src="http://image.nysdy.com/20190513155771190646517.png" alt="20190513155771190646517.png"></p><p>the results show including the ELMo in both input and output can preform better.</p><h2 id="What-information-is-captured-by-the-biLMâ€™s-representations"><a href="#What-information-is-captured-by-the-biLMâ€™s-representations" class="headerlink" title="What information is captured by the biLMâ€™s representations?"></a>What information is captured by the biLMâ€™s representations?</h2><p>Intuitively, the biLM must be disambiguating the meaning of words using their context.<img src="http://image.nysdy.com/20190513155771262634271.png" alt="20190513155771262634271.png"></p><p>The GloVe can only capure the speech. but the biLM is able to disambiguate both the part of speech and word sense in the source sentence.</p><h3 id="Word-sense-disambiguation"><a href="#Word-sense-disambiguation" class="headerlink" title="Word sense disambiguation"></a>Word sense disambiguation</h3><p>given a sentence, predicting  the sense of a target word using a simple 1-nearst negihbor approach</p><p><img src="http://image.nysdy.com/20190513155771312514655.png" alt="20190513155771312514655.png"></p><h3 id="POS-tagging"><a href="#POS-tagging" class="headerlink" title="POS tagging"></a>POS tagging</h3><p>to examine whether the biLM captures basic syntax.</p><p><img src="http://image.nysdy.com/20190513155771328944169.png" alt="20190513155771328944169.png"></p><h2 id="Sample-efficiency"><a href="#Sample-efficiency" class="headerlink" title="Sample efficiency"></a>Sample efficiency</h2><p>Adding ELMo to a model increases the sample efficiency considerably, both in terms of number of parameter updates to reach state-of-the-art performance and the overall training set size.<img src="http://image.nysdy.com/20190513155771349825964.png" alt="20190513155771349825964.png"></p><h2 id="Visualization-of-learned-weights"><a href="#Visualization-of-learned-weights" class="headerlink" title="Visualization of learned weights"></a>Visualization of learned weights</h2><p><img src="http://image.nysdy.com/20190513155771355211483.png" alt="20190513155771355211483.png"></p><h1 id="å‚è€ƒé“¾æ¥"><a href="#å‚è€ƒé“¾æ¥" class="headerlink" title="å‚è€ƒé“¾æ¥"></a>å‚è€ƒé“¾æ¥</h1><ul><li><a href="https://zhuanlan.zhihu.com/p/63115885" target="_blank" rel="noopener">NAACL2018:é«˜çº§è¯å‘é‡(ELMo)è¯¦è§£(è¶…è¯¦ç»†) ç»å…¸</a>ï¼Œè¿™ç¯‡æ–‡ç« ä¸­é˜è¿°äº†ä¸€äº›ä½¿ç”¨çš„ç»†èŠ‚ï¼Œå¹¶ç”¨å›¾æ¥è¡¨ç¤ºï¼Œæ›´åŠ æ¸…æ™°ã€‚</li><li><a href="https://blog.csdn.net/triplemeng/article/details/82380202" target="_blank" rel="noopener">ELMoç®—æ³•ä»‹ç»</a>ï¼Œè¿™ç¯‡åšå®¢ä¸­è‡ªå·±å¯¹æ•´ä¸ªè®ºæ–‡çš„æ¦‚è¿°å’Œæ€»ç»“å’Œå¥½ï¼Œéœ€è¦å­¦ä¹ ã€‚</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1802.05365.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;è®ºæ–‡ä¸‹è½½åœ°å€&lt;/a&gt;ï¼ŒELMoäº‹å…ˆç”¨è¯­è¨€æ¨¡å‹å­¦å¥½ä¸€ä¸ªå•è¯çš„ Word Embeddingï¼Œæ­¤æ—¶å¤šä¹‰è¯æ— æ³•åŒºåˆ†ï¼Œä¸è¿‡è¿™æ²¡å…³ç³»ã€‚åœ¨æˆ‘å®é™…ä½¿ç”¨ Word Embedding çš„æ—¶å€™ï¼Œå•è¯å·²ç»å…·å¤‡äº†ç‰¹å®šçš„ä¸Šä¸‹æ–‡äº†ï¼Œè¿™ä¸ªæ—¶å€™æˆ‘å¯ä»¥æ ¹æ®ä¸Šä¸‹æ–‡å•è¯çš„è¯­ä¹‰å»è°ƒæ•´å•è¯çš„ Word Embedding è¡¨ç¤ºï¼Œè¿™æ ·ç»è¿‡è°ƒæ•´åçš„ Word Embedding æ›´èƒ½è¡¨è¾¾åœ¨è¿™ä¸ªä¸Šä¸‹æ–‡ä¸­çš„å…·ä½“å«ä¹‰ï¼Œè‡ªç„¶ä¹Ÿå°±è§£å†³äº†å¤šä¹‰è¯çš„é—®é¢˜äº†ã€‚&lt;strong&gt;æ‰€ä»¥ ELMO æœ¬èº«æ˜¯ä¸ªæ ¹æ®å½“å‰ä¸Šä¸‹æ–‡å¯¹ Word Embedding åŠ¨æ€è°ƒæ•´çš„æ€è·¯ã€‚&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="è®ºæ–‡é˜…è¯»ç¬”è®°" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="embedding" scheme="http://yoursite.com/tags/embedding/"/>
    
  </entry>
  
  <entry>
    <title>ã€ŠEfficient Estimation of Word Representations in Vector Spaceã€‹é˜…è¯»ç¬”è®°</title>
    <link href="http://yoursite.com/post/Efficient%20Estimation%20of%20Word%20Representations%20in%20Vector%20Space/"/>
    <id>http://yoursite.com/post/Efficient Estimation of Word Representations in Vector Space/</id>
    <published>2019-04-30T02:37:23.000Z</published>
    <updated>2019-05-06T09:00:22.589Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://arxiv.org/pdf/1301.3781.pdf" target="_blank" rel="noopener">è®ºæ–‡ä¸‹è½½åœ°å€</a>ï¼Œè¯¥ç¯‡è®ºæ–‡çš„å¤§ç¯‡å¹…éƒ½åœ¨è®¨è®ºå®éªŒç»“æœçš„åˆ†æï¼Œæ¨¡å‹çš„éƒ¨åˆ†æ¯”è¾ƒç®€å•ï¼Œæ²¡æœ‰è¯¦ç»†åˆ†æï¼Œæœ¬æ¥æ˜¯æƒ³è¯»ä¸€ä¸‹CBOWå’Œskip-gramçš„åŸå§‹è®ºæ–‡ï¼Œå‘ç°å¹¶æ²¡æœ‰æƒ³è±¡ä¸­çš„é‚£ä¹ˆå¤§çš„ç”¨å¤„ã€‚</p></blockquote><a id="more"></a><h2 id="Goals-of-paper"><a href="#Goals-of-paper" class="headerlink" title="Goals of paper"></a>Goals of paper</h2><ul><li>å¼€å‘äº†ä¸¤ç§æ–°æ¨¡å‹ï¼Œå¹¶ä¿ç•™äº†å•è¯ä¹‹é—´çš„çº¿æ€§è§„å¾‹</li><li>è®¾è®¡äº†ä¸€ä¸ªæ–°çš„ç»¼åˆæµ‹è¯•é›†ï¼Œç”¨äºæµ‹é‡å¥æ³•å’Œè¯­ä¹‰è§„å¾‹</li><li>è®¨è®ºäº†è®­ç»ƒæ—¶é—´å’Œå‡†ç¡®æ€§å¦‚ä½•å–å†³äºå•è¯å‘é‡çš„ç»´åº¦å’Œè®­ç»ƒæ•°æ®çš„æ•°é‡</li></ul><h2 id="Model-Architectures"><a href="#Model-Architectures" class="headerlink" title="Model Architectures"></a>Model Architectures</h2><p>è®­ç»ƒå¤æ‚åº¦ï¼š</p><p><img src="http://image.nysdy.com/20190506155712909488421.png" alt="20190506155712909488421.png"></p><p>å…¶ä¸­ï¼ŒEæ˜¯è®­ç»ƒæ¬¡æ•°ï¼ŒTæ˜¯è®­ç»ƒé›†å•è¯æ•°é‡ï¼ŒQæ˜¯æ¨¡å‹ç»“æ„ã€‚</p><h3 id="Feedforward-Neural-Net-Language-Model-NNLM"><a href="#Feedforward-Neural-Net-Language-Model-NNLM" class="headerlink" title="Feedforward Neural Net Language Model (NNLM)"></a>Feedforward Neural Net Language Model (NNLM)</h3><p>å®ƒç”±è¾“å…¥ï¼Œæ˜ å°„ï¼Œéšè—å’Œè¾“å‡ºå±‚ç»„æˆã€‚é€šè¿‡ç®€åŒ–æ–¹æ³•ï¼ŒQ= N x D x H</p><h3 id="Recurrent-Neural-Net-Language-Model-RNNLM"><a href="#Recurrent-Neural-Net-Language-Model-RNNLM" class="headerlink" title="Recurrent Neural Net Language Model (RNNLM)"></a>Recurrent Neural Net Language Model (RNNLM)</h3><p>å…‹æœäº†æ¨¡å‹éœ€è¦å›ºå®šçš„ä¸Šä¸‹æ–‡é•¿åº¦çš„é—®é¢˜ï¼Œå¹¶ä¸”åªæœ‰è¾“å…¥ï¼Œéšè—å’Œè¾“å‡ºå±‚ã€‚</p><p>Q= H x H + H x Vï¼Œå…¶ä¸­H = Dï¼ˆå•è¯è¡¨ç¤ºï¼‰ï¼ŒH x V å¯ä»¥é€šè¿‡åˆ†çº§softmaxè¢«ç®€åŒ–ä¸ºH x log_2(V)ã€‚æ‰€ä»¥ä¸»è¦çš„å¤æ‚åº¦æ¥è‡ªäºH x Hã€‚</p><h3 id="Parallel-Training-of-Neural-Networks"><a href="#Parallel-Training-of-Neural-Networks" class="headerlink" title="Parallel Training of Neural Networks"></a>Parallel Training of Neural Networks</h3><p>æ¨¡å‹ä½¿ç”¨çš„DistBeliefæ¡†æ¶å…è®¸æˆ‘ä»¬å¹¶è¡Œè¿è¡ŒåŒä¸€æ¨¡å‹çš„å¤šä¸ªå‰¯æœ¬ï¼Œæ¯ä¸ªå‰¯æœ¬é€šè¿‡é›†ä¸­çš„æœåŠ¡å™¨åŒæ­¥å…¶æ¢¯åº¦æ›´æ–°ï¼Œè¯¥æœåŠ¡å™¨ä¿ç•™æ‰€æœ‰å‚æ•°</p><h2 id="New-Log-linear-Models"><a href="#New-Log-linear-Models" class="headerlink" title="New Log-linear Models"></a>New Log-linear Models</h2><p>å¤§å¤šæ•°å¤æ‚æ€§æ˜¯ç”±äºæ¨¡å‹ä¸­çš„éçº¿æ€§éšè—å±‚å¼•èµ·çš„ã€‚æ¨¡å‹ç»“æ„å¦‚ä¸‹ï¼š<img src="http://image.nysdy.com/20190506155713050638684.png" alt="20190506155713050638684.png"></p><h3 id="Continuous-Bag-of-Words-Model-CBOW"><a href="#Continuous-Bag-of-Words-Model-CBOW" class="headerlink" title="Continuous Bag-of-Words Model(CBOW)"></a>Continuous Bag-of-Words Model(CBOW)</h3><p>ç¬¬ä¸€ä¸ªæå‡ºçš„ä½“ç³»ç»“æ„ç±»ä¼¼äºå‰é¦ˆNNLMï¼Œå…¶ä¸­å»é™¤äº†éçº¿æ€§éšè—å±‚ï¼Œå¹¶ä¸”æ‰€æœ‰å•è¯ï¼ˆä¸ä»…ä»…æ˜¯æŠ•å½±çŸ©é˜µï¼‰å…±äº«æŠ•å½±å±‚ã€‚ å› æ­¤ï¼Œæ‰€æœ‰å•è¯éƒ½è¢«æŠ•å°„åˆ°ç›¸åŒçš„ä½ç½®ï¼ˆå®ƒä»¬çš„å‘é‡è¢«å¹³å‡ï¼‰ã€‚ å°†è¿™ä¸ªæ¶æ„ç§°ä¸ºè¯è¢‹æ¨¡å‹ï¼Œå› ä¸ºå†å²ä¸­çš„å•è¯é¡ºåºä¸ä¼šå½±å“æŠ•å½±ã€‚</p><p>æ¨¡å‹çš„å¤æ‚åº¦ï¼šQ = N Ã— D + D Ã— log_2(V )</p><h3 id="Continuous-Skip-gram-Model"><a href="#Continuous-Skip-gram-Model" class="headerlink" title="Continuous Skip-gram Model"></a>Continuous Skip-gram Model</h3><p>åŸºäºåŒä¸€å¥å­ä¸­çš„å¦ä¸€ä¸ªå•è¯æœ€å¤§åŒ–å•è¯çš„åˆ†ç±»ã€‚ æ›´å‡†ç¡®åœ°è¯´ï¼Œä½¿ç”¨æ¯ä¸ªå½“å‰å•è¯ä½œä¸ºå…·æœ‰è¿ç»­æŠ•å½±å±‚çš„å¯¹æ•°çº¿æ€§åˆ†ç±»å™¨çš„è¾“å…¥ï¼Œå¹¶é¢„æµ‹å½“å‰å•è¯ä¹‹å‰å’Œä¹‹åçš„ç‰¹å®šèŒƒå›´å†…çš„å•è¯ã€‚</p><p>æ¨¡å‹çš„å¤æ‚åº¦ï¼šQ = C Ã— (D + D Ã— log2(V ))ï¼Œå…¶ä¸­Cæ˜¯å•è¯çš„æœ€å¤§è·ç¦»ã€‚</p><h2 id="å®éªŒ"><a href="#å®éªŒ" class="headerlink" title="å®éªŒ"></a>å®éªŒ</h2><h3 id="ä»»åŠ¡æè¿°"><a href="#ä»»åŠ¡æè¿°" class="headerlink" title="ä»»åŠ¡æè¿°"></a>ä»»åŠ¡æè¿°</h3><p>ä¸ºäº†åº¦é‡è¯å‘é‡çš„è´¨é‡ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªå¤æ‚çš„æµ‹è¯•é›†ï¼Œå®ƒåŒ…æ‹¬äº†äº”ç§ç±»å‹çš„è¯­ä¹‰é—®é¢˜ã€‚ä¹ä¸ªç±»å‹çš„å¥æ³•é—®é¢˜ã€‚åŒ…æ‹¬æ¯ä¸ªç±»åˆ«çš„ä¸¤ä¸ªæ ·æœ¬é›†åœ¨ä¸Šè¡¨å±•ç¤ºï¼›æ€»ä¹‹ï¼Œå…±æ‹¥æœ‰8869ä¸ªè¯­ä¹‰é—®é¢˜å’Œ10675ä¸ªå¥æ³•é—®é¢˜</p><p>ä½œè€…é€šè¿‡ï¼šæœ€å¤§åŒ–ç²¾ç¡®åº¦ ï¼Œæ¨¡å‹ä½“ç³»ç»“æ„çš„æ¯”è¾ƒï¼Œæ¨¡å‹çš„å¤§è§„æ¨¡å¹¶è¡Œè®­ç»ƒæ¥è¯æ˜æå‡ºæ¨¡å‹çš„è¿é€Ÿåº¦å’Œç²¾ç¡®çš„ä¼˜åŠ¿ã€‚</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1301.3781.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;è®ºæ–‡ä¸‹è½½åœ°å€&lt;/a&gt;ï¼Œè¯¥ç¯‡è®ºæ–‡çš„å¤§ç¯‡å¹…éƒ½åœ¨è®¨è®ºå®éªŒç»“æœçš„åˆ†æï¼Œæ¨¡å‹çš„éƒ¨åˆ†æ¯”è¾ƒç®€å•ï¼Œæ²¡æœ‰è¯¦ç»†åˆ†æï¼Œæœ¬æ¥æ˜¯æƒ³è¯»ä¸€ä¸‹CBOWå’Œskip-gramçš„åŸå§‹è®ºæ–‡ï¼Œå‘ç°å¹¶æ²¡æœ‰æƒ³è±¡ä¸­çš„é‚£ä¹ˆå¤§çš„ç”¨å¤„ã€‚&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="è®ºæ–‡é˜…è¯»ç¬”è®°" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="word2vec" scheme="http://yoursite.com/tags/word2vec/"/>
    
  </entry>
  
  <entry>
    <title>Shared Embedding Based Neural Networks for Knowledge Graph Completioné˜…è¯»ç¬”è®°</title>
    <link href="http://yoursite.com/post/Shared%20Embedding%20Based%20Neural%20Networks%20for%20Knowledge%20Graph%20Completion/"/>
    <id>http://yoursite.com/post/Shared Embedding Based Neural Networks for Knowledge Graph Completion/</id>
    <published>2019-04-19T06:52:38.000Z</published>
    <updated>2019-04-19T08:09:35.184Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="http://delivery.acm.org/10.1145/3280000/3271704/p247-guan.pdf?ip=59.64.129.243&amp;id=3271704&amp;acc=ACTIVE%20SERVICE&amp;key=BF85BBA5741FDC6E%2E66A15327C2E204FC%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1555657159_db1582f1a6ea923a16011064e5cc7955" target="_blank" rel="noopener">åŸæ–‡ä¸‹è½½é“¾æ¥</a>ï¼ŒçŸ¥è¯†å›¾è°±è¡¥å…¨ï¼ˆKGCï¼ŒKnowledge Graph Completion)æ˜¯ä¸€ç§è‡ªåŠ¨å»ºç«‹å›¾è°±å†…éƒ¨çŸ¥è¯†å…³è”çš„å·¥ä½œã€‚ç›®æ ‡æ˜¯è¡¥å…¨çŸ¥è¯†å›¾è°±ä¸­ä¸‰å…ƒç»„çš„ç¼ºå¤±éƒ¨åˆ†ã€‚ä¸»è¦æ–¹æ³•ä¸ºåŸºäºå¼ é‡ï¼ˆæˆ–è€…çŸ©é˜µï¼‰å’ŒåŸºäºç¿»è¯‘ä¸¤ç±»ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºå…±äº«åµŒå…¥çš„ç¥ç»ç½‘ç»œçš„æ¨¡å‹ï¼ˆSENNï¼‰æ¥å¤„ç†KGCã€‚</p></blockquote><a id="more"></a><h2 id="Contribulation"><a href="#Contribulation" class="headerlink" title="Contribulation"></a>Contribulation</h2><ul><li>æå‡ºäº†SENNæ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ˜ç¡®åŒºåˆ†å¤´å®ä½“ã€å…³ç³»å’Œä¸ºå®ä½“é¢„æµ‹ä»»åŠ¡ï¼Œå¹¶æŠŠå®ƒä»¬æ•´åˆåˆ°ä¸€ä¸ªåŸºäºå…¨è¿æ¥ç¥ç»ç½‘ç»œæ¡†æ¶ä¸­ï¼Œè¯¥æ¡†æ¶å…±äº«çš„å®ä½“å’Œå…³ç³»åµŒå…¥ã€‚</li><li>SENNæå‡ºäº†ä¸€ä¸ªè‡ªé€‚åº”å…¨ä¸­æŸå¤±æœºåˆ¶ï¼Œè¯¥æ–¹æ³•å¯ä»¥å¾ˆå¥½çš„å¤„ç†å…·æœ‰ä¸åŒæ˜ å°„å±æ€§çš„ä¸‰å…ƒç»„ï¼Œå¹¶å¤„ç†ä¸åŒçš„é¢„æµ‹ä»»åŠ¡ã€‚</li><li>ç”±äºå…³ç³»é¢„æµ‹é€šå¸¸æ¯”å¤´å°¾å®ä½“é¢„æµ‹å…·æœ‰æ›´å¥½çš„æ€§èƒ½ï¼Œæˆ‘ä»¬æŠŠSENNåº”ç”¨åˆ°å¤´å°¾å®ä½“é¢„æµ‹ï¼Œä»è€Œå°†SENNæ‰©å±•åˆ°SENN+ã€‚</li></ul><h2 id="Related-works"><a href="#Related-works" class="headerlink" title="Related works"></a>Related works</h2><h3 id="Tensor-Matrix-Based-Methods"><a href="#Tensor-Matrix-Based-Methods" class="headerlink" title="Tensor/Matrix Based Methods"></a>Tensor/Matrix Based Methods</h3><p>RESCALæ˜¯ä¸€ä¸ªå…¸å‹çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŸºäºä¸‰å‘å¼ é‡å› å­åˆ†è§£çš„æ–¹æ³•ã€‚</p><p>ç›®æ ‡å‡½æ•°ä¸ºï¼š<img src="http://image.nysdy.com/20190419155565817094503.png" alt="20190419155565817094503.png"></p><p>$M_r$æ˜¯rçš„å…³ç³»çŸ©é˜µï¼Œå¤§å°ä¸ºk x kã€‚</p><p>ComlExæ˜¯æœ€è¿‘æå‡ºçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŸºäºçŸ©é˜µåˆ†è§£ï¼Œå¹¶ä¸”å®ƒä½¿ç”¨å¤æ•°å€¼æ¥å®šä¹‰å®ä½“å’Œå…³ç³»çš„åµŒå…¥ã€‚</p><p>ç›®æ ‡å‡½æ•°ä¸ºï¼š<img src="http://image.nysdy.com/20190419155565837140789.png" alt="20190419155565837140789.png"></p><p>Re(x)è¿”å›xçš„å®éƒ¨ã€‚</p><h3 id="Translation-Based-Methods"><a href="#Translation-Based-Methods" class="headerlink" title="Translation Based Methods"></a>Translation Based Methods</h3><p>ä»£è¡¨æ¨¡å‹ä¸ºç»å…¸çš„TransEæ¨¡å‹ï¼ˆè¿™é‡Œä¸å†èµ˜è¿°ï¼‰</p><h3 id="Translation-Based-Methods-1"><a href="#Translation-Based-Methods-1" class="headerlink" title="Translation Based Methods"></a>Translation Based Methods</h3><p>ER-MLPä½¿ç”¨å¤šå±‚æ„ŸçŸ¥å™¨æ¥æ•è·å¤´å®ä½“ï¼Œå…³ç³»å’Œå°¾å®ä½“ä¹‹é—´çš„éšå¼äº¤äº’ã€‚</p><p>ç›®æ ‡å‡½æ•°ä¸ºï¼š<img src="http://image.nysdy.com/2019041915556586361053.png" alt="2019041915556586361053.png"></p><p>ProjEä½¿ç”¨å…·æœ‰ç»„åˆå±‚å’ŒæŠ•å½±å±‚çš„ç¥ç»ç½‘ç»œæ¥å¯¹å¤´å°¾å®ä½“é¢„æµ‹å»ºæ¨¡ã€‚</p><h2 id="THE-SENN-METHOD"><a href="#THE-SENN-METHOD" class="headerlink" title="THE SENN METHOD"></a>THE SENN METHOD</h2><p>æ¨¡å‹ç»“æ„å¦‚å›¾æ‰€ç¤ºï¼š<img src="http://image.nysdy.com/20190419155565880062506.png" alt="20190419155565880062506.png"></p><p>ä½œè€…å°†æ¡†æ¶åˆ’åˆ†ä¸ºä»¥ä¸‹å››ä¸ªéƒ¨åˆ†ï¼š</p><ol><li>ä¸‰å…ƒç»„çš„æ‰¹é‡é¢„å¤„ç†</li><li>çŸ¥è¯†å›¾è°±çš„Shared embeddingsè¡¨ç¤ºå­¦ä¹ </li><li>ç‹¬ç«‹çš„å¤´å°¾å®ä½“åŠå…³ç³»é¢„æµ‹å­æ¨¡å‹è®­ç»ƒä¸èåˆ</li><li>è”åˆæŸå¤±å‡½æ•°æ„æˆ</li></ol><p>æ•´ä¸ªKGCçš„æµç¨‹å¯ä»¥æè¿°å¦‚ä¸‹ï¼š</p><ol><li>å°†è®­ç»ƒæ•°æ®ä¸­çš„å®Œæ•´ä¸‰å…ƒç»„ï¼ˆçŸ¥è¯†å›¾è°±ï¼‰åˆ’åˆ†æ‰¹é‡åä½œä¸ºæ¨¡å‹çš„è¾“å…¥</li><li>å¯¹äºè¾“å…¥çš„ä¸‰å…ƒç»„ï¼Œåˆ†åˆ«è®­ç»ƒå¾—åˆ°å®ä½“ï¼ˆåŒ…æ‹¬å¤´å°¾å®ä½“ï¼‰åµŒå…¥çŸ©é˜µä¸å…³ç³»åµŒå…¥çŸ©é˜µï¼ˆembeddingsï¼‰</li><li>å°†å¤´å°¾å®ä½“åŠå…³ç³»embeddingsåˆ†åˆ«è¾“å…¥åˆ°ä¸‰ä¸ªé¢„æµ‹æ¨¡å‹ä¸­ï¼ˆå¤´å®ä½“é¢„æµ‹ï¼ˆ?, r, tï¼‰ï¼Œå…³ç³»é¢„æµ‹(h, ?, t)ï¼Œå°¾å®ä½“é¢„æµ‹(h, r, ?)ï¼‰</li></ol><h3 id="The-Three-Substructures"><a href="#The-Three-Substructures" class="headerlink" title="The Three Substructures"></a>The Three Substructures</h3><p>é¢„æµ‹å­æ¨¡å‹å…·æœ‰ç›¸ä¼¼çš„ç»“æ„å¦‚ä¸‹å›¾ï¼Œæ¨¡å‹è¾“å…¥å…³ç³»å‘é‡ä¸å®ä½“å‘é‡åï¼Œè¿›å…¥nå±‚å…¨è¿æ¥å±‚ï¼Œå¾—åˆ°é¢„æµ‹å‘é‡ï¼Œå†ç»è¿‡ä¸€ä¸ªsigmoidï¼ˆæˆ–è€…softmaxï¼‰å±‚ï¼Œè¾“å‡ºé¢„æµ‹æ ‡ç­¾å‘é‡ã€‚<img src="http://image.nysdy.com/20190419155565925349707.png" alt="20190419155565925349707.png"></p><p>å¤´å®ä½“é¢„æµ‹ç›®æ ‡å‡½æ•°ï¼š<img src="http://image.nysdy.com/20190419155565929066802.png" alt="20190419155565929066802.png"></p><p>f(x)= max(0,x).</p><p>é¢„æµ‹æ ‡ç­¾ï¼š<img src="http://image.nysdy.com/20190419155565936981620.png" alt="20190419155565936981620.png"></p><p>å…¶å®ƒä¸¤ç§ä¸æ­¤å¤´å®ä½“ç±»ä¼¼ã€‚</p><h3 id="Model-Training"><a href="#Model-Training" class="headerlink" title="Model Training"></a>Model Training</h3><h4 id="The-General-Loss-Function"><a href="#The-General-Loss-Function" class="headerlink" title="The General Loss Function"></a>The General Loss Function</h4><p>æ¨¡å‹ç›®æ ‡æ ‡ç­¾å‘é‡è¡¨ç¤ºä¸ºï¼š<img src="http://image.nysdy.com/20190419155565949172586.png" alt="20190419155565949172586.png"></p><p>$I_h$æ˜¯åœ¨è®­ç»ƒé›†ä¸­ç»™å®šrå’Œtçš„æ‰€æœ‰æœ‰æ•ˆå¤´å®ä½“é›†ã€‚</p><p>ä¸‰è€…çš„å¹³æ»‘å‘é‡è¡¨ç¤ºä¸ºï¼š<img src="http://image.nysdy.com/20190419155565967448577.png" alt="20190419155565967448577.png"></p><p>ä¸‰ä¸ªé¢„æµ‹ä»»åŠ¡çš„æŸå¤±å‡½æ•°ä¸ºï¼š<img src="http://image.nysdy.com/20190419155565972110097.png" alt="20190419155565972110097.png"></p><p>æ€»æŸå¤±å‡½æ•°ä¸ºï¼š<img src="http://image.nysdy.com/20190419155565974814392.png" alt="20190419155565974814392.png"></p><h4 id="The-Adaptively-Weighted-Loss-Mechanism"><a href="#The-Adaptively-Weighted-Loss-Mechanism" class="headerlink" title="The Adaptively Weighted Loss Mechanism."></a>The Adaptively Weighted Loss Mechanism.</h4><p>è¯¥æ–¹æ³•çš„åŠ¨æœºï¼š</p><ul><li>åœ¨çŸ¥è¯†å›¾è°±ä¸­çš„ä¸‰å…ƒç»„æœ‰4ç§ç±»å‹ï¼š1-TO-1, 1-TO-M, M-TO-1 and M-TO-Mã€‚æ‰€ä»¥é¢„æµ‹åœ¨è®­ç»ƒé›†ä¸­å…·æœ‰çš„æœ‰æ•ˆå®ä½“/å…³ç³»è¶Šå¤šï¼Œå®ƒå°±è¶Šä¸ç¡®å®šã€‚æ‰€ä»¥ä½œè€…å°†å¯¹åº”äºå¤´éƒ¨å®ä½“é¢„æµ‹ï¼Œå…³ç³»é¢„æµ‹å’Œå°¾éƒ¨å®ä½“é¢„æµ‹çš„æŸå¤±çš„æƒé‡ä¸æœ‰æ•ˆå®ä½“çš„æ•°é‡ç›¸å…³è”ã€‚</li><li>å› ä¸ºå…³ç³»é¢„æµ‹æ¯”å®ä½“é¢„æµ‹æ›´åŠ å®¹æ˜“ã€‚æ‰€ä»¥ä½œè€…åŠ å¤§å¯¹å¤´å°¾å®ä½“çš„é”™è¯¯é¢„æµ‹çš„æƒ©ç½šã€‚</li></ul><p>æ‰€ä»¥ä½œè€…å¾—åˆ°æ–°çš„æŸå¤±å‡½æ•°ï¼š<img src="http://image.nysdy.com/20190419155566013038361.png" alt="20190419155566013038361.png"></p><p>æ€»æŸå¤±å‡½æ•°å˜ä¸ºï¼š<img src="http://image.nysdy.com/20190419155566016395908.png" alt="20190419155566016395908.png"></p><h2 id="THE-SENN-METHOD-1"><a href="#THE-SENN-METHOD-1" class="headerlink" title="THE SENN+METHOD"></a>THE SENN+METHOD</h2><p>ä½œè€…ç›¸ä¿¡å¯ä»¥è¿›ä¸€æ­¥åˆ©ç”¨å…³ç³»é¢„æµ‹çš„ç›¸å½“å¥½çš„æ€§èƒ½æ¥è¾…åŠ©æµ‹è¯•è¿‡ç¨‹ä¸­çš„å¤´éƒ¨å’Œå°¾éƒ¨å®ä½“é¢„æµ‹ã€‚</p><p>ç»™å®šå¤´éƒ¨é¢„æµ‹ä»»åŠ¡ï¼ˆï¼Ÿï¼Œrï¼Œtï¼‰å¹¶å‡è®¾hæ˜¯æœ‰æ•ˆçš„å¤´éƒ¨å®ä½“ã€‚ å¦‚æœæˆ‘ä»¬é‡‡ç”¨SENNæ–¹æ³•æ¥é¢„æµ‹hå’Œtä¹‹é—´çš„å…³ç³»ï¼Œå³æ‰§è¡Œå…³ç³»é¢„æµ‹ä»»åŠ¡ï¼ˆhï¼Œï¼Ÿï¼Œtï¼‰ï¼Œåˆ™å…³ç³»ræœ€æœ‰å¯èƒ½å…·æœ‰ é¢„æµ‹æ ‡ç­¾é«˜äºå…¶ä»–å…³ç³»ï¼Œå› æ­¤åº”æ’åé«˜äºå…¶ä»–å…³ç³»ã€‚</p><p><img src="http://image.nysdy.com/20190419155566073220975.png" alt="20190419155566073220975.png"></p><p>å…¶ä¸­Valueï¼ˆxï¼Œrï¼‰è¿”å›å¯¹åº”äºå…³ç³»rçš„å‘é‡xçš„æ¡ç›®; Rankï¼ˆxï¼Œrï¼‰ä»¥é™åºè¿”å›å¯¹åº”äºå…³ç³»rçš„å‘é‡xçš„æ¡ç›®çš„ç­‰çº§ã€‚</p><p>æœ€åSENN+ç§é¢„æµ‹æ ‡ç­¾ä¸ºï¼š<img src="http://image.nysdy.com/20190419155566089424123.png" alt="20190419155566089424123.png"></p><p>å…¶ä¸­<img src="http://image.nysdy.com/20190419155566090898589.png" alt="20190419155566090898589.png"></p><h2 id="EXPERIMENTS"><a href="#EXPERIMENTS" class="headerlink" title="EXPERIMENTS"></a>EXPERIMENTS</h2><h3 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h3><p><img src="http://image.nysdy.com/20190419155566095994824.png" alt="20190419155566095994824.png"></p><h3 id="Entity-Prediction"><a href="#Entity-Prediction" class="headerlink" title="Entity Prediction"></a>Entity Prediction</h3><p><img src="http://image.nysdy.com/20190419155566101939979.png" alt="20190419155566101939979.png"></p><p><img src="http://image.nysdy.com/20190419155566104394441.png" alt="20190419155566104394441.png"></p><h3 id="Relation-Prediction"><a href="#Relation-Prediction" class="headerlink" title="Relation Prediction"></a>Relation Prediction</h3><p><img src="http://image.nysdy.com/20190419155566106385514.png" alt="20190419155566106385514.png"></p><p>è®ºæ–‡è¿˜è¿›è¡Œäº†å…±äº«åµŒå…¥å’Œè‡ªé€‚åº”æƒé‡æŸå¤±æœºåˆ¶æœ‰æ•ˆæ€§çš„éªŒè¯ã€‚</p><h2 id="å‚è€ƒé“¾æ¥"><a href="#å‚è€ƒé“¾æ¥" class="headerlink" title="å‚è€ƒé“¾æ¥"></a>å‚è€ƒé“¾æ¥</h2><ul><li><a href="http://blog.openkg.cn/%E8%AE%BA%E6%96%87%E6%B5%85%E5%B0%9D-%E9%9D%A2%E5%90%91%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E8%A1%A5%E5%85%A8%E7%9A%84%E5%85%B1%E4%BA%AB%E5%B5%8C%E5%85%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">http://blog.openkg.cn/%E8%AE%BA%E6%96%87%E6%B5%85%E5%B0%9D-%E9%9D%A2%E5%90%91%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E8%A1%A5%E5%85%A8%E7%9A%84%E5%85%B1%E4%BA%AB%E5%B5%8C%E5%85%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;http://delivery.acm.org/10.1145/3280000/3271704/p247-guan.pdf?ip=59.64.129.243&amp;amp;id=3271704&amp;amp;acc=ACTIVE%20SERVICE&amp;amp;key=BF85BBA5741FDC6E%2E66A15327C2E204FC%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;amp;__acm__=1555657159_db1582f1a6ea923a16011064e5cc7955&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;åŸæ–‡ä¸‹è½½é“¾æ¥&lt;/a&gt;ï¼ŒçŸ¥è¯†å›¾è°±è¡¥å…¨ï¼ˆKGCï¼ŒKnowledge Graph Completion)æ˜¯ä¸€ç§è‡ªåŠ¨å»ºç«‹å›¾è°±å†…éƒ¨çŸ¥è¯†å…³è”çš„å·¥ä½œã€‚ç›®æ ‡æ˜¯è¡¥å…¨çŸ¥è¯†å›¾è°±ä¸­ä¸‰å…ƒç»„çš„ç¼ºå¤±éƒ¨åˆ†ã€‚ä¸»è¦æ–¹æ³•ä¸ºåŸºäºå¼ é‡ï¼ˆæˆ–è€…çŸ©é˜µï¼‰å’ŒåŸºäºç¿»è¯‘ä¸¤ç±»ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºå…±äº«åµŒå…¥çš„ç¥ç»ç½‘ç»œçš„æ¨¡å‹ï¼ˆSENNï¼‰æ¥å¤„ç†KGCã€‚&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="è®ºæ–‡é˜…è¯»ç¬”è®°" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="çŸ¥è¯†å›¾è°±" scheme="http://yoursite.com/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    
      <category term="ç¥ç»ç½‘ç»œ" scheme="http://yoursite.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="çŸ¥è¯†å›¾è°±è¡¥å…¨" scheme="http://yoursite.com/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E8%A1%A5%E5%85%A8/"/>
    
  </entry>
  
  <entry>
    <title>ã€ŠBootstrapping Entity Alignment with Knowledge Graph Embeddingã€‹é˜…è¯»ç¬”è®°</title>
    <link href="http://yoursite.com/post/Bootstrapping%20Entity%20Alignment%20with%20Knowledge%20Graph%20Embedding/"/>
    <id>http://yoursite.com/post/Bootstrapping Entity Alignment with Knowledge Graph Embedding/</id>
    <published>2019-04-09T01:01:13.000Z</published>
    <updated>2019-04-09T02:58:37.713Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://www.ijcai.org/proceedings/2018/0611.pdf" target="_blank" rel="noopener">è®ºæ–‡ä¸‹è½½åœ°å€</a>ï¼Œé‡‡ç”¨äº†bootstrappingæ–¹æ³•æ¥è§£å†³ç¼ºä¹è®­ç»ƒæ•°æ®çš„è¿‡ç¨‹ï¼Œæå‡ºäº†æˆªæ–­å‡åŒ€è´Ÿé‡‡æ ·æ¥æé«˜è´Ÿæ ·ä¾‹å¯¹äºç›®æ ‡å‡½æ•°çš„è´¡çŒ®ï¼Œé‡‡ç”¨åŸºäºé™åˆ¶çš„ç›®æ ‡å‡½æ•°æ¥æŒ‰éœ€è°ƒæ•´æ­£è´Ÿæ ·ä¾‹çš„å¾—åˆ†ã€‚</p></blockquote><a id="more"></a><p>åŸºäºåµŒå…¥çš„å®ä½“å¯¹é½å°†ä¸åŒçš„çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰è¡¨ç¤ºä¸ºä½ç»´åµŒå…¥ï¼Œå¹¶é€šè¿‡æµ‹é‡å®ä½“åµŒå…¥ä¹‹é—´çš„ç›¸ä¼¼æ€§æ¥æŸ¥æ‰¾å®ä½“å¯¹é½ã€‚å…¶ä¸­ï¼Œå¤§é‡æ–¹æ³•æ‰€é¢ä¸´çš„ä¸€ä¸ªæŒ‘æˆ˜æ˜¯ï¼šç¼ºä¹è¶³å¤Ÿçš„å…ˆå‰å¯¹é½ä½œä¸ºæ ‡è®°çš„è®­ç»ƒæ•°æ®ã€‚</p><h2 id="è´¡çŒ®"><a href="#è´¡çŒ®" class="headerlink" title="è´¡çŒ®"></a>è´¡çŒ®</h2><ul><li>ä½œè€…æŠŠå®ä½“å¯¹é½å»ºæ¨¡ä¸ºä¸€ä¸ªåˆ†ç±»é—®é¢˜ï¼Œå…¶åŸºäºKGåµŒå…¥æ¥å¯»æ±‚æœ€å¤§åŒ–æ‰€æœ‰æ ‡è®°å’Œæœªæ ‡è®°çš„å®ä½“å¯¹é½å¯èƒ½æ€§</li><li>å¯¹äºé¢å‘å¯¹é½çš„KGåµŒå…¥ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºé™åˆ¶çš„ç›®æ ‡å‡½æ•°ï¼›ä¸ºäº†å¯¹ä¸å¤ªå¯èƒ½åŒºåˆ†çš„è´Ÿä¸‰å…ƒç»„è¿›è¡ŒæŠ½æ ·ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æˆªæ–­å‡åŒ€çš„è´ŸæŠ½æ ·æ–¹æ³•ã€‚</li><li>ä½œè€…æå‡ºäº†ä¸€ä¸ªè‡ªä¸¾è¿‡ç¨‹ï¼ˆbootstrappingï¼‰æ¥å…‹æœç¼ºä¹è¶³å¤Ÿè®­ç»ƒæ•°æ®ï¼Œé€šè¿‡æ ‡è®°å¯èƒ½çš„å¯¹é½å¹¶è¿­ä»£åœ°å°†å…¶æ·»åŠ åˆ°è®­ç»ƒæ•°æ®ä¸­æ¥æ›´æ–°é¢å‘å¯¹é½çš„åµŒå…¥ã€‚</li><li>ä½œè€…åœ¨ä¸‰ä¸ªè·¨è¯­è¨€å’Œä¸¤ä¸ªå¤§å‹æ•°æ®é›†ä¸Šè¯„ä¼°äº†æ‰€æå‡ºçš„æ–¹æ³•ï¼Œè¡¨æ˜æ‰€æå‡ºçš„æ–¹æ³•æ˜æ˜¾ä¼˜äºä¸‰ç§æœ€å…ˆè¿›çš„å®ä½“å¯¹é½æ–¹æ³•ã€‚</li></ul><h2 id="é—®é¢˜æè¿°"><a href="#é—®é¢˜æè¿°" class="headerlink" title="é—®é¢˜æè¿°"></a>é—®é¢˜æè¿°</h2><p>æœ€å¤§ä¼¼ç„¶å‡†åˆ™æŒ‡å¯¼é€‰æ‹©å®ç°æœ€é«˜å¯¹é½å¯èƒ½æ€§çš„æœ€ä½³Î¸</p><p><img src="http://image.nysdy.com/20190409155477420219240.png" alt="20190409155477420219240.png"></p><p>å…¶ä¸­ï¼ŒL_xä»£è¡¨å®ä½“xçš„çœŸå®æ ‡ç­¾ï¼Œ1_[]æ˜¯ä¸€ä¸ªæŒ‡ç¤ºå‡½æ•°ï¼Œè¡¨ç¤ºç»™å®šå‘½é¢˜çš„çœŸå€¼ï¼ˆ0æˆ–1ï¼‰ã€‚ä½†æ˜¯å¯¹äºæ²¡æœ‰æ ‡ç­¾çš„å®ä½“ï¼Œæƒ³è¦é€šè¿‡ä¸Šè¿°æ¥å¾—åˆ°thetaå°±å¾ˆå›°éš¾ã€‚</p><h2 id="æ¨¡å‹"><a href="#æ¨¡å‹" class="headerlink" title="æ¨¡å‹"></a>æ¨¡å‹</h2><h3 id="é¢å‘å¯¹é½çš„KGåµŒå…¥"><a href="#é¢å‘å¯¹é½çš„KGåµŒå…¥" class="headerlink" title="é¢å‘å¯¹é½çš„KGåµŒå…¥"></a>é¢å‘å¯¹é½çš„KGåµŒå…¥</h3><p>ä½œè€…æå‡ºäº†ä¸€ä¸ªç›®æ ‡å‡½æ•°ï¼š<img src="http://image.nysdy.com/2019040915547745103625.png" alt="2019040915547745103625.png"></p><p>è¯¥ç›®æ ‡å‡½æ•°æœ‰ä¸¤ä¸ªæœŸæœ›çš„å±æ€§ï¼š</p><ul><li>é¢„æœŸæ­£ä¸‰å…ƒç»„å¾—åˆ†è¾ƒä½ï¼Œè€Œè´Ÿä¸‰å…ƒç»„å¾—åˆ†è¾ƒé«˜ã€‚ä¾‹å¦‚f(r)&lt;= r_1 å¹¶ä¸” f(râ€™)&gt;=r_2ï¼Œè®¾ç½®æ—¶r_2&gt;r_1,ä¸”r_1æ˜¯ä¸€ä¸ªå°çš„æ­£å€¼ã€‚</li><li>ä»ç„¶å¯ä»¥å¾—åˆ°f(râ€™)-f(r)&gt;=r_2 - r_1ï¼Œè¿™è¡¨æ˜æ‰€æå‡ºçš„ç›®æ ‡å‡½æ•°ä»ç„¶ä¿ç•™äº†åŸºäºè¾¹é™…æ’åºæŸå¤±çš„ç‰¹å¾ã€‚</li></ul><h4 id="æˆªæ–­å‡åŒ€è´Ÿé‡‡æ ·"><a href="#æˆªæ–­å‡åŒ€è´Ÿé‡‡æ ·" class="headerlink" title="æˆªæ–­å‡åŒ€è´Ÿé‡‡æ ·"></a>æˆªæ–­å‡åŒ€è´Ÿé‡‡æ ·</h4><p>å¦‚æœæ ·ä¾‹å¤ªå®¹æ˜“åŒºåˆ†ï¼Œé‚£ä¹ˆå¯¹æ•´ä¸ªçš„åµŒå…¥å­¦ä¹ çš„è´¡çŒ®ä¼šå¾ˆå°ã€‚</p><p>æ‰€ä»¥ï¼Œä½œè€…é‡‡ç”¨åœ¨åµŒå…¥ç©ºé—´ä¸­sæœ€è¿‘çš„é‚»å±…ä½œä¸ºå€™é€‰é›†ï¼Œå‰”é™¤é‚£äº›å’Œå®ä½“xç›¸ä¼¼åº¦è¿‡ä½çš„æ•°æ®ã€‚</p><h3 id="å¼•å¯¼å¯¹é½ï¼ˆBootstrapping-Alignmentï¼‰"><a href="#å¼•å¯¼å¯¹é½ï¼ˆBootstrapping-Alignmentï¼‰" class="headerlink" title="å¼•å¯¼å¯¹é½ï¼ˆBootstrapping Alignmentï¼‰"></a>å¼•å¯¼å¯¹é½ï¼ˆBootstrapping Alignmentï¼‰</h3><p>ä½œè€…è¿­ä»£åœ°å°†å¯èƒ½çš„å¯¹é½æ ‡è®°ä½œä¸ºè®­ç»ƒæ•°æ®ï¼Œå¹¶ä½¿ç”¨å®ƒæ¥è¿›ä¸€æ­¥æ”¹è¿›å®ä½“åµŒå…¥å’Œå¯¹é½ã€‚</p><h4 id="å¯èƒ½çš„å¯¹é½æ ‡ç­¾å’Œç¼–è¾‘"><a href="#å¯èƒ½çš„å¯¹é½æ ‡ç­¾å’Œç¼–è¾‘" class="headerlink" title="å¯èƒ½çš„å¯¹é½æ ‡ç­¾å’Œç¼–è¾‘"></a>å¯èƒ½çš„å¯¹é½æ ‡ç­¾å’Œç¼–è¾‘</h4><p>ä½œè€…ä¸ºäº†å®ç°æœ€å¤§åŒ–å¯¹é½å¯èƒ½æ€§å¹¶éµå®ˆä¸€å¯¹ä¸€å¯¹é½çº¦æŸï¼Œæå‡ºä»¥ä¸‹ä¼˜åŒ–é—®é¢˜æ¥æ ‡è®°ç¬¬tæ¬¡è¿­ä»£ï¼š</p><p><img src="http://image.nysdy.com/20190409155477548037155.png" alt="20190409155477548037155.png"></p><p>Yâ€™_x = {y|y âˆˆ Yâ€™ and  Ï€(y|x; Î˜^(t)) &gt; Î³3}ä»£è¡¨æ ‡ç­¾xçš„å€™é€‰é›†ï¼›Ïˆ^(t)(Â·)æ˜¯ä¸€ä¸ªæŒ‡ç¤ºå‡½æ•°ï¼Œåªæœ‰å½“xåœ¨ç¬¬tæ¬¡è¿­ä»£æ—¶æ ‡ç­¾ä¸ºyæ—¶ä¸º1ï¼Œå…¶å®ƒæƒ…å†µä¸º0ã€‚ä¸¤ä¸ªé™åˆ¶æ¡ä»¶ä¿è¯äº†ä¸€å¯¹ä¸€çš„æ ‡ç­¾ã€‚è¿™æ—¶å¾—åˆ°äº†ä¸€ä¸ªæ–°çš„æ ‡ç­¾å¯¹é½ï¼š<img src="http://image.nysdy.com/20190409155477586716499.png" alt="20190409155477586716499.png"></p><p>ä¸ºäº†æé«˜æ ‡ç­¾è´¨é‡å¹¶æ»¡è¶³ä¸€å¯¹ä¸€çš„å¯¹é½çº¦æŸï¼Œåœ¨è‡ªä¸¾è¿‡ç¨‹ä¸­ï¼Œä¸€æ—¦è¢«æ ‡è®°çš„å®ä½“å¯ä»¥åœ¨éšåçš„æ ‡è®°ä¸­é‡æ–°æ ‡è®°æˆ–å˜ä¸ºæœªæ ‡è®°çš„å®ä½“ã€‚</p><p>å½“å‘ç”Ÿä¸¤ä¸ªæ ‡ç­¾å†²çªæ—¶ï¼Œæˆ‘ä»¬é€šè¿‡è®¡ç®—ä¸‹é¢çš„ä¼¼ç„¶å·®å¼‚æ¥ç¡®å®šä¿ç•™å“ªä¸ªï¼š<img src="http://image.nysdy.com/20190409155477607781047.png" alt="20190409155477607781047.png"></p><p>å½“è¯¥å€¼å¤§äº0è¯´æ˜å‰è€…å…·æœ‰æ›´å¤§çš„å¯¹é½æ¦‚ç‡ã€‚</p><h4 id="ä»æ•´ä½“è§’åº¦å­¦ä¹ "><a href="#ä»æ•´ä½“è§’åº¦å­¦ä¹ " class="headerlink" title="ä»æ•´ä½“è§’åº¦å­¦ä¹ "></a>ä»æ•´ä½“è§’åº¦å­¦ä¹ </h4><p>ä¸ºäº†è·å¾—æ ‡è®°å’Œæœªæ ‡è®°å®ä½“çš„æ•´ç†è§‚å¯Ÿï¼Œä½œè€…å®šä¹‰äº†æ¦‚ç‡åˆ†å¸ƒÏ†xæ¥æè¿°æ‰€æœ‰xå¯èƒ½çš„æ¦‚ç‡åˆ†å¸ƒã€‚</p><p><img src="http://image.nysdy.com/20190409155477633963211.png" alt="20190409155477633963211.png"></p><p>ç”±æ­¤ï¼Œä½œè€…å¾—åˆ°äº†æœ€å°åŒ–ä¸‹é¢çš„ä¼¼ç„¶å‡½æ•°æ¥å¾—åˆ°Î˜ï¼š<img src="http://image.nysdy.com/20190409155477641976513.png" alt="20190409155477641976513.png"></p><p>å› ä¸ºï¼ŒåµŒå…¥ä¸ä»…åº”è¯¥æ•è·å¯¹é½å¯èƒ½æ€§ï¼Œè¿˜åº”è¯¥æ¨¡æ‹ŸKGçš„è¯­ä¹‰ï¼Œæ‰€ä»¥ä½œè€…æœ€åå®šä¹‰è”åˆç›®æ ‡å‡½æ•°ï¼š</p><p><img src="http://image.nysdy.com/20190409155477650541518.png" alt="20190409155477650541518.png"></p><h2 id="å®éªŒ"><a href="#å®éªŒ" class="headerlink" title="å®éªŒ"></a>å®éªŒ</h2><h3 id="æ•°æ®é›†"><a href="#æ•°æ®é›†" class="headerlink" title="æ•°æ®é›†"></a>æ•°æ®é›†</h3><ul><li>DBP15K [Sun et al., 2017]åŒ…å«ä¸‰ä¸ªè·¨è¯­è¨€æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†æ˜¯ä»DBpediaçš„å¤šè¯­è¨€ç‰ˆæœ¬æ„å»ºçš„ã€‚DBPZH-EN(Chinese to English), DBPJA-EN(Japanese to English) and DBPFR-EN(French to English)æ¯ä¸ªæ•°æ®é›†åŒ…å«15ï¼Œ000ä¸ªå‚è€ƒå®ä½“å¯¹é½ã€‚</li><li>DWY100KåŒ…å«ä»DBpediaï¼ŒWikidataå’ŒYAGO3ä¸­æå–çš„ä¸¤ä¸ªå¤§å‹æ•°æ®é›†ï¼Œç”±DBP-WDå’ŒDBP-YGè¡¨ç¤ºã€‚ æ¯ä¸ªæ•°æ®é›†éƒ½æœ‰10ä¸‡ä¸ªå‚è€ƒå®ä½“å¯¹é½<img src="http://image.nysdy.com/20190409155477678270217.png" alt="20190409155477678270217.png"></li></ul><h3 id="å®éªŒè®¾ç½®"><a href="#å®éªŒè®¾ç½®" class="headerlink" title="å®éªŒè®¾ç½®"></a>å®éªŒè®¾ç½®</h3><p>ä½œè€…é€‰å–äº†ä¸‰ç§æœ€å…ˆè¿›çš„åŸºäºåµŒå…¥çš„æ–¹æ³•æ¥å®ç°å®ä½“å¯¹é½ã€‚</p><ul><li>MTransE [Chen et al., 2017]ï¼Œé€‰å–äº†ç¬¬å››ç§å˜ä½“ï¼ˆè¡¨ç°æœ€ä½³ï¼‰ã€‚</li><li>IPTransE[Zhu et al., 2017]æ˜¯ä¸€ä¸ªè¿­ä»£æ–¹æ³•</li><li>JAPE [Sun et al., 2017]ç»“åˆäº†å®ä½“å¯¹é½çš„å…³ç³»å’Œå±æ€§åµŒå…¥</li><li>AlignEé¢å‘å¯¹é½çš„KGåµŒå…¥æ¨¡å‹çš„å®ç°ï¼Œå…·æœ‰æˆªæ–­çš„å‡åŒ€è´Ÿé‡‡æ ·å’Œå‚æ•°äº¤æ¢ï¼Œå®ƒä¼˜åŒ–äº†å…¬å¼ï¼ˆ3ï¼‰ï¼Œä½†æ˜¯æ²¡æœ‰è‡ªä¸¾</li></ul><h3 id="å®éªŒç»“æœ"><a href="#å®éªŒç»“æœ" class="headerlink" title="å®éªŒç»“æœ"></a>å®éªŒç»“æœ</h3><p>è¡¨2ä¸­æˆ‘ä»¬è§‚å¯Ÿåˆ°AlignEæ˜æ˜¾ä¼˜äºMTransEï¼ŒIPTransEå’ŒJAPEï¼Œå› ä¸ºå®ƒé‡‡ç”¨é¢å‘å¯¹é½çš„åµŒå…¥ã€‚è€ŒBootEAæ˜¾ç€æ”¹å–„äº†AlignEçš„ç»“æœï¼Œè¡¨æ˜äº†è‡ªä¸¾çš„è‰¯å¥½æ€§èƒ½æ˜¯ç”±äºå…¶èƒ½å¤Ÿå‡†ç¡®åœ°å°†å¯èƒ½çš„å¯¹é½æ ‡è®°ä¸ºè®­ç»ƒæ•°æ®ã€‚</p><p><img src="http://image.nysdy.com/20190409155477712242929.png" alt="20190409155477712242929.png"></p><h3 id="åˆ†æ"><a href="#åˆ†æ" class="headerlink" title="åˆ†æ"></a>åˆ†æ</h3><h4 id="æˆªæ–­å‡åŒ€è´ŸæŠ½æ ·çš„æœ‰æ•ˆæ€§"><a href="#æˆªæ–­å‡åŒ€è´ŸæŠ½æ ·çš„æœ‰æ•ˆæ€§" class="headerlink" title="æˆªæ–­å‡åŒ€è´ŸæŠ½æ ·çš„æœ‰æ•ˆæ€§"></a>æˆªæ–­å‡åŒ€è´ŸæŠ½æ ·çš„æœ‰æ•ˆæ€§</h4><p><img src="http://image.nysdy.com/20190409155477794964299.png" alt="20190409155477794964299.png">ä»å›¾ä¸­å¯ä»¥çœ‹å‡ºï¼Œä¸MTransEï¼ŒIPTransEå’ŒJAPEç›¸æ¯”ï¼Œå…·æœ‰å‡åŒ€è´Ÿé‡‡æ ·çš„AlignEä»ç„¶è·å¾—äº†ä¼˜å¼‚çš„ç»“æœï¼Œå¹¶ä¸”éšç€é‡‡æ ·ç¦»xæ›´åŠ æ¥è¿‘ï¼Œæ•ˆæœå‘ˆä¸Šå‡è¶‹åŠ¿ã€‚</p><h4 id="å¯èƒ½å¯¹é½çš„å‡†ç¡®æ€§"><a href="#å¯èƒ½å¯¹é½çš„å‡†ç¡®æ€§" class="headerlink" title="å¯èƒ½å¯¹é½çš„å‡†ç¡®æ€§"></a>å¯èƒ½å¯¹é½çš„å‡†ç¡®æ€§</h4><p><img src="http://image.nysdy.com/20190409155477815850691.png" alt="20190409155477815850691.png">å¯ä»¥çœ‹åˆ°ä»¥ä½œè€…çš„æ ‡è®°æ–¹æ³•S3è¡¨ç°æœ€ä½³ã€‚è¿™äº›ç»“æœè¯å®ä½œè€…çš„æ–¹æ³•å¯ä»¥ä¿è¯ä½¿ç”¨æœªæ ‡è®°æ•°æ®çš„å®‰å…¨æ€§ã€‚</p><h4 id="å¯¹å…ˆå‰å¯¹å‡†æ¯”ä¾‹çš„æ•æ„Ÿæ€§"><a href="#å¯¹å…ˆå‰å¯¹å‡†æ¯”ä¾‹çš„æ•æ„Ÿæ€§" class="headerlink" title="å¯¹å…ˆå‰å¯¹å‡†æ¯”ä¾‹çš„æ•æ„Ÿæ€§"></a>å¯¹å…ˆå‰å¯¹å‡†æ¯”ä¾‹çš„æ•æ„Ÿæ€§</h4><p><img src="http://image.nysdy.com/20190409155477832196109.png" alt="20190409155477832196109.png"></p><p>æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œéšç€æ¯”ä¾‹çš„å¢åŠ ï¼Œæ‰€æœ‰äº”ä¸ªæ•°æ®é›†çš„ç»“æœéƒ½å˜å¾—æ›´å¥½ï¼Œå› ä¸ºæ›´å¤šçš„å…ˆå‰å¯¹é½å¯ä»¥æä¾›æ›´å¤šä¿¡æ¯æ¥å¯¹é½ä¸¤ä¸ªKGã€‚</p><h4 id="F1-score-w-r-t-å…³ç³»ä¸‰å…ƒæ•°çš„åˆ†å¸ƒ"><a href="#F1-score-w-r-t-å…³ç³»ä¸‰å…ƒæ•°çš„åˆ†å¸ƒ" class="headerlink" title="F1-score w.r.t. å…³ç³»ä¸‰å…ƒæ•°çš„åˆ†å¸ƒ"></a>F1-score w.r.t. å…³ç³»ä¸‰å…ƒæ•°çš„åˆ†å¸ƒ</h4><p><img src="http://image.nysdy.com/20190409155477850975486.png" alt="20190409155477850975486.png"></p><p>BootEAåœ¨æ‰€æœ‰æ—¶é—´é—´éš”éƒ½ä¼˜äºMTransEï¼ŒIPTransEå’ŒJAPEï¼Œè¿™å†æ¬¡è¯å®äº†BootEAçš„æœ‰æ•ˆæ€§ã€‚è€Œä¸”BootEAå¯ä»¥åœ¨ç¨€ç–æ•°æ®ä¸Šå–å¾—æœ‰å¸Œæœ›çš„ç»“æœã€‚</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://www.ijcai.org/proceedings/2018/0611.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;è®ºæ–‡ä¸‹è½½åœ°å€&lt;/a&gt;ï¼Œé‡‡ç”¨äº†bootstrappingæ–¹æ³•æ¥è§£å†³ç¼ºä¹è®­ç»ƒæ•°æ®çš„è¿‡ç¨‹ï¼Œæå‡ºäº†æˆªæ–­å‡åŒ€è´Ÿé‡‡æ ·æ¥æé«˜è´Ÿæ ·ä¾‹å¯¹äºç›®æ ‡å‡½æ•°çš„è´¡çŒ®ï¼Œé‡‡ç”¨åŸºäºé™åˆ¶çš„ç›®æ ‡å‡½æ•°æ¥æŒ‰éœ€è°ƒæ•´æ­£è´Ÿæ ·ä¾‹çš„å¾—åˆ†ã€‚&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="è®ºæ–‡é˜…è¯»ç¬”è®°" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="çŸ¥è¯†å›¾è°±" scheme="http://yoursite.com/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    
      <category term="çŸ¥è¯†å›¾è°±åµŒå…¥" scheme="http://yoursite.com/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%B5%8C%E5%85%A5/"/>
    
      <category term="å®ä½“å¯¹é½" scheme="http://yoursite.com/tags/%E5%AE%9E%E4%BD%93%E5%AF%B9%E9%BD%90/"/>
    
  </entry>
  
  <entry>
    <title>ã€ŠEntity Alignment between Knowledge Graphs Using Attribute Embeddingsã€‹é˜…è¯»ç¬”è®°</title>
    <link href="http://yoursite.com/post/Entity%20Alignment%20between%20Knowledge%20Graphs%20Using%20Attribute%20Embeddings/"/>
    <id>http://yoursite.com/post/Entity Alignment between Knowledge Graphs Using Attribute Embeddings/</id>
    <published>2019-03-28T00:57:15.000Z</published>
    <updated>2019-04-01T01:32:23.185Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://people.eng.unimelb.edu.au/jianzhongq/papers/AAAI2019_EntityAlignment.pdf" target="_blank" rel="noopener">è®ºæ–‡ä¸‹è½½åœ°å€</a>ï¼ŒçŸ¥è¯†å›¾ä¹‹é—´çš„å®ä½“å¯¹é½çš„ä»»åŠ¡æ—¨åœ¨åœ¨ä»£è¡¨ç›¸åŒç°å®ä¸–ç•Œå®ä½“çš„ä¸¤ä¸ªçŸ¥è¯†å›¾ä¸­æ‰¾åˆ°å®ä½“ã€‚æœ¬æ–‡æœ€ä¸»è¦å°±æ˜¯æå‡ºäº†å±æ€§å­—ç¬¦åµŒå…¥(attribute character embeddings)çš„æ–¹æ³•ã€‚</p></blockquote><a id="more"></a><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>æˆ‘ä»¬çš„æ¨¡å‹åˆ©ç”¨çŸ¥è¯†å›¾ä¸­å­˜åœ¨çš„å¤§é‡å±æ€§ä¸‰å…ƒç»„(attribute triples)å¹¶ç”Ÿæˆå±æ€§å­—ç¬¦åµŒå…¥ã€‚ å±æ€§å­—ç¬¦åµŒå…¥(attribute character embeddings)é€šè¿‡åŸºäºå®ä½“çš„å±æ€§è®¡ç®—å®ä½“ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œå°†å®ä½“åµŒå…¥ä»ä¸¤ä¸ªçŸ¥è¯†å›¾ç§»ä½åˆ°åŒä¸€ç©ºé—´ä¸­ã€‚<br>æˆ‘ä»¬ä½¿ç”¨ä¼ é€’è§„åˆ™æ¥è¿›ä¸€æ­¥ä¸°å¯Œå®ä½“çš„å±æ€§æ•°é‡ä»¥å¢å¼ºå±æ€§å­—ç¬¦åµŒå…¥ã€‚</p><h2 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h2><ul><li>æå‡ºäº†ä¸¤ä¸ªKGä¹‹é—´å®ä½“å¯¹é½çš„æ¡†æ¶ï¼Œå®ƒç”±è°“è¯å¯¹é½æ¨¡å—ï¼ŒåµŒå…¥å­¦ä¹ æ¨¡å—å’Œå®ä½“å¯¹é½æ¨¡å—ç»„æˆã€‚</li><li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„åµŒå…¥æ¨¡å‹ï¼Œå®ƒå°†å®ä½“åµŒå…¥ä¸å±æ€§åµŒå…¥é›†æˆåœ¨ä¸€èµ·ï¼Œä»¥ä¾¿ä¸ºä¸¤ä¸ªKGå­¦ä¹ ç»Ÿä¸€çš„åµŒå…¥ç©ºé—´ã€‚</li><li>æˆ‘ä»¬åœ¨ä¸‰ä¸ªçœŸæ­£çš„KGå¯¹ä¸Šè¯„ä¼°å»ºè®®çš„æ¨¡å‹ã€‚<br>ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å®ä½“å¯¹é½ä»»åŠ¡ä¸Šå§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œhits@1è¶…è¿‡50ï¼…ã€‚</li></ul><h2 id="æ¨¡å‹"><a href="#æ¨¡å‹" class="headerlink" title="æ¨¡å‹"></a>æ¨¡å‹</h2><h3 id="æ¨¡å‹æ€»è§ˆ"><a href="#æ¨¡å‹æ€»è§ˆ" class="headerlink" title="æ¨¡å‹æ€»è§ˆ"></a>æ¨¡å‹æ€»è§ˆ</h3><p>predicate alignment, embedding learning, and entity alignment</p><p><img src="http://image.nysdy.com/20190329155385087492759.png" alt="20190329155385087492759.png"></p><h3 id="Predicate-Alignment"><a href="#Predicate-Alignment" class="headerlink" title="Predicate Alignment"></a>Predicate Alignment</h3><p>è°“è¯å¯¹é½æ¨¡å—é€šè¿‡ä½¿ç”¨ç»Ÿä¸€çš„å‘½åæ–¹æ¡ˆé‡å‘½åä¸¤ä¸ªKGçš„è°“è¯æ¥åˆå¹¶ä¸¤ä¸ªKGï¼Œä»¥ä¾¿ä¸ºå…³ç³»åµŒå…¥æä¾›ç»Ÿä¸€çš„å‘é‡ç©ºé—´ã€‚dbp:bornIn vs. yago:wasBornIn ç»Ÿä¸€å‘½åä¸º :bornInã€‚</p><p>ä¸ºäº†æ‰¾åˆ°éƒ¨åˆ†åŒ¹é…çš„è°“è¯ï¼Œä½œè€…è®¡ç®—è°“è¯URIçš„æœ€åéƒ¨åˆ†çš„ç¼–è¾‘è·ç¦»ï¼ˆä¾‹å¦‚ï¼ŒbornInä¸wasBornInï¼‰å¹¶å°†0.95è®¾ç½®ä¸ºç›¸ä¼¼æ€§é˜ˆå€¼ã€‚</p><h2 id="Embedding-Learning"><a href="#Embedding-Learning" class="headerlink" title="Embedding Learning"></a>Embedding Learning</h2><h3 id="Structure-Embedding"><a href="#Structure-Embedding" class="headerlink" title="Structure Embedding"></a>Structure Embedding</h3><p>ä½œè€…é‡‡ç”¨TransEæ¥å­¦ä¹ å¯¹äºå®ä½“çš„ç»“æ„åµŒå…¥ã€‚ä¸TransEä¸åŒçš„æ˜¯ï¼Œæ¨¡å‹å¸Œæœ›æ›´å…³æ³¨å·²å¯¹é½çš„ä¸‰å…ƒç»„ï¼Œä¹Ÿå°±æ˜¯åŒ…å«å¯¹é½è°“è¯çš„ä¸‰å…ƒç»„ã€‚æ¨¡å‹é€šè¿‡æ·»åŠ æƒé‡æ¥å®ç°è¿™ä¸€ç›®çš„ã€‚Structure embeddingçš„ç›®æ ‡å‡½æ•°å¦‚ä¸‹ï¼š</p><p><img src="http://image.nysdy.com/2019040115540798067483.png" alt="2019040115540798067483.png"></p><p>count(r)æ˜¯å…³ç³»rå‡ºç°çš„æ•°é‡ã€‚</p><h3 id="Attribute-Character-Embedding"><a href="#Attribute-Character-Embedding" class="headerlink" title="Attribute Character Embedding"></a>Attribute Character Embedding</h3><p>å¯¹äºå±æ€§å­—ç¬¦åµŒå…¥ï¼Œä¹Ÿå‚è€ƒTransEçš„æ€æƒ³ï¼Œå°†è°“è¯rè§£é‡Šä¸ºä»å¤´éƒ¨å®ä½“håˆ°å±æ€§açš„è½¬æ¢ã€‚ä½†æ˜¯ï¼Œç›¸åŒçš„å±æ€§aå¯ä»¥åœ¨ä¸¤ä¸ªKGä¸­ä»¥ä¸åŒçš„å½¢å¼å‡ºç°ï¼Œä¾‹å¦‚50.9989å¯¹50.9988888889ä½œä¸ºå®ä½“çš„çº¬åº¦; â€œBarack Obamaâ€ä¸â€œBarack Hussein Obamaâ€ä½œä¸ºäººåç­‰ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºä½¿ç”¨ç»„åˆå‡½æ•°å¯¹å±æ€§å€¼è¿›è¡Œç¼–ç ï¼Œå¹¶å°†å±æ€§ä¸‰å…ƒç»„ä¸­æ¯ä¸ªå…ƒç´ çš„å…³ç³»å®šä¹‰ä¸ºh +râ‰ˆfaï¼ˆaï¼‰ã€‚ è¿™é‡Œï¼Œfaï¼ˆaï¼‰æ˜¯ç»„åˆå‡½æ•°ï¼Œaæ˜¯å±æ€§å€¼a = {c1ï¼Œc2ï¼Œc3ï¼Œâ€¦ï¼Œct}çš„å­—ç¬¦åºåˆ—ã€‚ ç»„åˆå‡½æ•°å°†å±æ€§å€¼ç¼–ç ä¸ºå•ä¸ªå‘é‡ï¼Œå¹¶å°†ç±»ä¼¼çš„å±æ€§å€¼æ˜ å°„åˆ°ç±»ä¼¼çš„å‘é‡è¡¨ç¤ºã€‚ ä½œè€…å®šä¹‰äº†ä¸‰ä¸ªç»„æˆå‡½æ•°å¦‚ä¸‹ã€‚</p><h4 id="Sum-compositional-function-SUM"><a href="#Sum-compositional-function-SUM" class="headerlink" title="Sum compositional function (SUM)"></a>Sum compositional function (SUM)</h4><p>å­˜åœ¨é—®é¢˜ï¼šåŒ…å«ç›¸åŒå­—ç¬¦ä¸åŒé¡ºåºçš„å±æ€§å€¼ä¼šæœ‰ç›¸åŒçš„å‘é‡è¡¨ç¤º</p><p><img src="http://image.nysdy.com/20190401155408045744772.png" alt="20190401155408045744772.png"></p><h4 id="LSTM-based-compositional-function-LSTM"><a href="#LSTM-based-compositional-function-LSTM" class="headerlink" title="LSTM-based compositional function (LSTM)."></a>LSTM-based compositional function (LSTM).</h4><p><img src="http://image.nysdy.com/20190401155408065416786.png" alt="20190401155408065416786.png"></p><h4 id="N-gram-based-compositional-function-N-gram"><a href="#N-gram-based-compositional-function-N-gram" class="headerlink" title="N-gram-based compositional function (N-gram)"></a>N-gram-based compositional function (N-gram)</h4><p><img src="http://image.nysdy.com/20190401155408070480904.png" alt="20190401155408070480904.png"></p><p>æœ€åattribute character embeddingç›®æ ‡å‡½æ•°ï¼š<img src="http://image.nysdy.com/20190401155408075852436.png" alt="20190401155408075852436.png"></p><h3 id="Joint-Learning-of-Structure-Embedding-and-Attribute-Character-Embedding"><a href="#Joint-Learning-of-Structure-Embedding-and-Attribute-Character-Embedding" class="headerlink" title="Joint Learning of Structure Embedding and Attribute Character Embedding"></a>Joint Learning of Structure Embedding and Attribute Character Embedding</h3><p>ä½œè€…ä½¿ç”¨å±æ€§å­—ç¬¦åµŒå…¥é€šè¿‡æœ€å°åŒ–ä»¥ä¸‹ç›®æ ‡å‡½æ•°å°†ç»“æ„åµŒå…¥ç§»åŠ¨åˆ°ç›¸åŒçš„å‘é‡ç©ºé—´ï¼š</p><p><img src="http://image.nysdy.com/20190401155408233583946.png" alt="20190401155408233583946.png"></p><p>æœ¬æ–‡æ•´ä½“æŸå¤±å‡½æ•°ï¼š</p><p><img src="http://image.nysdy.com/20190401155408096945378.png" alt="20190401155408096945378.png"></p><h3 id="Entity-Alignment"><a href="#Entity-Alignment" class="headerlink" title="Entity Alignment"></a>Entity Alignment</h3><p>åœ¨ç»è¿‡ä¸Šè¿°è®­ç»ƒè¿‡ç¨‹ä¹‹åï¼Œæ¥è‡ªä¸åŒKGçš„ç›¸ä¼¼çš„å®ä½“å°†ä¼šæœ‰ç›¸ä¼¼çš„å‘é‡è¡¨ç¤ºï¼Œå› æ­¤å¯é€šè¿‡</p><p><img src="http://image.nysdy.com/2019040115540811204332.png" alt="2019040115540811204332.png"></p><p>è·å¾—æ½œåœ¨å®ä½“å¯¹é½å¯¹<h_1, h_map="">ã€‚æ­¤å¤–ï¼Œæ¨¡å‹è®¾å®šç›¸ä¼¼åº¦é˜ˆå€¼æ¥è¿‡æ»¤æ½œåœ¨å®ä½“å¯¹é½å¯¹ï¼Œå¾—åˆ°æœ€ç»ˆçš„å¯¹é½ç»“æœã€‚</h_1,></p><h3 id="Triple-Enrichment-via-Transitivity-Rule"><a href="#Triple-Enrichment-via-Transitivity-Rule" class="headerlink" title="Triple Enrichment via Transitivity Rule"></a>Triple Enrichment via Transitivity Rule</h3><p>ä½œè€…åˆ©ç”¨ä¸€é˜¶é€»è¾‘ä¼ é€’å…³ç³»æ¥ä¸°å¯Œä¸‰å…ƒç»„ã€‚å³ï¼šå­˜åœ¨<h_1,t_1,t>å’Œ<t, r_2,t_2="">åˆ™å¯ä»¥æ¨ç†å‡ºh_1+ (r_1.r_2) â‰ˆ t_2</t,></h_1,t_1,t></p><h2 id="Database"><a href="#Database" class="headerlink" title="Database"></a>Database</h2><p>æœ¬æ–‡ä» DBpedia (DBP)ã€LinkedGeoData (LGD)ã€Geonames (GEO) å’Œ YAGO å››ä¸ª KG ä¸­æŠ½å–æ„å»ºäº†ä¸‰ä¸ªæ•°æ®é›†ï¼Œåˆ†åˆ«æ˜¯DBP-LGDã€DBP-GEOå’ŒDBP-YAGOã€‚å…·ä½“çš„æ•°æ®ç»Ÿè®¡å¦‚ä¸‹ï¼š</p><p><img src="http://image.nysdy.com/20190401155408149973345.png" alt="20190401155408149973345.png"></p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Entity-Alignment-Results"><a href="#Entity-Alignment-Results" class="headerlink" title="Entity Alignment Results"></a>Entity Alignment Results</h3><p>æœ¬æ–‡å¯¹æ¯”äº†ä¸‰ä¸ªç›¸å…³çš„æ¨¡å‹ï¼Œåˆ†åˆ«æ˜¯ TransEã€MTransE å’Œ JAPEã€‚è¯•éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ¨¡å‹åœ¨å®ä½“å¯¹é½ä»»åŠ¡ä¸Šå–å¾—äº†å…¨é¢çš„è¾ƒå¤§çš„æå‡ï¼Œåœ¨ä¸‰ç§ç»„åˆå‡½æ•°ä¸­ï¼ŒN-gramå‡½æ•°çš„ä¼˜åŠ¿è¾ƒä¸ºæ˜æ˜¾ã€‚æ­¤å¤–ï¼ŒåŸºäºä¼ é€’è§„åˆ™çš„ä¸‰å…ƒç»„ä¸°å¯Œæ¨¡å‹å¯¹ç»“æœä¹Ÿæœ‰ä¸€å®šçš„æå‡ã€‚å…·ä½“ç»“æœå¦‚ä¸‹<img src="http://image.nysdy.com/20190401155408163227980.png" alt="20190401155408163227980.png"></p><h3 id="Rule-based-Entity-Alignment-Results"><a href="#Rule-based-Entity-Alignment-Results" class="headerlink" title="Rule-based Entity Alignment Results"></a>Rule-based Entity Alignment Results</h3><p>ä¸ºäº†è¿›ä¸€æ­¥è¡¡é‡ attribute character embedding æ•è·å®ä½“é—´ç›¸ä¼¼ä¿¡æ¯çš„èƒ½åŠ›ï¼Œæœ¬æ–‡è®¾è®¡äº†åŸºäºè§„åˆ™çš„å®ä½“å¯¹é½æ¨¡å‹ã€‚æœ¬å®éªŒå¯¹æ¯”äº†ä¸‰ç§ä¸åŒçš„æ¨¡å‹ï¼šä»¥labelçš„å­—ç¬¦ä¸²ç›¸ä¼¼åº¦ä½œä¸ºåŸºç¡€æ¨¡å‹ï¼›é’ˆå¯¹æ•°æ®é›†ç‰¹ç‚¹ï¼Œåœ¨åŸºç¡€æ¨¡å‹çš„åŸºç¡€ä¹‹ä¸Šå¢åŠ äº†åæ ‡å±æ€§ï¼Œä»¥æ­¤ä½œä¸ºç¬¬äºŒä¸ªæ¨¡å‹ï¼›ç¬¬ä¸‰ä¸ªæ¨¡å‹æ˜¯æŠŠæœ¬æ–‡æå‡ºçš„æ¨¡å‹ä½œä¸ºé™„åŠ æ¨¡å‹ï¼Œä¸åŸºç¡€æ¨¡å‹ç›¸ç»“åˆã€‚å…·ä½“ç»“æœå¦‚ä¸‹ï¼š</p><p><img src="http://image.nysdy.com/20190401155408175594725.png" alt="20190401155408175594725.png"></p><h3 id="KG-Completion-Results"><a href="#KG-Completion-Results" class="headerlink" title="KG Completion Results"></a>KG Completion Results</h3><p>æœ¬æ–‡è¿˜åœ¨KGè¡¥å…¨ä»»åŠ¡ä¸ŠéªŒè¯äº†æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚æ¨¡å‹ä¸»è¦æµ‹è¯•äº†é“¾æ¥é¢„æµ‹å’Œä¸‰å…ƒç»„åˆ†ç±»ä¸¤ä¸ªæ ‡å‡†ä»»åŠ¡ï¼Œåœ¨è¿™ä¸¤ä¸ªä»»åŠ¡ä¸­ï¼Œæ¨¡å‹ä¹Ÿå–å¾—äº†ä¸é”™çš„æ•ˆæœã€‚å…·ä½“ç»“æœå¦‚ä¸‹ï¼š</p><p><img src="http://image.nysdy.com/20190401155408179146202.png" alt="20190401155408179146202.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://people.eng.unimelb.edu.au/jianzhongq/papers/AAAI2019_EntityAlignment.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;è®ºæ–‡ä¸‹è½½åœ°å€&lt;/a&gt;ï¼ŒçŸ¥è¯†å›¾ä¹‹é—´çš„å®ä½“å¯¹é½çš„ä»»åŠ¡æ—¨åœ¨åœ¨ä»£è¡¨ç›¸åŒç°å®ä¸–ç•Œå®ä½“çš„ä¸¤ä¸ªçŸ¥è¯†å›¾ä¸­æ‰¾åˆ°å®ä½“ã€‚æœ¬æ–‡æœ€ä¸»è¦å°±æ˜¯æå‡ºäº†å±æ€§å­—ç¬¦åµŒå…¥(attribute character embeddings)çš„æ–¹æ³•ã€‚&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="è®ºæ–‡é˜…è¯»ç¬”è®°" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="çŸ¥è¯†å›¾è°±" scheme="http://yoursite.com/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    
      <category term="çŸ¥è¯†å›¾è°±åµŒå…¥" scheme="http://yoursite.com/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%B5%8C%E5%85%A5/"/>
    
  </entry>
  
  <entry>
    <title>ã€ŠRelNN A Deep Neural Model for Relational Learningã€‹é˜…è¯»ç¬”è®°</title>
    <link href="http://yoursite.com/post/RelNN%20A%20Deep%20Neural%20Model%20for%20Relational%20Learning/"/>
    <id>http://yoursite.com/post/RelNN A Deep Neural Model for Relational Learning/</id>
    <published>2019-03-25T01:34:57.000Z</published>
    <updated>2019-04-01T02:34:41.280Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://arxiv.org/pdf/1712.02831.pdf" target="_blank" rel="noopener">è®ºæ–‡ä¸‹è½½åœ°å€</a>ï¼Œè¿™ç¯‡æ–‡ç« ç›¸å½“äºç»“åˆäº†ç»Ÿè®¡å­¦ä¹ å’Œæ·±åº¦ç¥ç»ç½‘ç»œã€‚é‡Œé¢æœ‰äº›å…¬å¼æ²¡æœ‰ç†è§£ï¼Œåº”è¯¥æ˜¯æœ‰è®¸å¤šå…ˆå‰è®ºæ–‡éœ€è¦é˜…è¯»ã€‚ä½†æ˜¯æœ¬ç¯‡è®ºæ–‡æ‰©å±•äº†æ€è·¯å¦‚ä½•ç»“åˆç»Ÿè®¡å­¦å’Œæ·±åº¦å­¦ä¹ ï¼Œå¹¶ä¸”åŸºäºå…¶ä½™æ•°æ®æ¥é¢„æµ‹ä¸€ä¸ªç±»ä¸­å¯¹è±¡çš„ä¸€ä¸ªå±æ€§ï¼Œæƒ³æ³•ä¹Ÿæ¯”è¾ƒå¥½ã€‚</p></blockquote><a id="more"></a><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>ä½œè€…ä¸»è¦é›†ä¸­äºåŸºäºå…¶ä½™æ•°æ®æ¥é¢„æµ‹ä¸€ä¸ªç±»ä¸­å¯¹è±¡çš„ä¸€ä¸ªå±æ€§ã€‚</p><h2 id="Challenge"><a href="#Challenge" class="headerlink" title="Challenge"></a>Challenge</h2><p>å½“ç±»ä¸­æ¯ä¸ªå¯¹è±¡çš„å±æ€§ä¾èµ–äºä¸åŒæ•°é‡çš„å…¶ä»–å¯¹è±¡çš„å±æ€§å’Œå…³ç³»æ—¶ï¼Œæ­¤é—®é¢˜å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ åœ¨StarAIç¤¾åŒºä¸­ï¼Œæ­¤é—®é¢˜ç§°ä¸ºèšåˆï¼ˆaggregationï¼‰ã€‚</p><h2 id="Relational-Logistic-Regression-and-Markov-Logic-Networks"><a href="#Relational-Logistic-Regression-and-Markov-Logic-Networks" class="headerlink" title="Relational Logistic Regression and Markov Logic Networks"></a>Relational Logistic Regression and Markov Logic Networks</h2><p>StarAIæ¨¡å‹æ—¨åœ¨æ¨¡æ‹Ÿå¯¹è±¡ä¹‹é—´å…³ç³»çš„æ¦‚ç‡ã€‚ </p><h3 id="Relational-logistic-regression-RLR-Kazemi-et-al-2014"><a href="#Relational-logistic-regression-RLR-Kazemi-et-al-2014" class="headerlink" title="Relational logistic regression (RLR) (Kazemi et al. 2014)"></a>Relational logistic regression (RLR) (Kazemi et al. 2014)</h3><p>å®šä¹‰çš„æ¦‚ç‡å…¬å¼å¦‚ä¸‹ï¼š</p><p><img src="http://image.nysdy.com/20190327155364804947436.png" alt="20190327155364804947436.png"></p><p>ä¸Šé¢å®šä¹‰çš„RLRæ¨¡å‹ä»…é€‚ç”¨äºå¸ƒå°”å€¼æˆ–å¤šå€¼çˆ¶é¡¹ã€‚ä½œè€…é‡‡ç”¨çš„æ˜¯è¿ç»­çš„åŸå­ï¼ˆcontinuous atomsï¼‰ï¼ˆFatemi, Kazemi, and Poole (2016)ï¼‰</p><h3 id="Relational-Neural-Networks"><a href="#Relational-Neural-Networks" class="headerlink" title="Relational Neural Networks"></a>Relational Neural Networks</h3><p>ä½œè€…é€šè¿‡è®¾è®¡ç¥ç»ç½‘ç»œä¸­çº¿æ€§å±‚ï¼ˆLLï¼‰ï¼Œæ¿€æ´»å±‚ï¼ˆALï¼‰å’Œè¯¯å·®å±‚ï¼ˆELï¼‰çš„å…³ç³»å¯¹åº”ç‰©ï¼Œå¯¹å…·æœ‰åˆ†å±‚æ¶æ„çš„RLR / MLNæ¨¡å‹è¿›è¡Œç¼–ç ã€‚</p><p>å…³ç³»ç¥ç»ç½‘ç»œï¼ˆRelNNï¼‰æ˜¯åŒ…å«ä½œä¸ºå›¾å½¢å½¼æ­¤è¿æ¥çš„è‹¥å¹²RLLå’ŒRALçš„ç»“æ„ã€‚</p><p><img src="http://image.nysdy.com/20190327155364863172988.png" alt="20190327155364863172988.png"></p><h2 id="Motivations-for-hidden-layers"><a href="#Motivations-for-hidden-layers" class="headerlink" title="Motivations for hidden layers"></a>Motivations for hidden layers</h2><ul><li>ä½¿å–œæ¬¢çœ‹åŠ¨ä½œç”µå½±çš„äººæ•°å¢åŠ æ—¶ï¼Œç”·æ€§çš„æ¦‚ç‡å˜ä¸º[0, 1]é‡çš„ä»»ä½•æ•°å€¼ï¼Œä¸è‡³äºç›´æ¥å˜ä¸º0æˆ–è€…1ã€‚</li><li>å› æ­¤ï¼Œéšè—å±‚é€šè¿‡ä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ é€šç”¨è§„åˆ™å¹¶ç›¸åº”åœ°å¯¹å¯¹è±¡è¿›è¡Œåˆ†ç±»ï¼Œç„¶åä»¥ä¸åŒæ–¹å¼å¤„ç†ä¸åŒç±»åˆ«çš„å¯¹è±¡ï¼Œä»è€Œæé«˜äº†å»ºæ¨¡èƒ½åŠ›ã€‚</li><li>ä½¿ç”¨RLRè¡¨ç¤ºä¸åŒç±»å‹çš„ç°æœ‰æ˜¾å¼èšåˆå™¨ï¼Œç„¶è€Œæœ‰äº›æƒ…å†µéœ€è¦ä½¿ç”¨2ä¸ªRLLså’Œ2ä¸ªRALs</li></ul><h2 id="Learning-latent-properties-directly"><a href="#Learning-latent-properties-directly" class="headerlink" title="Learning latent properties directly"></a>Learning latent properties directly</h2><p>å¯¹è±¡å¯èƒ½åŒ…å«æ— æ³•ä½¿ç”¨å¸¸è§„è§„åˆ™æŒ‡å®šçš„æ½œåœ¨å±æ€§ï¼Œä½†å¯ä»¥åœ¨è®­ç»ƒæœŸé—´ç›´æ¥ä»æ•°æ®ä¸­å­¦ä¹ ã€‚</p><p><img src="http://image.nysdy.com/20190327155364936417890.png" alt="20190327155364936417890.png"></p><p>è€ƒè™‘å›¾2ä¸­çš„æ¨¡å‹ï¼Œè®©Latentï¼ˆmï¼‰æˆä¸ºç”µå½±çš„æ•°å­—æ½œåœ¨å±æ€§ï¼Œå…¶å€¼å°†åœ¨è®­ç»ƒæœŸé—´å­¦ä¹ ã€‚</p><h2 id="From-ConvNet-Primitives-to-RelNNs"><a href="#From-ConvNet-Primitives-to-RelNNs" class="headerlink" title="From ConvNet Primitives to RelNNs"></a>From ConvNet Primitives to RelNNs</h2><p>æˆ‘ä»¬è§£é‡Šä¸ºä»€ä¹ˆRelNNä¹Ÿå¯ä»¥è¢«è§†ä¸ºConvNetsçš„ä¸€ä¸ªå®ä¾‹ã€‚</p><p>ConvNetsçš„è¾“å…¥çŸ©é˜µä¸­çš„å•å…ƒï¼ˆä¾‹å¦‚ï¼Œå›¾åƒåƒç´ ï¼‰å…·æœ‰ç©ºé—´ç›¸å…³æ€§å’Œç©ºé—´å†—ä½™ï¼šå½¼æ­¤æ›´æ¥è¿‘çš„å•å…ƒæ¯”æ›´è¿œçš„å•å…ƒæ›´ä¾èµ–ã€‚ ä¾‹å¦‚ï¼Œå¦‚æœMè¡¨ç¤ºå›¾åƒçš„è¾“å…¥é€šé“ï¼Œåˆ™M [iï¼Œj]å’ŒM [i + 1ï¼Œj + 1]ä¹‹é—´çš„ä¾èµ–æ€§å¯èƒ½è¿œå¤§äºM [iï¼Œj]å’ŒM [iï¼Œj+20]ä¹‹é—´çš„ä¾èµ–æ€§ã€‚</p><p>å¯¹äºå…³ç³»æ•°æ®ï¼Œè¾“å…¥çŸ©é˜µä¸­çš„ä¾èµ–å…³ç³»ï¼ˆå…³ç³»ï¼‰æ˜¯ä¸åŒçš„ï¼šåŒä¸€è¡Œæˆ–åˆ—ä¸­çš„å•å…ƒï¼ˆå³åŒä¸€å¯¹è±¡çš„å…³ç³»ï¼‰å…·æœ‰æ¯”ä¸åŒè¡Œå’Œåˆ—ä¸­çš„å•å…ƒæ›´é«˜çš„ä¾èµ–æ€§ï¼ˆå³ä¸åŒå¯¹è±¡çš„å…³ç³»ï¼‰ã€‚ å› æ­¤ï¼Œä¸ºäº†ä½¿ConvNetsé€‚åº”å…³ç³»æ•°æ®ï¼Œæˆ‘ä»¬éœ€è¦çŸ¢é‡å½¢çŠ¶çš„è¿‡æ»¤å™¨ï¼Œè¿™äº›è¿‡æ»¤å™¨å¯¹è¡Œå’Œåˆ—äº¤æ¢æ˜¯ä¸å˜çš„ï¼Œå¹¶ä¸”æ›´å¥½åœ°æ•è·å…³ç³»ä¾èµ–æ€§å’Œå¯äº¤æ¢æ€§å‡è®¾ã€‚</p><p><img src="http://image.nysdy.com/2019032715536511162060.png" alt="2019032715536511162060.png"></p><h2 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h2><h3 id="Movielens-1M-dataset-Harper-and-Konstan-2015"><a href="#Movielens-1M-dataset-Harper-and-Konstan-2015" class="headerlink" title="Movielens 1M dataset (Harper and Konstan 2015)"></a>Movielens 1M dataset (Harper and Konstan 2015)</h3><p>ç¬¬ä¸€ä¸ªæ•°æ®é›†æ˜¯Movielens 1M dataset (Harper and Konstan 2015)ï¼Œå¿½ç•¥äº†å®é™…çš„è¯„çº§ï¼Œåªè€ƒè™‘ç”µå½±æ˜¯å¦è¢«è¯„çº§ï¼Œåªè€ƒè™‘åŠ¨ä½œå’Œæˆå‰§ç±»å‹ã€‚</p><h3 id="PAKDD15"><a href="#PAKDD15" class="headerlink" title="PAKDD15"></a>PAKDD15</h3><p><a href="https://knowledgepit.fedcsis.org/contest/view.php?id=107" target="_blank" rel="noopener">è·å–åœ°å€</a></p><h3 id="all-Chinese-and-Mexican-restaurants-in-Yelp-dataset-challenge"><a href="#all-Chinese-and-Mexican-restaurants-in-Yelp-dataset-challenge" class="headerlink" title="all Chinese and Mexican restaurants in Yelp dataset challenge"></a>all Chinese and Mexican restaurants in Yelp dataset challenge</h3><p><a href="https://www.yelp.com/dataset_challenge" target="_blank" rel="noopener">è·å–åœ°å€</a></p><h2 id="Empirical-Results"><a href="#Empirical-Results" class="headerlink" title="Empirical Results"></a>Empirical Results</h2><p>ä½œè€…æå‡ºäº†ä¸‰ä¸ªé—®é¢˜æ¥è¿›è¡Œå®éªŒï¼š</p><h3 id="Q1ï¼šRelNNçš„æ€§èƒ½ä¸å…¶ä»–ä¼—æ‰€å‘¨çŸ¥çš„å…³ç³»å­¦ä¹ ç®—æ³•ç›¸æ¯”å¦‚ä½•ï¼Ÿ"><a href="#Q1ï¼šRelNNçš„æ€§èƒ½ä¸å…¶ä»–ä¼—æ‰€å‘¨çŸ¥çš„å…³ç³»å­¦ä¹ ç®—æ³•ç›¸æ¯”å¦‚ä½•ï¼Ÿ" class="headerlink" title="Q1ï¼šRelNNçš„æ€§èƒ½ä¸å…¶ä»–ä¼—æ‰€å‘¨çŸ¥çš„å…³ç³»å­¦ä¹ ç®—æ³•ç›¸æ¯”å¦‚ä½•ï¼Ÿ"></a>Q1ï¼šRelNNçš„æ€§èƒ½ä¸å…¶ä»–ä¼—æ‰€å‘¨çŸ¥çš„å…³ç³»å­¦ä¹ ç®—æ³•ç›¸æ¯”å¦‚ä½•ï¼Ÿ</h3><p><img src="http://image.nysdy.com/20190327155365126319780.png" alt="20190327155365126319780.png"></p><h3 id="Q2ï¼šåŸºäºæ•°å­—å’Œè§„åˆ™çš„æ½œåœ¨å±æ€§å¦‚ä½•å½±å“RelNNçš„æ€§èƒ½"><a href="#Q2ï¼šåŸºäºæ•°å­—å’Œè§„åˆ™çš„æ½œåœ¨å±æ€§å¦‚ä½•å½±å“RelNNçš„æ€§èƒ½" class="headerlink" title="Q2ï¼šåŸºäºæ•°å­—å’Œè§„åˆ™çš„æ½œåœ¨å±æ€§å¦‚ä½•å½±å“RelNNçš„æ€§èƒ½?"></a>Q2ï¼šåŸºäºæ•°å­—å’Œè§„åˆ™çš„æ½œåœ¨å±æ€§å¦‚ä½•å½±å“RelNNçš„æ€§èƒ½?</h3><p>æ›´æ”¹äº†RelNNä¸­éšè—å›¾å±‚å’Œæ•°å­—æ½œåœ¨å±æ€§çš„æ•°é‡ï¼Œä»¥æŸ¥çœ‹å®ƒä»¬å¦‚ä½•å½±å“æ€§èƒ½ã€‚</p><p><img src="http://image.nysdy.com/20190327155365168099042.png" alt="20190327155365168099042.png"></p><p>è¯·æ³¨æ„ï¼Œæ·»åŠ å›¾å±‚åªä¼šæ·»åŠ ä¸€å®šæ•°é‡çš„å‚æ•°ï¼Œä½†æ·»åŠ kä¸ªæ•°å­—æ½œåœ¨å±æ€§ä¼šå¢åŠ k * |Î”m|å‚æ•°ã€‚</p><h3 id="Q3ï¼šRelNNå¦‚ä½•æ¨æ–­å‡ºçœ‹ä¸è§çš„æ¡ˆä¾‹å¹¶è§£å†³æŒ‡å‘çš„è§„æ¨¡å¤§å°é—®é¢˜-ï¼ˆPoole-et-al-2014ï¼‰"><a href="#Q3ï¼šRelNNå¦‚ä½•æ¨æ–­å‡ºçœ‹ä¸è§çš„æ¡ˆä¾‹å¹¶è§£å†³æŒ‡å‘çš„è§„æ¨¡å¤§å°é—®é¢˜-ï¼ˆPoole-et-al-2014ï¼‰" class="headerlink" title="Q3ï¼šRelNNå¦‚ä½•æ¨æ–­å‡ºçœ‹ä¸è§çš„æ¡ˆä¾‹å¹¶è§£å†³æŒ‡å‘çš„è§„æ¨¡å¤§å°é—®é¢˜ ï¼ˆPoole et al.2014ï¼‰?"></a>Q3ï¼šRelNNå¦‚ä½•æ¨æ–­å‡ºçœ‹ä¸è§çš„æ¡ˆä¾‹å¹¶è§£å†³æŒ‡å‘çš„è§„æ¨¡å¤§å°é—®é¢˜ ï¼ˆPoole et al.2014ï¼‰?</h3><p>ä½œè€…å®æ–½äº†ä¸¤ä¸ªå®éªŒï¼š</p><ul><li>æˆ‘ä»¬åœ¨å¤§é‡æ•°æ®ä¸­è®­ç»ƒä¸€ä¸ªRelNNï¼Œå¹¶åœ¨ä¸€å°æ•°æ®ä¸Šè¿›è¡Œæµ‹è¯•ï¼šè¯¥å®éªŒå¯ä»¥çœ‹ä½œæ¯ä¸ªæ¨¡å‹å—å†·å¯åŠ¨é—®é¢˜çš„ä¸¥é‡ç¨‹åº¦</li><li>ç„¶åæˆ‘ä»¬åœ¨ä¸€å°æ•°æ®ä¸Šè®­ç»ƒä¸€ä¸ªRelNNå¹¶åœ¨ä¸€å¤§æ•°æ®ä¸Šè¿›è¡Œæµ‹è¯•ï¼šå¯ä»¥çœ‹ä½œè¿™äº›æ¨¡å‹å¯¹æ›´å¤§ç¾¤ä½“çš„æ¨æ–­</li></ul><p><img src="http://image.nysdy.com/20190327155365190971381.png" alt="20190327155365190971381.png"></p><h2 id="Future"><a href="#Future" class="headerlink" title="Future"></a>Future</h2><p>ä½œè€…å°†ä»æ•°æ®ä¸­è‡ªåŠ¨å­¦ä¹ è¿™äº›ç»“æ„çš„é—®é¢˜ç•™ä½œæœªæ¥çš„å·¥ä½œã€‚</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1712.02831.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;è®ºæ–‡ä¸‹è½½åœ°å€&lt;/a&gt;ï¼Œè¿™ç¯‡æ–‡ç« ç›¸å½“äºç»“åˆäº†ç»Ÿè®¡å­¦ä¹ å’Œæ·±åº¦ç¥ç»ç½‘ç»œã€‚é‡Œé¢æœ‰äº›å…¬å¼æ²¡æœ‰ç†è§£ï¼Œåº”è¯¥æ˜¯æœ‰è®¸å¤šå…ˆå‰è®ºæ–‡éœ€è¦é˜…è¯»ã€‚ä½†æ˜¯æœ¬ç¯‡è®ºæ–‡æ‰©å±•äº†æ€è·¯å¦‚ä½•ç»“åˆç»Ÿè®¡å­¦å’Œæ·±åº¦å­¦ä¹ ï¼Œå¹¶ä¸”åŸºäºå…¶ä½™æ•°æ®æ¥é¢„æµ‹ä¸€ä¸ªç±»ä¸­å¯¹è±¡çš„ä¸€ä¸ªå±æ€§ï¼Œæƒ³æ³•ä¹Ÿæ¯”è¾ƒå¥½ã€‚&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="è®ºæ–‡é˜…è¯»ç¬”è®°" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="å…³ç³»æŠ½å–" scheme="http://yoursite.com/tags/%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/"/>
    
      <category term="ç»Ÿè®¡å­¦ä¹ " scheme="http://yoursite.com/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>ã€ŠInteraction Embeddings for Prediction and Explanationã€‹é˜…è¯»ç¬”è®°</title>
    <link href="http://yoursite.com/post/Interaction%20Embeddings%20for%20Prediction%20and%20Explanation/"/>
    <id>http://yoursite.com/post/Interaction Embeddings for Prediction and Explanation/</id>
    <published>2019-03-21T06:07:37.000Z</published>
    <updated>2019-03-25T01:59:25.370Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://www.zora.uzh.ch/id/eprint/162876/1/interaction-embeddings-prediction_merlin_version.pdf" target="_blank" rel="noopener">è®ºæ–‡ä¸‹è½½åœ°å€</a>ï¼Œæ­¤è®ºæ–‡ä¸»è¦æå‡ºäº†å®ä½“å’Œå…³ç³»çš„äº¤äº’ä½œç”¨å¯¹äºçŸ¥è¯†å›¾è°±åµŒå…¥çš„å½±å“ï¼Œå’Œæå‡ºäº†æ–°çš„åµŒå…¥è¯„ä¼°æ–¹æ¡ˆ - æœç´¢é¢„æµ‹è§£é‡Šã€‚</p></blockquote><a id="more"></a><h2 id="è®ºæ–‡è´¡çŒ®"><a href="#è®ºæ–‡è´¡çŒ®" class="headerlink" title="è®ºæ–‡è´¡çŒ®"></a>è®ºæ–‡è´¡çŒ®</h2><ul><li>æå‡ºäº†CrossEï¼Œä¸€ç§é€šè¿‡å­¦ä¹ ä¸€ä¸ªäº¤äº’çŸ©é˜µæ¥ç»™å®ä½“å’Œå…³ç³»çš„äº¤äº’å»ºæ¨¡çš„æ–°å‹çŸ¥è¯†å›¾è°±åµŒå…¥ã€‚</li><li>æˆ‘ä»¬ä½¿ç”¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†è¯„ä¼°CrossEä¸é“¾æ¥é¢„æµ‹ä»»åŠ¡ä¸Šçš„å„ç§å…¶ä»–KGEçš„æ¯”è¾ƒï¼Œå¹¶æ˜¾ç¤ºCrossEåœ¨å…·æœ‰é€‚åº¦å‚æ•°å¤§å°çš„å¤æ‚ä¸”æ›´å…·æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šå®ç°æœ€å…ˆè¿›çš„ç»“æœã€‚</li><li>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åµŒå…¥è¯„ä¼°æ–¹æ¡ˆ - æœç´¢é¢„æµ‹è§£é‡Šï¼Œå¹¶è¡¨æ˜CrossEèƒ½å¤Ÿç”Ÿæˆæ¯”å…¶ä»–æ–¹æ³•æ›´å¯é çš„è§£é‡Šã€‚ è¿™è¡¨æ˜äº¤äº’åµŒå…¥æ›´èƒ½åœ¨ä¸åŒçš„ä¸‰å…ƒç»„ç¯å¢ƒä¸­æ•æ‰å®ä½“å’Œå…³ç³»ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚</li></ul><h2 id="ä»‹ç»"><a href="#ä»‹ç»" class="headerlink" title="ä»‹ç»"></a>ä»‹ç»</h2><p>ç»™å®šçŸ¥è¯†å›¾è°±å’Œä¸€ä¸ªè¦é¢„æµ‹çš„ä¸‰å…ƒç»„çš„å¤´å®ä½“å’Œå…³ç³»ï¼Œåœ¨é¢„æµ‹å°¾å®ä½“çš„è¿‡ç¨‹ä¸­ï¼Œå¤´å®ä½“å’Œå…³ç³»ä¹‹é—´æ˜¯æœ‰äº¤å‰äº¤äº’çš„crossover interaction, å³å…³ç³»å†³å®šäº†åœ¨é¢„æµ‹çš„è¿‡ç¨‹ä¸­å“ªäº›å¤´å®ä½“çš„ä¿¡æ¯æ˜¯æœ‰ç”¨çš„ï¼Œè€Œå¯¹é¢„æµ‹æœ‰ç”¨çš„å¤´å®ä½“çš„ä¿¡æ¯åˆå†³å®šäº†é‡‡ç”¨ä»€ä¹ˆé€»è¾‘å»æ¨ç†å‡ºå°¾å®ä½“ï¼Œæ–‡ä¸­é€šè¿‡ä¸€ä¸ªæ¨¡æ‹Ÿçš„çŸ¥è¯†å›¾è°±è¿›è¡Œäº†è¯´æ˜å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š</p><p><img src="http://image.nysdy.com/20190321155314902217434.png" alt="20190321155314902217434.png"></p><h2 id="ç›¸å…³å·¥ä½œ"><a href="#ç›¸å…³å·¥ä½œ" class="headerlink" title="ç›¸å…³å·¥ä½œ"></a>ç›¸å…³å·¥ä½œ</h2><p>è®ºæ–‡ä¸­åœ¨è¿™éƒ¨åˆ†å¯¹KGEï¼ˆKnowledge graph embeddingï¼‰è¿›è¡Œäº†åˆ†ç±»æ€»ç»“ï¼š</p><ul><li>KGEs with general embeddings</li><li>KGEs with multiple embeddings.</li><li>KGEs that utilize extra information.</li></ul><p>è¿™éƒ¨åˆ†æ€»ç»“ä¸­å¯¹å¤§é‡çš„æ–¹æ³•è¿›è¡Œæè¿°ï¼Œå¯ä»¥ä½œä¸ºèƒŒæ™¯çŸ¥è¯†è¿›è¡Œé˜…è¯»ã€‚</p><h2 id="CrossEæ¨¡å‹"><a href="#CrossEæ¨¡å‹" class="headerlink" title="CrossEæ¨¡å‹"></a>CrossEæ¨¡å‹</h2><p>åŸºäºå¯¹å¤´å®ä½“å’Œå…³ç³»ä¹‹é—´äº¤å‰äº¤äº’çš„è§‚å¯Ÿï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„çŸ¥è¯†å›¾è°±è¡¨ç¤ºå­¦ä¹ æ¨¡å‹CrossE. CrossEé™¤äº†å­¦ä¹ å®ä½“å’Œå…³ç³»çš„å‘é‡è¡¨ç¤ºï¼ŒåŒæ—¶è¿˜å­¦ä¹ äº†ä¸€ä¸ªäº¤äº’çŸ©é˜µCï¼ŒCä¸å…³ç³»ç›¸å…³ï¼Œå¹¶ä¸”ç”¨äºç”Ÿæˆå®ä½“å’Œå…³ç³»ç»è¿‡äº¤äº’ä¹‹åçš„å‘é‡è¡¨ç¤ºï¼Œæ‰€ä»¥åœ¨CrossEä¸­å®ä½“å’Œå…³ç³»ä¸ä»…ä»…æœ‰é€šç”¨å‘é‡è¡¨ç¤ºï¼ŒåŒæ—¶è¿˜æœ‰å¾ˆå¤šäº¤äº’å‘é‡è¡¨ç¤ºã€‚CrossEæ ¸å¿ƒæƒ³æ³•å¦‚ä¸‹å›¾ï¼š<img src="http://image.nysdy.com/20190321155314970714298.png" alt="20190321155314970714298.png"></p><p>ç›®æ ‡å‡½æ•°ç²‰å››æ­¥ç”Ÿæˆï¼š</p><ol><li>Interaction Embedding for Entitiesï¼šæ ¹æ®å¤´å®ä½“å‘é‡å’Œäº¤äº’çŸ©é˜µï¼ˆä»¥å…³ç³»ç¡®å®šçš„ï¼‰æ¥ç¡®å®šå¤´å®ä½“çš„äº¤äº’è¡¨ç¤ºã€‚</li><li>Interaction Embedding for Relationsï¼šæ ¹æ®å¤´å®ä½“çš„äº¤äº’è¡¨ç¤ºå’Œå…³ç³»ä½œç”¨ç”Ÿæˆå…³ç³»çš„äº¤äº’è¡¨ç¤º</li><li>Combination Operatorï¼šå°†å¤´å®ä½“çš„äº¤äº’è¡¨ç¤ºå’Œå…³ç³»çš„äº¤äº’è¡¨ç¤ºç›¸ç»“åˆï¼Œå¹¶è¿›è¡Œéçº¿æ€§å¤„ç†ï¼ˆtanhï¼‰</li><li>Similarity Operatorï¼šè®¡ç®—ç»“åˆåè¡¨ç¤ºå’Œå°¾å®ä½“è¡¨ç¤ºä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚</li></ol><p>æœ€ååˆ†æ•°å‡½æ•°ï¼š</p><p><img src="http://image.nysdy.com/20190321155315073712112.png" alt="20190321155315073712112.png"></p><p>æŸå¤±å‡½æ•°ï¼šï¼ˆè¿™é‡Œå°±æ˜¯ä¸€ä¸ªäº¤å‰ç†µå‡½æ•°ï¼Œä½†æ˜¯å†™çš„æœ‰é—®é¢˜f(x)é¡¹åº”è¯¥åœ¨æ‹¬å·å¤–ï¼‰<img src="http://image.nysdy.com/20190321155315081421205.png" alt="20190321155315081421205.png"></p><h2 id="å¯¹äºé¢„æµ‹çš„è§£é‡Š"><a href="#å¯¹äºé¢„æµ‹çš„è§£é‡Š" class="headerlink" title="å¯¹äºé¢„æµ‹çš„è§£é‡Š"></a>å¯¹äºé¢„æµ‹çš„è§£é‡Š</h2><p>è¿™éƒ¨åˆ†ä½œè€…æè¿°äº†å¦‚ä½•ç”Ÿæˆé¢„æµ‹ä¸‰å…ƒç»„çš„è§£é‡Šï¼Œå¹¶ä»‹ç»äº†åŸºäºåµŒå…¥çš„è·¯å¾„æœç´¢ç®—æ³•ï¼Œä¸»è¦æ­¥éª¤å¦‚ä¸‹ï¼š</p><ol><li>Search for similar relationsï¼šä¿®å‰ªæ‰ä¸åˆç†è·¯å¾„</li><li>Search for paths between h and tï¼šä½œè€…å®šä¹‰äº†6ç§è·¯å¾„ï¼ˆç­æ±‰ä¸€ä¸ªæˆ–ä¸¤ä¸ªå…³ç³»ï¼‰</li><li>Search similar entitiesï¼šæ•è·å®ä½“ä¹‹é—´çš„ç›¸ä¼¼æ€§æ–¹é¢è¶Šæœ‰èƒ½åŠ›ï¼Œå°±è¶Šæœ‰å¯èƒ½å­˜åœ¨ï¼ˆhsï¼Œrï¼Œtsï¼‰</li><li>: Search for similar structures as supportsï¼šæˆ‘ä»¬åªå°†çŸ¥è¯†å›¾ä¸­è‡³å°‘æœ‰ä¸€ä¸ªæ”¯æŒçš„è·¯å¾„è§†ä¸ºè§£é‡Šã€‚</li></ol><h2 id="å®éªŒ"><a href="#å®éªŒ" class="headerlink" title="å®éªŒ"></a>å®éªŒ</h2><h3 id="æ•°æ®é›†"><a href="#æ•°æ®é›†" class="headerlink" title="æ•°æ®é›†"></a>æ•°æ®é›†</h3><p><img src="http://image.nysdy.com/20190321155315142112584.png" alt="20190321155315142112584.png"></p><h3 id="é“¾æ¥é¢„æµ‹"><a href="#é“¾æ¥é¢„æµ‹" class="headerlink" title="é“¾æ¥é¢„æµ‹"></a>é“¾æ¥é¢„æµ‹</h3><p><img src="/Users/dy/Library/Application Support/typora-user-images/image-20190321145750608.png" alt="image-20190321145750608"></p><p><img src="http://image.nysdy.com/20190321155315147862891.png" alt="20190321155315147862891.png"></p><p>ä»å®éªŒç»“æœä¸­æˆ‘ä»¬å¯ä»¥çœ‹å‡ºï¼ŒCrossEå®ç°äº†è¾ƒå¥½çš„é“¾æ¥é¢„æµ‹ç»“æœã€‚æˆ‘ä»¬å»é™¤CrossEä¸­çš„å¤´å®ä½“å’Œå…³ç³»çš„äº¤å‰äº¤äº’ï¼Œæ„é€ äº†æ¨¡å‹ CrossESï¼ŒCrossE å’Œ CrossES çš„æ¯”è¾ƒè¯´æ˜äº†äº¤å‰äº¤äº’çš„æœ‰æ•ˆæ€§ã€‚</p><h3 id="ç”Ÿæˆè§£é‡Š"><a href="#ç”Ÿæˆè§£é‡Š" class="headerlink" title="ç”Ÿæˆè§£é‡Š"></a>ç”Ÿæˆè§£é‡Š</h3><p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç›¸ä¼¼ç»“æ„é€šè¿‡çŸ¥è¯†å›¾è°±çš„è¡¨ç¤ºå­¦ä¹ ç»“æœç”Ÿæˆé¢„æµ‹ç»“æœè§£é‡Šçš„æ–¹æ³•ï¼Œå¹¶æå‡ºäº†ä¸¤ç§è¡¡é‡è§£é‡Šç»“æœçš„æŒ‡æ ‡ï¼ŒAvgSupportå’ŒRecallã€‚Recallæ˜¯æŒ‡æ¨¡å‹èƒ½ç»™å‡ºè§£é‡Šçš„é¢„æµ‹ç»“æœçš„å æ¯”ï¼Œå…¶ä»‹äº0å’Œ1ä¹‹é—´ä¸”å€¼è¶Šå¤§è¶Šå¥½ï¼›AvgSupportæ˜¯æ¨¡å‹èƒ½ç»™å‡ºè§£é‡Šçš„é¢„æµ‹ç»“æœçš„å¹³å‡supportä¸ªæ•°ï¼ŒAvgSupportæ˜¯ä¸€ä¸ªå¤§äº0çš„æ•°ä¸”è¶Šå¤§è¶Šå¥½ã€‚å¯è§£é‡Šçš„è¯„ä¼°ç»“æœå¦‚ä¸‹ï¼š</p><p><img src="http://image.nysdy.com/2019032115531515625385.png" alt="2019032115531515625385.png"></p><p>é“¾æ¥é¢„æµ‹å’Œå¯è§£é‡Šçš„å®éªŒä»ä¸¤ä¸ªä¸åŒçš„æ–¹é¢è¯„ä¼°äº†çŸ¥è¯†å›¾è°±è¡¨ç¤ºå­¦ä¹ çš„æ•ˆæœï¼ŒåŒæ—¶ä¹Ÿè¯´æ˜äº†é“¾æ¥é¢„æµ‹çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§æ²¡æœ‰å¿…ç„¶è”ç³»ï¼Œé“¾æ¥é¢„æµ‹æ•ˆæœå¥½çš„æ¨¡å‹å¹¶ä¸ä¸€å®šèƒ½å¤Ÿæ›´å¥½åœ°æä¾›è§£é‡Šï¼Œåä¹‹äº¦ç„¶ã€‚</p><h2 id="å‚è€ƒé“¾æ¥"><a href="#å‚è€ƒé“¾æ¥" class="headerlink" title="å‚è€ƒé“¾æ¥"></a>å‚è€ƒé“¾æ¥</h2><ul><li><a href="http://blog.openkg.cn/%E8%AE%BA%E6%96%87%E6%B5%85%E5%B0%9D-interaction-embeddings-for-prediction-and-explanation/" target="_blank" rel="noopener">http://blog.openkg.cn/%E8%AE%BA%E6%96%87%E6%B5%85%E5%B0%9D-interaction-embeddings-for-prediction-and-explanation/</a></li></ul><h2 id=""><a href="#" class="headerlink" title=" "></a> </h2>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://www.zora.uzh.ch/id/eprint/162876/1/interaction-embeddings-prediction_merlin_version.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;è®ºæ–‡ä¸‹è½½åœ°å€&lt;/a&gt;ï¼Œæ­¤è®ºæ–‡ä¸»è¦æå‡ºäº†å®ä½“å’Œå…³ç³»çš„äº¤äº’ä½œç”¨å¯¹äºçŸ¥è¯†å›¾è°±åµŒå…¥çš„å½±å“ï¼Œå’Œæå‡ºäº†æ–°çš„åµŒå…¥è¯„ä¼°æ–¹æ¡ˆ - æœç´¢é¢„æµ‹è§£é‡Šã€‚&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="è®ºæ–‡é˜…è¯»ç¬”è®°" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="çŸ¥è¯†å›¾è°±" scheme="http://yoursite.com/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    
      <category term="çŸ¥è¯†å›¾è°±åµŒå…¥" scheme="http://yoursite.com/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%B5%8C%E5%85%A5/"/>
    
      <category term="çŸ¥è¯†å›¾è°±æ¨ç†" scheme="http://yoursite.com/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%8E%A8%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>ã€ŠDifferentiable Learning of Logical Rules for Knowledge Base Reasoningã€‹é˜…è¯»ç¬”è®°</title>
    <link href="http://yoursite.com/post/Differentiable%20Learning%20of%20Logical%20Rules%20for%20Knowledge%20Base%20Reasoning/"/>
    <id>http://yoursite.com/post/Differentiable Learning of Logical Rules for Knowledge Base Reasoning/</id>
    <published>2019-03-05T07:34:05.000Z</published>
    <updated>2019-03-10T13:25:50.136Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>è®ºæ–‡<a href="http://papers.nips.cc/paper/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning.pdf" target="_blank" rel="noopener">ä¸‹è½½åœ°å€</a>ï¼Œæœ¬æ–‡ç ”ç©¶ç”¨äºäºçŸ¥è¯†å›¾è°±æ¨ç†çš„å­¦ä¹ æ¦‚ç‡ä¸€é˜¶é€»è¾‘è§„åˆ™çš„é—®é¢˜ï¼Œæå‡ºäº†Neural Logic Programmingï¼ˆNeural-LPï¼‰æ¡†æ¶ï¼Œå®ƒç»“åˆäº†ç«¯åˆ°ç«¯å¯å¾®åˆ†æ¨¡å‹ä¸­ä¸€é˜¶é€»è¾‘è§„åˆ™çš„å‚æ•°å’Œç»“æ„å­¦ä¹ ã€‚ä¸ºäº†åœ¨å¯å¾®åˆ†çš„æ¡†æ¶ä¸­åŒæ—¶å­¦ä¹ å‚æ•°å’Œç»“æ„ï¼Œä½œè€…è®¾è®¡äº†ä¸€ä¸ªå…·æœ‰æ³¨æ„æœºåˆ¶å’Œè®°å¿†çš„ç¥ç»æ§åˆ¶å™¨ç³»ç»Ÿï¼Œä»¥å­¦ä¹ é¡ºåºç»„æˆTensorLogä½¿ç”¨çš„åŸå§‹å¯å¾®æ“ä½œã€‚ä½œè€…é‡‡ç”¨çš„æ³¨æ„æœºåˆ¶æ˜¯ä½œä¸ºé€»è¾‘è§„åˆ™çš„ç½®ä¿¡åº¦å¹¶ä¸”æœ‰å¯“æ„å«ä¹‰çš„ã€‚</p></blockquote><a id="more"></a><p>ä¸‹å›¾å±•ç¤ºäº†ä¸€ä¸ªä½¿ç”¨é€»è¾‘è§„åˆ™è¿›è¡ŒçŸ¥è¯†å›¾è°±æ¨ç†çš„ä¾‹å­</p><p><img src="https://i.loli.net/2019/03/05/5c7dc90edacd1.jpg" alt=""></p><p>ä½¿ç”¨æ¦‚ç‡é€»è¾‘çš„ä¼˜ç‚¹æ˜¯é€šè¿‡ä¸ºé€»è¾‘è§„åˆ™é…å¤‡æ¦‚ç‡ï¼Œå¯ä»¥æ›´å¥½åœ°æ¨¡æ‹Ÿç»Ÿè®¡å¤æ‚å’Œå™ªå£°æ•°æ®ã€‚</p><p>statistical relational learningï¼ˆç»Ÿè®¡å…³ç³»å­¦ä¹ ï¼‰ï¼šå­¦ä¹ å…³ç³»è§„åˆ™çš„é›†åˆ</p><p>==inductive logic programmingï¼ˆå½’çº³é€»è¾‘è§„åˆ’ï¼‰ï¼š==å½“å­¦ä¹ æ¶‰åŠæå‡ºæ–°çš„é€»è¾‘è§„åˆ™æ—¶ã€‚ï¼ˆè¿™åº”è¯¥å’Œæˆ‘æ­£åœ¨åšçš„æ–¹å‘æ˜¯ç›¸å…³çš„ï¼Œéƒ½æ˜¯å¸¦æœ‰å½’çº³æ€§è´¨çš„ï¼Œæœ‰æ–°çš„ä¸œè¥¿äº§ç”Ÿï¼‰ã€‚</p><h1 id="Framework"><a href="#Framework" class="headerlink" title="Framework"></a>Framework</h1><h2 id="Knowledge-base-reasoning"><a href="#Knowledge-base-reasoning" class="headerlink" title="Knowledge base reasoning"></a>Knowledge base reasoning</h2><p>ä¸ºäº†æ¨ç†çŸ¥è¯†åº“ï¼Œå¯¹äºæ¯ä¸ªæŸ¥è¯¢æˆ‘ä»¬éƒ½æœ‰å…´è¶£å­¦ä¹ ä»¥ä¸‹å½¢å¼çš„åŠ æƒé“¾å¼é€»è¾‘è§„åˆ™ï¼Œç±»ä¼¼äº<strong>==éšæœºé€»è¾‘ç¨‹åº==</strong>ï¼š</p><p><img src="https://i.loli.net/2019/03/05/5c7dcd6e1a5ac.jpg" alt=""></p><p>å…¶ä¸­$\alpha$æ˜¯å’Œè§„åˆ™æœ‰å…³çš„ç½®ä¿¡åº¦ï¼ŒRæ˜¯çŸ¥è¯†åº“ä¸­çš„å…³ç³»ï¼Œquery(Y,X) è¡¨ç¤ºä¸€ä¸ªä¸‰å…ƒç»„ï¼Œquery è¡¨ç¤ºä¸€ä¸ªå…³ç³»ã€‚</p><h2 id="TensorLog-for-KB-reasoning"><a href="#TensorLog-for-KB-reasoning" class="headerlink" title="TensorLog for KB reasoning"></a>TensorLog for KB reasoning</h2><p>å°†å®ä½“è½¬æ¢æˆone-hotå˜é‡ï¼›å¹¶ç”¨ä¸€ä¸ªçŸ©é˜µ$M_R$è¡¨ç¤ºå…³ç³»ï¼Œè¯¥çŸ©é˜µåªåœ¨ï¼ˆiï¼Œjï¼‰å¤„ä¸º1ï¼Œiã€jä¸ºç¬¬iã€jä¸ªå®ä½“ã€‚</p><p>ç»“åˆä¸¤ä¸ªæ“ä½œï¼Œé€»è¾‘è§„åˆ™æ¨ç†$R(Y,X) \gets P(Y,X) \bigwedge Q(Z,X)$å¯ä»¥è¢«è¡¨ç¤ºä¸ºï¼š$M_P \cdot M_P \cdot v_x \doteq s$ï¼Œå‘é‡sä¸­ä¸º1çš„ä½ç½®å°±æ˜¯Yçš„ç­”æ¡ˆã€‚</p><p>å¯¹äºä¸€æ¡æŸ¥è¯¢ï¼Œæ‰€æœ‰çš„é€»è¾‘è§„åˆ™çš„å³è¾¹éƒ¨åˆ†è¢«è¡¨ç¤ºä¸ºä»¥ä¸‹å½¢å¼ï¼š</p><p><img src="https://i.loli.net/2019/03/05/5c7dda071bcef.jpg" alt=""></p><p>å…¶ä¸­ï¼Œlè¡¨ç¤ºæ‰€æœ‰çš„å¯èƒ½è§„åˆ™çš„ä¸ªæ•°ï¼Œ$\alpha_l$æ˜¯è§„åˆ™lçš„ç½®ä¿¡åº¦ï¼Œ$\beta_l$æ˜¯æŸç‰¹å®šå…³ç³»é‡Œçš„æœ‰åºå…³ç³»åˆ—è¡¨ï¼Œæ‰€ä»¥åœ¨inferenceæ—¶ï¼Œç»™å®šå®ä½“$v_xâ€‹$ï¼Œå®ä½“yçš„scoreç­‰äºå‘é‡sä¸­çš„å¯¹åº”yçš„ä½ç½®çš„å€¼ã€‚å¯¹äºæ¨ç†ï¼Œç»™å®šå®ä½“xï¼Œå®ä½“yçš„scoreç­‰äºå‘é‡sä¸­çš„å¯¹åº”yçš„ä½ç½®çš„å€¼ã€‚</p><p><img src="https://i.loli.net/2019/03/10/5c85104e2a53b.jpg" alt=""></p><p>æ‰€ä»¥æ€»ç»“æœ¬æ–‡å…³å¿ƒçš„ä¼˜åŒ–é—®é¢˜å¦‚ä¸‹ï¼š</p><p><img src="https://i.loli.net/2019/03/05/5c7e1568d9ec7.jpg" alt=""></p><h2 id="Learning-the-logical-rules"><a href="#Learning-the-logical-rules" class="headerlink" title="Learning the logical rules"></a>Learning the logical rules</h2><p>åœ¨ä¸Šå¼çš„ä¼˜åŒ–é—®é¢˜ä¸­ï¼Œç®—æ³•éœ€è¦å­¦ä¹ çš„éƒ¨åˆ†åˆ†ä¸ºä¸¤ä¸ªï¼šä¸€ä¸ªæ˜¯è§„åˆ™çš„ç»“æ„ï¼Œå³ä¸€ä¸ªè§„åˆ™æ˜¯ç”±å“ªäº›æ¡ä»¶ç»„åˆè€Œæˆçš„ï¼›å¦ä¸€ä¸ªæ˜¯è§„åˆ™çš„ç½®ä¿¡åº¦ã€‚ç”±äºæ¯ä¸€æ¡è§„åˆ™çš„ç½®ä¿¡åº¦éƒ½æ˜¯ä¾èµ–äºå…·ä½“çš„è§„åˆ™å½¢å¼ï¼Œè€Œè§„åˆ™ç»“æ„çš„ç»„æˆä¹Ÿæ˜¯ä¸€ä¸ªç¦»æ•£åŒ–çš„è¿‡ç¨‹ï¼Œå› æ­¤ä¸Šå¼æ•´ä½“æ˜¯ä¸å¯å¾®çš„ã€‚å› æ­¤ä½œè€…å¯¹å‰é¢çš„å¼å­åšäº†ä»¥ä¸‹æ›´æ”¹ï¼š<img src="https://i.loli.net/2019/03/05/5c7e160404436.jpg" alt=""></p><p>å¯¹æ¯”ä¸å¼ï¼ˆ2ï¼‰ï¼šä¸»è¦äº¤æ¢äº†è¿ä¹˜å’Œç´¯åŠ çš„è®¡ç®—é¡ºåºï¼Œå¯¹é¢„ä¸€ä¸ªå…³ç³»çš„ç›¸å…³çš„è§„åˆ™ï¼Œä¸ºæ¯ä¸ªå…³ç³»åœ¨æ¯ä¸ªæ­¥éª¤éƒ½å­¦ä¹ äº†ä¸€ä¸ªæƒé‡ï¼Œå³ä¸Šå¼çš„ $a_t^k$ã€‚</p><p>ç”±äºä¸Šå¼å›ºå®šäº†æ¯ä¸ªè§„åˆ™çš„é•¿åº¦éƒ½ä¸º Tï¼Œè¿™æ˜¾ç„¶æ˜¯ä¸åˆé€‚çš„ã€‚ä¸ºäº†èƒ½å¤Ÿå­¦ä¹ åˆ°å˜é•¿çš„è§„åˆ™ï¼ŒNeural LPä¸­è®¾è®¡äº†è®°å¿†å‘é‡ $u_t$,è¡¨ç¤ºæ¯ä¸ªæ­¥éª¤è¾“å‡ºçš„ç­”æ¡ˆâ€”æ¯ä¸ªå®ä½“ä½œä¸ºç­”æ¡ˆçš„æ¦‚ç‡åˆ†å¸ƒï¼Œè¿˜è®¾è®¡äº†ä¸¤ä¸ªæ³¨æ„åŠ›å‘é‡ï¼šä¸€ä¸ªä¸ºè®°å¿†æ³¨æ„åŠ›å‘é‡ $b_t$ â€”â€”è¡¨ç¤ºåœ¨æ­¥éª¤ t æ—¶å¯¹äºä¹‹å‰æ¯ä¸ªæ­¥éª¤çš„æ³¨æ„åŠ›ï¼›ä¸€ä¸ªä¸ºç®—å­æ³¨æ„åŠ›å‘é‡ $a_t$ â€”â€”è¡¨ç¤ºåœ¨æ­¥éª¤ t æ—¶å¯¹äºæ¯ä¸ªå…³ç³»ç®—å­çš„æ³¨æ„åŠ›ã€‚æ¯ä¸ªæ­¥éª¤çš„è¾“å‡ºç”±ä¸‹é¢ä¸‰ä¸ªå¼å­ç”Ÿæˆï¼š<img src="https://i.loli.net/2019/03/05/5c7e18ac09662.jpg" alt=""></p><p>å…¶ä¸­$b_t$å’Œ$a_t$ç”±ä»¥ä¸‹å…¬å¼é€šè¿‡RNNè·å¾—ï¼š</p><p><img src="https://i.loli.net/2019/03/05/5c7e1a015f0ac.jpg" alt=""></p><p>æ¨ç†æœºçš„æ•´ä½“æ¡†æ¶æ˜¯ï¼š</p><p><img src="https://i.loli.net/2019/03/05/5c7e1aec1aea9.jpg" alt=""></p><p>å…¶ä¸­memoryå­˜çš„å°±æ˜¯æ¯æ­¥çš„æ¨ç†ç»“æœï¼ˆå®ä½“ï¼‰ï¼Œæœ€åçš„è¾“å‡ºï¼ˆä¾‹å¦‚$u_{T+1}$ï¼Œç›®æ ‡å°±æ˜¯æœ€å¤§åŒ– $logv_y^Tu$ï¼ŒåŠ logæ˜¯å› ä¸ºéçº¿æ€§èƒ½è®©æ•ˆæœå˜å¥½ã€‚</p><p>æ•´ä¸ªç®—æ³•å¦‚ä¸‹ï¼š<img src="https://i.loli.net/2019/03/05/5c7e1c0d5cf32.jpg" alt=""></p><h1 id="å®éªŒ"><a href="#å®éªŒ" class="headerlink" title="å®éªŒ"></a>å®éªŒ</h1><h2 id="ï¼ˆ1ï¼‰-ä¸¤ä¸ªæ ‡å‡†æ•°æ®é›†ä¸Šçš„ç»Ÿè®¡å…³ç³»å­¦ä¹ ç›¸å…³çš„å®éªŒ"><a href="#ï¼ˆ1ï¼‰-ä¸¤ä¸ªæ ‡å‡†æ•°æ®é›†ä¸Šçš„ç»Ÿè®¡å…³ç³»å­¦ä¹ ç›¸å…³çš„å®éªŒ" class="headerlink" title="ï¼ˆ1ï¼‰ ä¸¤ä¸ªæ ‡å‡†æ•°æ®é›†ä¸Šçš„ç»Ÿè®¡å…³ç³»å­¦ä¹ ç›¸å…³çš„å®éªŒ"></a>ï¼ˆ1ï¼‰ ä¸¤ä¸ªæ ‡å‡†æ•°æ®é›†ä¸Šçš„ç»Ÿè®¡å…³ç³»å­¦ä¹ ç›¸å…³çš„å®éªŒ</h2><ul><li>Unified Medical Language System (UMLS)ï¼šThe entities are biomedical concepts (e.g. disease, antibiotic) and relations are like treats and diagnoses.</li><li>Kinshipï¼šcontains kinship relationships among members of the Alyawarra tribe from Central Australia [</li></ul><p><img src="https://i.loli.net/2019/03/05/5c7e2270289d0.jpg" alt=""></p><h2 id="ï¼ˆ2ï¼‰-åœ¨-16-16-çš„ç½‘æ ¼ä¸Šçš„è·¯å¾„å¯»æ‰¾çš„å®éªŒ"><a href="#ï¼ˆ2ï¼‰-åœ¨-16-16-çš„ç½‘æ ¼ä¸Šçš„è·¯å¾„å¯»æ‰¾çš„å®éªŒ" class="headerlink" title="ï¼ˆ2ï¼‰ åœ¨$16*16$çš„ç½‘æ ¼ä¸Šçš„è·¯å¾„å¯»æ‰¾çš„å®éªŒ"></a>ï¼ˆ2ï¼‰ åœ¨$16*16$çš„ç½‘æ ¼ä¸Šçš„è·¯å¾„å¯»æ‰¾çš„å®éªŒ</h2><p><img src="https://i.loli.net/2019/03/05/5c7e235b430cb.jpg" alt=""></p><h2 id="ï¼ˆ3ï¼‰-çŸ¥è¯†åº“è¡¥å…¨å®éªŒ"><a href="#ï¼ˆ3ï¼‰-çŸ¥è¯†åº“è¡¥å…¨å®éªŒ" class="headerlink" title="ï¼ˆ3ï¼‰ çŸ¥è¯†åº“è¡¥å…¨å®éªŒ"></a>ï¼ˆ3ï¼‰ çŸ¥è¯†åº“è¡¥å…¨å®éªŒ</h2><p>å®éªŒæ‰€ç”¨æ•°æ®é›†ä¿¡æ¯ï¼š</p><p>FB15KSelectedï¼šè¿™æ˜¯é€šè¿‡ä»FB15Kä¸­å»é™¤è¿‘ä¼¼é‡å¤å’Œåå‘å…³ç³»è€Œæ„é€ çš„</p><p><img src="https://i.loli.net/2019/03/05/5c7e2378e1c64.jpg" alt=""></p><p>å®éªŒç»“æœï¼š<img src="https://i.loli.net/2019/03/05/5c7e23c43db48.jpg" alt=""></p><p>ä¸ºäº†è¯æ˜Neural LPçš„å½’çº³æ¨ç†çš„èƒ½åŠ›ï¼Œæœ¬æ–‡è¿˜ç‰¹åˆ«è®¾è®¡äº†ä¸€ä¸ªå®éªŒï¼Œåœ¨è®­ç»ƒæ•°æ®é›†ä¸­å»æ‰æ‰€æœ‰æ¶‰åŠæµ‹è¯•é›†ä¸­åŒ…å«çš„å®ä½“çš„ä¸‰å…ƒç»„ï¼Œç„¶åè®­ç»ƒå¹¶é¢„æµ‹ï¼Œå¾—åˆ°ç»“æœå¦‚ä¸‹ï¼š<img src="https://i.loli.net/2019/03/05/5c7e23ebbca69.jpg" alt=""></p><h2 id="ï¼ˆ4ï¼‰-çŸ¥è¯†åº“é—®ç­”çš„å®éªŒ"><a href="#ï¼ˆ4ï¼‰-çŸ¥è¯†åº“é—®ç­”çš„å®éªŒ" class="headerlink" title="ï¼ˆ4ï¼‰ çŸ¥è¯†åº“é—®ç­”çš„å®éªŒ"></a>ï¼ˆ4ï¼‰ çŸ¥è¯†åº“é—®ç­”çš„å®éªŒ</h2><p><img src="https://i.loli.net/2019/03/05/5c7e24aa6a427.jpg" alt=""></p><h1 id="æ€»ç»“"><a href="#æ€»ç»“" class="headerlink" title="æ€»ç»“"></a><strong>æ€»ç»“</strong></h1><p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå¯å¾®çš„è§„åˆ™å­¦ä¹ æ¨¡å‹ï¼Œå¹¶å¼ºè°ƒäº†çŸ¥è¯†åº“ä¸­çš„è§„åˆ™åº”è¯¥æ˜¯å®ä½“æ— å…³çš„ï¼Œå¯¹äºæˆ‘ç›®å‰åœ¨åšçš„æ–¹å‘ï¼Œæœ¬ä½“è®ºä¹Ÿæ˜¯ä¸å®ä½“æ— å…³çš„ï¼Œè¿™ç§è§„åˆ™å­¦ä¹ æœ‰ä¸€å®šçš„å€Ÿé‰´æ€§ï¼Œä½†æ˜¯å¥½åƒæ‰€åŒºåˆ«ã€‚è¿™ä¸ªè§„åˆ™æ¨ç†ä¹Ÿå¯ä»¥çœ‹æˆæŸäº›å…³ç³»ä¹‹é—´çš„åŒ…å«å…³ç³»3.1ä¸­ä¸¾çš„HasOfficeInCity(New York,Uber) and CityInCountry(USA,New York)çš„ä¾‹å­ï¼Œå¯ä»¥çœ‹ä½œæ˜¯2å¯¹äº1æœ‰åŒ…å«å…³ç³»ã€‚å¹¶ä¸”å¯ä»¥çœ‹åˆ°æœ¬ç¯‡è®ºæ–‡ä¸­ï¼Œä½œè€…è®¾è®¡äº†ä¸°å¯Œçš„å®éªŒã€‚</p><h1 id="å‚è€ƒé“¾æ¥"><a href="#å‚è€ƒé“¾æ¥" class="headerlink" title="å‚è€ƒé“¾æ¥"></a>å‚è€ƒé“¾æ¥</h1><ul><li><a href="https://toutiao.io/posts/wrxf4z/preview" target="_blank" rel="noopener">https://toutiao.io/posts/wrxf4z/preview</a></li><li><a href="https://zhuanlan.zhihu.com/p/46024825" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/46024825</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;è®ºæ–‡&lt;a href=&quot;http://papers.nips.cc/paper/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ä¸‹è½½åœ°å€&lt;/a&gt;ï¼Œæœ¬æ–‡ç ”ç©¶ç”¨äºäºçŸ¥è¯†å›¾è°±æ¨ç†çš„å­¦ä¹ æ¦‚ç‡ä¸€é˜¶é€»è¾‘è§„åˆ™çš„é—®é¢˜ï¼Œæå‡ºäº†Neural Logic Programmingï¼ˆNeural-LPï¼‰æ¡†æ¶ï¼Œå®ƒç»“åˆäº†ç«¯åˆ°ç«¯å¯å¾®åˆ†æ¨¡å‹ä¸­ä¸€é˜¶é€»è¾‘è§„åˆ™çš„å‚æ•°å’Œç»“æ„å­¦ä¹ ã€‚ä¸ºäº†åœ¨å¯å¾®åˆ†çš„æ¡†æ¶ä¸­åŒæ—¶å­¦ä¹ å‚æ•°å’Œç»“æ„ï¼Œä½œè€…è®¾è®¡äº†ä¸€ä¸ªå…·æœ‰æ³¨æ„æœºåˆ¶å’Œè®°å¿†çš„ç¥ç»æ§åˆ¶å™¨ç³»ç»Ÿï¼Œä»¥å­¦ä¹ é¡ºåºç»„æˆTensorLogä½¿ç”¨çš„åŸå§‹å¯å¾®æ“ä½œã€‚ä½œè€…é‡‡ç”¨çš„æ³¨æ„æœºåˆ¶æ˜¯ä½œä¸ºé€»è¾‘è§„åˆ™çš„ç½®ä¿¡åº¦å¹¶ä¸”æœ‰å¯“æ„å«ä¹‰çš„ã€‚&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="è®ºæ–‡é˜…è¯»ç¬”è®°" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="çŸ¥è¯†å›¾è°±" scheme="http://yoursite.com/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    
      <category term="çŸ¥è¯†å›¾è°±æ¨ç†" scheme="http://yoursite.com/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%8E%A8%E7%90%86/"/>
    
      <category term="è§„åˆ™å­¦ä¹ " scheme="http://yoursite.com/tags/%E8%A7%84%E5%88%99%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
