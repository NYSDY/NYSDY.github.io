<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>NYSDY</title>
  
  <subtitle>NYSDY</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://nysdy.com/"/>
  <updated>2019-10-25T07:43:08.240Z</updated>
  <id>http://nysdy.com/</id>
  
  <author>
    <name>NYSDY</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>2016 TransG : A Generative Model for Knowledge Graph Embedding阅读笔记</title>
    <link href="http://nysdy.com/post/TransG_:_A_Generative_Model_for_Knowledge_Graph_Embedding/"/>
    <id>http://nysdy.com/post/TransG_:_A_Generative_Model_for_Knowledge_Graph_Embedding/</id>
    <published>2019-10-12T02:03:44.000Z</published>
    <updated>2019-10-25T07:43:08.240Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://www.aclweb.org/anthology/P16-1219.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;论文下载地址&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://nysdy.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="KGE" scheme="http://nysdy.com/tags/KGE/"/>
    
      <category term="2016" scheme="http://nysdy.com/tags/2016/"/>
    
      <category term="ACL" scheme="http://nysdy.com/tags/ACL/"/>
    
  </entry>
  
  <entry>
    <title>2016 Knowledge Graph Completion with Adaptive Sparse Transfer Matrix阅读笔记</title>
    <link href="http://nysdy.com/post/2016_Knowledge_Graph_Completion_with_Adaptive_Sparse_Transfer_Matrix/"/>
    <id>http://nysdy.com/post/2016_Knowledge_Graph_Completion_with_Adaptive_Sparse_Transfer_Matrix/</id>
    <published>2019-09-10T01:16:53.000Z</published>
    <updated>2019-10-12T05:30:51.532Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;为解决heterogeneity和imbalance问题，作者针对同一关系链接实体对的数量和同一关系不同头尾实体数量，分别设计了不同稀疏程度的转移矩阵。存在缺点：并没有同时解决这两个问题。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/11982/11693&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;论文下载地址&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://nysdy.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="KGE" scheme="http://nysdy.com/tags/KGE/"/>
    
      <category term="2016" scheme="http://nysdy.com/tags/2016/"/>
    
      <category term="AAAI" scheme="http://nysdy.com/tags/AAAI/"/>
    
  </entry>
  
  <entry>
    <title>2015 TransA An Adaptive Approach for Knowledge Graph Embeddin阅读笔记</title>
    <link href="http://nysdy.com/post/2015_TransA_An_Adaptive_Approach_for_Knowledge_Graph_Embeddin/"/>
    <id>http://nysdy.com/post/2015_TransA_An_Adaptive_Approach_for_Knowledge_Graph_Embeddin/</id>
    <published>2019-09-02T06:32:48.000Z</published>
    <updated>2019-09-09T13:46:46.350Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;这篇文章具体的公式操作比较难以理解，但是对于我来说，它的每一维度加权和最后判断时也应该考虑不同维度差异和我的思路比较像。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1509.05490&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;论文下载地址&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://nysdy.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
      <category term="KG" scheme="http://nysdy.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/KG/"/>
    
    
      <category term="TransA" scheme="http://nysdy.com/tags/TransA/"/>
    
      <category term="KGE" scheme="http://nysdy.com/tags/KGE/"/>
    
      <category term="2015" scheme="http://nysdy.com/tags/2015/"/>
    
      <category term="arxiv" scheme="http://nysdy.com/tags/arxiv/"/>
    
  </entry>
  
  <entry>
    <title>Transition-based Knowledge Graph Embedding with Relational Mapping Properties阅读笔记</title>
    <link href="http://nysdy.com/post/Transition-based_Knowledge_Graph_Embedding_with_Relational_Mapping_Properties/"/>
    <id>http://nysdy.com/post/Transition-based_Knowledge_Graph_Embedding_with_Relational_Mapping_Properties/</id>
    <published>2019-08-31T13:36:06.000Z</published>
    <updated>2019-09-15T13:59:40.912Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pdfs.semanticscholar.org/0ddd/f37145689e5f2899f8081d9971882e6ff1e9.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;论文下载地址&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://nysdy.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
      <category term="KG" scheme="http://nysdy.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/KG/"/>
    
    
      <category term="KGE" scheme="http://nysdy.com/tags/KGE/"/>
    
      <category term="2014" scheme="http://nysdy.com/tags/2014/"/>
    
  </entry>
  
  <entry>
    <title>Embedding Edge-attributed Relational Hierarchies阅读笔记</title>
    <link href="http://nysdy.com/post/Embedding_Edge-attributed_Relational_Hierarchies/"/>
    <id>http://nysdy.com/post/Embedding_Edge-attributed_Relational_Hierarchies/</id>
    <published>2019-08-28T08:12:30.000Z</published>
    <updated>2019-08-28T08:15:05.849Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;http://yellowstone.cs.ucla.edu/~muhao/articles/_SIGIR__hre.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;论文下载地址&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://nysdy.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>Knowledge Graph Embedding via Dynamic Mapping Matrix阅读笔记</title>
    <link href="http://nysdy.com/post/Knowledge_Graph_Embedding_via_Dynamic_Mapping_Matrix/"/>
    <id>http://nysdy.com/post/Knowledge_Graph_Embedding_via_Dynamic_Mapping_Matrix/</id>
    <published>2019-08-24T11:58:52.000Z</published>
    <updated>2019-08-28T04:58:45.444Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;论文下载地址&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://nysdy.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="KGE" scheme="http://nysdy.com/tags/KGE/"/>
    
      <category term="TransD" scheme="http://nysdy.com/tags/TransD/"/>
    
  </entry>
  
  <entry>
    <title>From Knowledge Graph Embedding to Ontology Embedding  An Analysis of the Compatibility between Vector Space Representations and Rules阅读笔记</title>
    <link href="http://nysdy.com/post/From_Knowledge_Graph_Embedding_to_Ontology_Embedding_An_Analysis_of_the_Compatibility_between_Vector_Space_Representations_and_Rules/"/>
    <id>http://nysdy.com/post/From_Knowledge_Graph_Embedding_to_Ontology_Embedding_An_Analysis_of_the_Compatibility_between_Vector_Space_Representations_and_Rules/</id>
    <published>2019-07-25T08:57:55.000Z</published>
    <updated>2019-07-25T09:04:31.963Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;论文下载地址&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://nysdy.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>Knowledge graph embedding with concepts阅读笔记</title>
    <link href="http://nysdy.com/post/Knowledge_graph_embedding_with_concepts/"/>
    <id>http://nysdy.com/post/Knowledge_graph_embedding_with_concepts/</id>
    <published>2019-07-25T05:19:44.000Z</published>
    <updated>2020-03-25T14:12:06.667Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;这篇论文，运用skip-gram方法，将实体对应相关概念引入实体向量表示，以增强表示效果。实体和概念在同一空间中，但是概念是空间中的一个超平面（类似于transH）。文中举例很多例子来辅助说明，使得文章可读性大幅提升。文中实验最后俩个比较有意思。本文值得思考借鉴的东西不少，值得再好好回顾。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0950705118304945/pdfft?md5=b9bad12f2bc771990ad0feaefa7402d4&amp;amp;pid=1-s2.0-S0950705118304945-main.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;论文下载地址&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://nysdy.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
      <category term="KG" scheme="http://nysdy.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/KG/"/>
    
    
      <category term="KGE" scheme="http://nysdy.com/tags/KGE/"/>
    
      <category term="ontology" scheme="http://nysdy.com/tags/ontology/"/>
    
      <category term="concept" scheme="http://nysdy.com/tags/concept/"/>
    
  </entry>
  
  <entry>
    <title>Universal Representation Learning of Knowledge Bases by Jointly Embedding Instances and Ontological Concepts阅读笔记</title>
    <link href="http://nysdy.com/post/Universal_Representation_Learning_of_Knowledge_Bases_by_Jointly_Embedding_Instances_and_Ontological_Concepts/"/>
    <id>http://nysdy.com/post/Universal_Representation_Learning_of_Knowledge_Bases_by_Jointly_Embedding_Instances_and_Ontological_Concepts/</id>
    <published>2019-07-17T08:45:52.000Z</published>
    <updated>2019-07-25T08:41:32.245Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://web.cs.ucla.edu/~yzsun/papers/2019_KDD_JOIE.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;论文下载地址&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://nysdy.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
      <category term="KG" scheme="http://nysdy.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/KG/"/>
    
    
      <category term="KGE" scheme="http://nysdy.com/tags/KGE/"/>
    
      <category term="ontology" scheme="http://nysdy.com/tags/ontology/"/>
    
  </entry>
  
  <entry>
    <title>DocRED A Large-Scale Document-Level Relation Extraction Dataset阅读笔记</title>
    <link href="http://nysdy.com/post/DocRED_A_Large-Scale_Document-Level_Relation_Extraction_Dataset/"/>
    <id>http://nysdy.com/post/DocRED_A_Large-Scale_Document-Level_Relation_Extraction_Dataset/</id>
    <published>2019-07-01T01:11:49.000Z</published>
    <updated>2019-07-01T01:49:37.278Z</updated>
    
    <summary type="html">
    
      
      
        
        
          &lt;blockquote&gt;
&lt;p&gt;这是一个介绍数据集的论文，主要是文档级别的关系抽取数据集。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://arxiv.org/abs/1906.06127&quot; target=&quot;_blank&quot;
        
      
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://nysdy.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="RE" scheme="http://nysdy.com/tags/RE/"/>
    
      <category term="dataset" scheme="http://nysdy.com/tags/dataset/"/>
    
  </entry>
  
  <entry>
    <title>Learning Entity and Relation Embeddings for Knowledge Graph Completion阅读笔记</title>
    <link href="http://nysdy.com/post/Learning_Entity_and_Relation_Embeddings_for_Knowledge_Graph_Completion/"/>
    <id>http://nysdy.com/post/Learning_Entity_and_Relation_Embeddings_for_Knowledge_Graph_Completion/</id>
    <published>2019-06-26T08:18:15.000Z</published>
    <updated>2019-06-27T07:51:44.136Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;TransR embeds entities and relations in distinct entity space and relation space, and learns embeddings via translation between projected entities.CTransR models internal complicated correlations within each relation type.&lt;/p&gt;
&lt;p&gt;论文下载地址&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://nysdy.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="KGE" scheme="http://nysdy.com/tags/KGE/"/>
    
      <category term="KGR" scheme="http://nysdy.com/tags/KGR/"/>
    
      <category term="TransR" scheme="http://nysdy.com/tags/TransR/"/>
    
  </entry>
  
  <entry>
    <title>Neural Relation Extraction with Selective Attention over Instances阅读笔记</title>
    <link href="http://nysdy.com/post/Neural_Relation_Extraction_with_Selective_Attention_over_Instances/"/>
    <id>http://nysdy.com/post/Neural_Relation_Extraction_with_Selective_Attention_over_Instances/</id>
    <published>2019-06-26T05:49:57.000Z</published>
    <updated>2019-06-26T08:12:44.937Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;这篇文章之前看过😂。&lt;/p&gt;
&lt;p&gt;论下载地址&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://nysdy.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="RE" scheme="http://nysdy.com/tags/RE/"/>
    
  </entry>
  
  <entry>
    <title>Learning as the Unsupervised Alignment of Conceptual Systems阅读笔记</title>
    <link href="http://nysdy.com/post/Learning_as_the_Unsupervised_Alignment_of_Conceptual_Systems/"/>
    <id>http://nysdy.com/post/Learning_as_the_Unsupervised_Alignment_of_Conceptual_Systems/</id>
    <published>2019-06-25T06:42:57.000Z</published>
    <updated>2019-06-26T04:02:25.369Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;这篇文章没怎么看懂，主要思想应该是代表同时概念的不同形式（文本，图像，语音等）应该具有相似的分布，以此来进行无监督的概念对齐。这种思路挺不错的，不过还没有深入的想法，算是拓展视野吧！&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://nysdy.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ERNIE Enhanced Language Representation with Informative Entities阅读笔记</title>
    <link href="http://nysdy.com/post/ERNIE_Enhanced_Language_Representation_with_Informative_Entities/"/>
    <id>http://nysdy.com/post/ERNIE_Enhanced_Language_Representation_with_Informative_Entities/</id>
    <published>2019-06-24T03:10:26.000Z</published>
    <updated>2019-06-25T06:32:57.460Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;该篇论文借鉴BERT，试图将实体信息（TransE）融入token(singal word)中，通过类似实体对齐的方法将实体与token对齐（并采取mask方式进行预训练），通过infromation fusion 将token与实体融合映射入相关联的两个向量空间。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1905.07129&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;论文下载地址&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://nysdy.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
      <category term="KG" scheme="http://nysdy.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/KG/"/>
    
    
      <category term="KGR" scheme="http://nysdy.com/tags/KGR/"/>
    
      <category term="BERT" scheme="http://nysdy.com/tags/BERT/"/>
    
      <category term="KG" scheme="http://nysdy.com/tags/KG/"/>
    
  </entry>
  
  <entry>
    <title>Incorporating Literals into Knowledge Graph Embeddings阅读笔记</title>
    <link href="http://nysdy.com/post/Incorporating_Literals_into_Knowledge_Graph_Embeddings/"/>
    <id>http://nysdy.com/post/Incorporating_Literals_into_Knowledge_Graph_Embeddings/</id>
    <published>2019-06-03T06:57:50.000Z</published>
    <updated>2019-06-03T07:33:16.832Z</updated>
    
    <summary type="html">
    
      
      
        
        
          &lt;blockquote&gt;
&lt;p&gt;读完了前两章，简单的看了一下作者提出的模型，感觉并没有太大价值，就是给实体输入多加入了一个literal的信息（加入方法可以采用线性、非线性或者神经网络）。&lt;/p&gt;
&lt;p&gt;读论文前需要先熟悉DistMult、ComlLEx和ConvE模型，此论文方
        
      
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://nysdy.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
      <category term="KG" scheme="http://nysdy.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/KG/"/>
    
    
      <category term="KGE" scheme="http://nysdy.com/tags/KGE/"/>
    
      <category term="link prediction" scheme="http://nysdy.com/tags/link-prediction/"/>
    
  </entry>
  
  <entry>
    <title>Learning Knowledge Embeddings by Combining Limit-based Scoring Loss阅读笔记</title>
    <link href="http://nysdy.com/post/Learning_Knowledge_Embeddings_by_Combining_Limit-based_Scoring_Loss/"/>
    <id>http://nysdy.com/post/Learning_Knowledge_Embeddings_by_Combining_Limit-based_Scoring_Loss/</id>
    <published>2019-06-03T02:57:20.000Z</published>
    <updated>2019-06-03T03:06:20.944Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;此篇文章最为重要的就是作者设计的 margin-based ranking loss 的改进，对两个超参数$\lambda$和$\gamma$的实验，对于实验结果有很多值得分析与思考的地方。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://dl.acm.org/ft_gateway.cfm?id=3132939&amp;amp;ftid=1920664&amp;amp;dwn=1&amp;amp;CFID=135630312&amp;amp;CFTOKEN=71536805165d7c9d-10D2074A-AD9B-A596-5CA31DB63C36A322&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;论文下载地址&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://nysdy.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
      <category term="KG" scheme="http://nysdy.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/KG/"/>
    
    
      <category term="KG" scheme="http://nysdy.com/tags/KG/"/>
    
      <category term="transH" scheme="http://nysdy.com/tags/transH/"/>
    
      <category term="margin loss" scheme="http://nysdy.com/tags/margin-loss/"/>
    
      <category term="transE" scheme="http://nysdy.com/tags/transE/"/>
    
  </entry>
  
  <entry>
    <title>Knowledge Graph Embedding by Translating on Hyperplanes阅读笔记</title>
    <link href="http://nysdy.com/post/Knowledge%20Graph%20Embedding%20by%20Translating%20on%20Hyperplanes/"/>
    <id>http://nysdy.com/post/Knowledge Graph Embedding by Translating on Hyperplanes/</id>
    <published>2019-05-28T08:17:44.000Z</published>
    <updated>2019-06-01T05:38:20.042Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;作为trans系列经典文献，必读。文章主要精华在于这种超平面想法的由来解决了同一实体的多关系问题。&lt;/p&gt;
&lt;p&gt;Authors proposed TransH which models a relation as a hyperplane together with a translation operation on it. It solves the problem of multi-relation and makes a good trade-off between model capacity and efficiency.&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://nysdy.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
      <category term="KG" scheme="http://nysdy.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/KG/"/>
    
    
      <category term="KGE" scheme="http://nysdy.com/tags/KGE/"/>
    
      <category term="transH" scheme="http://nysdy.com/tags/transH/"/>
    
  </entry>
  
  <entry>
    <title>Attention Is All You Need阅读笔记</title>
    <link href="http://nysdy.com/post/Attention%20Is%20All%20You%20Need/"/>
    <id>http://nysdy.com/post/Attention Is All You Need/</id>
    <published>2019-05-25T05:57:35.000Z</published>
    <updated>2019-05-28T07:58:49.154Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;transformer 是一个完全由注意力机制组成的搭建的模型，模型复杂度低，并可以进行并行计算，使得计算速度快。在翻译模型上取得了较好的效果。本篇论文属于经典必读论文，阅读笔记中对一些不清楚的地方进行了汉语解释，读完论文后阅读参考链接以加深理解。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1706.03762&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;论文下载地址&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://nysdy.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
      <category term="classical" scheme="http://nysdy.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/classical/"/>
    
    
      <category term="attention" scheme="http://nysdy.com/tags/attention/"/>
    
      <category term="transformer" scheme="http://nysdy.com/tags/transformer/"/>
    
      <category term="translation" scheme="http://nysdy.com/tags/translation/"/>
    
      <category term="classical" scheme="http://nysdy.com/tags/classical/"/>
    
  </entry>
  
  <entry>
    <title>Graph Neural Networks with Generated Parameters for Relation Extraction阅读笔记</title>
    <link href="http://nysdy.com/post/Graph_Neural_Networks_with_Generated_Parameters_for_Relation/"/>
    <id>http://nysdy.com/post/Graph_Neural_Networks_with_Generated_Parameters_for_Relation/</id>
    <published>2019-05-23T02:41:51.000Z</published>
    <updated>2019-05-23T09:24:39.487Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文将GNNs应用到处理非结构化文本的（多跳）关系推理任务来进行关系抽取。采用从句子序列中获取的实体构建全链接图，应用编码（sequence model），传播（节点间信息）和分类（预测）三个模块来处理关系推理。本文提供了三个数据集。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://nysdy.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="GNNs" scheme="http://nysdy.com/tags/GNNs/"/>
    
      <category term="relation extraction" scheme="http://nysdy.com/tags/relation-extraction/"/>
    
      <category term="relation reasoning" scheme="http://nysdy.com/tags/relation-reasoning/"/>
    
  </entry>
  
  <entry>
    <title>allennlp安装踩坑</title>
    <link href="http://nysdy.com/post/allennlp_install/"/>
    <id>http://nysdy.com/post/allennlp_install/</id>
    <published>2019-05-22T14:09:27.000Z</published>
    <updated>2019-05-22T14:11:19.740Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;安装allennlp的踩坑之路，踩了不少坑最后选择’Installing from source’的安装方法，排坑后下面方法亲测可用&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="install" scheme="http://nysdy.com/categories/install/"/>
    
    
      <category term="allennlp" scheme="http://nysdy.com/tags/allennlp/"/>
    
      <category term="包安装" scheme="http://nysdy.com/tags/%E5%8C%85%E5%AE%89%E8%A3%85/"/>
    
  </entry>
  
</feed>
