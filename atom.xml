<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>NYSDY</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-08-26T07:50:41.328Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>NYSDY</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Knowledge Graph Embedding via Dynamic Mapping Matrix阅读笔记</title>
    <link href="http://yoursite.com/post/Knowledge_Graph_Embedding_via_Dynamic_Mapping_Matrix/"/>
    <id>http://yoursite.com/post/Knowledge_Graph_Embedding_via_Dynamic_Mapping_Matrix/</id>
    <published>2019-08-24T11:58:52.000Z</published>
    <updated>2019-08-26T07:50:41.328Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>论文下载地址</p></blockquote><a id="more"></a><h1 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h1><ol><li>对于特定的关系$r$，所有的实体都共享相同的映射矩阵$M_r$。然而，由关系链接的实体总是包含各种类型和属性。</li><li>投影是实体和关系之间的交互过程，映射矩阵只能由关系决定是不合理的。</li><li>矩阵向量乘法使其具有大量计算，并且当关系数大时，它还具有比TransE和TransH更多的参数。 由于复杂性，TransR / CTransR难以应用于大规模知识图</li></ol><h1 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h1><ol><li>作者构建了一个新颖的模型TransD，通过同时考虑实体和关系的多样性，为每一个实体-关系构建动态映射矩阵。它为实体表示映射到关系向量空间提供灵活的样式。</li><li>与TransR / CTransR相比，TransD具有更少的参数并且没有矩阵向量乘法</li><li>在实验中，作者的方法优于之前的模型。</li></ol><h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><p>模型在TransD中，每个命名的符号对象（实体和关系）由两个向量表示。 第一个捕获实体（关系）的含义，另一个用于构造映射矩阵。</p><p><img src="http://image.nysdy.com/20190826156680488849411.png" alt="20190826156680488849411.png"></p><p>对于一个三元组$(h,r,t)$,向量一共有$h, h_p, r, r_p, t, t_p$,其中带$p$的为映射向量，则有</p><script type="math/tex; mode=display">\begin{aligned} \mathbf{M}_{r h} &=\mathbf{r}_{p} \mathbf{h}_{p}^{\top}+\mathbf{I}^{m \times n} \\ \mathbf{M}_{r t} &=\mathbf{r}_{p} \mathbf{t}_{p}^{\top}+\mathbf{I}^{m \times n} \end{aligned}</script><p>故</p><script type="math/tex; mode=display">\mathbf{h}_{\perp}=\mathbf{M}_{r h} \mathbf{h}, \quad \mathbf{t}_{\perp}=\mathbf{M}_{r t} \mathbf{t}</script><p>可以综合为：</p><script type="math/tex; mode=display">\begin{aligned} \mathbf{h}_{\perp} &=\mathbf{M}_{r h} \mathbf{h}=\mathbf{h}+\mathbf{h}_{p}^{\top} \mathbf{h} \mathbf{r}_{p} \\ \mathbf{t}_{\perp} &=\mathbf{M}_{r t} \mathbf{t}=\mathbf{t}+\mathbf{t}_{p}^{\top} \mathbf{t} \mathbf{r}_{p} \end{aligned}</script><p>这样就没有矩阵和向量间的乘法运算，变成向量间运算，提升计算速度。</p><h1 id="Experiments-and-Results-Analysis"><a href="#Experiments-and-Results-Analysis" class="headerlink" title="Experiments and Results Analysis"></a>Experiments and Results Analysis</h1><p>常规实验：triplets classification and link prediction不再赘述。</p><p>作者在实验过程中关注了一些具有更低accuracy的关系。</p><p><img src="http://image.nysdy.com/20190826156680558013943.png" alt="20190826156680558013943.png"></p><p>分析：</p><pre><code>1. 对于$similar_to$关系主要因为训练数据不充足，只占了1.5%。 2. 对于最右侧的图说明了bern方法的效果要好于unif</code></pre><h3 id="Properties-of-Projection-Vectors"><a href="#Properties-of-Projection-Vectors" class="headerlink" title="Properties of Projection Vectors"></a>Properties of Projection Vectors</h3><p>作者还做了case study，通过不同类型实体和关系的投影向量的相似性表明了作者方法的合理性</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;论文下载地址&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="KGE" scheme="http://yoursite.com/tags/KGE/"/>
    
      <category term="TransD" scheme="http://yoursite.com/tags/TransD/"/>
    
  </entry>
  
  <entry>
    <title>From Knowledge Graph Embedding to Ontology Embedding  An Analysis of the Compatibility between Vector Space Representations and Rules阅读笔记</title>
    <link href="http://yoursite.com/post/From_Knowledge_Graph_Embedding_to_Ontology_Embedding_An_Analysis_of_the_Compatibility_between_Vector_Space_Representations_and_Rules/"/>
    <id>http://yoursite.com/post/From_Knowledge_Graph_Embedding_to_Ontology_Embedding_An_Analysis_of_the_Compatibility_between_Vector_Space_Representations_and_Rules/</id>
    <published>2019-07-25T08:57:55.000Z</published>
    <updated>2019-07-25T09:04:31.963Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>论文下载地址</p></blockquote><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;论文下载地址&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>Knowledge graph embedding with concepts阅读笔记</title>
    <link href="http://yoursite.com/post/Knowledge_graph_embedding_with_concepts/"/>
    <id>http://yoursite.com/post/Knowledge_graph_embedding_with_concepts/</id>
    <published>2019-07-25T05:19:44.000Z</published>
    <updated>2019-07-25T08:38:02.760Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>这篇论文，运用skip-gram方法，将实体对应相关概念引入实体向量表示，以增强表示效果。实体和概念在同一空间中，但是概念是空间中的一个超平面（类似于transH）。文中举例很多例子来辅助说明，使得文章可读性大幅提升。文中实验最后俩个比较有意思。本文值得思考借鉴的东西不少，值得再好好回顾。</p><p><a href="https://www.sciencedirect.com/science/article/pii/S0950705118304945/pdfft?md5=b9bad12f2bc771990ad0feaefa7402d4&amp;pid=1-s2.0-S0950705118304945-main.pdf" target="_blank" rel="noopener">论文下载地址</a></p></blockquote><a id="more"></a><h1 id="problem-statement"><a href="#problem-statement" class="headerlink" title="problem statement"></a>problem statement</h1><ul><li>已经存在的KGE模型主要集中于实体-关系-实体三元组或者文本语料交互。<ul><li>三元组是缺少信息的，并且域内文本不总是可以获得的——导致嵌入结果偏离实际</li></ul></li><li>常识概念知识发挥很重要的作用。</li></ul><h1 id="background"><a href="#background" class="headerlink" title="background"></a>background</h1><blockquote><p>For example, for two triplets (Apple, Developer, IPhone) and (Apple, Developer, Samsung Mobile), it is quite difficult to distinguish which is the true triplet that contains fact triplets only, because ‘‘IPhone’’ and ‘‘Samsung Mobile’’ both belong to mobile phones. However, in the concept graph, ‘‘IPhone’’ has a concept ‘‘apple device’’, but ‘‘Samsung Mobile’’ does not. Thus, it is easy to infer the correct triplet by mapping ‘‘IPhone’’ to the ‘‘apple device’’concept</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">很好的一个举例关于如何运用concept来辅助关系识别</span><br></pre></td></tr></table></figure><blockquote><p>Specifically, when a corpus about technology is provided, embedding methods with technical textual descriptions could easily infer the fact (Apple, Developer, IPhone), because the keywords ‘‘hardware products’’ and ‘‘iPhone smartphone’’ occur frequently in the textual description of ‘‘Apple’’. However, it is difficult to infer the fact (Apple, Taste, Sweet), which is irrelevant to textual descriptions of ‘‘Apple’’ about the specific topic of ‘‘technology company</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这里作者举例说明：与具有文本信息的嵌入方法相比，具有概念信息的嵌入方法在其任务中更加通用，并且它不依赖于语料库的主题。</span><br></pre></td></tr></table></figure><p>作者把KGE分成了三类，如下：</p><ul><li>Embedding with symbolic triplets：trans系列都放到了这部分中</li><li>Embedding with textual information</li><li>Embedding with category information</li></ul><h1 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h1><h2 id="concept-graph-embedding"><a href="#concept-graph-embedding" class="headerlink" title="concept graph embedding"></a>concept graph embedding</h2><p>作者采用skip-gram来学习可以捕获其语义相关性的概念和实体的表示。</p><p><img src="http://image.nysdy.com/20190725156403562012591.png" alt="20190725156403562012591.png"></p><p>其中，每个实体对应多个概念，每个概念又包含多个实体（这些实体作为实体的上下文）。</p><p>则，skip-gram函数可以写为：</p><script type="math/tex; mode=display">\begin{array}{l}{P\left(e_{c} | e_{t}\right)=\frac{\exp \left(e_{c} \cdot e_{t}\right)}{\sum_{e \in E} \exp \left(e \cdot e_{t}\right)}} \\ {P\left(e_{c} | c_{i}\right)=\frac{\exp \left(e_{c} \cdot c_{i}\right)}{\sum_{e \in E} \exp \left(e \cdot c_{i}\right)}}\end{array}</script><p>故损失函数为：</p><script type="math/tex; mode=display">L=\frac{1}{|D|} \sum_{\left(e_{c}, e_{t}\right) \in D}\left[\log P\left(e_{c} | e_{t}\right)+\sum_{c_{i} \in C\left(e_{t}\right)} \log P\left(e_{c} | c_{i}\right)\right]</script><p>学习率设为：</p><p>α = starting_alpha×(1−count_actual/(real)(iter × total_size+1))</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这里作者说为了避免过拟合，对优化目标采用“负抽样”方法。&quot;负抽样&quot;方法还可以避免过拟合？</span><br></pre></td></tr></table></figure><h2 id="knowledge-graph-embedding"><a href="#knowledge-graph-embedding" class="headerlink" title="knowledge graph embedding"></a>knowledge graph embedding</h2><p>将特定三元组嵌入到概念子空间中，首先构建一个超平面，其中法向量$c$为概念子空间：</p><script type="math/tex; mode=display">c=C\left(e_{h}, e_{t}\right)=\frac{e_{h}-e_{t}}{\left\|e_{h}-e_{t}\right\|_{2}^{2}}</script><p>根据TransE三元组的嵌入损失为：</p><script type="math/tex; mode=display">l=h+r-t</script><p>所以，可以计算出法向量方向上的损失分量是：</p><script type="math/tex; mode=display">\left(c^{T} l c\right)</script><p>然后，投影到超平面上的另一个正交分量是：</p><script type="math/tex; mode=display">\left(l-c^{T} l c\right)</script><p><img src="http://image.nysdy.com/20190725156403899071000.png" alt="20190725156403899071000.png"></p><p>定义总损失函数：</p><script type="math/tex; mode=display">f_{r}(h, t)=-\lambda\left\|l-c^{T} l c\right\|_{2}^{2}+\|l\|_{2}^{2}</script><h2 id="Model-interpretation"><a href="#Model-interpretation" class="headerlink" title="Model interpretation"></a>Model interpretation</h2><ol><li>可以通过概念来辅助三元组识别，文中以(Christopher Plummer, /people/person/nationality, Canada)举例</li><li>可以解决在当两个候选实体在KGE，中计算loss相等时辨别这两个哪个是真实的。文中以“which the director made the film ‘‘WALL-E’’”为例来进行说明</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">都是通过查询实体对应概念来进行辅助</span><br></pre></td></tr></table></figure><h1 id="Objectives-and-training"><a href="#Objectives-and-training" class="headerlink" title="Objectives and training"></a>Objectives and training</h1><p>margin-based loss function：</p><script type="math/tex; mode=display">L=\sum_{(h, r, t) \in S} \sum_{\left(h^{\prime}, r, t^{\prime}\right) \in S_{(h, r, t)}^{\prime}}\left[\gamma+f_{r}(h, t)-f_{r^{\prime}}\left(h^{\prime}, t^{\prime}\right)\right]_{+}</script><h2 id="train"><a href="#train" class="headerlink" title="train"></a>train</h2><ol><li>先预训练概念图模型嵌入，获得在概念空间中的实体向量</li><li>利用1中获得的实体向量进行更新。</li></ol><h1 id="datasets"><a href="#datasets" class="headerlink" title="datasets"></a>datasets</h1><ul><li><p>WN18 and FB15K</p></li><li><p>Microsoft Concept Graph<img src="http://image.nysdy.com/20190725156404326996082.png" alt="20190725156404326996082.png"></p><p>其中，relations表示频率</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">真的有统计频率的这种</span><br></pre></td></tr></table></figure></li></ul><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h3 id="Knowledge-graph-completion"><a href="#Knowledge-graph-completion" class="headerlink" title="Knowledge graph completion"></a>Knowledge graph completion</h3><h3 id="Entity-classification"><a href="#Entity-classification" class="headerlink" title="Entity classification"></a>Entity classification</h3><h3 id="Concept-relevance-analysis"><a href="#Concept-relevance-analysis" class="headerlink" title="Concept relevance analysis"></a>Concept relevance analysis</h3><p><img src="http://image.nysdy.com/20190725156404278694395.png" alt="20190725156404278694395.png"></p><p>这个实验比较有意思：每个单元格中的数字表示在TransE中排名大于m且在我们的模型中小于n的三元组的数量。</p><h3 id="Precise-semantic-expression-analysis"><a href="#Precise-semantic-expression-analysis" class="headerlink" title="Precise semantic expression analysis"></a>Precise semantic expression analysis</h3><p>我们在链接预测（换句话说，这些是TransE的难以证明的例子）中收集那些得分略高于真实三元组作为负三元组</p><p>然后在KEC中对比两者的分数差值。</p><p><img src="http://image.nysdy.com/20190725156404301070773.png" alt="20190725156404301070773.png"></p><p>右边条表示KEC在TransE失败时作出正确决定，左边条表示KEC和TransE都失败。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;这篇论文，运用skip-gram方法，将实体对应相关概念引入实体向量表示，以增强表示效果。实体和概念在同一空间中，但是概念是空间中的一个超平面（类似于transH）。文中举例很多例子来辅助说明，使得文章可读性大幅提升。文中实验最后俩个比较有意思。本文值得思考借鉴的东西不少，值得再好好回顾。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0950705118304945/pdfft?md5=b9bad12f2bc771990ad0feaefa7402d4&amp;amp;pid=1-s2.0-S0950705118304945-main.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;论文下载地址&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
      <category term="KG" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/KG/"/>
    
    
      <category term="KGE" scheme="http://yoursite.com/tags/KGE/"/>
    
      <category term="ontology" scheme="http://yoursite.com/tags/ontology/"/>
    
      <category term="concept" scheme="http://yoursite.com/tags/concept/"/>
    
  </entry>
  
  <entry>
    <title>Universal Representation Learning of Knowledge Bases by Jointly Embedding Instances and Ontological Concepts阅读笔记</title>
    <link href="http://yoursite.com/post/Universal_Representation_Learning_of_Knowledge_Bases_by_Jointly_Embedding_Instances_and_Ontological_Concepts/"/>
    <id>http://yoursite.com/post/Universal_Representation_Learning_of_Knowledge_Bases_by_Jointly_Embedding_Instances_and_Ontological_Concepts/</id>
    <published>2019-07-17T08:45:52.000Z</published>
    <updated>2019-07-25T08:41:32.245Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p></p><p><a href="http://web.cs.ucla.edu/~yzsun/papers/2019_KDD_JOIE.pdf" target="_blank" rel="noopener">论文下载地址</a></p></blockquote><a id="more"></a><h1 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h1><p>Existing KG embedding models merely focus on representing of an ontology view for abstract and commonsense concepts or an instance view for special entities that are instantiated from ontological concepts. </p><h1 id="Challenge"><a href="#Challenge" class="headerlink" title="Challenge"></a>Challenge</h1><ul><li><strong>mappings difficult</strong> :the semantic mappings from entities to concepts and from relations to meta-relations are complicated and difficult to be precisely captured by any current embedding models</li><li><strong>inadequate cross-view links:</strong> the known cross-view links inadequately cover a vast number of entities, which leads to insufficient information to align both views of the KB, and curtails discovering new cross-view links</li><li>the scales and topological structures are different in ontological views and instance views</li></ul><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p><img src="http://image.nysdy.com/20190722156378173495213.jpg" alt="20190722156378173495213.jpg"></p><p>从两种视图来学习表示有以下两点好处：</p><ul><li>instance embeddings provide detailed and rich information for their corresponding ontological concepts.</li><li>a concept embedding provides a high-level summary of its instances, which is extremely helpful when an instance is rarely observed.</li></ul><h1 id="contribution"><a href="#contribution" class="headerlink" title="contribution"></a>contribution</h1><ul><li>a novel KG embedding model named JOIE, which jointly encodes both the ontology and instance views of a KB<ul><li>cross-view association model : a novel KG embedding model named JOIE, which jointly encodes both the ontology and instance views of a KB<ul><li>cross-view grouping technique : assumes that the two views can be forced into the same embedding space</li><li>cross-view transformation technique : enables non-linear transformations from the instance embedding space to the ontology embedding space</li></ul></li><li>intra-view embedding model : characterizes the relational facts of ontology and instance views in two separate embedding spaces<ul><li>three state-of-the-art translational or similarity-based relational embedding techniques</li><li>hierarchy-aware embedding: based on intra-view non- linear transformations to preserve ontologies hierarchical substructures.</li></ul></li></ul></li><li>implement two experiments:<ul><li>the triple completion task : confirm the effectiveness of JOIE for populating knowledge in both ontology and instance-view KGs, which has significantly outperformed various baseline models.</li><li>the entity typing task :  show that JOIE is competent in discovering cross-view links to align the ontology-view and the instance-view KGs.</li></ul></li></ul><h2 id="Modeling"><a href="#Modeling" class="headerlink" title="Modeling"></a>Modeling</h2><p><img src="http://image.nysdy.com/2019072215637837731893.jpg" alt="2019072215637837731893.jpg"></p><h2 id="Cross-view-Association-Model"><a href="#Cross-view-Association-Model" class="headerlink" title="Cross-view Association Model"></a>Cross-view Association Model</h2><p><img src="http://image.nysdy.com/20190722156378594486796.png" alt="20190722156378594486796.png"></p><h3 id="Cross-view-Grouping-CG"><a href="#Cross-view-Grouping-CG" class="headerlink" title="Cross-view Grouping (CG)"></a>Cross-view Grouping (CG)</h3><p>该模型可以视为grouping-based regularization， 假设本体视图KG和实例视图KG可以被嵌入到同一空间中，并强制使实例向量靠近与它相关联的概念向量，如图3(a)所示</p><p>定义损失函数如下：</p><script type="math/tex; mode=display">J_{\text { Cross }}^{\mathrm{CG}}=\frac{1}{|\mathcal{S}|} \sum_{(e, c) \in \mathcal{S}}\left[\|\mathbf{c}-\mathbf{e}\|_{2}-\gamma^{\mathrm{CG}}\right]_{+}</script><h3 id="Cross-view-Transformation-CT"><a href="#Cross-view-Transformation-CT" class="headerlink" title="Cross-view Transformation (CT)"></a>Cross-view Transformation (CT)</h3><p>试图在实体嵌入空间和概念空间之间转换信息，如图3(b)所示</p><p>定义映射函数，将实例映射到本体视图空间，该映射后向量应该靠近它的相关联概念：</p><script type="math/tex; mode=display">\mathbf{c} \leftarrow f_{\mathrm{CT}}(\mathbf{e}), \forall(e, c) \in \mathcal{S}</script><p>其中，</p><script type="math/tex; mode=display">f_{\mathrm{CT}}(\mathbf{e})=\sigma\left(\mathbf{W}_{\mathrm{ct}} \cdot \mathbf{e}+\mathbf{b}_{\mathrm{ct}}\right)</script><p>整个CT的损失函数为：</p><script type="math/tex; mode=display">J_{\text { Cross }}^{\mathrm{CT}}=\frac{1}{|\mathcal{S}|} \sum_{(e, c) \in \mathcal{S} \atop \wedge\left(e, c^{\prime}\right) \in \mathcal{S}}\left[\gamma^{\mathrm{CT}}+\left\|\mathbf{c}-f_{\mathrm{CT}}(\mathbf{e})\right\|_{2}-\left\|\mathbf{c}^{\prime}-f_{\mathrm{CT}}(\mathbf{e})\right\|_{2}\right]_{+}</script><h2 id="Intra-view-Model"><a href="#Intra-view-Model" class="headerlink" title="Intra-view Model"></a>Intra-view Model</h2><p>该模型的目的：在两个嵌入空间中分别保留KB的每个视图中的原始结构信息。</p><h3 id="Default-Intra-view-Model"><a href="#Default-Intra-view-Model" class="headerlink" title="Default Intra-view Model"></a>Default Intra-view Model</h3><p>作者采用三种方式：</p><script type="math/tex; mode=display">\begin{aligned} f_{\text { TransE }}(\mathbf{h}, \mathbf{r}, \mathbf{t}) &=-\|\mathbf{h}+\mathbf{r}-\mathbf{t}\|_{2} \\ f_{\text { Mult }}(\mathbf{h}, \mathbf{r}, \mathbf{t}) &=(\mathbf{h} \circ \mathbf{t}) \cdot \mathbf{r} \\ f_{\text { HolE }}(\mathbf{h}, \mathbf{r}, \mathbf{t}) &=(\mathbf{h} \star \mathbf{t}) \cdot \mathbf{r} \end{aligned}</script><p>损失函数：</p><script type="math/tex; mode=display">J_{\text { Intra }}^{G}=\frac{1}{|\mathcal{G}|} \sum_{(h, r, t) \in \mathcal{G}}\left[\gamma^{\mathcal{G}}+f\left(\mathbf{h}^{\prime}, \mathbf{r}, \mathbf{t}^{\prime}\right)-f(\mathbf{h}, \mathbf{r}, \mathbf{t})\right]_{+}</script><p>intra损失函数：</p><script type="math/tex; mode=display">J_{\text { Intra }}=J_{\text { Intra }}^{\mathcal{G}_{I}}+\alpha_{1} \cdot J_{\text { Intra }}^{\mathcal{G}_{O}}</script><h3 id="Hierarchy-Aware-Intra-view-Model-for-the-Ontology"><a href="#Hierarchy-Aware-Intra-view-Model-for-the-Ontology" class="headerlink" title="Hierarchy-Aware Intra-view Model for the Ontology"></a>Hierarchy-Aware Intra-view Model for the Ontology</h3><p>进一步区分了构成本体层次结构的元关系和视图内模型中的规则语义关系(如“related_to”)。</p><p>给定概念对（cl，ch），我们将这种层次结构建模为粗略概念和相关更精细概念之间的非线性变换:</p><script type="math/tex; mode=display">g_{\mathrm{HA}}\left(\mathbf{c}_{h}\right)=\sigma\left(\mathbf{W}_{\mathrm{HA}} \cdot \mathbf{c}_{l}+\mathbf{b}_{\mathrm{HA}}\right)</script><p>损失函数为：</p><script type="math/tex; mode=display">J_{\text { Intra }}^{\mathrm{HA}}=\frac{1}{|\mathcal{T}|} \sum_{\left(c_{l}, c_{h}\right) \in \mathcal{T}}\left[\gamma^{\mathrm{HA}}+\left\|\mathbf{c}_{h}-g\left(\mathbf{c}_{l}\right)\right\|_{2}-\left\|\mathbf{c}_{\mathrm{h}}^{\prime}-g\left(\mathbf{c}_{1}\right)\right\|_{2}\right]_{+}</script><p>故，该部分损失函数为：</p><script type="math/tex; mode=display">J_{\text { Intra }}=J_{\text { Intra }}^{G_{I}}+\alpha_{1} \cdot J_{\text { Intra }}^{\mathcal{G} o \backslash \mathcal{T}}+\alpha_{2} \cdot J_{\text { Intra }}^{\mathrm{HA}}</script><ul><li>$J_{\text { Intra }}^{\mathcal{G} o} \backslash \mathcal{T}$: 默认的视图内模型的丢失，该模型仅在具有规则语义关系的三元组上训练</li><li>$J_{\text { Intra }}^{\mathrm{HA}}$明确训练三元组与形成本体层次结构的元关系</li></ul><blockquote><p>感觉这部分就是传递关系，类似推理性质的。</p><p>没明白两种ontology关系的区分点</p></blockquote><h2 id="Joint-Training-on-Two-View-KBs"><a href="#Joint-Training-on-Two-View-KBs" class="headerlink" title="Joint Training on Two-View KBs"></a>Joint Training on Two-View KBs</h2><p>联合损失函数：</p><script type="math/tex; mode=display">J=J_{\text { Intra }}+\omega \cdot J_{\text { Cross }}</script><p>作者并不直接更新$J$，而是交替更新$J_{\text { Intra }}^{\mathcal{G}_{I}}, J_{\text { Intra }}^{\mathcal{G} O} \text { and } J_{\text { Cross }}$.</p><h1 id="EXPERIMENTS"><a href="#EXPERIMENTS" class="headerlink" title="EXPERIMENTS"></a>EXPERIMENTS</h1><p>具体细节直接见论文</p><h2 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h2><p><img src="http://image.nysdy.com/20190722156379991044398.png" alt="20190722156379991044398.png">数据集是作者自己构建的，信息如上图所示。</p><h2 id="Case-Study"><a href="#Case-Study" class="headerlink" title="Case Study"></a>Case Study</h2><h3 id="Ontology-Population"><a href="#Ontology-Population" class="headerlink" title="Ontology Population"></a>Ontology Population</h3><p>作者想预测在元关系词表中并不存在的元关系，例如：预测(“Office Holder”, ?r, “Country”)</p><p>这里，作者采取的方式是将概念通过之前提到的实体空间到概念空间的映射来进行反映射。然后按照$f_{\mathrm{CT}}^{\mathrm{inv}}\left(\mathbf{c}_{\text { country }}\right)-f_{\mathrm{CT}}^{\mathrm{inv}}\left(\mathbf{c}_{\text { office }}\right)$来在实体嵌入空间进行搜索与之相近的实体间关系。</p><p><img src="http://image.nysdy.com/20190722156380029962081.png" alt="20190722156380029962081.png"></p><h3 id="Long-tail-entity-typing"><a href="#Long-tail-entity-typing" class="headerlink" title="Long-tail entity typing"></a>Long-tail entity typing</h3><p><img src="http://image.nysdy.com/2019072215638007155223.png" alt="2019072215638007155223.png"></p><p>In KGs, the frequency of entities and relations often follow a long-tail distribution (Zipf’s law)</p><p>作者抽取了低频次实体进行了训练，发现JOIE模型的效果虽然有下降，但尚在可以接受的程度内。</p><h1 id="FUTURE-WORK"><a href="#FUTURE-WORK" class="headerlink" title="FUTURE WORK"></a>FUTURE WORK</h1><ul><li>Particularly, instead of optimizing structure loss with triples (first-order neighborhood) locally, we plan to adopt more complex embedding models which leverage information from higher order neighborhood, logic paths or even global knowledge graph structures. </li><li>We also plan to explore the alignment on relations and meta-relations like entity-concept.</li><li>exploring different triple encoding techniques</li><li>Note that we are also aware of the fact that there are more comprehensive properties of relations and meta-relations in the two views such as logical rules of relations and entity types. Incorporating such properties into the learning process is left as future work.</li></ul><h1 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h1><p>这篇论文和之前跟张老师定的我的论文的思路基本一致，额，有点感觉有点受打击。这篇文章也是该作者博士毕业论文中的一部分，所以应该是这个作者早就有这个思路了，所以也没什么好纠结的。这篇文章也是走的trans的路线，和刘的论文又不一样的思路，但是都是概念本体这类的。其中有一点不一样，就是is_a关系可能两篇论文用的不一样。这篇论文中提到了数据集开源，可是github的链接中并没有数据集。虽然轮文中说他结合了概念和实例的视图，但是其实像刘的论文就已经提出结合了概念和实例的角度了。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://web.cs.ucla.edu/~yzsun/papers/2019_KDD_JOIE.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;论文下载地址&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
      <category term="KG" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/KG/"/>
    
    
      <category term="KGE" scheme="http://yoursite.com/tags/KGE/"/>
    
      <category term="ontology" scheme="http://yoursite.com/tags/ontology/"/>
    
  </entry>
  
  <entry>
    <title>DocRED A Large-Scale Document-Level Relation Extraction Dataset阅读笔记</title>
    <link href="http://yoursite.com/post/DocRED_A_Large-Scale_Document-Level_Relation_Extraction_Dataset/"/>
    <id>http://yoursite.com/post/DocRED_A_Large-Scale_Document-Level_Relation_Extraction_Dataset/</id>
    <published>2019-07-01T01:11:49.000Z</published>
    <updated>2019-07-01T01:49:37.278Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>这是一个介绍数据集的论文，主要是文档级别的关系抽取数据集。</p><p><a href="http://arxiv.org/abs/1906.06127" target="_blank" rel="noopener">论文下载地址</a></p></blockquote><h1 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h1><p>existing datasets for document-level RE </p><ul><li>either only have a small number of manually-annotated relations and entities, </li><li>or exhibit noisy annotations from distant supervision, </li><li>or serve specific domains or approaches.</li></ul><h1 id="Contribution-DocRED"><a href="#Contribution-DocRED" class="headerlink" title="Contribution (DocRED)"></a>Contribution (DocRED)</h1><ul><li>constructed from Wikipedia and Wikidata</li><li>DocRED contains 132, 375 entities and 56, 354 relational facts annotated on 5, 053 Wikipedia documents</li><li>As at least 40.7% of the relational facts in DocRED can only be extracted from multiple sentences</li><li><p>also provide large-scale distantly supervised data to support weakly supervised RE research</p></li><li><p>indicate the existing methods deal with the taks document level RE is  more difficult sentence-level RE.</p></li></ul><h1 id="data"><a href="#data" class="headerlink" title="data"></a>data</h1><p><img src="http://image.nysdy.com/20190701156194521958350.jpg" alt="20190701156194521958350.jpg"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;这是一个介绍数据集的论文，主要是文档级别的关系抽取数据集。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://arxiv.org/abs/1906.06127&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;论文下载地址&lt;/a&gt;&lt;/p&gt;
      
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="RE" scheme="http://yoursite.com/tags/RE/"/>
    
      <category term="dataset" scheme="http://yoursite.com/tags/dataset/"/>
    
  </entry>
  
  <entry>
    <title>Learning Entity and Relation Embeddings for Knowledge Graph Completion阅读笔记</title>
    <link href="http://yoursite.com/post/Learning_Entity_and_Relation_Embeddings_for_Knowledge_Graph_Completion/"/>
    <id>http://yoursite.com/post/Learning_Entity_and_Relation_Embeddings_for_Knowledge_Graph_Completion/</id>
    <published>2019-06-26T08:18:15.000Z</published>
    <updated>2019-06-27T07:51:44.136Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>TransR embeds entities and relations in distinct entity space and relation space, and learns embeddings via translation between projected entities.CTransR models internal complicated correlations within each relation type.</p><p>论文下载地址</p></blockquote><a id="more"></a><h1 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h1><p>In fact, an entity may have multiple aspects and various relaitons may focus on different aspects of entities, which makes a common space insurficient for modeling.</p><h1 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h1><ul><li>propose a TransR model which models entities and relations in distinct spaces</li><li>CTransR models internal complicated correlations within each relation type.</li><li>experiment on benchmark datasets of WordNet and Freebase and gain consistent improvements compared to state-of-the-art models</li></ul><h1 id="Future-work"><a href="#Future-work" class="headerlink" title="Future work"></a>Future work</h1><ul><li>Existing models including TransR consider each relational fact separately.<ul><li>relation transitive</li></ul></li><li>explore a unified embedding model of both text side and knowledge graph</li><li>modeling internal correlations within each relation type</li></ul><h1 id="TransR"><a href="#TransR" class="headerlink" title="TransR"></a>TransR</h1><p><img src="http://image.nysdy.com/20190627156159912894954.jpg" alt="20190627156159912894954.jpg"></p><ol><li><p>for each triple$(h, r, t)$, entities embeddings are set as $\mathbf{h}, \mathbf{t} \in \mathbb{R}^{k}$ and relation embedding is set as $\mathbf{r} \in \mathbb{R}^{d}$, $k \neq d$</p></li><li><p>for each relation $r$, set a projection matrix $\mathbf{M}_{r} \in\mathbb{R}^{k \times d}$</p><ul><li>projects entities from entity space to relation space</li></ul></li><li><p>projected vectors of entities as </p><script type="math/tex; mode=display">\mathbf{h}_{r}=\mathbf{h} \mathbf{M}_{r}, \quad \mathbf{t}_{r}=\mathbf{t} \mathbf{M}_{r}</script></li><li><p>score function:</p><script type="math/tex; mode=display">f_{r}(h, t)=\left\|\mathbf{h}_{r}+\mathbf{r}-\mathbf{t}_{r}\right\|_{2}^{2}</script></li></ol><h1 id="Cluster-based-TransR-CTransR"><a href="#Cluster-based-TransR-CTransR" class="headerlink" title="Cluster-based TransR (CTransR)"></a>Cluster-based TransR (CTransR)</h1><h3 id="why-propose-CTransR"><a href="#why-propose-CTransR" class="headerlink" title="why propose CTransR"></a>why propose CTransR</h3><p>TransE, TransH and TransR, learn a unique vector for each relation, which may be under-representative to fit all entity pairs under this relation, because these relations are usually rather diverse.</p><h3 id="basic-idea"><a href="#basic-idea" class="headerlink" title="basic idea"></a>basic idea</h3><ul><li>incorporate the idea of piecewise linear regression Ritzema and others 1994</li><li>segment input instances into several groups</li></ul><h3 id="process"><a href="#process" class="headerlink" title="process"></a>process</h3><ol><li><p>for a specific relation r, all entity pairs (h, t) in the training data are clustered into multiple groups, and entity pairs in each group are expected to exhibit similar r relation.</p><ul><li>All entity pairs (h, t) are represented with their vector offsets (h − t) for clustering, where h and t are obtained with TransE.</li></ul></li><li><p>learn a separate relation vector $r_c$for each cluster and matrix $M_r$ for each relation, respectively</p></li><li><p>projected vectors of entities as $\mathbf{h}_{r, c}=\mathbf{h} \mathbf{M}_{r} \text { and } \mathbf{t}_{r, c}=\mathbf{t} \mathbf{M}_{r}$</p></li><li><p>sorce fuction</p><script type="math/tex; mode=display">f_{r}(h, t)=\left\|\mathbf{h}_{r, c}+\mathbf{r}_{c}-\mathbf{t}_{r, c}\right\|_{2}^{2}+\alpha\left\|\mathbf{r}_{c}-\mathbf{r}\right\|_{2}^{2}</script><p>the later item aims to ensure cluster-specific relation vector rcnot too far away from the original relation vector r</p></li></ol><h1 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h1><p>采用和前人所用一样的数据集</p><div class="table-container"><table><thead><tr><th>Dataset</th><th>#Rel</th><th>#Ent</th><th>#Train</th><th>#Valid</th><th># Test</th></tr></thead><tbody><tr><td>WN18</td><td>18</td><td>40,943</td><td>141,442</td><td>5,000</td><td>5,000</td></tr><tr><td>FB15K</td><td>1,345</td><td>14,951</td><td>483,142</td><td>50,000</td><td>59,071</td></tr><tr><td>WN11</td><td>11</td><td>38,696</td><td>112,581</td><td>2,609</td><td>10,544</td></tr><tr><td>FB13</td><td>13</td><td>75,043</td><td>316,232</td><td>5,908</td><td>23,733</td></tr><tr><td>FB40K</td><td>1,336</td><td>39528</td><td>370,648</td><td>67,946</td><td>96,678</td></tr></tbody></table></div><h1 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h1><p>作者采取常规实验</p><h3 id="Link-Prediction"><a href="#Link-Prediction" class="headerlink" title="Link Prediction"></a>Link Prediction</h3><p>这里作者对关系中聚类进行了展示： <img src="http://image.nysdy.com/2019062715616031534723.jpg" alt="2019062715616031534723.jpg"></p><blockquote><p>我觉得这种方式是值得尝试的。</p></blockquote><h3 id="Triple-classification"><a href="#Triple-classification" class="headerlink" title="Triple classification"></a>Triple classification</h3><p>Moreover, the “bern” sampling technique improves the performance of TransE, TransH and TransR on all three data sets.</p><blockquote><p>bern采样方法需要掌握。</p></blockquote><h3 id="Relation-Extraction-from-Text"><a href="#Relation-Extraction-from-Text" class="headerlink" title="Relation Extraction from Text"></a>Relation Extraction from Text</h3>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;TransR embeds entities and relations in distinct entity space and relation space, and learns embeddings via translation between projected entities.CTransR models internal complicated correlations within each relation type.&lt;/p&gt;
&lt;p&gt;论文下载地址&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="KGE" scheme="http://yoursite.com/tags/KGE/"/>
    
      <category term="KGR" scheme="http://yoursite.com/tags/KGR/"/>
    
      <category term="TransR" scheme="http://yoursite.com/tags/TransR/"/>
    
  </entry>
  
  <entry>
    <title>Neural Relation Extraction with Selective Attention over Instances阅读笔记</title>
    <link href="http://yoursite.com/post/Neural_Relation_Extraction_with_Selective_Attention_over_Instances/"/>
    <id>http://yoursite.com/post/Neural_Relation_Extraction_with_Selective_Attention_over_Instances/</id>
    <published>2019-06-26T05:49:57.000Z</published>
    <updated>2019-06-26T08:12:44.937Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>这篇文章之前看过😂。</p><p>论下载地址</p></blockquote><a id="more"></a><h1 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h1><p>​    Distant supervision inevitably accompanies with the wrong labelling problem, and thse noisy data will substantially hurt the performance of relation extraction.</p><h1 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h1><ul><li>As compared to existing neural relation extraction model, our model can make full use of all informative sentences of each entity pair.</li><li>To address the wrong labelling problem in distant supervision, we propose selective attention to de-emphasize those noisy instances.</li><li>In the experiments, we show that selective attention is beneficial to two kinds of CNN models in the task of relation extraction.</li></ul><h1 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h1><p>模型整体架构如下所示：</p><p><img src="http://image.nysdy.com/20190626156153649268323.jpg" alt="20190626156153649268323.jpg"></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;这篇文章之前看过😂。&lt;/p&gt;
&lt;p&gt;论下载地址&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="RE" scheme="http://yoursite.com/tags/RE/"/>
    
  </entry>
  
  <entry>
    <title>Learning as the Unsupervised Alignment of Conceptual Systems阅读笔记</title>
    <link href="http://yoursite.com/post/Learning_as_the_Unsupervised_Alignment_of_Conceptual_Systems/"/>
    <id>http://yoursite.com/post/Learning_as_the_Unsupervised_Alignment_of_Conceptual_Systems/</id>
    <published>2019-06-25T06:42:57.000Z</published>
    <updated>2019-06-26T04:02:25.369Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>这篇文章没怎么看懂，主要思想应该是代表同时概念的不同形式（文本，图像，语音等）应该具有相似的分布，以此来进行无监督的概念对齐。这种思路挺不错的，不过还没有深入的想法，算是拓展视野吧！</p></blockquote><a id="more"></a><h1 id="KEY"><a href="#KEY" class="headerlink" title="KEY"></a>KEY</h1><p>The key insight is that each concept has a unique signature within one conceptual system (e.g., images) that is recapitulated in other systems (e.g., text or audio)</p><h1 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h1><ul><li>For supervised approaches, as the number of concepts grows, so does the number of required training examples</li><li>V. W. Quine argued, even supervised instruction contains a substantial amount of ambiguity (Quine, 1960).Quine suggested that meaning may derive from something’s place within a conceptual system.</li></ul><h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><p>In order to solve Quinne’s problem, we align a system of word labels and a system of visual semantics that both refer to the same underlying reality and therefore have related structure that can be discovered by unsupervised means (Figure 1）</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;这篇文章没怎么看懂，主要思想应该是代表同时概念的不同形式（文本，图像，语音等）应该具有相似的分布，以此来进行无监督的概念对齐。这种思路挺不错的，不过还没有深入的想法，算是拓展视野吧！&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ERNIE Enhanced Language Representation with Informative Entities阅读笔记</title>
    <link href="http://yoursite.com/post/ERNIE_Enhanced_Language_Representation_with_Informative_Entities/"/>
    <id>http://yoursite.com/post/ERNIE_Enhanced_Language_Representation_with_Informative_Entities/</id>
    <published>2019-06-24T03:10:26.000Z</published>
    <updated>2019-06-25T06:32:57.460Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>该篇论文借鉴BERT，试图将实体信息（TransE）融入token(singal word)中，通过类似实体对齐的方法将实体与token对齐（并采取mask方式进行预训练），通过infromation fusion 将token与实体融合映射入相关联的两个向量空间。</p><p><a href="https://arxiv.org/pdf/1905.07129" target="_blank" rel="noopener">论文下载地址</a></p></blockquote><a id="more"></a><h1 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h1><p>the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding.</p><h1 id="Challenge"><a href="#Challenge" class="headerlink" title="Challenge"></a>Challenge</h1><p>For incorporating external knowledge into language representation models</p><ul><li>Structured Knowledge Encoding<ul><li>regarding to the given text, how to effectively extract and encode its related informative facts in KGs for language representation models</li></ul></li><li>Heterogeneous Information Fusion<ul><li>how to design a special pre-training objective to fuse the lexical, syntactic, and knowledge information is another challenge.</li></ul></li></ul><h1 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h1><p><img src="http://image.nysdy.com/20190625156142554544546.jpg" alt="20190625156142554544546.jpg"></p><h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p>ERNIE</p><ul><li><p>the underlying textual encoder (T-Encoder)负责从文本中捕获基本的词法和语法信息</p><ul><li><script type="math/tex; mode=display">\left\{\boldsymbol{w}_{1}, \ldots, \boldsymbol{w}_{n}\right\}=\mathrm{T}-\operatorname{Encoder}\left(\left\{w_{1}, \ldots, w_{n}\right\}\right)</script><p>T-Encoder(·) is a multi-layer bidirectional Transformer encoder</p></li></ul></li><li><p>the upper knowledgeable encoder (K-Encoder)</p><ul><li>entity embeddings are pre-trained by TransE负责将知识图谱集成到底层的文本信息中</li></ul></li></ul><h2 id="Knowledgeable-Encoder"><a href="#Knowledgeable-Encoder" class="headerlink" title="Knowledgeable Encoder"></a>Knowledgeable Encoder</h2><ul><li>the knowledgeable encoder K-Encoder consists of stacked aggregators</li><li>designed for encoding both tokens and entities as well as fusing their heterogeneous features.</li></ul><p>In the i-th aggregator</p><ul><li><p>the input:</p><ul><li>token embeddings: $\left\{\boldsymbol{w}_{1}^{(i-1)}, \ldots, \boldsymbol{w}_{n}^{(i-1)}\right\}$</li><li>entity embeddings :$\left\{\boldsymbol{e}_{1}^{(i-1)}, \ldots, \boldsymbol{e}_{m}^{(i-1)}\right\}$</li></ul></li><li><p>fed into two multi-head self-attentions(MH-ATTs)</p><ul><li>$\left\{\tilde{\boldsymbol{w}}_{1}^{(i)}, \ldots, \tilde{\boldsymbol{w}}_{n}^{(i)}\right\}=\mathrm{MH}-\operatorname{ATT}\left(\left\{\boldsymbol{w}_{1}^{(i-1)}, \ldots, \boldsymbol{w}_{n}^{(i-1)}\right\}\right)$</li><li>$\left\{\tilde{\boldsymbol{e}}_{1}^{(i)}, \ldots, \tilde{\boldsymbol{e}}_{m}^{(i)}\right\}=\mathrm{MH}-\operatorname{ATT}\left(\left\{\boldsymbol{e}_{1}^{(i-1)}, \ldots, \boldsymbol{e}_{m}^{(i-1)}\right\}\right)$</li></ul></li><li><p>an information fusion layer</p><ul><li><script type="math/tex; mode=display">\begin{aligned} \boldsymbol{h}_{j} &=\sigma\left(\tilde{\boldsymbol{W}}_{t}^{(i)} \tilde{\boldsymbol{w}}_{j}^{(i)}+\tilde{\boldsymbol{W}}_{e}^{(i)} \tilde{\boldsymbol{e}}_{k}^{(i)}+\tilde{\boldsymbol{b}}^{(i)}\right) \\ \boldsymbol{w}_{j}^{(i)} &=\sigma\left(\boldsymbol{W}_{t}^{(i)} \boldsymbol{h}_{j}+\boldsymbol{b}_{t}^{(i)}\right) \\ \boldsymbol{e}_{k}^{(i)} &=\sigma\left(\boldsymbol{W}_{e}^{(i)} \boldsymbol{h}_{j}+\boldsymbol{b}_{e}^{(i)}\right) \end{aligned}</script></li></ul><p>$h_j$ is the inner hidden state</p></li></ul><p>For the tokens without corresponding entities</p><script type="math/tex; mode=display">\begin{aligned} \boldsymbol{h}_{j} &=\sigma\left(\tilde{\boldsymbol{W}}_{t}^{(i)} \tilde{\boldsymbol{w}}_{j}^{(i)}+\tilde{\boldsymbol{b}}^{(i)}\right) \\ \boldsymbol{w}_{j}^{(i)} &=\sigma\left(\boldsymbol{W}_{t}^{(i)} \boldsymbol{h}_{j}+\boldsymbol{b}_{t}^{(i)}\right) \end{aligned}</script><h2 id="Pre-training-for-Injecting-Knowledge"><a href="#Pre-training-for-Injecting-Knowledge" class="headerlink" title="Pre-training for Injecting Knowledge"></a>Pre-training for Injecting Knowledge</h2><p>In order to inject knowledge into language rep- resentation by informative entities.</p><p>Randomly masks some token-entity alignments and then requires the system to predict all corresponding entities based on aligned tokens.</p><ul><li><p>denoising entity auto-encoder (dEA)</p></li><li><p>define the aligned entity distribution for the token $w_i$ as follows:</p><script type="math/tex; mode=display">p\left(e_{j} | w_{i}\right)=\frac{\exp \left(\text { linear }\left(\boldsymbol{w}_{i}^{o}\right) \cdot \boldsymbol{e}_{j}\right)}{\sum_{k=1}^{m} \exp \left(\text { linear }\left(\boldsymbol{w}_{i}^{o}\right) \cdot \boldsymbol{e}_{k}\right)}</script></li></ul><p>  linear(·) is a linear layer</p><p>For dEA, perform the following operations:</p><ul><li>in 5% of the time, replace the entity with another random<ul><li>aims to train model to correct the errors that the token is aligned with a wrong entity;</li></ul></li><li>In 15% of the time, mask token-entity alignments<ul><li>aims to train model to correct the errors that entity alignment system doesn’t extract all existing alignments;</li></ul></li><li>in the rest of the time, keep token-entity alignments unchanged <ul><li>aims to encourage our model to integrate the entity information into token representations for better language understanding.</li></ul></li></ul><h2 id="Fine-tuning-for-Specific-Tasks"><a href="#Fine-tuning-for-Specific-Tasks" class="headerlink" title="Fine-tuning for Specific Tasks"></a>Fine-tuning for Specific Tasks</h2><p><img src="http://image.nysdy.com/20190625156143137852366.jpg" alt="20190625156143137852366.jpg"></p><p>We can take the final output embedding for the first token, which corresponds to the special [CLS] token, as the representation of the input sequence for specific tasks.</p><p>For some knowledge-driven tasks, we design special fine-tuning procedure:</p><ul><li>relation classification<ul><li>design different tokens [HD] and [TL] for head entities and tail entities respectively</li><li>a similar role like position embeddings in the conventional relation classification models (Zeng et al., 2015)</li></ul></li><li>entity typing<ul><li>the mention mark token [ENT]</li></ul></li></ul><blockquote><ul><li>这里的CLS不知道有什么作用，所有的任务都有，是不同的任务重CLS的embedding有所不同吗？个人目前觉得是这样的。</li><li>作者这里采用的mark token的方法代替position embedding，不知道两个对比那种效果会更好一些。直观觉得都是标记位置信息。</li></ul></blockquote><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h3 id="Pre-training-Dataset"><a href="#Pre-training-Dataset" class="headerlink" title="Pre-training Dataset"></a>Pre-training Dataset</h3><ul><li>we use English Wikipedia as our pre-training corpus and align text to Wiki-data<ul><li>4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities</li></ul></li><li>before pre-training ERINE, entity embeddings by TransE<ul><li>sample part of Wikidata which contains 5, 040, 986 entities and 24, 267, 796 fact triples</li></ul></li></ul><h3 id="Training-Details"><a href="#Training-Details" class="headerlink" title="Training Details"></a>Training Details</h3><ul><li>We also fine-tune ERNIE on the distant supervised dataset, i.e., FIGER (Ling et al., 2015)</li><li>we use TAGME (Ferragina and Scaiella, 2010) to extract the entity mentions in the sentences and link them to their corresponding entities in KGs</li></ul><h2 id="Entity-Typing"><a href="#Entity-Typing" class="headerlink" title="Entity Typing"></a>Entity Typing</h2><h3 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h3><p>two well-established datasets FIGER (Ling et al., 2015) and Open Entity (Choi et al., 2018).</p><ul><li>The training set of FIGER is labeled with distant supervision, and its test set is annotated by human.</li><li>Open Entity is a completely manually-annotated dataset.</li></ul><p><img src="http://image.nysdy.com/20190625156143360958681.jpg" alt="20190625156143360958681.jpg"></p><h3 id="Comparble-model"><a href="#Comparble-model" class="headerlink" title="Comparble model"></a>Comparble model</h3><ul><li>NFGEC<ul><li>NFGEC is a hybrid model proposed by Shimaoka et al. (2016)</li></ul></li><li>UFET<ul><li>(Choi et al., 2018)</li></ul></li></ul><h4 id="The-results-on-FIGER"><a href="#The-results-on-FIGER" class="headerlink" title="The results on FIGER:"></a>The results on FIGER:</h4><p>However, BERT has lower accuracy than the best NFGEC model. As strict accuracy is the ratio of instances whose predictions are identical to human annotations, it illustrates <strong>some wrong labels from distant supervision are learned by BERT</strong> due to its powerful fitting ability.</p><h2 id="Relation-Classification"><a href="#Relation-Classification" class="headerlink" title="Relation Classification"></a>Relation Classification</h2><h3 id="dataset-1"><a href="#dataset-1" class="headerlink" title="dataset"></a>dataset</h3><p>two well-established datasets FewRel (Han et al., 2018b) and TACRED (Zhang et al., 2017).</p><ul><li>FewRel<ul><li>As FewRel does not have any null instance where there isn’t any relation between entities, we adopt macro averaged metrics to present the model performances. Since FewRel is built by checking whether the sentences contain facts in Wiki-data, we drop the related facts in KGs before pretraining for fair comparison</li></ul></li><li>TACRED<ul><li>In TACRED, there are nearly 80% null instances so that we follow the previous work (Zhang et al., 2017) to adopt micro averaged metrics to represent the model performances instead of the macro</li></ul></li></ul><p><img src="http://image.nysdy.com/20190625156143680247028.jpg" alt="20190625156143680247028.jpg"></p><h3 id="Comparble-model-1"><a href="#Comparble-model-1" class="headerlink" title="Comparble model"></a>Comparble model</h3><ul><li>CNN:(Zeng et al., 2015).</li><li>PA-LSTM</li><li>C-GCN :Zhang et al. (2018) adopt the graph convolution operations to model dependency trees for relation classification.<graph convolution="" over="" pruned="" dependency="" trees="" improves="" relation="" extraction.=""></graph></li></ul><h2 id="GLUE"><a href="#GLUE" class="headerlink" title="GLUE"></a>GLUE</h2><p>The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is a collection of diverse natural language understanding tasks</p><h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><p>explore the effects of the informative entities and the knowledgeable pretraining task (dEA) for ERNIE using FewRel dataset</p><blockquote><p>实验部分做的很丰富，既有两个任务的对比实验，也有对自身模块的对比实验，并且还对比了bert来检测自己模型是否对GLUE任务效果有降低。</p></blockquote><h1 id="future-research"><a href="#future-research" class="headerlink" title="future research"></a>future research</h1><p>1) inject knowledge into feature-based pre-training models such as ELMo (Peters et al., 2018); </p><p>(2) introduce diverse structured knowledge into language representation models such as ConceptNet (Speer and Havasi, 2012) which is different from world knowledge database Wikidata; </p><p>(3) annotate more real-world corpora heuristically for larger pre-training data</p><blockquote></blockquote><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul><li><a href="https://blog.csdn.net/summerhmh/article/details/91042273" target="_blank" rel="noopener">https://blog.csdn.net/summerhmh/article/details/91042273</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;该篇论文借鉴BERT，试图将实体信息（TransE）融入token(singal word)中，通过类似实体对齐的方法将实体与token对齐（并采取mask方式进行预训练），通过infromation fusion 将token与实体融合映射入相关联的两个向量空间。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1905.07129&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;论文下载地址&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
      <category term="KG" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/KG/"/>
    
    
      <category term="KGR" scheme="http://yoursite.com/tags/KGR/"/>
    
      <category term="KG" scheme="http://yoursite.com/tags/KG/"/>
    
      <category term="BERT" scheme="http://yoursite.com/tags/BERT/"/>
    
  </entry>
  
  <entry>
    <title>Incorporating Literals into Knowledge Graph Embeddings阅读笔记</title>
    <link href="http://yoursite.com/post/Incorporating_Literals_into_Knowledge_Graph_Embeddings/"/>
    <id>http://yoursite.com/post/Incorporating_Literals_into_Knowledge_Graph_Embeddings/</id>
    <published>2019-06-03T06:57:50.000Z</published>
    <updated>2019-06-03T07:33:16.832Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>读完了前两章，简单的看了一下作者提出的模型，感觉并没有太大价值，就是给实体输入多加入了一个literal的信息（加入方法可以采用线性、非线性或者神经网络）。</p><p>读论文前需要先熟悉DistMult、ComlLEx和ConvE模型，此论文方法是添加在这些方法上的。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;读完了前两章，简单的看了一下作者提出的模型，感觉并没有太大价值，就是给实体输入多加入了一个literal的信息（加入方法可以采用线性、非线性或者神经网络）。&lt;/p&gt;
&lt;p&gt;读论文前需要先熟悉DistMult、ComlLEx和ConvE模型，此论文方
      
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
      <category term="KG" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/KG/"/>
    
    
      <category term="KGE" scheme="http://yoursite.com/tags/KGE/"/>
    
      <category term="link prediction" scheme="http://yoursite.com/tags/link-prediction/"/>
    
  </entry>
  
  <entry>
    <title>Learning Knowledge Embeddings by Combining Limit-based Scoring Loss阅读笔记</title>
    <link href="http://yoursite.com/post/Learning_Knowledge_Embeddings_by_Combining_Limit-based_Scoring_Loss/"/>
    <id>http://yoursite.com/post/Learning_Knowledge_Embeddings_by_Combining_Limit-based_Scoring_Loss/</id>
    <published>2019-06-03T02:57:20.000Z</published>
    <updated>2019-06-03T03:06:20.944Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>此篇文章最为重要的就是作者设计的 margin-based ranking loss 的改进，对两个超参数$\lambda$和$\gamma$的实验，对于实验结果有很多值得分析与思考的地方。</p><p><a href="https://dl.acm.org/ft_gateway.cfm?id=3132939&amp;ftid=1920664&amp;dwn=1&amp;CFID=135630312&amp;CFTOKEN=71536805165d7c9d-10D2074A-AD9B-A596-5CA31DB63C36A322" target="_blank" rel="noopener">论文下载地址</a></p></blockquote><a id="more"></a><h1 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h1><p>The margin-based ranking loss function cannot ensure the fact that the scoring of correct triplets must be low enough to fulfill the translation.</p><h1 id="research-objective"><a href="#research-objective" class="headerlink" title="research objective"></a>research objective</h1><p>reduce the scoring of correct triplets to fulfill the translation by mending the margin-based ranking loss function</p><h1 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h1><ul><li>proposing a limit-based ranking loss item combined with margin-based ranking loss </li><li>extending TransE and TransH to TransE-RS and TransH-RS</li></ul><h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><h2 id="Margin-based-Tanking-Loss"><a href="#Margin-based-Tanking-Loss" class="headerlink" title="Margin-based Tanking Loss"></a>Margin-based Tanking Loss</h2><p>formula:</p><script type="math/tex; mode=display">L_{R}=\sum_{(h, r, t) \in \Delta} \sum_{\left(h^{\prime}, r, t^{\prime}\right) \in \Delta^{\prime}}\left[\gamma_{1}+f_{r}(h, t)-f_{r}\left(h^{\prime}, t^{\prime}\right)\right) ]_{+}</script><ul><li><p>The margin-based ranking loss function aims to make the score $f_{r}\left(h^{\prime}, t^{\prime}\right)$ of corrupted triplet higher by at least $\gamma_{1}$ than  of positive triplet.</p></li><li><p>cannot be proved $f_{r}(h, t)&lt;\varepsilon$ </p></li></ul><h2 id="Limit-based-Scoring-Loss"><a href="#Limit-based-Scoring-Loss" class="headerlink" title="Limit-based Scoring Loss"></a>Limit-based Scoring Loss</h2><p>formula:</p><script type="math/tex; mode=display">L_{S}=\sum_{(h, r, t) \in \Delta}\left[f_{r}(h, t)-\gamma_{2}\right]_{+}</script><h2 id="Finally-loss"><a href="#Finally-loss" class="headerlink" title="Finally loss"></a>Finally loss</h2><p>formula:</p><script type="math/tex; mode=display">L_{R S}=L_{R}+\lambda L_{S}, \quad(\lambda>0)</script><p>detail is :</p><script type="math/tex; mode=display">\begin{array}{c}{L_{R S}=\sum_{(h, r, t) \in \Delta} \sum_{\left(h^{\prime}, r, t^{\prime}\right) \in \Delta^{\prime}}\left\{\left[\gamma_{1}+f_{r}(h, t)-f_{r}\left(h^{\prime}, t^{\prime}\right)\right]_{+}\right.} \\ {+\lambda\left[f_{r}(h, t)-\gamma_{2}\right]_{+} \}}\end{array}</script><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h2><p><img src="http://image.nysdy.com/20190603155952634332494.jpg" alt="20190603155952634332494.jpg"></p><h2 id="Link-prediction"><a href="#Link-prediction" class="headerlink" title="Link prediction"></a>Link prediction</h2><blockquote><p>思考</p><p>作者只是对表格的数据进行了陈述，有一些问题并没有进行分析解释</p><ul><li>并没有分析比如说为什么改进loss后的transE为什么会比TransH（R、D）效果要好？</li><li>为什么在n-to-1中的表现效果没有达到最好（其他的都达到了最好）？</li><li>通过这种改进可以发现，transH相比于TransE并没有显著提升，原因是什么？</li></ul></blockquote><h2 id="Triple-Classification"><a href="#Triple-Classification" class="headerlink" title="Triple Classification"></a>Triple Classification</h2><ul><li>TransE-RS and TransH-RS have same parameter and operation complexities as TransE and TransH, which is lower than TransR and TransD.</li><li>Our models randomly initial the entities, not use the learned embeddings by TransE as TransR and TransD.<ul><li>It means that our models have much better ability to overcome the problem of overfitting</li></ul></li></ul><h2 id="Distributions-of-Triplets’-Scores"><a href="#Distributions-of-Triplets’-Scores" class="headerlink" title="Distributions of Triplets’ Scores"></a>Distributions of Triplets’ Scores</h2><h3 id="aim"><a href="#aim" class="headerlink" title="aim"></a>aim</h3><p>analyze the difference between $L_R$ Loss and our $L_RS$ Loss</p><h3 id="Parameters"><a href="#Parameters" class="headerlink" title="Parameters"></a>Parameters</h3><p><img src="http://image.nysdy.com/20190603155952784132260.jpg" alt="20190603155952784132260.jpg"></p><blockquote><p>思考：</p><ul><li>对于我自己正在做的实验：是不是我自己用的间隔太小了</li></ul></blockquote><h3 id="result"><a href="#result" class="headerlink" title="result"></a>result</h3><p><img src="http://image.nysdy.com/20190603155952821850690.jpg" alt="20190603155952821850690.jpg"></p><p><img src="http://image.nysdy.com/20190603155952848841252.jpg" alt="20190603155952848841252.jpg"></p><blockquote><p>思考</p><ul><li>这部分的实验值得借鉴，它可以相对于直观的可以展示出为什么效果会好。</li><li>比如对于上述为什么改进后的transE的效果会更好<ul><li>看到最后的分数分布transE-RS的分布效果和Trans-H的十分接近，</li><li>而transE的模型较为简单，可能最终loss最小化会使得模型充分表达，而其他模型引入了更多的假设可能会带来更多的噪声</li><li>也可能当loss很小时，其他的假设条件发挥作用的很小（至少从实验结果来看是的，但是还有待于进一步设计实验验证）</li></ul></li></ul></blockquote><h2 id="Discussion-of-Parameters"><a href="#Discussion-of-Parameters" class="headerlink" title="Discussion of Parameters"></a>Discussion of Parameters</h2><h3 id="Discussion-on-γ1-and-γ2"><a href="#Discussion-on-γ1-and-γ2" class="headerlink" title="Discussion on γ1 and γ2."></a>Discussion on γ1 and γ2.</h3><p><img src="http://image.nysdy.com/20190603155952911595962.jpg" alt="20190603155952911595962.jpg"></p><ul><li>We find that γ2 = 3γ1 or γ2 = 4γ1 is better for link prediction, but for triplet classification there are not obvious characteristics on γ1 and γ2.</li><li>a lower γ2 is expected to ensure the golden condition $\mathbf{h}+\mathbf{r} \approx \mathbf{t}$ for positive triplets, but an entity needs to satisfy many golden coditions at the same time.</li></ul><blockquote><p>思考</p><ul><li>既然如作者说，那么理论上transH的效果应该很好才对，但是结果并不是这样的，这又产生矛盾。</li></ul></blockquote><h3 id="Discussion-on-λ"><a href="#Discussion-on-λ" class="headerlink" title="Discussion on λ"></a>Discussion on λ</h3><p><img src="http://image.nysdy.com/20190603155953002076189.jpg" alt="20190603155953002076189.jpg"></p><blockquote><p>思考</p><ul><li>看到λ并没有对模型影响并没很大</li><li>λ在1左右是效果会比较好</li><li>λ和margin会不会产生关联？</li></ul></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;此篇文章最为重要的就是作者设计的 margin-based ranking loss 的改进，对两个超参数$\lambda$和$\gamma$的实验，对于实验结果有很多值得分析与思考的地方。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://dl.acm.org/ft_gateway.cfm?id=3132939&amp;amp;ftid=1920664&amp;amp;dwn=1&amp;amp;CFID=135630312&amp;amp;CFTOKEN=71536805165d7c9d-10D2074A-AD9B-A596-5CA31DB63C36A322&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;论文下载地址&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
      <category term="KG" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/KG/"/>
    
    
      <category term="transH" scheme="http://yoursite.com/tags/transH/"/>
    
      <category term="margin loss" scheme="http://yoursite.com/tags/margin-loss/"/>
    
      <category term="transE" scheme="http://yoursite.com/tags/transE/"/>
    
      <category term="KG" scheme="http://yoursite.com/tags/KG/"/>
    
  </entry>
  
  <entry>
    <title>Knowledge Graph Embedding by Translating on Hyperplanes阅读笔记</title>
    <link href="http://yoursite.com/post/Knowledge%20Graph%20Embedding%20by%20Translating%20on%20Hyperplanes/"/>
    <id>http://yoursite.com/post/Knowledge Graph Embedding by Translating on Hyperplanes/</id>
    <published>2019-05-28T08:17:44.000Z</published>
    <updated>2019-06-01T05:38:20.042Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>作为trans系列经典文献，必读。文章主要精华在于这种超平面想法的由来解决了同一实体的多关系问题。</p><p>Authors proposed TransH which models a relation as a hyperplane together with a translation operation on it. It solves the problem of multi-relation and makes a good trade-off between model capacity and efficiency.</p></blockquote><a id="more"></a><h1 id="推测transH的想法来源"><a href="#推测transH的想法来源" class="headerlink" title="推测transH的想法来源"></a>推测transH的想法来源</h1><blockquote><p>既然实际是表达同一关系不同实体最后通过TransE后会趋于一致，那么我直接通过一个中介来进行映射将同一表示映射成不同向量表示，那么这些向量表示就可以代表不同的实体，就达到了不同实体拥有不同表示的目的。因为关系是不变的所以想到了将关系作为映射平面，让实体向量向其中映射。</p></blockquote><h1 id="research-objective"><a href="#research-objective" class="headerlink" title="research objective"></a>research objective</h1><ul><li>solves the problem of multi-relation </li><li>makes a good trade-off between model capacity and efficiency</li></ul><h1 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h1><ul><li>TransE can’t deal with reflexive, one-to-many, many-to-many and many -to-one relations</li><li>some complex model sacrifice efficiency in the process(although can deal with transE’s problem)</li></ul><h1 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h1><ul><li>proposing a method named <em>translation on hyperplanes</em>(TransH)<ul><li>interpreting a relation as a translating operation on a hyperplane</li></ul></li><li>proposing a simple trick to reduce the chance of false negative labeling</li></ul><h1 id="Embedding-by-Translating-on-Hyperplanes"><a href="#Embedding-by-Translating-on-Hyperplanes" class="headerlink" title="Embedding by Translating on Hyperplanes"></a>Embedding by Translating on Hyperplanes</h1><h2 id="Relations’-Mapping-Properties-in-Embedding"><a href="#Relations’-Mapping-Properties-in-Embedding" class="headerlink" title="Relations’ Mapping Properties in Embedding"></a>Relations’ Mapping Properties in Embedding</h2><p>transE</p><ul><li>the representation of an  entity is the same when involved in any relations, ignoring <strong>distributed representations of entities when invovled in different relaions</strong></li></ul><h2 id="Translating-on-Hyperplanes-TransH"><a href="#Translating-on-Hyperplanes-TransH" class="headerlink" title="Translating on Hyperplanes (TransH)"></a>Translating on Hyperplanes (TransH)</h2><p><strong>同一个实体在不同关系中的意义不同</strong>，同时<strong>不同实体，在同一关系中的意义，也可以相同</strong>。</p><blockquote><p>将每个关系定义在一个独特的平面呢，在该平面内有符合该关系的transE的表示（h,r,t)，多加入的代表该平面的法向量完成了将不同实体向平面内和h，t转化的任务，使得同一关系的不同实体拥有不同的表示，但是在关系平面内的投影相同；同一实体可以在不同的关系平面内拥有不同的含义（平面内的投影）</p></blockquote><p><img src="http://image.nysdy.com/20190601155935483248827.jpg" alt="20190601155935483248827.jpg"></p><p>如图所示，对于正确的三元组来说$(h, r, t) \in \Delta$，所需满足的关系如图所示。那么对于一个实体$h’’$如果满足$\left(h^{\prime \prime}, r, t\right) \in \Delta    $，在transE中是需要$h’’=h$，而在transH中则将约束放宽到$h,h’’$在$W_r$上的投影相同就可以了，也可以实现将$h,h’’$区分开并且具有不同的表示。</p><h4 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h4><p>scoring function：</p><script type="math/tex; mode=display">d(h+r, t)=f_{r}(h, t)=\left\|h_{\perp}+d_{r}-t_{\perp}\right\|_{2}^{2}</script><p>As the hyperplane $W_r$, the $w_r$ is the normal vector of it, and $\left|w_{r}\right|_{2}^{2}=1$, so the projection $h$ in $w_r$ is:</p><script type="math/tex; mode=display">h_{w_{r}}=w_r^{T} h w_r</script><p>其中，$w_r^{T} h=|w_r||h| \cos \theta$可以表示$h$在$w_r$上的投影的长度和$w_r$长度的乘积，因为$\left|w_{r}\right|_{2}^{2}=1$,所以可以代表投影的长度，再乘上单位向量即可表示投影向量。所以：</p><script type="math/tex; mode=display">\mathbf{h}_{\perp}=\mathbf{h}-\mathbf{w}_{r}^{\top} \mathbf{h w}_{r}, \quad \mathbf{t}_{\perp}=\mathbf{t}-\mathbf{w}_{r}^{\top} \mathbf{t} \mathbf{w}_{r}</script><p>如图所示：<img src="http://image.nysdy.com/2019060115593616504994.jpg" alt="2019060115593616504994.jpg"></p><p>the score function is:</p><script type="math/tex; mode=display">f_{r}(\mathbf{h}, \mathbf{t})=\left\|\left(\mathbf{h}-\mathbf{w}_{r}^{\top} \mathbf{h w}_{r}\right)+\mathbf{d}_{r}-\left(\mathbf{t}-\mathbf{w}_{r}^{\top} \mathbf{t} \mathbf{w}_{r}\right)\right\|_{2}^{2}</script><h4 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h4><p>loss function consists of margin-based ranking loss and some constraints:</p><script type="math/tex; mode=display">\begin{aligned} \mathcal{L} &=\sum_{(h, r, t) \in \Delta\left(h^{\prime}, r^{\prime}, t^{\prime}\right) \in \Delta_{(h, r, t)}}\left[f_{r}(\mathbf{h}, \mathbf{t})+\gamma-f_{r^{\prime}}\left(\mathbf{h}^{\prime}, \mathbf{t}^{\prime}\right)\right]_{+} \\ &+C\left\{\sum_{e \in E}\left[\|\mathbf{e}\|_{2}^{2}-1\right]_{+}+\sum_{r \in R}\left[\frac{\left(\mathbf{w}_{r}^{\top} \mathbf{d}_{r}\right)^{2}}{\left\|\mathbf{d}_{r}\right\|_{2}^{2}}-\epsilon^{2}\right]_{+}\right\}, \text { (4) } \end{aligned}</script><p>the constraints:</p><script type="math/tex; mode=display">\forall e \in E,\|\mathrm{e}\|_{2} \leq 1, // \text { scale }\\\forall r \in R,\left|\mathbf{w}_{r}^{\top} \mathbf{d}_{r}\right| /\left\|\mathbf{d}_{r}\right\|_{2} \leq \epsilon, / / \text { orthogonal }\\\forall r \in R,\left\|\mathbf{w}_{r}\right\|_{2}=1, / / \text { unit normal vector }</script><ul><li><strong>the second grantees the translation vectot $d_r$ is in the hyperplane</strong></li><li>they project each $w_r$ to unit $l_2$-ball before visiting each mini-batch</li></ul><blockquote><p>既然transH可以完成将同一实体映射到不同的关系平面来获得不同的含义，那么我觉得</p><ul><li>是不是不同代表同一含义的投影表示应该相同或者相似</li><li>这样是不是可以解决同一个实体的多义性问题。</li></ul></blockquote><h2 id="Reducing-Ralse-Negative-Labels"><a href="#Reducing-Ralse-Negative-Labels" class="headerlink" title="Reducing Ralse Negative Labels"></a>Reducing Ralse Negative Labels</h2><p>Authors set different probabilities for replacing the head or tail entity depending on the mapping property of the relation (one-to-many, many-to-one, many-to-many)</p><ul><li><p>give more chance to replacing the head entity if the relation is one-to-many</p><ul><li>分别统计每个头实体对应尾实体的数量（反之亦然），按占比进行生成负样例</li></ul></li></ul><blockquote><ul><li>通过这样的方式，例如one-many关系，替换头实体显然更不容易得到正样例（因为只有一种头实体是对的，然而替换尾实体因为对于头实体对应该关系的尾实体更多，说不定就有其他不在此many中的尾实体符合这个关系。</li><li>相比之下我认为在《Bootstrapping-Entity-Alignment-with-Knowledge-Graph-Embedding》采用的均匀截断负采样效果会更好一些</li></ul></blockquote><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>the detail can be seen in the paper</p><h2 id="Link-prediction"><a href="#Link-prediction" class="headerlink" title="Link prediction"></a>Link prediction</h2><h3 id="outperform-TransE-in-one-to-one"><a href="#outperform-TransE-in-one-to-one" class="headerlink" title="outperform TransE in one-to-one"></a>outperform TransE in one-to-one</h3><p>Authors explain:</p><ul><li>entities are connected with relations so that better embeddings of some parts lead to better results on the whole.</li></ul><blockquote><p>我是觉得有些牵强，不过要是硬理解也是可以，毕竟通过投影相当于把实体和关系进行了一个联系，可能这个增强了效果。</p></blockquote><h2 id="Triplets-Classification"><a href="#Triplets-Classification" class="headerlink" title="Triplets Classification"></a>Triplets Classification</h2><p>This means FB13 is a very dense subgraph where strong correlations exist between entities</p><h2 id="Relational-Fact-Extraction-from-Text"><a href="#Relational-Fact-Extraction-from-Text" class="headerlink" title="Relational Fact Extraction from Text"></a>Relational Fact Extraction from Text</h2><ul><li>Actually, knowledge graph embedding is able to score a candidate fact, without observing any evidence from ex- ternal text corpus</li></ul><blockquote><p>可以看到从14年开始就有利用知识图谱来从文本抽取关系，最近这个应用好像又有起色，这个也可作为自己实验的一部分。</p></blockquote><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://blog.csdn.net/MonkeyDSummer/article/details/85273843" target="_blank" rel="noopener">https://blog.csdn.net/MonkeyDSummer/article/details/85273843</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;作为trans系列经典文献，必读。文章主要精华在于这种超平面想法的由来解决了同一实体的多关系问题。&lt;/p&gt;
&lt;p&gt;Authors proposed TransH which models a relation as a hyperplane together with a translation operation on it. It solves the problem of multi-relation and makes a good trade-off between model capacity and efficiency.&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
      <category term="KG" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/KG/"/>
    
    
      <category term="KGE" scheme="http://yoursite.com/tags/KGE/"/>
    
      <category term="transH" scheme="http://yoursite.com/tags/transH/"/>
    
  </entry>
  
  <entry>
    <title>Attention Is All You Need阅读笔记</title>
    <link href="http://yoursite.com/post/Attention%20Is%20All%20You%20Need/"/>
    <id>http://yoursite.com/post/Attention Is All You Need/</id>
    <published>2019-05-25T05:57:35.000Z</published>
    <updated>2019-05-28T07:58:49.154Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>transformer 是一个完全由注意力机制组成的搭建的模型，模型复杂度低，并可以进行并行计算，使得计算速度快。在翻译模型上取得了较好的效果。本篇论文属于经典必读论文，阅读笔记中对一些不清楚的地方进行了汉语解释，读完论文后阅读参考链接以加深理解。</p><p><a href="https://arxiv.org/pdf/1706.03762" target="_blank" rel="noopener">论文下载地址</a></p></blockquote><a id="more"></a><h1 id="research-objective"><a href="#research-objective" class="headerlink" title="research objective"></a>research objective</h1><p>based solely on attention mechanisms, increase parallezable computation and decrease train time</p><h1 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h1><p>recurrent models hidden states depended on previous hidden state and the input for position precludes parallelization</p><h1 id="contribution"><a href="#contribution" class="headerlink" title="contribution"></a>contribution</h1><ul><li>Transformer,<ul><li>eschewing recurrence and instead relying entirely on an attention mechanism, solve the long dependency problem.</li><li>draw global dependecies between input and output </li><li>allow for significantly more parallelization</li></ul></li></ul><h1 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h1><p>The Transformer uses stacked self-attention and point-wise, fully connected layers for both the encoder and decoder.<img src="http://image.nysdy.com/2019052515587656321218.jpg" alt="2019052515587656321218.jpg"></p><h2 id="Encoder-and-Decoder-Stacks"><a href="#Encoder-and-Decoder-Stacks" class="headerlink" title="Encoder and Decoder Stacks"></a>Encoder and Decoder Stacks</h2><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><ul><li>compose of a stack of N identical layers</li><li>each layers has two sub-layers<ol><li>multi-head self-attention mechanism</li><li>position-wise fully connected feed forward network</li></ol></li><li>employ a residual connection around each of the two sub-layers, followed by layer normalization</li><li>the output of each sub-layer is $\text { LayerNorm }(x+\text { Sublayer }(x))$</li><li>encoder中的Q，K，V都是学出来的</li></ul><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><ul><li>composed of a stack of N identical layers</li><li>has the same two sub-layers as the encoder</li><li>the third sub-layer between the two sub-layers<ul><li>perform multi-head attention over the output of the encoder stack</li></ul></li><li>add a mask to modify the self-attention sub-layer to ensure that the predictions for position $i$ can depend only the known outputs at positions less than $i$</li><li>除了第一子层中Q，K，V是自己学出来的，第二个子层利用了encoder中的K，V。</li></ul><h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p><img src="http://image.nysdy.com/20190526155885488441950.jpg" alt="20190526155885488441950.jpg"></p><h3 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h3><p>the calculation process as the left at the figure 2. <strong>formula：</strong></p><script type="math/tex; mode=display">\text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V</script><ul><li>where $\sqrt{d_{k}}$ is to  prevent value from getting too large, which will push the softmax function into regions where it has extremely small gradients. 因为量级太大，softmax后就非0即1了，不够“soft”了。也会导致softmax的梯度非常小。也就是让softmax结果<strong>不稀疏</strong>(问号脸，通常人们希望得到更稀疏的attention吧)。</li><li>$Q, K,V$ is a matrix needed to learn from input.</li></ul><h3 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h3><p><strong>helps the encoder look at other words in the input sentence as it encodes a specific word</strong></p><p>in the figure 2 right. </p><ul><li>it’s beneficial to <strong>lineraly project</strong> the quries, keys and values $h$ times with different, learned projections to $d_k, d_k, d_v$ dimensions, respectively</li><li>concatenate the output </li></ul><script type="math/tex; mode=display">\begin{aligned} \text { MultiHead }(Q, K, V) &=\text { Concat (head }_{1}, \ldots, \text { head }_{h} ) W^{O} \\ \text { where head }_{i} &=\text { Attention }\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right) \end{aligned}</script><p>where $W_{i}^{Q} \in \mathbb{R}^{d_{\text { model }} \times d_{k}}, W_{i}^{K} \in \mathbb{R}^{d_{\text { model }} \times d_{k}}, W_{i}^{V} \in \mathbb{R}^{d_{\text { model }} \times d_{v}}, W^{O} \in \mathbb{R}^{h d_{v} \times d_{\mathrm{model}}}$</p><h3 id="Applications-of-Attention-in-our-Model"><a href="#Applications-of-Attention-in-our-Model" class="headerlink" title="Applications of Attention in our Model"></a>Applications of Attention in our Model</h3><ul><li>the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. mimicing the seq-to-seq</li><li>self -attention can make that each position in the encoder can attend to all positions in the previous layer of the encoder</li><li><strong>We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. See Figure 2</strong>。即我们只能attend到前面已经翻译过的输出的词语，因为翻译过程我们当前还并不知道下一个输出词语，这是我们之后才会推测到的。即将$QK^T$中每行该单词之后的数值做处理，使得前面的单词看不到后面单词所占的重要性程度。</li></ul><h2 id="Position-wise-Feed-Forward-Networks"><a href="#Position-wise-Feed-Forward-Networks" class="headerlink" title="Position-wise Feed-Forward Networks"></a>Position-wise Feed-Forward Networks</h2><ul><li>applied to each position separately and identically</li><li>feed-forward network consists of tow linear transformations with a ReLU activation. formula:</li></ul><script type="math/tex; mode=display">\mathrm{FFN}(x)=\max \left(0, x W_{1}+b_{1}\right) W_{2}+b_{2}</script><blockquote><p><strong>小结</strong></p><ol><li>为什么叫强调<code>position-wise</code>?<ul><li>解释一: 这里FFN层是每个position进行相同且独立的操作，所以叫position-wise。对每个position独立做FFN。</li><li>解释二：从卷积的角度解释，这里的FFN等价于kernel_size=1的卷积，这样每个position都是独立运算的。如果kernel_size=2，或者其他，position之间就具有依赖性了，貌似就不能叫做position-wise了</li></ul></li><li>为什么要采用全连接层？<ul><li>目的: 增加非线性变换</li><li>如果不采用FFN呢？有什么替代的设计？</li></ul></li><li>为什么采用2层全连接，而且中间升维？<ul><li>这也是所谓的bottle neck，只不过低维在IO上，中间采用high rank</li></ul></li></ol></blockquote><h2 id="Embeddings-and-Softmax"><a href="#Embeddings-and-Softmax" class="headerlink" title="Embeddings and Softmax"></a>Embeddings and Softmax</h2><p>Sharing the same weight maatrix between the two embedding layers and the pre-softmax linear transformation</p><h2 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h2><p>Using sine and xosine functions of different frequencies:</p><script type="math/tex; mode=display">P E_{(p o s, 2 i)}=\sin \left(p o s / 10000^{2 i / d_{\text { model }}}\right)\\P E_{(p o s, 2 i+1)}=\cos \left(p o s / 10000^{2 i / d_{\mathrm{model}}}\right)</script><ul><li><p>where $pos$ is the postiiton and $i$ is the dimension</p></li><li><p><strong>Authors hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$, $PE_{pos+k}$can be represented as a linear function of $PE_{pos}$</strong></p><p>但在语言中，<code>相对位置</code>也很重要，Google选择前述的位置向量公式的一个重要原因是：由于我们有$\sin (\alpha+\beta)=\sin \alpha \cos \beta+\cos \alpha \sin \beta$以及$\cos (\alpha+\beta)=\cos \alpha \cos \beta-\sin \alpha \sin \beta$，这表明位置$p+k$的向量可以表示成位置$p$的向量的线性变换，这提供了表达相对位置信息的可能性。</p></li><li><p>Compared with using learned positional embeddings, the sinusoidal version may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.</p></li></ul><p>注意由于该模型没有recurrence或convolution操作，所以没有明确的关于单词在源句子中位置的相对或绝对的信息，为了更好的让模型学习位置信息，所以添加了position encoding并将其叠加在word embedding上。</p><h1 id="Why-Self-Attention"><a href="#Why-Self-Attention" class="headerlink" title="Why Self-Attention"></a>Why Self-Attention</h1><ul><li>total computational complexity per layer</li><li>the amount of computation that can be parallelized</li><li>the path between long-range dependencies in the network</li></ul><p><img src="http://image.nysdy.com/20190527155892126287777.jpg" alt="20190527155892126287777.jpg"></p><p><img src="http://image.nysdy.com/20190528155902465937774.jpg" alt="20190528155902465937774.jpg"></p><p>self-attention|：</p><ul><li>$QK^TV$相乘，根据矩阵大小（分别为$n<em>d, n</em>d, n<em>d$需要的复杂度为$O(n^2d</em>2)$（忽略softmax）</li><li>maximum path length：图说明了， 对于self-attention, target node (生成的那个点) 实际上和 输入中的任意一点的距离是相同的</li></ul><p>convolutional:  </p><ul><li><p>每层有k个卷积和，对于input matix（$n<em>d$)矩阵执行卷积需要运算复杂度是$n</em>d*(d-m)$, m为卷积和宽度是一个比较小的常数，所以总复杂度为$O\left(k \cdot n \cdot d^{2}\right)$,作者提到可分离的卷基层暂时还不了解，可以以后查阅。</p></li><li><p>maximum path length: 正常卷积和的距离是$O(n/k)$, 但如果是堆叠卷积如图：　　<img src="http://image.nysdy.com/2019052815590251436862.jpg" alt="2019052815590251436862.jpg"></p><p>就可以减小到$O\left(\log _{k}(n)\right)$</p></li></ul><p>recurrent:</p><ul><li>计算是每个词向量乘隐藏权重($d*d$)，所以易得计算复杂度：$O\left(n \cdot d^{2}\right)$</li><li>maximum path length: 长度就是n。</li><li>操作步骤要从第一个到第n个为n步，是有顺序的。其他的都没有顺序要求</li></ul><p>self-attentin(restricted)</p><ul><li>相当于只输入r邻近的句子长度，自然可以得到如图结果</li></ul><h1 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h1><h2 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h2><script type="math/tex; mode=display">\text { lrate }=d_{\text { model }}^{-0.5} \cdot \min \left(\text {step}_{-} n u m^{-0.5}, \text { step }_{-} n u m \cdot \text { warmup steps }^{-1.5}\right)</script><ul><li>increasing the learning rate linearly for the first warmup_steps training steps</li><li>decreasing it thereafter proportionally to the inverse square root of the step number</li></ul><h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><h3 id="Residual-Dropout"><a href="#Residual-Dropout" class="headerlink" title="Residual Dropout"></a>Residual Dropout</h3><ul><li>apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized</li><li>apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks</li></ul><h3 id="Label-Smoothing"><a href="#Label-Smoothing" class="headerlink" title="Label Smoothing"></a>Label Smoothing</h3><ul><li>This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score</li></ul><h1 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h1><h2 id="machine-Translation"><a href="#machine-Translation" class="headerlink" title="machine Translation"></a>machine Translation</h2><p>Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models</p><p><img src="http://image.nysdy.com/2019052715589406009133.jpg" alt="2019052715589406009133.jpg"></p><h2 id="Model-Variations"><a href="#Model-Variations" class="headerlink" title="Model Variations"></a>Model Variations</h2><p><img src="http://image.nysdy.com/20190527155894062794865.jpg" alt="20190527155894062794865.jpg"></p><h1 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a><strong>缺点</strong></h1><p>缺点在原文中没有提到，是后来在Universal Transformers中指出的，在这里加一下吧，主要是两点：</p><ol><li>实践上：有些rnn轻易可以解决的问题transformer没做到，比如复制string，尤其是碰到比训练时的sequence更长的时</li><li>理论上：transformers非computationally universal（<a href="https://www.zhihu.com/question/20115374/answer/288346717" target="_blank" rel="noopener">图灵完备</a>），（我认为）因为无法实现“while”循环</li></ol><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h1><p>Transformer是第一个用纯attention搭建的模型，不仅计算速度更快，在翻译任务上也获得了更好的结果。</p><p>Google现在的翻译应该是在此基础上做的，但是数据量大可能用transformer好一些，小的话还是继续用rnn-based model。</p><p>花了不少时间，算是理解了attention和transformer，对其中不是很清楚的点如attention的内部中Q，K，V具体是什么在self-attention和multi-head attention中大小是不同的，如何mask，如何计算复杂，等进行查阅资料弄懂了。总体来说还是收获很大的。准备在看一些代码讲解。</p><h1 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h1><ul><li>Attention机制详解（二）——Self-Attention与Transformer - 川陀学者的文章 - 知乎<br><a href="https://zhuanlan.zhihu.com/p/47282410" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/47282410</a></li><li><strong><a href="https://jalammar.github.io/illustrated-transformer/（这个讲的比较详细，建议看完论文后再看一遍这个会加深理解）" target="_blank" rel="noopener">https://jalammar.github.io/illustrated-transformer/（这个讲的比较详细，建议看完论文后再看一遍这个会加深理解）</a></strong></li><li>【NLP】Transformer详解 - 李如的文章 - 知乎<br><a href="https://zhuanlan.zhihu.com/p/44121378" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/44121378</a></li><li><a href="https://blog.eson.org/pub/664e9bad/" target="_blank" rel="noopener">https://blog.eson.org/pub/664e9bad/</a></li><li><a href="https://mp.weixin.qq.com/s/J-anyCuwLd5UYjTsUFNT1g" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/J-anyCuwLd5UYjTsUFNT1g</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;transformer 是一个完全由注意力机制组成的搭建的模型，模型复杂度低，并可以进行并行计算，使得计算速度快。在翻译模型上取得了较好的效果。本篇论文属于经典必读论文，阅读笔记中对一些不清楚的地方进行了汉语解释，读完论文后阅读参考链接以加深理解。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1706.03762&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;论文下载地址&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
      <category term="classical" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/classical/"/>
    
    
      <category term="attention" scheme="http://yoursite.com/tags/attention/"/>
    
      <category term="transformer" scheme="http://yoursite.com/tags/transformer/"/>
    
      <category term="translation" scheme="http://yoursite.com/tags/translation/"/>
    
      <category term="classical" scheme="http://yoursite.com/tags/classical/"/>
    
  </entry>
  
  <entry>
    <title>Graph Neural Networks with Generated Parameters for Relation Extraction阅读笔记</title>
    <link href="http://yoursite.com/post/Graph_Neural_Networks_with_Generated_Parameters_for_Relation/"/>
    <id>http://yoursite.com/post/Graph_Neural_Networks_with_Generated_Parameters_for_Relation/</id>
    <published>2019-05-23T02:41:51.000Z</published>
    <updated>2019-05-23T09:24:39.487Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文将GNNs应用到处理非结构化文本的（多跳）关系推理任务来进行关系抽取。采用从句子序列中获取的实体构建全链接图，应用编码（sequence model），传播（节点间信息）和分类（预测）三个模块来处理关系推理。本文提供了三个数据集。</p></blockquote><a id="more"></a><h1 id="problem-statement"><a href="#problem-statement" class="headerlink" title="problem statement"></a>problem statement</h1><ul><li>existing relation extraction models fail to infer the relationship without multi-hop relational reasoning.</li><li>existing GNNs can’t process multi-hop relational reasoning in natural language relational reasoning </li></ul><h1 id="research-objective"><a href="#research-objective" class="headerlink" title="research objective"></a>research objective</h1><p>enable GNNs to porcess relational reasoning on unstructed text inputs</p><h1 id="contribution"><a href="#contribution" class="headerlink" title="contribution"></a>contribution</h1><ul><li>extend a GNN with generated parameters, which could be applied to process relational reasoning on unstructured inputs</li><li>verify GP-GNNs in the taks of relation extraction from text; present three datasets</li></ul><h1 id="GP-GNNs"><a href="#GP-GNNs" class="headerlink" title="GP-GNNs"></a>GP-GNNs</h1><ul><li>construct a fully-connected graph with the entities in the sequence of text</li><li>employs three models to process relational reasoning<ul><li>an encoding modul: enable edges to encode rich information from natural languages </li><li>a propagation modul: propagates realtional information among various nodes </li><li>a classification modul: make prediction with node representations </li></ul></li></ul><p>As compared to tradtional GNNs, GP-GNNs could learn edges’ parameters from natural lanuages</p><h1 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h1><h2 id="Graph-Neural-Networks-GNNs"><a href="#Graph-Neural-Networks-GNNs" class="headerlink" title="Graph Neural Networks(GNNs)"></a>Graph Neural Networks(GNNs)</h2><ul><li>existing models still perfom message-passing on predefined graphs</li><li><em>Learning Graphical State Transitions</em> is most related<ul><li>introduecs a nove lnerual architecture to generate a graph based on the textal input</li><li>dynamically update the relationship during the learning process</li></ul></li></ul><h2 id="relational-reasoning"><a href="#relational-reasoning" class="headerlink" title="relational reasoning"></a>relational reasoning</h2><ul><li>existing models could not make full use of the multi-hop inference patterns among multiple entity pair and their relaitons within the sentence </li><li><em>LEARNING GRAPHICAL STATE TRANSITIONS</em> is the most related work<ul><li>the proposed model incorporates contextual relations with attention mechanism when predicting the relation of a target entity pair</li></ul></li></ul><h1 id="Graph-Neural-Network-with-Grenerated-Parameters-GP-GNNs"><a href="#Graph-Neural-Network-with-Grenerated-Parameters-GP-GNNs" class="headerlink" title="Graph Neural Network with Grenerated Parameters(GP-GNNs)"></a>Graph Neural Network with Grenerated Parameters(GP-GNNs)</h1><p>The picture is overall architecture: encoding module, propagation module and classification module</p><p><img src="http://image.nysdy.com/20190523155858968594021.jpg" alt="20190523155858968594021.jpg"></p><h2 id="Encoding-Module"><a href="#Encoding-Module" class="headerlink" title="Encoding Module"></a>Encoding Module</h2><p>formula:</p><script type="math/tex; mode=display">\mathcal{A}_{i, j}^{(n)}=f\left(E\left(x_{0}^{i, j}\right), E\left(x_{1}^{i, j}\right), \cdots, E\left(x_{l-1}^{i, j}\right) ; \theta_{e}^{n}\right)</script><p>where $f(\cdot)$ could be any model that could sequential(such as LSTMs); $E(\cdot)$ indicates an embedding function. $x^{i, j}$ is the word in sentence labeled( $i,j$)</p><h2 id="Porpagation-Module"><a href="#Porpagation-Module" class="headerlink" title="Porpagation Module"></a>Porpagation Module</h2><p>the representations of layer n + 1 are calculated by:</p><script type="math/tex; mode=display">\mathbf{h}_{i}^{(n+1)}=\sum_{v_{j} \in \mathcal{N}\left(v_{i}\right)} \sigma\left(\mathcal{A}_{i, j}^{(n)} \mathbf{h}_{j}^{(n)}\right)</script><p>where $\mathcal{N}\left(v_{i}\right)$ denotes the neighbors of node $v_i$</p><h2 id="Classification-Module"><a href="#Classification-Module" class="headerlink" title="Classification Module"></a>Classification Module</h2><p>the loss of GP-GNNs:</p><script type="math/tex; mode=display">\mathcal{L}=g\left(\mathbf{h}_{0 :|\mathcal{V}|-1}^{0}, \mathbf{h}_{0 :|\mathcal{V}|-1}^{1}, \ldots, \mathbf{h}_{0 :|\mathcal{V}|-1}^{K}, Y ; \theta_{c}\right)</script><h1 id="Relation-Extraction-with-GP-GNNs"><a href="#Relation-Extraction-with-GP-GNNs" class="headerlink" title="Relation Extraction with GP-GNNs"></a>Relation Extraction with GP-GNNs</h1><p>Authors introduce how to apply GP-GNNs to relation extraction</p><h2 id="Encoding-Module-1"><a href="#Encoding-Module-1" class="headerlink" title="Encoding Module"></a>Encoding Module</h2><p>encoding then context of entity pairs (or edges in the graph)</p><script type="math/tex; mode=display">E\left(x_{t}^{i, j}\right)=\left[\boldsymbol{x}_{t} ; \boldsymbol{p}_{t}^{i, j}\right]</script><p>where $x_t$ denotes the word embedding; $\boldsymbol{p}_{t}^{i, j}$denotes the position embedding of word posistion t relative to the entity pair’s position $i, j$.</p><h3 id="position-embedding"><a href="#position-embedding" class="headerlink" title="position embedding"></a>position embedding</h3><p>we mark each token in the sentence as either belonging to the first entity $v_i$, the second entity $v_j$ or to neither of those</p><h2 id="Propagation-Module"><a href="#Propagation-Module" class="headerlink" title="Propagation Module"></a>Propagation Module</h2><p> the formula is the same as the front</p><h3 id="The-Initial-Embeddings-of-Nodes"><a href="#The-Initial-Embeddings-of-Nodes" class="headerlink" title="The Initial Embeddings of Nodes"></a>The Initial Embeddings of Nodes</h3><ul><li>when extracting the relationship between entity $v_i$ and entity $v_j$, the initial embeddings of them are annotated as $\mathbf{h}_{v_{i}}^{(0)}=a_{\text { subject }}$, and $h_{v_{j}}^{(0)}=a_{\text { object }}$, while the intial embeddings of other entities are set to all zeros.</li><li>In our experiments, we generalize the idea of Gated Graph Neural Networks (Li et al., 2016) by setting $a_{\text { subject }}=[1 ; 0]^{\top}$and $a_{\text { object }}=[0 ; 1]^{\top}$.</li></ul><h2 id="classification-Module"><a href="#classification-Module" class="headerlink" title="classification Module"></a>classification Module</h2><p><strong>As the  target entity pair $(v_i, v_j)$:</strong></p><script type="math/tex; mode=display">\boldsymbol{r}_{v_{i}, v_{j}}=\left[\left[\boldsymbol{h}_{v_{i}}^{(1)} \odot \boldsymbol{h}_{v_{j}}^{(1)}\right]^{\top} ;\left[\boldsymbol{h}_{v_{i}}^{(2)} \odot \boldsymbol{h}_{v_{j}}^{(2)}\right]^{\top} ; \ldots ;\left[\boldsymbol{h}_{v_{i}}^{(K)} \odot \boldsymbol{h}_{v_{j}}^{(K)}\right]^{\top}\right]</script><p>where $\odot$ represents element-wise multiplication</p><p><strong>classification:</strong></p><script type="math/tex; mode=display">\mathbb{P}\left(r_{v_{i}, v_{j}} | h, t, s\right)=\operatorname{softmax}\left(M L P\left(\boldsymbol{r}_{v_{i}, v_{j}}\right)\right)</script><p><strong>loss:</strong></p><script type="math/tex; mode=display">\mathcal{L}=\sum_{s \in S} \sum_{i \neq j} \log \mathbb{P}\left(r_{v_{i}, v_{j}} | i, j, s\right)</script><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="aim"><a href="#aim" class="headerlink" title="aim"></a>aim</h2><ul><li>showing their best models could improve the performance of relation extraction under a variety of settings</li><li>illlustrating that how the number of layers affect the performance of their model</li><li>performing a qualitiative investigation to highlight the diference between their models and baseline models</li></ul><h2 id="design"><a href="#design" class="headerlink" title="design"></a>design</h2><p>as the first and second aim</p><ul><li>show that our models could improve instance-level relation extraction on a human annotated test set</li><li>we will show that our models could also help enhance the performance of bag-level relation extraction on a distantly labeled test set</li><li>split a subset of distantly labeled test set, where the number of entities and edges is large</li></ul><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><h3 id="distantly-label-set"><a href="#distantly-label-set" class="headerlink" title="distantly label set"></a>distantly label set</h3><ul><li>Sorokin and Gurevych (2017) proposed </li><li>modify their dataset<ul><li>added reversed edges</li><li>for all of the entity pairs with no relations, added “NA” labels to them</li></ul></li></ul><h3 id="Human-annotated-test-set"><a href="#Human-annotated-test-set" class="headerlink" title="Human annotated test set"></a>Human annotated test set</h3><ul><li>Sorokin and Gurevych (2017)</li><li>select the distantly lablel pairs which all 5 annotaters are accepted.</li><li>There are 350 sentences and 1,230 triples in this test set </li></ul><h3 id="Dense-distantly-labeled-test-set"><a href="#Dense-distantly-labeled-test-set" class="headerlink" title="Dense distantly labeled test set"></a>Dense distantly labeled test set</h3><ul><li>criteria<ul><li>the number of entities should be strictly larger than 2</li><li>there must be at least one circle (with at least three entities) in the ground-truth label of the sentence</li></ul></li><li>There are 1,350 sentences and more than 17,915 triples and 7,906 relational facts in this test set.</li></ul><h2 id="Models-for-comparison"><a href="#Models-for-comparison" class="headerlink" title="Models for comparison"></a>Models for comparison</h2><ul><li>Context-aware RE</li><li>Multi-Window CNN</li><li>PCNN</li><li>LSTM or GP-GNN with K = 1 layer</li><li>GP-GNN with K = 2 or K = 3 layerss</li></ul><h2 id="Evaluation-Details"><a href="#Evaluation-Details" class="headerlink" title="Evaluation Details"></a>Evaluation Details</h2><p><strong>To evaluation models in bag-level:</strong></p><script type="math/tex; mode=display">E\left(r | v_{i}, v_{j}, S\right)=\max _{s \in S} \mathbb{P}\left(r_{v_{i}, v_{j}} | i, j, s\right)</script><p><strong>result</strong>:</p><p><img src="http://image.nysdy.com/20190523155859369893411.jpg" alt="20190523155859369893411.jpg"></p><h2 id="Effectiveness-of-Reasoning-Mechanism"><a href="#Effectiveness-of-Reasoning-Mechanism" class="headerlink" title="Effectiveness of Reasoning Mechanism"></a>Effectiveness of Reasoning Mechanism</h2><p><img src="http://image.nysdy.com/2019052315585938129506.jpg" alt="2019052315585938129506.jpg"></p><ul><li>Context-Aware RE may <strong>introduce more noise,</strong> for it may mistakenly increase the probability of a relation with the similar topic with the context relations</li><li>sentences from Wikipedia corpus are always complex, which may be hard to model for CNN and PCNN</li></ul><h2 id="The-Effectiveness-of-the-Number-of-Layers"><a href="#The-Effectiveness-of-the-Number-of-Layers" class="headerlink" title="The Effectiveness of the Number of Layers"></a>The Effectiveness of the Number of Layers</h2><p><img src="http://image.nysdy.com/20190523155859455443298.jpg" alt="20190523155859455443298.jpg"></p><ul><li>the improvement of the third layer is much smaller on the overall distantly supervised test set than the one on the dense subset<ul><li>This observation reveals that the reasoning mechanism could help us identify relations especially on sentences where there are more entities</li></ul></li><li>as the number of layers grows, the curves get higher and higher precision, <ul><li>indicating considering more hops in reasoning leads to better performance</li></ul></li></ul><h2 id="Qualitative-Results-Case-Study"><a href="#Qualitative-Results-Case-Study" class="headerlink" title="Qualitative Results: Case Study"></a>Qualitative Results: Case Study</h2><p><img src="http://image.nysdy.com/20190523155859457710223.jpg" alt="20190523155859457710223.jpg"></p><p>Context-Aware RE makes a mistake by predicting (Kentucky, share boarder with, Ohio). As we have discussed before, this is due to its mechanism to model co-occurrence of multiple relations</p><h1 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h1><p>文章是刘知远组的论文，针对的方向是关系抽取，在其中结合了关系推理，最近许多任务都在结合推理的思想。文章整体的结构，逻辑十分清晰，论述的也比较详细，属于标准论文。感觉文章中GP-GNNs结构图还可以画的更好一点，展现一下encoding module的层，可以更好理解。文章的精髓应该是这个propagation module的部分，还需要消化一下，不过这部分可能是有先前的知识支撑的。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文将GNNs应用到处理非结构化文本的（多跳）关系推理任务来进行关系抽取。采用从句子序列中获取的实体构建全链接图，应用编码（sequence model），传播（节点间信息）和分类（预测）三个模块来处理关系推理。本文提供了三个数据集。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="GNNs" scheme="http://yoursite.com/tags/GNNs/"/>
    
      <category term="relation extraction" scheme="http://yoursite.com/tags/relation-extraction/"/>
    
      <category term="relation reasoning" scheme="http://yoursite.com/tags/relation-reasoning/"/>
    
  </entry>
  
  <entry>
    <title>allennlp安装踩坑</title>
    <link href="http://yoursite.com/post/allennlp_install/"/>
    <id>http://yoursite.com/post/allennlp_install/</id>
    <published>2019-05-22T14:09:27.000Z</published>
    <updated>2019-05-22T14:11:19.740Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>安装allennlp的踩坑之路，踩了不少坑最后选择’Installing from source’的安装方法，排坑后下面方法亲测可用</p></blockquote><a id="more"></a><h2 id="Installing-from-source"><a href="#Installing-from-source" class="headerlink" title="Installing from source"></a>Installing from source</h2><p>安装步骤：</p><h3 id="1-下载GitHub文件"><a href="#1-下载GitHub文件" class="headerlink" title="1.下载GitHub文件"></a>1.下载GitHub文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/allenai/allennlp.git</span><br></pre></td></tr></table></figure><h3 id="2-创建conda环境"><a href="#2-创建conda环境" class="headerlink" title="2.创建conda环境"></a>2.创建conda环境</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n allennlp python=3.6</span><br></pre></td></tr></table></figure><h3 id="3-激活环境下载依赖文件"><a href="#3-激活环境下载依赖文件" class="headerlink" title="3.激活环境下载依赖文件"></a>3.激活环境下载依赖文件</h3><ul><li><p>激活环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source activate allennlp</span><br></pre></td></tr></table></figure></li><li><p>进入github上下载的文件夹</p></li><li><p>下载依赖文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure><p>遇到报错问题，参考下一小节，所欲问题解决。</p></li></ul><h3 id="4-安装allennlp"><a href="#4-安装allennlp" class="headerlink" title="4.安装allennlp"></a>4.安装allennlp</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install --editable .</span><br></pre></td></tr></table></figure><h3 id="5-测试"><a href="#5-测试" class="headerlink" title="5.测试"></a>5.测试</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">allennlp</span><br></pre></td></tr></table></figure><p>成功后效果如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> allennlp</span></span><br><span class="line">2019-05-22 21:58:42,297 - INFO - pytorch_pretrained_bert.modeling - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .</span><br><span class="line">usage: allennlp</span><br><span class="line"></span><br><span class="line">Run AllenNLP</span><br><span class="line"></span><br><span class="line">optional arguments:</span><br><span class="line">  -h, --help     show this help message and exit</span><br><span class="line">  --version      show program's version number and exit</span><br><span class="line"></span><br><span class="line">Commands:</span><br><span class="line"></span><br><span class="line">    configure    Run the configuration wizard.</span><br><span class="line">    train        Train a model.</span><br><span class="line">    evaluate     Evaluate the specified model + dataset.</span><br><span class="line">    predict      Use a trained model to make predictions.</span><br><span class="line">    make-vocab   Create a vocabulary.</span><br><span class="line">    elmo         Create word vectors using a pretrained ELMo model.</span><br><span class="line">    fine-tune    Continue training a model on a new dataset.</span><br><span class="line">    dry-run      Create a vocabulary, compute dataset statistics and other</span><br><span class="line">                 training utilities.</span><br><span class="line">    test-install</span><br><span class="line">                 Run the unit tests.</span><br><span class="line">    find-lr      Find a learning rate range.</span><br><span class="line">    print-results</span><br><span class="line">                 Print results from allennlp serialization directories to the</span><br><span class="line">                 console.</span><br></pre></td></tr></table></figure><h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><h3 id="问题1"><a href="#问题1" class="headerlink" title="问题1"></a>问题1</h3><h4 id="报错信息："><a href="#报错信息：" class="headerlink" title="报错信息："></a>报错信息：</h4><p>ERROR: Failed building wheel for jsonnet</p><p><img src="http://image.nysdy.com/20190522155853293350470.jpg" alt="20190522155853293350470.jpg"></p><h4 id="解决方法："><a href="#解决方法：" class="headerlink" title="解决方法："></a>解决方法：</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install -c conda-forge jsonnet</span><br></pre></td></tr></table></figure><h3 id="问题2"><a href="#问题2" class="headerlink" title="问题2"></a>问题2</h3><h4 id="报错信息：-1"><a href="#报错信息：-1" class="headerlink" title="报错信息："></a>报错信息：</h4><p>报的都是某些包的版本问题</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ERROR: botocore 1.12.152 has requirement urllib3&lt;1.25,&gt;=1.20; python_version &gt;= "3.4", but you'll have urllib3 1.25.2 which is incompatible.</span><br><span class="line">ERROR: aws-sam-translator 1.11.0 has requirement jsonschema~=2.6, but you'll have jsonschema 3.0.1 which is incompatible.</span><br><span class="line">ERROR: cfn-lint 0.20.3 has requirement jsonschema~=2.6, but you'll have jsonschema 3.0.1 which is incompatible.</span><br><span class="line">ERROR: cfn-lint 0.20.3 has requirement requests&lt;=2.21.0,&gt;=2.15.0, but you'll have requests 2.22.0 which is incompatible</span><br></pre></td></tr></table></figure><h4 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h4><p>根据报错信息下载相应安装包即可</p><h2 id="问题3"><a href="#问题3" class="headerlink" title="问题3"></a>问题3</h2><h4 id="报错信息：-2"><a href="#报错信息：-2" class="headerlink" title="报错信息："></a>报错信息：</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">ImportError: dlopen: cannot load any more object with static TLS</span><br><span class="line">___________________________________________________________________________</span><br><span class="line">Contents of /home/minelab/anaconda3/envs/allennlp/lib/python3.6/site-packages/sklearn/__check_build:</span><br><span class="line">__init__.py               setup.py                  _check_build.cpython-36m-x86_64-linux-gnu.so</span><br><span class="line">__pycache__</span><br><span class="line">___________________________________________________________________________</span><br><span class="line">It seems that scikit-learn has not been built correctly.</span><br><span class="line"></span><br><span class="line">If you have installed scikit-learn from source, please do not forget</span><br><span class="line">to build the package before using it: run `python setup.py install` or</span><br><span class="line">`make` in the source directory.</span><br><span class="line"></span><br><span class="line">If you have used an installer, please check that it is suited for your</span><br><span class="line">Python version, your operating system and your platform.</span><br></pre></td></tr></table></figure><h4 id="解决方法：-1"><a href="#解决方法：-1" class="headerlink" title="解决方法："></a>解决方法：</h4><p>下载更低版本的scikit-learn,例如</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install scikit-learn=0.20.3</span><br></pre></td></tr></table></figure><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://github.com/pytorch/pytorch/issues/10443" target="_blank" rel="noopener">https://github.com/pytorch/pytorch/issues/10443</a></li><li><a href="https://github.com/pypa/pip/issues/4330" target="_blank" rel="noopener">https://github.com/pypa/pip/issues/4330</a></li></ul><h1 id="安装的启示"><a href="#安装的启示" class="headerlink" title="安装的启示"></a>安装的启示</h1><h3 id="环境问题"><a href="#环境问题" class="headerlink" title="环境问题"></a>环境问题</h3><ul><li>最基本的就是先去网上查这个错误的解决方法</li><li>网上的解决不了的，先猜猜大概率是哪方面的问题。<ul><li>比如大概率是各种版本互相之间不适配的问题，那就调试版本，一般都会告诉你哪个有问题，比如上面的scikit-learn问题。</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;安装allennlp的踩坑之路，踩了不少坑最后选择’Installing from source’的安装方法，排坑后下面方法亲测可用&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="install" scheme="http://yoursite.com/categories/install/"/>
    
    
      <category term="allennlp" scheme="http://yoursite.com/tags/allennlp/"/>
    
      <category term="包安装" scheme="http://yoursite.com/tags/%E5%8C%85%E5%AE%89%E8%A3%85/"/>
    
  </entry>
  
  <entry>
    <title>Triple Trustworthiness Measurement for Knowledge Graph阅读笔记</title>
    <link href="http://yoursite.com/post/Triple%20Trustworthiness%20Measurement%20for%20Knowledge%20Graph/"/>
    <id>http://yoursite.com/post/Triple Trustworthiness Measurement for Knowledge Graph/</id>
    <published>2019-05-21T06:45:26.000Z</published>
    <updated>2019-05-22T13:33:20.088Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文提出了一种通过计算triple trustworthiness来评估知识图谱的准确程度的方法。模型利用神经网络综合来自实体（借鉴Resource allocation）、关系（借鉴翻译模型的思想，如TransE）和KG全局（借鉴关系路径，RNN）三个层面的语义和全局信息，输出最后的 trustworthiness作为判断依据。</p><p><a href="http://delivery.acm.org/10.1145/3320000/3313586/p2865-jia.pdf?ip=59.64.129.22&amp;id=3313586&amp;acc=OPEN&amp;key=BF85BBA5741FDC6E%2E66A15327C2E204FC%2E4D4702B0C3E38B35%2E6D218144511F3437&amp;__acm__=1558515578_57e0bf75d529cf4656975ffe7da506b9" target="_blank" rel="noopener">下载地址</a></p></blockquote><a id="more"></a><h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>This paper proposed a method for estimating the accuracy of a knowledge graph by computing triple trustworthiness. The model uses neural network to synthesize semantic and global information from three levels: entity(resource allocation), relationship(translation model ideas, such as TransE)m and KG global(relationship path, RNN) and outputting the final trustworthiness as the basis for judgment.</p><h1 id="Problem-statement"><a href="#Problem-statement" class="headerlink" title="Problem statement"></a>Problem statement</h1><p>possible noises and conflicts are inevitably intoduced in the process of constructing the KG</p><h1 id="research-objective"><a href="#research-objective" class="headerlink" title="research objective"></a>research objective</h1><p>quantify the KG’s semantic correctness and the true degree of the facts expressed</p><h1 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h1><ul><li>Knowledge graph triple trustworthiness measurement<ul><li>use the  triple semantic information and globally inferring information</li><li>three levels measurement and an intergration of confidence value</li></ul></li><li>experiment result verified the model valid on large-scale KG Freebase</li><li>the KGTtm could be utilized in knowledge graph construction or improvement</li></ul><h1 id="THE-TRIPLE-TRUSTWORTHINESS-MEASUREMENT-MODEL"><a href="#THE-TRIPLE-TRUSTWORTHINESS-MEASUREMENT-MODEL" class="headerlink" title="THE TRIPLE TRUSTWORTHINESS MEASUREMENT MODEL"></a>THE TRIPLE TRUSTWORTHINESS MEASUREMENT MODEL</h1><p><img src="http://image.nysdy.com/20190521155842605796518.jpg" alt="20190521155842605796518.jpg"></p><ul><li>Longitudinally, the model can be divided into two level.<ul><li>the upper is a pool of multiple trustworthiness estimate cells(estimator)</li><li>the output of these Estimator forms the input of lower-level fusion device(Fusioner)</li></ul></li><li>Viewed laterally, three progressive levels   are be considered, as following.</li></ul><h2 id="Is-there-a-possible-relationship-between-the-entity-pairs"><a href="#Is-there-a-possible-relationship-between-the-entity-pairs" class="headerlink" title="Is there a possible relationship between the entity pairs?"></a>Is there a possible relationship between the entity pairs?</h2><p><img src="http://image.nysdy.com/20190521155842810227816.jpg" alt="20190521155842810227816.jpg"></p><p>ResourceRank:</p><ul><li>The algorithm assumes that the association between entity paires $(h,t)$ will be stronger, and more resource is passed from the  head $h$ through all associated paths to the tail $t$ in a graph</li><li>The amount of resource aggregated into $t$ ingeniously indicateds the association strength from $h$ to $t$.</li></ul><p>As pair $(e_1,e_2)$, there only one directed edge from $e_1$ to $e_2$ in the graph, where the different bandwidth of the edge indicates the number of the multiple relations.</p><p>output:</p><script type="math/tex; mode=display">\left\{\begin{array}{c}{u=\alpha\left(W_{1} V+b_{1}\right)} \\ {R R(h, t)=W_{2} u+b_{2}}\end{array}\right.</script><p>Authors constructed a $V$ vector by combining six characteristics.</p><ol><li>R (t | h); </li><li>In-degree of head node ID(h); </li><li>Out-degree of head node OD(h); </li><li>In-degree of tail node ID(t);</li><li>Out-degree of tail node OD(t);</li><li>The depth from head node to tail node Dep</li></ol><p>As for 1. the formula:</p><script type="math/tex; mode=display">R(t | h)=(1-\theta) \sum_{e_{i} \in M_{t}} \frac{R\left(e_{i} | h\right) \cdot B W_{e_{i} t}}{O D\left(e_{i}\right)}+\frac{\theta}{N}</script><ul><li>$M_t$is the set of all nodes that have outgoing links to the node $t$, $OD (e_i)$ is the out-degree of the node eiand the $BW_{e_it}$ is the bandwidth from the $e_i$ to $t$.</li><li>In order to improve the model fault-tolerance, we assume that the resource fow from each node may directly jump to a random node with the same probability θ</li></ul><h2 id="Can-the-determined-relationship-r-occur-between-the-entity-pair-h-t"><a href="#Can-the-determined-relationship-r-occur-between-the-entity-pair-h-t" class="headerlink" title="Can the determined relationship $r$ occur between the entity pair $(h,t)$ ?"></a>Can the determined relationship $r$ occur between the entity pair $(h,t)$ ?</h2><p><img src="http://image.nysdy.com/20190522155850919251079.jpg" alt="20190522155850919251079.jpg"></p><p>Translation-based energy function (TEF)：depended on TransE</p><p>$E(h, r, t)=|\mathbf{h}+\mathbf{r}-\mathbf{t}|$</p><p>output:</p><script type="math/tex; mode=display">P(E(h, r, t))=\frac{1}{1+e^{-\lambda\left(\delta_{r}-E(h, r, t)\right)}}</script><h2 id="Can-the-relevant-triples-in-the-KG-infer-that-the-triple-is-trustworthy"><a href="#Can-the-relevant-triples-in-the-KG-infer-that-the-triple-is-trustworthy" class="headerlink" title="Can the relevant triples in the KG infer that the triple is trustworthy?"></a>Can the relevant triples in the KG infer that the triple is trustworthy?</h2><p><img src="http://image.nysdy.com/20190522155851013726520.jpg" alt="20190522155851013726520.jpg"></p><p>Reachable paths inference (RPI):</p><p>There two challenges to exploit the reachable paths for inferring triple trustworthiness:</p><h3 id="reachable-paths-selection"><a href="#reachable-paths-selection" class="headerlink" title="reachable paths selection"></a>reachable paths selection</h3><p>Semantic distance-based path selection<img src="http://image.nysdy.com/2019052215585105592950.jpg" alt="2019052215585105592950.jpg"></p><h3 id="Reachable-Paths-Representation"><a href="#Reachable-Paths-Representation" class="headerlink" title="Reachable Paths Representation"></a>Reachable Paths Representation</h3><p>using a RNN to deal with the embeddings of the three elements of each triple in the selected path</p><h2 id="Fusing-the-Estimators"><a href="#Fusing-the-Estimators" class="headerlink" title="Fusing the Estimators"></a>Fusing the Estimators</h2><p>a classifer based on a multi-layer perceptron </p><h1 id="EXPERIMENTS"><a href="#EXPERIMENTS" class="headerlink" title="EXPERIMENTS"></a>EXPERIMENTS</h1><h2 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h2><p>FB15K</p><h2 id="Interpreting-the-Validity-of-the-Trustworthiness"><a href="#Interpreting-the-Validity-of-the-Trustworthiness" class="headerlink" title="Interpreting the Validity of the Trustworthiness"></a>Interpreting the Validity of the Trustworthiness</h2><p><img src="http://image.nysdy.com/20190522155851139148623.jpg" alt="20190522155851139148623.jpg"></p><ul><li>The left picture shows that the positives examples are mainly concentrated in the upper region, vice versa.</li><li>As for the right picture<ul><li>only if the value of a triple is higher than the threshold can it be considered trustworthy</li><li>shows that the positive examples universally have higher confidence values</li></ul></li></ul><h2 id="Comparing-With-Other-Models-on-The-Knowledge-Graph-Error-Detection-Task"><a href="#Comparing-With-Other-Models-on-The-Knowledge-Graph-Error-Detection-Task" class="headerlink" title="Comparing With Other Models on The Knowledge Graph Error Detection Task"></a>Comparing With Other Models on The Knowledge Graph Error Detection Task</h2><p><img src="http://image.nysdy.com/20190522155851269267340.jpg" alt="20190522155851269267340.jpg"></p><p>Authors’ model has beter results in terms of accuracy and the F1-score than the other models.</p><h2 id="Analyzing-the-ability-of-models-to-tackle-the-three-type-noises"><a href="#Analyzing-the-ability-of-models-to-tackle-the-three-type-noises" class="headerlink" title="Analyzing the ability of models to tackle the three type noises."></a>Analyzing the ability of models to tackle the three type noises.</h2><p><img src="http://image.nysdy.com/20190522155851290149973.jpg" alt="20190522155851290149973.jpg"></p><ul><li>a higher recall shows that authors’ model can more accurately find the right from noisy triples</li><li>higher average trustworthiness values show that authors’ model can better identify the correct instances and with high confidence </li><li>the worst among the $(h, ?, t)$, because the various relations between a certain entity  increase the difficulty of model judgment.</li></ul><h2 id="Analyzing-the-Efects-of-Single-Estimators"><a href="#Analyzing-the-Efects-of-Single-Estimators" class="headerlink" title="Analyzing the Efects of Single Estimators"></a>Analyzing the Efects of Single Estimators</h2><p><img src="http://image.nysdy.com/20190522155851337652153.jpg" alt="20190522155851337652153.jpg"></p><p>It can be found that the accuracy obtained by each model is above 0.8, which proves the effectiveness of each Estimator</p><h1 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h1><p>本文在方法上几乎没有什么创新，本质上就是一个老方法的多个组合。最大亮点就是作者能提出trustworthiness来把这个评价知识图谱准确度的问题进行了量化。这种能力比提出方法上的创新更加厉害，也是需要学习的地方。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文提出了一种通过计算triple trustworthiness来评估知识图谱的准确程度的方法。模型利用神经网络综合来自实体（借鉴Resource allocation）、关系（借鉴翻译模型的思想，如TransE）和KG全局（借鉴关系路径，RNN）三个层面的语义和全局信息，输出最后的 trustworthiness作为判断依据。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://delivery.acm.org/10.1145/3320000/3313586/p2865-jia.pdf?ip=59.64.129.22&amp;amp;id=3313586&amp;amp;acc=OPEN&amp;amp;key=BF85BBA5741FDC6E%2E66A15327C2E204FC%2E4D4702B0C3E38B35%2E6D218144511F3437&amp;amp;__acm__=1558515578_57e0bf75d529cf4656975ffe7da506b9&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;下载地址&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="KG" scheme="http://yoursite.com/tags/KG/"/>
    
      <category term="Knowledge Graph" scheme="http://yoursite.com/tags/Knowledge-Graph/"/>
    
      <category term="Triple" scheme="http://yoursite.com/tags/Triple/"/>
    
  </entry>
  
  <entry>
    <title>GloVe: Global Vectors for Word Representation阅读笔记</title>
    <link href="http://yoursite.com/post/GloVe:Global%20Vectors%20for%20Word%20Representation/"/>
    <id>http://yoursite.com/post/GloVe:Global Vectors for Word Representation/</id>
    <published>2019-05-20T13:35:25.000Z</published>
    <updated>2019-05-21T06:43:45.987Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>论文<a href="https://www.aclweb.org/anthology/D14-1162" target="_blank" rel="noopener">下载地址</a>，GloVe是一个新的全球对数双线性回归模型，属于经典的词向量表示方法之一。</p></blockquote><a id="more"></a><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>evaluate the intrinsic quality</p><ul><li>Most word vector methods rely on the distance or angle between pairs of word vectors </li><li>Mikolov et al. (2013c) introduced word analogies that examines word vector’s various dimensions of difference.</li></ul><p>two main model families for learning vectors:</p><ul><li>global matrix factorization methods</li><li>local context window methods</li></ul><p>Authors propose a specific weighted least squares model that trains on globla word-word co-occurrence counts and thus makes efficient use of statistics.</p><h1 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h1><h2 id="Matix-Facroization-Methods"><a href="#Matix-Facroization-Methods" class="headerlink" title="Matix Facroization Methods"></a>Matix Facroization Methods</h2><p>These methods utilize low-rank approximations to decompose large matrices that capture statistical information about a corpus.</p><h3 id="shortcoming"><a href="#shortcoming" class="headerlink" title="shortcoming"></a>shortcoming</h3><p>the most frequent words contribute a dispropoertionate amount to the similarity measure.</p><h2 id="Shallow-Window-Based-Methods"><a href="#Shallow-Window-Based-Methods" class="headerlink" title="Shallow Window-Based Methods"></a>Shallow Window-Based Methods</h2><p>Another approach is to learn word representations that aid in making predictins within local context windows.</p><h3 id="shortcoming-1"><a href="#shortcoming-1" class="headerlink" title="shortcoming"></a>shortcoming</h3><p>do not operate directly on the co-occurrence statistics of the corpus and fails to take advantage of the vast amount of repetition in the data.</p><h1 id="The-GloVe-Model"><a href="#The-GloVe-Model" class="headerlink" title="The GloVe Model"></a>The GloVe Model</h1><h3 id="GloVe-Global-Vectors"><a href="#GloVe-Global-Vectors" class="headerlink" title="GloVe: Global Vectors"></a>GloVe: Global Vectors</h3><p>the global corpus statistics are captured directly by the model</p><h3 id="the-question-about-the-model-using-the-statistics-of-word-occurrences-in-a-corpus"><a href="#the-question-about-the-model-using-the-statistics-of-word-occurrences-in-a-corpus" class="headerlink" title="the question about the model using the statistics of word occurrences in a corpus"></a>the question about the model using the statistics of word occurrences in a corpus</h3><ul><li>how meaning is generated from these statistics</li><li>how the resulting word vectors might represent that meaning</li></ul><h2 id="some-notation"><a href="#some-notation" class="headerlink" title="some notation"></a>some notation</h2><p>$X_{ij}$ : the number of times word j occurs in the context of word i</p><p>$X_i = \sum_{k} X_{i k}$ : the number of times any word appears in the context of word i</p><p>$P_{i j}=P(j | i)=X_{i j} / X_{i}$: the probability that word j appear in the context of word i</p><p><img src="http://image.nysdy.com/20190515155788329289127.jpg" alt="20190515155788329289127.jpg"></p><p>above that, werd vector learning should be with ratios of co-occurrence probabilities:</p><p><img src="http://image.nysdy.com/20190515155788352871603.jpg" alt="20190515155788352871603.jpg"></p><p>$w \in \mathbb{R}^{d}$are word vectors and $\tilde{w} \in \mathbb{R}^{d}$are separate context word vectors</p><p>For F, we should select a unique choice by enforcing a few desiderata.</p><ul><li><p>encoding the information present the ratio $P_{i k} / P_{j k}$ in the word vector space. </p><p>Since vector spaces are inherently linear structures</p><p><img src="http://image.nysdy.com/20190515155788379647745.jpg" alt="20190515155788379647745.jpg"></p></li><li><p>put F to be a compicated function parameterized, and avoiding bofuscating the linear structure<img src="http://image.nysdy.com/20190515155788397136355.jpg" alt="20190515155788397136355.jpg"></p></li><li><p>the word-word co-occurrence matrices, we can exchange a word and a context word(because a word can also be a context word)</p><ol><li><p>F should be a homomorphism<img src="http://image.nysdy.com/2019051515578842869345.jpg" alt="2019051515578842869345.jpg"></p><p>by Eqn.(3)<img src="http://image.nysdy.com/20190515155788435674239.png" alt="20190515155788435674239.png"></p><p>F = exp or <img src="http://image.nysdy.com/20190515155788448759173.jpg" alt="20190515155788448759173.jpg"></p></li><li><p>the Eqn(6) would have the exchange symmetry if not $\log \left(X_{i}\right)$ and $\log \left(X_{i}\right)$ is independent of k, so it can be absorbed into a bias $b_i$<img src="http://image.nysdy.com/20190515155788566687599.jpg" alt="20190515155788566687599.jpg"></p></li><li><p>for avoiding diverge, $\log \left(X_{i k}\right) \rightarrow \log \left(1+X_{i k}\right)$</p></li><li><p>a new weighted least squares regression model to address the problem that LSA wirhts all co-occuttences equally.</p><p>cost function:<img src="http://image.nysdy.com/20190515155788560186237.jpg" alt="20190515155788560186237.jpg"></p></li><li><p><img src="http://image.nysdy.com/20190515155788571777804.jpg" alt="20190515155788571777804.jpg"></p></li></ol></li></ul><h2 id="Relationship-to-Other-Models"><a href="#Relationship-to-Other-Models" class="headerlink" title="Relationship to Other Models"></a>Relationship to Other Models</h2><p>In this subsection authors show how these models are related to their proposed model.</p><h4 id="the-defect-of-cross-entropy"><a href="#the-defect-of-cross-entropy" class="headerlink" title="the defect of cross entropy"></a>the defect of cross entropy</h4><ul><li>it has the unfortunate property that distributions with long tails are often modeled poorly with too much wieght given to the unlikely events.</li></ul><h2 id="Complexity-of-the-model"><a href="#Complexity-of-the-model" class="headerlink" title="Complexity of the model"></a>Complexity of the model</h2><p>the computational complexity of the model depends on the number of nonzero elects in the matrix $X$</p><h4 id="some-assumptions-about-the-distribution-of-word-co-occurrences"><a href="#some-assumptions-about-the-distribution-of-word-co-occurrences" class="headerlink" title="some assumptions about the distribution of word co-occurrences"></a>some assumptions about the distribution of word co-occurrences</h4><ul><li><p>the number of co-occurrences of word $i$ with word $j$, $X_{ij}$, can be modeled as a power-law function of the frequency rank of that word pair, $r_{ij}$:</p><p>$X_{i j}=\frac{k}{\left(r_{i j}\right)^{\alpha}}$</p></li></ul><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Evaluation-methods"><a href="#Evaluation-methods" class="headerlink" title="Evaluation methods"></a>Evaluation methods</h2><p>authors conduct experiments on the word analogy taks of Mikolov et al. (2013a)</p><h3 id="Word-analogies"><a href="#Word-analogies" class="headerlink" title="Word analogies"></a>Word analogies</h3><p>The word analogy task consists of questions like, “a is to b as c is to ?”</p><h3 id="Word-similarity"><a href="#Word-similarity" class="headerlink" title="Word similarity"></a>Word similarity</h3><p><img src="http://image.nysdy.com/20190520155833277478435.jpg" alt="20190520155833277478435.jpg"></p><h3 id="Named-entity-recognition"><a href="#Named-entity-recognition" class="headerlink" title="Named entity recognition"></a>Named entity recognition</h3><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>Table 2 shows the CloVe model performs significantly better than the other baslines, often with smaller vector sizes and smaller corpora.</p><p><img src="http://image.nysdy.com/20190520155833353468570.jpg" alt="20190520155833353468570.jpg"></p><p>Table 3 shows results on five different word similarity datasets.</p><p>Table 4 shows results on the NER task with the CRF-based model.</p><p><img src="http://image.nysdy.com/20190520155833377540169.jpg" alt="20190520155833377540169.jpg"></p><h2 id="Model-Analysis-Vector-Length-and-Context-Size"><a href="#Model-Analysis-Vector-Length-and-Context-Size" class="headerlink" title="Model Analysis: Vector Length and Context Size"></a>Model Analysis: Vector Length and Context Size</h2><p><img src="http://image.nysdy.com/2019052015583339399087.jpg" alt="2019052015583339399087.jpg"></p><h3 id="Model-Analysis-Corpus-Size"><a href="#Model-Analysis-Corpus-Size" class="headerlink" title="Model Analysis: Corpus Size"></a>Model Analysis: Corpus Size</h3><p><img src="http://image.nysdy.com/20190520155833403240640.jpg" alt="20190520155833403240640.jpg"></p><ul><li>On the syntactic subtask, larger corpora typically produce better statistics so that there is a monotonic increase in performance as the cor- pus size increases.</li><li>But the same trend is not true for the semantic subtask, which is probably because of analogy dataset</li></ul><h2 id="Model-Analysis-Run-time"><a href="#Model-Analysis-Run-time" class="headerlink" title="Model Analysis: Run-time"></a>Model Analysis: Run-time</h2><p><img src="http://image.nysdy.com/20190520155833432462881.jpg" alt="20190520155833432462881.jpg"></p><h2 id="Model-Analysis-Comparison-with-word2vec"><a href="#Model-Analysis-Comparison-with-word2vec" class="headerlink" title="Model Analysis: Comparison with word2vec"></a>Model Analysis: Comparison with word2vec</h2><p>For the same corpus, vocabulary, window size, and training time, GloVe consistently outperforms word2vec</p><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul><li><a href="https://blog.csdn.net/coderTC/article/details/73864097" target="_blank" rel="noopener">https://blog.csdn.net/coderTC/article/details/73864097</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;论文&lt;a href=&quot;https://www.aclweb.org/anthology/D14-1162&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;下载地址&lt;/a&gt;，GloVe是一个新的全球对数双线性回归模型，属于经典的词向量表示方法之一。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="word vector" scheme="http://yoursite.com/tags/word-vector/"/>
    
      <category term="GloVe" scheme="http://yoursite.com/tags/GloVe/"/>
    
  </entry>
  
  <entry>
    <title>Deep contextualized word representations 阅读笔记</title>
    <link href="http://yoursite.com/post/Deep%20contextualized%20word%20representations/"/>
    <id>http://yoursite.com/post/Deep contextualized word representations/</id>
    <published>2019-05-10T07:28:06.000Z</published>
    <updated>2019-05-13T06:50:58.808Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://arxiv.org/pdf/1802.05365.pdf" target="_blank" rel="noopener">论文下载地址</a>，ELMo事先用语言模型学好一个单词的 Word Embedding，此时多义词无法区分，不过这没关系。在我实际使用 Word Embedding 的时候，单词已经具备了特定的上下文了，这个时候我可以根据上下文单词的语义去调整单词的 Word Embedding 表示，这样经过调整后的 Word Embedding 更能表达在这个上下文中的具体含义，自然也就解决了多义词的问题了。<strong>所以 ELMO 本身是个根据当前上下文对 Word Embedding 动态调整的思路。</strong></p></blockquote><a id="more"></a><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><h2 id="ELMo-Embedddings-from-Language-Models"><a href="#ELMo-Embedddings-from-Language-Models" class="headerlink" title="ELMo(Embedddings from Language Models):"></a>ELMo(Embedddings from Language Models):</h2><h3 id="why-call-ELMo"><a href="#why-call-ELMo" class="headerlink" title="why call ELMo:"></a>why call ELMo:</h3><p>Using vectors derived from a bidirectional LSTM that is trained with a coupled language model(LM) objective on a large text corups.</p><h3 id="characteristics"><a href="#characteristics" class="headerlink" title="characteristics"></a>characteristics</h3><ul><li><p>ELMo representations are a function of all of the internal layers of the biLM.</p></li><li><p>learn a linear combination of the vectors stacked above each input word for each end task</p></li><li><p>the higher-level LSTM states capture context-dependent aspects of word meaning</p><p>the lower-level states model aspects of syntax</p></li></ul><h3 id="Extensive-experiments"><a href="#Extensive-experiments" class="headerlink" title="Extensive experiments"></a>Extensive experiments</h3><ul><li>EMLo representations can be easily added to existing models</li><li>improve the state of art in every case</li><li>ELMo outperform those derived from just the top layer of a LSTM</li></ul><h1 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h1><ul><li><p>Some approaches for learning word vectors only allow a single context-independent representation for each word.</p></li><li><p>to overcome some shortcomings of traditional word vectors:</p><ul><li>enriching them with subword information</li><li>learning separate vectors for each word sense</li></ul><p>Authors uses subword units through the use of character convolutions, seamlessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes.</p></li><li><p>context-depends representations</p><p> Authors take full advantage of access to plentiful monolingual data</p></li><li><p>Previous work also shown that different layers of deep biRNNs encode different types of information</p><ul><li>introducing multi-task syntactic supervision at the lower levels of a deep LSTM can improve overall performance of higher level tasks</li><li>the top layer of an LSTM for encoding word context (Melamud et al., 2016) has been shown to learn representations of word sense.</li></ul><p>ELMo representations can also induce similar signals.</p></li></ul><h1 id="ELMo-Embeddings-from-Language-Models"><a href="#ELMo-Embeddings-from-Language-Models" class="headerlink" title="ELMo: Embeddings from Language Models"></a>ELMo: Embeddings from Language Models</h1><h2 id="Bidirectional-language-models"><a href="#Bidirectional-language-models" class="headerlink" title="Bidirectional language models"></a>Bidirectional language models</h2><ul><li><p>model the probability of token $t_k$ given the history($t_1, … , t_{k-1}$):</p><p><img src="http://image.nysdy.com/20190512155766627486478.png" alt="20190512155766627486478.png"></p></li><li><p>a backward LM:<img src="http://image.nysdy.com/2019051215576663543534.png" alt="2019051215576663543534.png"></p></li></ul><p>Authors’ formulation jointly maximizes the log likelihood of the forward and backward directions:</p><p><img src="http://image.nysdy.com/20190512155766643539454.png" alt="20190512155766643539454.png"></p><h2 id="ELMo"><a href="#ELMo" class="headerlink" title="ELMo"></a>ELMo</h2><ul><li><p>For each token $t_k$, a L-layer biLM computes a set of 2L + 1 representations:<img src="http://image.nysdy.com/20190512155767113638451.png" alt="20190512155767113638451.png"></p></li><li><p>For a downstream model, ELMo collapses all layers in R into a single vector.</p><p>In the simplest case, ELMo just selects the top layer.</p></li><li><p>For a task specific weighting of all biLM layers:<img src="http://image.nysdy.com/20190512155767132777658.png" alt="20190512155767132777658.png"></p><p>$s^{task}$ are softmax-normalized weithts and the scalar parameter $γ^{task}$ allows the task model to scale the entire ELMo vector</p></li></ul><h2 id="Using-biLMs-for-supervised-NLP-tasks"><a href="#Using-biLMs-for-supervised-NLP-tasks" class="headerlink" title="Using biLMs for supervised NLP tasks"></a>Using biLMs for supervised NLP tasks</h2><ul><li>Given a pre-trained biLM and a supervised architecture for a target NLP task</li><li>let the end task model learn a linear combination of these representations<ol><li>consider the lowest layers of th supervised model without the biLM</li><li>add ELMo to the supervised model<ul><li>freeze the weights of the biLM</li><li>concatenate the ELMo vector $ELMo^{task}_k$ with $x_k$ and pass the ELMo enhanced representation $[x_k,;ELMo^{task}_k ]$ into the task RNN.</li><li>for some tasks, authors also include ELMo ar the output of task RNN by introducing another set of out put specific linear weights and replacing $h_k$ with $[h_k,;ELMo^{task}_k ]$</li><li>add a moderate amount of dropout to ELMo and in some case to regularize the ELMo weights</li></ul></li></ol></li></ul><h2 id="Pre-trained-bidirectional-language-model-architecture"><a href="#Pre-trained-bidirectional-language-model-architecture" class="headerlink" title="Pre-trained bidirectional language model architecture"></a>Pre-trained bidirectional language model architecture</h2><ul><li>the biLM provides three layers of representations for each input token, both directions and a residual connection between LSTM layers </li><li>fine tuning the biLM on domain specific data</li></ul><h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><p>the following picture shows the performance of ELMo in Question answering, Textual entailment, Semantic role labeling, Corefrence resolution, Named entity extraction, Sentiment analysis.</p><p><img src="http://image.nysdy.com/2019051315577106394943.png" alt="2019051315577106394943.png"></p><p>In every task considered, simply adding ELMo establishes a new state-of-the-art result.</p><h1 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h1><h2 id="Alternate-layer-weighting-schemes"><a href="#Alternate-layer-weighting-schemes" class="headerlink" title="Alternate layer weighting schemes"></a>Alternate layer weighting schemes</h2><p><img src="http://image.nysdy.com/20190512155767132777658.png" alt="20190512155767132777658.png"></p><p>the following picture compares these alternatives.</p><p><img src="http://image.nysdy.com/20190513155771140936480.png" alt="20190513155771140936480.png"></p><p>Including representations from all layers improves overall performance over just using the last layer, and including contextual representations from the last layer improves performace over the baseline.</p><p>Also shows the $\lambda$ is important.</p><h2 id="Where-to-include-ELMo"><a href="#Where-to-include-ELMo" class="headerlink" title="Where to include ELMo?"></a>Where to include ELMo?</h2><p>The ELMo can be included in both the input and output.</p><p><img src="http://image.nysdy.com/20190513155771190646517.png" alt="20190513155771190646517.png"></p><p>the results show including the ELMo in both input and output can preform better.</p><h2 id="What-information-is-captured-by-the-biLM’s-representations"><a href="#What-information-is-captured-by-the-biLM’s-representations" class="headerlink" title="What information is captured by the biLM’s representations?"></a>What information is captured by the biLM’s representations?</h2><p>Intuitively, the biLM must be disambiguating the meaning of words using their context.<img src="http://image.nysdy.com/20190513155771262634271.png" alt="20190513155771262634271.png"></p><p>The GloVe can only capure the speech. but the biLM is able to disambiguate both the part of speech and word sense in the source sentence.</p><h3 id="Word-sense-disambiguation"><a href="#Word-sense-disambiguation" class="headerlink" title="Word sense disambiguation"></a>Word sense disambiguation</h3><p>given a sentence, predicting  the sense of a target word using a simple 1-nearst negihbor approach</p><p><img src="http://image.nysdy.com/20190513155771312514655.png" alt="20190513155771312514655.png"></p><h3 id="POS-tagging"><a href="#POS-tagging" class="headerlink" title="POS tagging"></a>POS tagging</h3><p>to examine whether the biLM captures basic syntax.</p><p><img src="http://image.nysdy.com/20190513155771328944169.png" alt="20190513155771328944169.png"></p><h2 id="Sample-efficiency"><a href="#Sample-efficiency" class="headerlink" title="Sample efficiency"></a>Sample efficiency</h2><p>Adding ELMo to a model increases the sample efficiency considerably, both in terms of number of parameter updates to reach state-of-the-art performance and the overall training set size.<img src="http://image.nysdy.com/20190513155771349825964.png" alt="20190513155771349825964.png"></p><h2 id="Visualization-of-learned-weights"><a href="#Visualization-of-learned-weights" class="headerlink" title="Visualization of learned weights"></a>Visualization of learned weights</h2><p><img src="http://image.nysdy.com/20190513155771355211483.png" alt="20190513155771355211483.png"></p><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul><li><a href="https://zhuanlan.zhihu.com/p/63115885" target="_blank" rel="noopener">NAACL2018:高级词向量(ELMo)详解(超详细) 经典</a>，这篇文章中阐述了一些使用的细节，并用图来表示，更加清晰。</li><li><a href="https://blog.csdn.net/triplemeng/article/details/82380202" target="_blank" rel="noopener">ELMo算法介绍</a>，这篇博客中自己对整个论文的概述和总结和好，需要学习。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1802.05365.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;论文下载地址&lt;/a&gt;，ELMo事先用语言模型学好一个单词的 Word Embedding，此时多义词无法区分，不过这没关系。在我实际使用 Word Embedding 的时候，单词已经具备了特定的上下文了，这个时候我可以根据上下文单词的语义去调整单词的 Word Embedding 表示，这样经过调整后的 Word Embedding 更能表达在这个上下文中的具体含义，自然也就解决了多义词的问题了。&lt;strong&gt;所以 ELMO 本身是个根据当前上下文对 Word Embedding 动态调整的思路。&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="embedding" scheme="http://yoursite.com/tags/embedding/"/>
    
  </entry>
  
  <entry>
    <title>《Efficient Estimation of Word Representations in Vector Space》阅读笔记</title>
    <link href="http://yoursite.com/post/Efficient%20Estimation%20of%20Word%20Representations%20in%20Vector%20Space/"/>
    <id>http://yoursite.com/post/Efficient Estimation of Word Representations in Vector Space/</id>
    <published>2019-04-30T02:37:23.000Z</published>
    <updated>2019-05-06T09:00:22.589Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://arxiv.org/pdf/1301.3781.pdf" target="_blank" rel="noopener">论文下载地址</a>，该篇论文的大篇幅都在讨论实验结果的分析，模型的部分比较简单，没有详细分析，本来是想读一下CBOW和skip-gram的原始论文，发现并没有想象中的那么大的用处。</p></blockquote><a id="more"></a><h2 id="Goals-of-paper"><a href="#Goals-of-paper" class="headerlink" title="Goals of paper"></a>Goals of paper</h2><ul><li>开发了两种新模型，并保留了单词之间的线性规律</li><li>设计了一个新的综合测试集，用于测量句法和语义规律</li><li>讨论了训练时间和准确性如何取决于单词向量的维度和训练数据的数量</li></ul><h2 id="Model-Architectures"><a href="#Model-Architectures" class="headerlink" title="Model Architectures"></a>Model Architectures</h2><p>训练复杂度：</p><p><img src="http://image.nysdy.com/20190506155712909488421.png" alt="20190506155712909488421.png"></p><p>其中，E是训练次数，T是训练集单词数量，Q是模型结构。</p><h3 id="Feedforward-Neural-Net-Language-Model-NNLM"><a href="#Feedforward-Neural-Net-Language-Model-NNLM" class="headerlink" title="Feedforward Neural Net Language Model (NNLM)"></a>Feedforward Neural Net Language Model (NNLM)</h3><p>它由输入，映射，隐藏和输出层组成。通过简化方法，Q= N x D x H</p><h3 id="Recurrent-Neural-Net-Language-Model-RNNLM"><a href="#Recurrent-Neural-Net-Language-Model-RNNLM" class="headerlink" title="Recurrent Neural Net Language Model (RNNLM)"></a>Recurrent Neural Net Language Model (RNNLM)</h3><p>克服了模型需要固定的上下文长度的问题，并且只有输入，隐藏和输出层。</p><p>Q= H x H + H x V，其中H = D（单词表示），H x V 可以通过分级softmax被简化为H x log_2(V)。所以主要的复杂度来自于H x H。</p><h3 id="Parallel-Training-of-Neural-Networks"><a href="#Parallel-Training-of-Neural-Networks" class="headerlink" title="Parallel Training of Neural Networks"></a>Parallel Training of Neural Networks</h3><p>模型使用的DistBelief框架允许我们并行运行同一模型的多个副本，每个副本通过集中的服务器同步其梯度更新，该服务器保留所有参数</p><h2 id="New-Log-linear-Models"><a href="#New-Log-linear-Models" class="headerlink" title="New Log-linear Models"></a>New Log-linear Models</h2><p>大多数复杂性是由于模型中的非线性隐藏层引起的。模型结构如下：<img src="http://image.nysdy.com/20190506155713050638684.png" alt="20190506155713050638684.png"></p><h3 id="Continuous-Bag-of-Words-Model-CBOW"><a href="#Continuous-Bag-of-Words-Model-CBOW" class="headerlink" title="Continuous Bag-of-Words Model(CBOW)"></a>Continuous Bag-of-Words Model(CBOW)</h3><p>第一个提出的体系结构类似于前馈NNLM，其中去除了非线性隐藏层，并且所有单词（不仅仅是投影矩阵）共享投影层。 因此，所有单词都被投射到相同的位置（它们的向量被平均）。 将这个架构称为词袋模型，因为历史中的单词顺序不会影响投影。</p><p>模型的复杂度：Q = N × D + D × log_2(V )</p><h3 id="Continuous-Skip-gram-Model"><a href="#Continuous-Skip-gram-Model" class="headerlink" title="Continuous Skip-gram Model"></a>Continuous Skip-gram Model</h3><p>基于同一句子中的另一个单词最大化单词的分类。 更准确地说，使用每个当前单词作为具有连续投影层的对数线性分类器的输入，并预测当前单词之前和之后的特定范围内的单词。</p><p>模型的复杂度：Q = C × (D + D × log2(V ))，其中C是单词的最大距离。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="任务描述"><a href="#任务描述" class="headerlink" title="任务描述"></a>任务描述</h3><p>为了度量词向量的质量，我们定义了一个复杂的测试集，它包括了五种类型的语义问题。九个类型的句法问题。包括每个类别的两个样本集在上表展示；总之，共拥有8869个语义问题和10675个句法问题</p><p>作者通过：最大化精确度 ，模型体系结构的比较，模型的大规模并行训练来证明提出模型的运速度和精确的优势。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1301.3781.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;论文下载地址&lt;/a&gt;，该篇论文的大篇幅都在讨论实验结果的分析，模型的部分比较简单，没有详细分析，本来是想读一下CBOW和skip-gram的原始论文，发现并没有想象中的那么大的用处。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="word2vec" scheme="http://yoursite.com/tags/word2vec/"/>
    
  </entry>
  
  <entry>
    <title>Shared Embedding Based Neural Networks for Knowledge Graph Completion阅读笔记</title>
    <link href="http://yoursite.com/post/Shared%20Embedding%20Based%20Neural%20Networks%20for%20Knowledge%20Graph%20Completion/"/>
    <id>http://yoursite.com/post/Shared Embedding Based Neural Networks for Knowledge Graph Completion/</id>
    <published>2019-04-19T06:52:38.000Z</published>
    <updated>2019-04-19T08:09:35.184Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="http://delivery.acm.org/10.1145/3280000/3271704/p247-guan.pdf?ip=59.64.129.243&amp;id=3271704&amp;acc=ACTIVE%20SERVICE&amp;key=BF85BBA5741FDC6E%2E66A15327C2E204FC%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1555657159_db1582f1a6ea923a16011064e5cc7955" target="_blank" rel="noopener">原文下载链接</a>，知识图谱补全（KGC，Knowledge Graph Completion)是一种自动建立图谱内部知识关联的工作。目标是补全知识图谱中三元组的缺失部分。主要方法为基于张量（或者矩阵）和基于翻译两类。在本文中，作者提出了一种基于共享嵌入的神经网络的模型（SENN）来处理KGC。</p></blockquote><a id="more"></a><h2 id="Contribulation"><a href="#Contribulation" class="headerlink" title="Contribulation"></a>Contribulation</h2><ul><li>提出了SENN模型，该模型明确区分头实体、关系和为实体预测任务，并把它们整合到一个基于全连接神经网络框架中，该框架共享的实体和关系嵌入。</li><li>SENN提出了一个自适应全中损失机制，该方法可以很好的处理具有不同映射属性的三元组，并处理不同的预测任务。</li><li>由于关系预测通常比头尾实体预测具有更好的性能，我们把SENN应用到头尾实体预测，从而将SENN扩展到SENN+。</li></ul><h2 id="Related-works"><a href="#Related-works" class="headerlink" title="Related works"></a>Related works</h2><h3 id="Tensor-Matrix-Based-Methods"><a href="#Tensor-Matrix-Based-Methods" class="headerlink" title="Tensor/Matrix Based Methods"></a>Tensor/Matrix Based Methods</h3><p>RESCAL是一个典型的方法，该方法基于三向张量因子分解的方法。</p><p>目标函数为：<img src="http://image.nysdy.com/20190419155565817094503.png" alt="20190419155565817094503.png"></p><p>$M_r$是r的关系矩阵，大小为k x k。</p><p>ComlEx是最近提出的方法，该方法基于矩阵分解，并且它使用复数值来定义实体和关系的嵌入。</p><p>目标函数为：<img src="http://image.nysdy.com/20190419155565837140789.png" alt="20190419155565837140789.png"></p><p>Re(x)返回x的实部。</p><h3 id="Translation-Based-Methods"><a href="#Translation-Based-Methods" class="headerlink" title="Translation Based Methods"></a>Translation Based Methods</h3><p>代表模型为经典的TransE模型（这里不再赘述）</p><h3 id="Translation-Based-Methods-1"><a href="#Translation-Based-Methods-1" class="headerlink" title="Translation Based Methods"></a>Translation Based Methods</h3><p>ER-MLP使用多层感知器来捕获头实体，关系和尾实体之间的隐式交互。</p><p>目标函数为：<img src="http://image.nysdy.com/2019041915556586361053.png" alt="2019041915556586361053.png"></p><p>ProjE使用具有组合层和投影层的神经网络来对头尾实体预测建模。</p><h2 id="THE-SENN-METHOD"><a href="#THE-SENN-METHOD" class="headerlink" title="THE SENN METHOD"></a>THE SENN METHOD</h2><p>模型结构如图所示：<img src="http://image.nysdy.com/20190419155565880062506.png" alt="20190419155565880062506.png"></p><p>作者将框架划分为以下四个部分：</p><ol><li>三元组的批量预处理</li><li>知识图谱的Shared embeddings表示学习</li><li>独立的头尾实体及关系预测子模型训练与融合</li><li>联合损失函数构成</li></ol><p>整个KGC的流程可以描述如下：</p><ol><li>将训练数据中的完整三元组（知识图谱）划分批量后作为模型的输入</li><li>对于输入的三元组，分别训练得到实体（包括头尾实体）嵌入矩阵与关系嵌入矩阵（embeddings）</li><li>将头尾实体及关系embeddings分别输入到三个预测模型中（头实体预测（?, r, t），关系预测(h, ?, t)，尾实体预测(h, r, ?)）</li></ol><h3 id="The-Three-Substructures"><a href="#The-Three-Substructures" class="headerlink" title="The Three Substructures"></a>The Three Substructures</h3><p>预测子模型具有相似的结构如下图，模型输入关系向量与实体向量后，进入n层全连接层，得到预测向量，再经过一个sigmoid（或者softmax）层，输出预测标签向量。<img src="http://image.nysdy.com/20190419155565925349707.png" alt="20190419155565925349707.png"></p><p>头实体预测目标函数：<img src="http://image.nysdy.com/20190419155565929066802.png" alt="20190419155565929066802.png"></p><p>f(x)= max(0,x).</p><p>预测标签：<img src="http://image.nysdy.com/20190419155565936981620.png" alt="20190419155565936981620.png"></p><p>其它两种与此头实体类似。</p><h3 id="Model-Training"><a href="#Model-Training" class="headerlink" title="Model Training"></a>Model Training</h3><h4 id="The-General-Loss-Function"><a href="#The-General-Loss-Function" class="headerlink" title="The General Loss Function"></a>The General Loss Function</h4><p>模型目标标签向量表示为：<img src="http://image.nysdy.com/20190419155565949172586.png" alt="20190419155565949172586.png"></p><p>$I_h$是在训练集中给定r和t的所有有效头实体集。</p><p>三者的平滑向量表示为：<img src="http://image.nysdy.com/20190419155565967448577.png" alt="20190419155565967448577.png"></p><p>三个预测任务的损失函数为：<img src="http://image.nysdy.com/20190419155565972110097.png" alt="20190419155565972110097.png"></p><p>总损失函数为：<img src="http://image.nysdy.com/20190419155565974814392.png" alt="20190419155565974814392.png"></p><h4 id="The-Adaptively-Weighted-Loss-Mechanism"><a href="#The-Adaptively-Weighted-Loss-Mechanism" class="headerlink" title="The Adaptively Weighted Loss Mechanism."></a>The Adaptively Weighted Loss Mechanism.</h4><p>该方法的动机：</p><ul><li>在知识图谱中的三元组有4种类型：1-TO-1, 1-TO-M, M-TO-1 and M-TO-M。所以预测在训练集中具有的有效实体/关系越多，它就越不确定。所以作者将对应于头部实体预测，关系预测和尾部实体预测的损失的权重与有效实体的数量相关联。</li><li>因为关系预测比实体预测更加容易。所以作者加大对头尾实体的错误预测的惩罚。</li></ul><p>所以作者得到新的损失函数：<img src="http://image.nysdy.com/20190419155566013038361.png" alt="20190419155566013038361.png"></p><p>总损失函数变为：<img src="http://image.nysdy.com/20190419155566016395908.png" alt="20190419155566016395908.png"></p><h2 id="THE-SENN-METHOD-1"><a href="#THE-SENN-METHOD-1" class="headerlink" title="THE SENN+METHOD"></a>THE SENN+METHOD</h2><p>作者相信可以进一步利用关系预测的相当好的性能来辅助测试过程中的头部和尾部实体预测。</p><p>给定头部预测任务（？，r，t）并假设h是有效的头部实体。 如果我们采用SENN方法来预测h和t之间的关系，即执行关系预测任务（h，？，t），则关系r最有可能具有 预测标签高于其他关系，因此应排名高于其他关系。</p><p><img src="http://image.nysdy.com/20190419155566073220975.png" alt="20190419155566073220975.png"></p><p>其中Value（x，r）返回对应于关系r的向量x的条目; Rank（x，r）以降序返回对应于关系r的向量x的条目的等级。</p><p>最后SENN+种预测标签为：<img src="http://image.nysdy.com/20190419155566089424123.png" alt="20190419155566089424123.png"></p><p>其中<img src="http://image.nysdy.com/20190419155566090898589.png" alt="20190419155566090898589.png"></p><h2 id="EXPERIMENTS"><a href="#EXPERIMENTS" class="headerlink" title="EXPERIMENTS"></a>EXPERIMENTS</h2><h3 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h3><p><img src="http://image.nysdy.com/20190419155566095994824.png" alt="20190419155566095994824.png"></p><h3 id="Entity-Prediction"><a href="#Entity-Prediction" class="headerlink" title="Entity Prediction"></a>Entity Prediction</h3><p><img src="http://image.nysdy.com/20190419155566101939979.png" alt="20190419155566101939979.png"></p><p><img src="http://image.nysdy.com/20190419155566104394441.png" alt="20190419155566104394441.png"></p><h3 id="Relation-Prediction"><a href="#Relation-Prediction" class="headerlink" title="Relation Prediction"></a>Relation Prediction</h3><p><img src="http://image.nysdy.com/20190419155566106385514.png" alt="20190419155566106385514.png"></p><p>论文还进行了共享嵌入和自适应权重损失机制有效性的验证。</p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="http://blog.openkg.cn/%E8%AE%BA%E6%96%87%E6%B5%85%E5%B0%9D-%E9%9D%A2%E5%90%91%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E8%A1%A5%E5%85%A8%E7%9A%84%E5%85%B1%E4%BA%AB%E5%B5%8C%E5%85%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">http://blog.openkg.cn/%E8%AE%BA%E6%96%87%E6%B5%85%E5%B0%9D-%E9%9D%A2%E5%90%91%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E8%A1%A5%E5%85%A8%E7%9A%84%E5%85%B1%E4%BA%AB%E5%B5%8C%E5%85%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;http://delivery.acm.org/10.1145/3280000/3271704/p247-guan.pdf?ip=59.64.129.243&amp;amp;id=3271704&amp;amp;acc=ACTIVE%20SERVICE&amp;amp;key=BF85BBA5741FDC6E%2E66A15327C2E204FC%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;amp;__acm__=1555657159_db1582f1a6ea923a16011064e5cc7955&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;原文下载链接&lt;/a&gt;，知识图谱补全（KGC，Knowledge Graph Completion)是一种自动建立图谱内部知识关联的工作。目标是补全知识图谱中三元组的缺失部分。主要方法为基于张量（或者矩阵）和基于翻译两类。在本文中，作者提出了一种基于共享嵌入的神经网络的模型（SENN）来处理KGC。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="知识图谱" scheme="http://yoursite.com/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    
      <category term="神经网络" scheme="http://yoursite.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="知识图谱补全" scheme="http://yoursite.com/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E8%A1%A5%E5%85%A8/"/>
    
  </entry>
  
</feed>
