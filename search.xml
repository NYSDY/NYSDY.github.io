<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Graph Neural Networks with Generated Parameters for Relation Extraction阅读笔记]]></title>
    <url>%2Fpost%2FGraph_Neural_Networks_with_Generated_Parameters_for_Relation%2F</url>
    <content type="text"><![CDATA[本文将GNNs应用到处理非结构化文本的（多跳）关系推理任务来进行关系抽取。采用从句子序列中获取的实体构建全链接图，应用编码（sequence model），传播（节点间信息）和分类（预测）三个模块来处理关系推理。本文提供了三个数据集。 problem statement existing relation extraction models fail to infer the relationship without multi-hop relational reasoning. existing GNNs can’t process multi-hop relational reasoning in natural language relational reasoning research objectiveenable GNNs to porcess relational reasoning on unstructed text inputs contribution extend a GNN with generated parameters, which could be applied to process relational reasoning on unstructured inputs verify GP-GNNs in the taks of relation extraction from text; present three datasets GP-GNNs construct a fully-connected graph with the entities in the sequence of text employs three models to process relational reasoning an encoding modul: enable edges to encode rich information from natural languages a propagation modul: propagates realtional information among various nodes a classification modul: make prediction with node representations As compared to tradtional GNNs, GP-GNNs could learn edges’ parameters from natural lanuages Related workGraph Neural Networks(GNNs) existing models still perfom message-passing on predefined graphs Learning Graphical State Transitions is most related introduecs a nove lnerual architecture to generate a graph based on the textal input dynamically update the relationship during the learning process relational reasoning existing models could not make full use of the multi-hop inference patterns among multiple entity pair and their relaitons within the sentence LEARNING GRAPHICAL STATE TRANSITIONS is the most related work the proposed model incorporates contextual relations with attention mechanism when predicting the relation of a target entity pair Graph Neural Network with Grenerated Parameters(GP-GNNs)The picture is overall architecture: encoding module, propagation module and classification module Encoding Moduleformula: \mathcal{A}_{i, j}^{(n)}=f\left(E\left(x_{0}^{i, j}\right), E\left(x_{1}^{i, j}\right), \cdots, E\left(x_{l-1}^{i, j}\right) ; \theta_{e}^{n}\right)where $f(\cdot)$ could be any model that could sequential(such as LSTMs); $E(\cdot)$ indicates an embedding function. $x^{i, j}$ is the word in sentence labeled( $i,j$) Porpagation Modulethe representations of layer n + 1 are calculated by: \mathbf{h}_{i}^{(n+1)}=\sum_{v_{j} \in \mathcal{N}\left(v_{i}\right)} \sigma\left(\mathcal{A}_{i, j}^{(n)} \mathbf{h}_{j}^{(n)}\right)where $\mathcal{N}\left(v_{i}\right)$ denotes the neighbors of node $v_i$ Classification Modulethe loss of GP-GNNs: \mathcal{L}=g\left(\mathbf{h}_{0 :|\mathcal{V}|-1}^{0}, \mathbf{h}_{0 :|\mathcal{V}|-1}^{1}, \ldots, \mathbf{h}_{0 :|\mathcal{V}|-1}^{K}, Y ; \theta_{c}\right)Relation Extraction with GP-GNNsAuthors introduce how to apply GP-GNNs to relation extraction Encoding Moduleencoding then context of entity pairs (or edges in the graph) E\left(x_{t}^{i, j}\right)=\left[\boldsymbol{x}_{t} ; \boldsymbol{p}_{t}^{i, j}\right]where $x_t$ denotes the word embedding; $\boldsymbol{p}_{t}^{i, j}$denotes the position embedding of word posistion t relative to the entity pair’s position $i, j$. position embeddingwe mark each token in the sentence as either belonging to the first entity $v_i$, the second entity $v_j$ or to neither of those Propagation Module the formula is the same as the front The Initial Embeddings of Nodes when extracting the relationship between entity $v_i$ and entity $v_j$, the initial embeddings of them are annotated as $\mathbf{h}_{v_{i}}^{(0)}=a_{\text { subject }}$, and $h_{v_{j}}^{(0)}=a_{\text { object }}$, while the intial embeddings of other entities are set to all zeros. In our experiments, we generalize the idea of Gated Graph Neural Networks (Li et al., 2016) by setting $a_{\text { subject }}=[1 ; 0]^{\top}$and $a_{\text { object }}=[0 ; 1]^{\top}$. classification ModuleAs the target entity pair $(v_i, v_j)$: \boldsymbol{r}_{v_{i}, v_{j}}=\left[\left[\boldsymbol{h}_{v_{i}}^{(1)} \odot \boldsymbol{h}_{v_{j}}^{(1)}\right]^{\top} ;\left[\boldsymbol{h}_{v_{i}}^{(2)} \odot \boldsymbol{h}_{v_{j}}^{(2)}\right]^{\top} ; \ldots ;\left[\boldsymbol{h}_{v_{i}}^{(K)} \odot \boldsymbol{h}_{v_{j}}^{(K)}\right]^{\top}\right]where $\odot$ represents element-wise multiplication classification: \mathbb{P}\left(r_{v_{i}, v_{j}} | h, t, s\right)=\operatorname{softmax}\left(M L P\left(\boldsymbol{r}_{v_{i}, v_{j}}\right)\right)loss: \mathcal{L}=\sum_{s \in S} \sum_{i \neq j} \log \mathbb{P}\left(r_{v_{i}, v_{j}} | i, j, s\right)Experimentsaim showing their best models could improve the performance of relation extraction under a variety of settings illlustrating that how the number of layers affect the performance of their model performing a qualitiative investigation to highlight the diference between their models and baseline models designas the first and second aim show that our models could improve instance-level relation extraction on a human annotated test set we will show that our models could also help enhance the performance of bag-level relation extraction on a distantly labeled test set split a subset of distantly labeled test set, where the number of entities and edges is large Datasetdistantly label set Sorokin and Gurevych (2017) proposed modify their dataset added reversed edges for all of the entity pairs with no relations, added “NA” labels to them Human annotated test set Sorokin and Gurevych (2017) select the distantly lablel pairs which all 5 annotaters are accepted. There are 350 sentences and 1,230 triples in this test set Dense distantly labeled test set criteria the number of entities should be strictly larger than 2 there must be at least one circle (with at least three entities) in the ground-truth label of the sentence There are 1,350 sentences and more than 17,915 triples and 7,906 relational facts in this test set. Models for comparison Context-aware RE Multi-Window CNN PCNN LSTM or GP-GNN with K = 1 layer GP-GNN with K = 2 or K = 3 layerss Evaluation DetailsTo evaluation models in bag-level: E\left(r | v_{i}, v_{j}, S\right)=\max _{s \in S} \mathbb{P}\left(r_{v_{i}, v_{j}} | i, j, s\right)result: Effectiveness of Reasoning Mechanism Context-Aware RE may introduce more noise, for it may mistakenly increase the probability of a relation with the similar topic with the context relations sentences from Wikipedia corpus are always complex, which may be hard to model for CNN and PCNN The Effectiveness of the Number of Layers the improvement of the third layer is much smaller on the overall distantly supervised test set than the one on the dense subset This observation reveals that the reasoning mechanism could help us identify relations especially on sentences where there are more entities as the number of layers grows, the curves get higher and higher precision, indicating considering more hops in reasoning leads to better performance Qualitative Results: Case Study Context-Aware RE makes a mistake by predicting (Kentucky, share boarder with, Ohio). As we have discussed before, this is due to its mechanism to model co-occurrence of multiple relations 思考文章是刘知远组的论文，针对的方向是关系抽取，在其中结合了关系推理，最近许多任务都在结合推理的思想。文章整体的结构，逻辑十分清晰，论述的也比较详细，属于标准论文。感觉文章中GP-GNNs结构图还可以画的更好一点，展现一下encoding module的层，可以更好理解。文章的精髓应该是这个propagation module的部分，还需要消化一下，不过这部分可能是有先前的知识支撑的。]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>GNNs</tag>
        <tag>relation extraction</tag>
        <tag>relation reasoning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[allennlp安装踩坑]]></title>
    <url>%2Fpost%2Fallennlp_install%2F</url>
    <content type="text"><![CDATA[安装allennlp的踩坑之路，踩了不少坑最后选择’Installing from source’的安装方法，排坑后下面方法亲测可用 Installing from source安装步骤： 1.下载GitHub文件1git clone https://github.com/allenai/allennlp.git 2.创建conda环境1conda create -n allennlp python=3.6 3.激活环境下载依赖文件 激活环境 1source activate allennlp 进入github上下载的文件夹 下载依赖文件 1pip install -r requirements.txt 遇到报错问题，参考下一小节，所欲问题解决。 4.安装allennlp1pip install --editable . 5.测试1allennlp 成功后效果如下： 123456789101112131415161718192021222324252627$ allennlp2019-05-22 21:58:42,297 - INFO - pytorch_pretrained_bert.modeling - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .usage: allennlpRun AllenNLPoptional arguments: -h, --help show this help message and exit --version show program's version number and exitCommands: configure Run the configuration wizard. train Train a model. evaluate Evaluate the specified model + dataset. predict Use a trained model to make predictions. make-vocab Create a vocabulary. elmo Create word vectors using a pretrained ELMo model. fine-tune Continue training a model on a new dataset. dry-run Create a vocabulary, compute dataset statistics and other training utilities. test-install Run the unit tests. find-lr Find a learning rate range. print-results Print results from allennlp serialization directories to the console. 遇到的问题问题1报错信息：ERROR: Failed building wheel for jsonnet 解决方法：1conda install -c conda-forge jsonnet 问题2报错信息：报的都是某些包的版本问题 1234ERROR: botocore 1.12.152 has requirement urllib3&lt;1.25,&gt;=1.20; python_version &gt;= "3.4", but you'll have urllib3 1.25.2 which is incompatible.ERROR: aws-sam-translator 1.11.0 has requirement jsonschema~=2.6, but you'll have jsonschema 3.0.1 which is incompatible.ERROR: cfn-lint 0.20.3 has requirement jsonschema~=2.6, but you'll have jsonschema 3.0.1 which is incompatible.ERROR: cfn-lint 0.20.3 has requirement requests&lt;=2.21.0,&gt;=2.15.0, but you'll have requests 2.22.0 which is incompatible 解决方法根据报错信息下载相应安装包即可 问题3报错信息：1234567891011121314ImportError: dlopen: cannot load any more object with static TLS___________________________________________________________________________Contents of /home/minelab/anaconda3/envs/allennlp/lib/python3.6/site-packages/sklearn/__check_build:__init__.py setup.py _check_build.cpython-36m-x86_64-linux-gnu.so__pycache_____________________________________________________________________________It seems that scikit-learn has not been built correctly.If you have installed scikit-learn from source, please do not forgetto build the package before using it: run `python setup.py install` or`make` in the source directory.If you have used an installer, please check that it is suited for yourPython version, your operating system and your platform. 解决方法：下载更低版本的scikit-learn,例如 1pip install scikit-learn=0.20.3 参考链接 https://github.com/pytorch/pytorch/issues/10443 https://github.com/pypa/pip/issues/4330 安装的启示环境问题 最基本的就是先去网上查这个错误的解决方法 网上的解决不了的，先猜猜大概率是哪方面的问题。 比如大概率是各种版本互相之间不适配的问题，那就调试版本，一般都会告诉你哪个有问题，比如上面的scikit-learn问题。]]></content>
      <categories>
        <category>install</category>
      </categories>
      <tags>
        <tag>allennlp</tag>
        <tag>包安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Triple Trustworthiness Measurement for Knowledge Graph阅读笔记]]></title>
    <url>%2Fpost%2FTriple%20Trustworthiness%20Measurement%20for%20Knowledge%20Graph%2F</url>
    <content type="text"><![CDATA[本文提出了一种通过计算triple trustworthiness来评估知识图谱的准确程度的方法。模型利用神经网络综合来自实体（借鉴Resource allocation）、关系（借鉴翻译模型的思想，如TransE）和KG全局（借鉴关系路径，RNN）三个层面的语义和全局信息，输出最后的 trustworthiness作为判断依据。 下载地址 SummaryThis paper proposed a method for estimating the accuracy of a knowledge graph by computing triple trustworthiness. The model uses neural network to synthesize semantic and global information from three levels: entity(resource allocation), relationship(translation model ideas, such as TransE)m and KG global(relationship path, RNN) and outputting the final trustworthiness as the basis for judgment. Problem statementpossible noises and conflicts are inevitably intoduced in the process of constructing the KG research objectivequantify the KG’s semantic correctness and the true degree of the facts expressed Contribution Knowledge graph triple trustworthiness measurement use the triple semantic information and globally inferring information three levels measurement and an intergration of confidence value experiment result verified the model valid on large-scale KG Freebase the KGTtm could be utilized in knowledge graph construction or improvement THE TRIPLE TRUSTWORTHINESS MEASUREMENT MODEL Longitudinally, the model can be divided into two level. the upper is a pool of multiple trustworthiness estimate cells(estimator) the output of these Estimator forms the input of lower-level fusion device(Fusioner) Viewed laterally, three progressive levels are be considered, as following. Is there a possible relationship between the entity pairs? ResourceRank: The algorithm assumes that the association between entity paires $(h,t)$ will be stronger, and more resource is passed from the head $h$ through all associated paths to the tail $t$ in a graph The amount of resource aggregated into $t$ ingeniously indicateds the association strength from $h$ to $t$. As pair $(e_1,e_2)$, there only one directed edge from $e_1$ to $e_2$ in the graph, where the different bandwidth of the edge indicates the number of the multiple relations. output: \left\{\begin{array}{c}{u=\alpha\left(W_{1} V+b_{1}\right)} \\ {R R(h, t)=W_{2} u+b_{2}}\end{array}\right.Authors constructed a $V$ vector by combining six characteristics. R (t | h); In-degree of head node ID(h); Out-degree of head node OD(h); In-degree of tail node ID(t); Out-degree of tail node OD(t); The depth from head node to tail node Dep As for 1. the formula: R(t | h)=(1-\theta) \sum_{e_{i} \in M_{t}} \frac{R\left(e_{i} | h\right) \cdot B W_{e_{i} t}}{O D\left(e_{i}\right)}+\frac{\theta}{N} $M_t$is the set of all nodes that have outgoing links to the node $t$, $OD (e_i)$ is the out-degree of the node eiand the $BW_{e_it}$ is the bandwidth from the $e_i$ to $t$. In order to improve the model fault-tolerance, we assume that the resource fow from each node may directly jump to a random node with the same probability θ Can the determined relationship $r$ occur between the entity pair $(h,t)$ ? Translation-based energy function (TEF)：depended on TransE $E(h, r, t)=|\mathbf{h}+\mathbf{r}-\mathbf{t}|$ output: P(E(h, r, t))=\frac{1}{1+e^{-\lambda\left(\delta_{r}-E(h, r, t)\right)}}Can the relevant triples in the KG infer that the triple is trustworthy? Reachable paths inference (RPI): There two challenges to exploit the reachable paths for inferring triple trustworthiness: reachable paths selectionSemantic distance-based path selection Reachable Paths Representationusing a RNN to deal with the embeddings of the three elements of each triple in the selected path Fusing the Estimatorsa classifer based on a multi-layer perceptron EXPERIMENTSdatasetFB15K Interpreting the Validity of the Trustworthiness The left picture shows that the positives examples are mainly concentrated in the upper region, vice versa. As for the right picture only if the value of a triple is higher than the threshold can it be considered trustworthy shows that the positive examples universally have higher confidence values Comparing With Other Models on The Knowledge Graph Error Detection Task Authors’ model has beter results in terms of accuracy and the F1-score than the other models. Analyzing the ability of models to tackle the three type noises. a higher recall shows that authors’ model can more accurately find the right from noisy triples higher average trustworthiness values show that authors’ model can better identify the correct instances and with high confidence the worst among the $(h, ?, t)$, because the various relations between a certain entity increase the difficulty of model judgment. Analyzing the Efects of Single Estimators It can be found that the accuracy obtained by each model is above 0.8, which proves the effectiveness of each Estimator 思考本文在方法上几乎没有什么创新，本质上就是一个老方法的多个组合。最大亮点就是作者能提出trustworthiness来把这个评价知识图谱准确度的问题进行了量化。这种能力比提出方法上的创新更加厉害，也是需要学习的地方。]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>Knowledge Graph</tag>
        <tag>KG</tag>
        <tag>Triple</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GloVe: Global Vectors for Word Representation阅读笔记]]></title>
    <url>%2Fpost%2FGloVe%3AGlobal%20Vectors%20for%20Word%20Representation%2F</url>
    <content type="text"><![CDATA[论文下载地址，GloVe是一个新的全球对数双线性回归模型，属于经典的词向量表示方法之一。 Introductionevaluate the intrinsic quality Most word vector methods rely on the distance or angle between pairs of word vectors Mikolov et al. (2013c) introduced word analogies that examines word vector’s various dimensions of difference. two main model families for learning vectors: global matrix factorization methods local context window methods Authors propose a specific weighted least squares model that trains on globla word-word co-occurrence counts and thus makes efficient use of statistics. Related WorkMatix Facroization MethodsThese methods utilize low-rank approximations to decompose large matrices that capture statistical information about a corpus. shortcomingthe most frequent words contribute a dispropoertionate amount to the similarity measure. Shallow Window-Based MethodsAnother approach is to learn word representations that aid in making predictins within local context windows. shortcomingdo not operate directly on the co-occurrence statistics of the corpus and fails to take advantage of the vast amount of repetition in the data. The GloVe ModelGloVe: Global Vectorsthe global corpus statistics are captured directly by the model the question about the model using the statistics of word occurrences in a corpus how meaning is generated from these statistics how the resulting word vectors might represent that meaning some notation$X_{ij}$ : the number of times word j occurs in the context of word i $X_i = \sum_{k} X_{i k}$ : the number of times any word appears in the context of word i $P_{i j}=P(j | i)=X_{i j} / X_{i}$: the probability that word j appear in the context of word i above that, werd vector learning should be with ratios of co-occurrence probabilities: $w \in \mathbb{R}^{d}$are word vectors and $\tilde{w} \in \mathbb{R}^{d}$are separate context word vectors For F, we should select a unique choice by enforcing a few desiderata. encoding the information present the ratio $P_{i k} / P_{j k}$ in the word vector space. Since vector spaces are inherently linear structures put F to be a compicated function parameterized, and avoiding bofuscating the linear structure the word-word co-occurrence matrices, we can exchange a word and a context word(because a word can also be a context word) F should be a homomorphism by Eqn.(3) F = exp or the Eqn(6) would have the exchange symmetry if not $\log \left(X_{i}\right)$ and $\log \left(X_{i}\right)$ is independent of k, so it can be absorbed into a bias $b_i$ for avoiding diverge, $\log \left(X_{i k}\right) \rightarrow \log \left(1+X_{i k}\right)$ a new weighted least squares regression model to address the problem that LSA wirhts all co-occuttences equally. cost function: Relationship to Other ModelsIn this subsection authors show how these models are related to their proposed model. the defect of cross entropy it has the unfortunate property that distributions with long tails are often modeled poorly with too much wieght given to the unlikely events. Complexity of the modelthe computational complexity of the model depends on the number of nonzero elects in the matrix $X$ some assumptions about the distribution of word co-occurrences the number of co-occurrences of word $i$ with word $j$, $X_{ij}$, can be modeled as a power-law function of the frequency rank of that word pair, $r_{ij}$: $X_{i j}=\frac{k}{\left(r_{i j}\right)^{\alpha}}$ ExperimentsEvaluation methodsauthors conduct experiments on the word analogy taks of Mikolov et al. (2013a) Word analogiesThe word analogy task consists of questions like, “a is to b as c is to ?” Word similarity Named entity recognitionResultsTable 2 shows the CloVe model performs significantly better than the other baslines, often with smaller vector sizes and smaller corpora. Table 3 shows results on five different word similarity datasets. Table 4 shows results on the NER task with the CRF-based model. Model Analysis: Vector Length and Context Size Model Analysis: Corpus Size On the syntactic subtask, larger corpora typically produce better statistics so that there is a monotonic increase in performance as the cor- pus size increases. But the same trend is not true for the semantic subtask, which is probably because of analogy dataset Model Analysis: Run-time Model Analysis: Comparison with word2vecFor the same corpus, vocabulary, window size, and training time, GloVe consistently outperforms word2vec 参考链接 https://blog.csdn.net/coderTC/article/details/73864097]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>word vector</tag>
        <tag>GloVe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep contextualized word representations 阅读笔记]]></title>
    <url>%2Fpost%2FDeep%20contextualized%20word%20representations%2F</url>
    <content type="text"><![CDATA[论文下载地址，ELMo事先用语言模型学好一个单词的 Word Embedding，此时多义词无法区分，不过这没关系。在我实际使用 Word Embedding 的时候，单词已经具备了特定的上下文了，这个时候我可以根据上下文单词的语义去调整单词的 Word Embedding 表示，这样经过调整后的 Word Embedding 更能表达在这个上下文中的具体含义，自然也就解决了多义词的问题了。所以 ELMO 本身是个根据当前上下文对 Word Embedding 动态调整的思路。 IntroductionELMo(Embedddings from Language Models):why call ELMo:Using vectors derived from a bidirectional LSTM that is trained with a coupled language model(LM) objective on a large text corups. characteristics ELMo representations are a function of all of the internal layers of the biLM. learn a linear combination of the vectors stacked above each input word for each end task the higher-level LSTM states capture context-dependent aspects of word meaning the lower-level states model aspects of syntax Extensive experiments EMLo representations can be easily added to existing models improve the state of art in every case ELMo outperform those derived from just the top layer of a LSTM Related work Some approaches for learning word vectors only allow a single context-independent representation for each word. to overcome some shortcomings of traditional word vectors: enriching them with subword information learning separate vectors for each word sense Authors uses subword units through the use of character convolutions, seamlessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes. context-depends representations Authors take full advantage of access to plentiful monolingual data Previous work also shown that different layers of deep biRNNs encode different types of information introducing multi-task syntactic supervision at the lower levels of a deep LSTM can improve overall performance of higher level tasks the top layer of an LSTM for encoding word context (Melamud et al., 2016) has been shown to learn representations of word sense. ELMo representations can also induce similar signals. ELMo: Embeddings from Language ModelsBidirectional language models model the probability of token $t_k$ given the history($t_1, … , t_{k-1}$): a backward LM: Authors’ formulation jointly maximizes the log likelihood of the forward and backward directions: ELMo For each token $t_k$, a L-layer biLM computes a set of 2L + 1 representations: For a downstream model, ELMo collapses all layers in R into a single vector. In the simplest case, ELMo just selects the top layer. For a task specific weighting of all biLM layers: $s^{task}$ are softmax-normalized weithts and the scalar parameter $γ^{task}$ allows the task model to scale the entire ELMo vector Using biLMs for supervised NLP tasks Given a pre-trained biLM and a supervised architecture for a target NLP task let the end task model learn a linear combination of these representations consider the lowest layers of th supervised model without the biLM add ELMo to the supervised model freeze the weights of the biLM concatenate the ELMo vector $ELMo^{task}_k$ with $x_k$ and pass the ELMo enhanced representation $[x_k,;ELMo^{task}_k ]$ into the task RNN. for some tasks, authors also include ELMo ar the output of task RNN by introducing another set of out put specific linear weights and replacing $h_k$ with $[h_k,;ELMo^{task}_k ]$ add a moderate amount of dropout to ELMo and in some case to regularize the ELMo weights Pre-trained bidirectional language model architecture the biLM provides three layers of representations for each input token, both directions and a residual connection between LSTM layers fine tuning the biLM on domain specific data Evaluationthe following picture shows the performance of ELMo in Question answering, Textual entailment, Semantic role labeling, Corefrence resolution, Named entity extraction, Sentiment analysis. In every task considered, simply adding ELMo establishes a new state-of-the-art result. AnalysisAlternate layer weighting schemes the following picture compares these alternatives. Including representations from all layers improves overall performance over just using the last layer, and including contextual representations from the last layer improves performace over the baseline. Also shows the $\lambda$ is important. Where to include ELMo?The ELMo can be included in both the input and output. the results show including the ELMo in both input and output can preform better. What information is captured by the biLM’s representations?Intuitively, the biLM must be disambiguating the meaning of words using their context. The GloVe can only capure the speech. but the biLM is able to disambiguate both the part of speech and word sense in the source sentence. Word sense disambiguationgiven a sentence, predicting the sense of a target word using a simple 1-nearst negihbor approach POS taggingto examine whether the biLM captures basic syntax. Sample efficiencyAdding ELMo to a model increases the sample efficiency considerably, both in terms of number of parameter updates to reach state-of-the-art performance and the overall training set size. Visualization of learned weights 参考链接 NAACL2018:高级词向量(ELMo)详解(超详细) 经典，这篇文章中阐述了一些使用的细节，并用图来表示，更加清晰。 ELMo算法介绍，这篇博客中自己对整个论文的概述和总结和好，需要学习。]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>embedding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Efficient Estimation of Word Representations in Vector Space》阅读笔记]]></title>
    <url>%2Fpost%2FEfficient%20Estimation%20of%20Word%20Representations%20in%20Vector%20Space%2F</url>
    <content type="text"><![CDATA[论文下载地址，该篇论文的大篇幅都在讨论实验结果的分析，模型的部分比较简单，没有详细分析，本来是想读一下CBOW和skip-gram的原始论文，发现并没有想象中的那么大的用处。 Goals of paper 开发了两种新模型，并保留了单词之间的线性规律 设计了一个新的综合测试集，用于测量句法和语义规律 讨论了训练时间和准确性如何取决于单词向量的维度和训练数据的数量 Model Architectures训练复杂度： 其中，E是训练次数，T是训练集单词数量，Q是模型结构。 Feedforward Neural Net Language Model (NNLM)它由输入，映射，隐藏和输出层组成。通过简化方法，Q= N x D x H Recurrent Neural Net Language Model (RNNLM)克服了模型需要固定的上下文长度的问题，并且只有输入，隐藏和输出层。 Q= H x H + H x V，其中H = D（单词表示），H x V 可以通过分级softmax被简化为H x log_2(V)。所以主要的复杂度来自于H x H。 Parallel Training of Neural Networks模型使用的DistBelief框架允许我们并行运行同一模型的多个副本，每个副本通过集中的服务器同步其梯度更新，该服务器保留所有参数 New Log-linear Models大多数复杂性是由于模型中的非线性隐藏层引起的。模型结构如下： Continuous Bag-of-Words Model(CBOW)第一个提出的体系结构类似于前馈NNLM，其中去除了非线性隐藏层，并且所有单词（不仅仅是投影矩阵）共享投影层。 因此，所有单词都被投射到相同的位置（它们的向量被平均）。 将这个架构称为词袋模型，因为历史中的单词顺序不会影响投影。 模型的复杂度：Q = N × D + D × log_2(V ) Continuous Skip-gram Model基于同一句子中的另一个单词最大化单词的分类。 更准确地说，使用每个当前单词作为具有连续投影层的对数线性分类器的输入，并预测当前单词之前和之后的特定范围内的单词。 模型的复杂度：Q = C × (D + D × log2(V ))，其中C是单词的最大距离。 实验任务描述为了度量词向量的质量，我们定义了一个复杂的测试集，它包括了五种类型的语义问题。九个类型的句法问题。包括每个类别的两个样本集在上表展示；总之，共拥有8869个语义问题和10675个句法问题 作者通过：最大化精确度 ，模型体系结构的比较，模型的大规模并行训练来证明提出模型的运速度和精确的优势。]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>word2vec</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shared Embedding Based Neural Networks for Knowledge Graph Completion阅读笔记]]></title>
    <url>%2Fpost%2FShared%20Embedding%20Based%20Neural%20Networks%20for%20Knowledge%20Graph%20Completion%2F</url>
    <content type="text"><![CDATA[原文下载链接，知识图谱补全（KGC，Knowledge Graph Completion)是一种自动建立图谱内部知识关联的工作。目标是补全知识图谱中三元组的缺失部分。主要方法为基于张量（或者矩阵）和基于翻译两类。在本文中，作者提出了一种基于共享嵌入的神经网络的模型（SENN）来处理KGC。 Contribulation 提出了SENN模型，该模型明确区分头实体、关系和为实体预测任务，并把它们整合到一个基于全连接神经网络框架中，该框架共享的实体和关系嵌入。 SENN提出了一个自适应全中损失机制，该方法可以很好的处理具有不同映射属性的三元组，并处理不同的预测任务。 由于关系预测通常比头尾实体预测具有更好的性能，我们把SENN应用到头尾实体预测，从而将SENN扩展到SENN+。 Related worksTensor/Matrix Based MethodsRESCAL是一个典型的方法，该方法基于三向张量因子分解的方法。 目标函数为： $M_r$是r的关系矩阵，大小为k x k。 ComlEx是最近提出的方法，该方法基于矩阵分解，并且它使用复数值来定义实体和关系的嵌入。 目标函数为： Re(x)返回x的实部。 Translation Based Methods代表模型为经典的TransE模型（这里不再赘述） Translation Based MethodsER-MLP使用多层感知器来捕获头实体，关系和尾实体之间的隐式交互。 目标函数为： ProjE使用具有组合层和投影层的神经网络来对头尾实体预测建模。 THE SENN METHOD模型结构如图所示： 作者将框架划分为以下四个部分： 三元组的批量预处理 知识图谱的Shared embeddings表示学习 独立的头尾实体及关系预测子模型训练与融合 联合损失函数构成 整个KGC的流程可以描述如下： 将训练数据中的完整三元组（知识图谱）划分批量后作为模型的输入 对于输入的三元组，分别训练得到实体（包括头尾实体）嵌入矩阵与关系嵌入矩阵（embeddings） 将头尾实体及关系embeddings分别输入到三个预测模型中（头实体预测（?, r, t），关系预测(h, ?, t)，尾实体预测(h, r, ?)） The Three Substructures预测子模型具有相似的结构如下图，模型输入关系向量与实体向量后，进入n层全连接层，得到预测向量，再经过一个sigmoid（或者softmax）层，输出预测标签向量。 头实体预测目标函数： f(x)= max(0,x). 预测标签： 其它两种与此头实体类似。 Model TrainingThe General Loss Function模型目标标签向量表示为： $I_h$是在训练集中给定r和t的所有有效头实体集。 三者的平滑向量表示为： 三个预测任务的损失函数为： 总损失函数为： The Adaptively Weighted Loss Mechanism.该方法的动机： 在知识图谱中的三元组有4种类型：1-TO-1, 1-TO-M, M-TO-1 and M-TO-M。所以预测在训练集中具有的有效实体/关系越多，它就越不确定。所以作者将对应于头部实体预测，关系预测和尾部实体预测的损失的权重与有效实体的数量相关联。 因为关系预测比实体预测更加容易。所以作者加大对头尾实体的错误预测的惩罚。 所以作者得到新的损失函数： 总损失函数变为： THE SENN+METHOD作者相信可以进一步利用关系预测的相当好的性能来辅助测试过程中的头部和尾部实体预测。 给定头部预测任务（？，r，t）并假设h是有效的头部实体。 如果我们采用SENN方法来预测h和t之间的关系，即执行关系预测任务（h，？，t），则关系r最有可能具有 预测标签高于其他关系，因此应排名高于其他关系。 其中Value（x，r）返回对应于关系r的向量x的条目; Rank（x，r）以降序返回对应于关系r的向量x的条目的等级。 最后SENN+种预测标签为： 其中 EXPERIMENTSDatasets Entity Prediction Relation Prediction 论文还进行了共享嵌入和自适应权重损失机制有效性的验证。 参考链接 http://blog.openkg.cn/%E8%AE%BA%E6%96%87%E6%B5%85%E5%B0%9D-%E9%9D%A2%E5%90%91%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E8%A1%A5%E5%85%A8%E7%9A%84%E5%85%B1%E4%BA%AB%E5%B5%8C%E5%85%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
        <tag>神经网络</tag>
        <tag>知识图谱补全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Bootstrapping Entity Alignment with Knowledge Graph Embedding》阅读笔记]]></title>
    <url>%2Fpost%2FBootstrapping%20Entity%20Alignment%20with%20Knowledge%20Graph%20Embedding%2F</url>
    <content type="text"><![CDATA[论文下载地址，采用了bootstrapping方法来解决缺乏训练数据的过程，提出了截断均匀负采样来提高负样例对于目标函数的贡献，采用基于限制的目标函数来按需调整正负样例的得分。 基于嵌入的实体对齐将不同的知识图谱（KG）表示为低维嵌入，并通过测量实体嵌入之间的相似性来查找实体对齐。其中，大量方法所面临的一个挑战是：缺乏足够的先前对齐作为标记的训练数据。 贡献 作者把实体对齐建模为一个分类问题，其基于KG嵌入来寻求最大化所有标记和未标记的实体对齐可能性 对于面向对齐的KG嵌入，作者提出了一种基于限制的目标函数；为了对不太可能区分的负三元组进行抽样，作者提出了一种截断均匀的负抽样方法。 作者提出了一个自举过程（bootstrapping）来克服缺乏足够训练数据，通过标记可能的对齐并迭代地将其添加到训练数据中来更新面向对齐的嵌入。 作者在三个跨语言和两个大型数据集上评估了所提出的方法，表明所提出的方法明显优于三种最先进的实体对齐方法。 问题描述最大似然准则指导选择实现最高对齐可能性的最佳θ 其中，L_x代表实体x的真实标签，1_[]是一个指示函数，表示给定命题的真值（0或1）。但是对于没有标签的实体，想要通过上述来得到theta就很困难。 模型面向对齐的KG嵌入作者提出了一个目标函数： 该目标函数有两个期望的属性： 预期正三元组得分较低，而负三元组得分较高。例如f(r)&lt;= r_1 并且 f(r’)&gt;=r_2，设置时r_2&gt;r_1,且r_1是一个小的正值。 仍然可以得到f(r’)-f(r)&gt;=r_2 - r_1，这表明所提出的目标函数仍然保留了基于边际排序损失的特征。 截断均匀负采样如果样例太容易区分，那么对整个的嵌入学习的贡献会很小。 所以，作者采用在嵌入空间中s最近的邻居作为候选集，剔除那些和实体x相似度过低的数据。 引导对齐（Bootstrapping Alignment）作者迭代地将可能的对齐标记作为训练数据，并使用它来进一步改进实体嵌入和对齐。 可能的对齐标签和编辑作者为了实现最大化对齐可能性并遵守一对一对齐约束，提出以下优化问题来标记第t次迭代： Y’_x = {y|y ∈ Y’ and π(y|x; Θ^(t)) &gt; γ3}代表标签x的候选集；ψ^(t)(·)是一个指示函数，只有当x在第t次迭代时标签为y时为1，其它情况为0。两个限制条件保证了一对一的标签。这时得到了一个新的标签对齐： 为了提高标签质量并满足一对一的对齐约束，在自举过程中，一旦被标记的实体可以在随后的标记中重新标记或变为未标记的实体。 当发生两个标签冲突时，我们通过计算下面的似然差异来确定保留哪个： 当该值大于0说明前者具有更大的对齐概率。 从整体角度学习为了获得标记和未标记实体的整理观察，作者定义了概率分布φx来描述所有x可能的概率分布。 由此，作者得到了最小化下面的似然函数来得到Θ： 因为，嵌入不仅应该捕获对齐可能性，还应该模拟KG的语义，所以作者最后定义联合目标函数： 实验数据集 DBP15K [Sun et al., 2017]包含三个跨语言数据集，这些数据集是从DBpedia的多语言版本构建的。DBPZH-EN(Chinese to English), DBPJA-EN(Japanese to English) and DBPFR-EN(French to English)每个数据集包含15，000个参考实体对齐。 DWY100K包含从DBpedia，Wikidata和YAGO3中提取的两个大型数据集，由DBP-WD和DBP-YG表示。 每个数据集都有10万个参考实体对齐 实验设置作者选取了三种最先进的基于嵌入的方法来实现实体对齐。 MTransE [Chen et al., 2017]，选取了第四种变体（表现最佳）。 IPTransE[Zhu et al., 2017]是一个迭代方法 JAPE [Sun et al., 2017]结合了实体对齐的关系和属性嵌入 AlignE面向对齐的KG嵌入模型的实现，具有截断的均匀负采样和参数交换，它优化了公式（3），但是没有自举 实验结果表2中我们观察到AlignE明显优于MTransE，IPTransE和JAPE，因为它采用面向对齐的嵌入。而BootEA显着改善了AlignE的结果，表明了自举的良好性能是由于其能够准确地将可能的对齐标记为训练数据。 分析截断均匀负抽样的有效性从图中可以看出，与MTransE，IPTransE和JAPE相比，具有均匀负采样的AlignE仍然获得了优异的结果，并且随着采样离x更加接近，效果呈上升趋势。 可能对齐的准确性可以看到以作者的标记方法S3表现最佳。这些结果证实作者的方法可以保证使用未标记数据的安全性。 对先前对准比例的敏感性 正如预期的那样，随着比例的增加，所有五个数据集的结果都变得更好，因为更多的先前对齐可以提供更多信息来对齐两个KG。 F1-score w.r.t. 关系三元数的分布 BootEA在所有时间间隔都优于MTransE，IPTransE和JAPE，这再次证实了BootEA的有效性。而且BootEA可以在稀疏数据上取得有希望的结果。]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
        <tag>知识图谱嵌入</tag>
        <tag>实体对齐</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Entity Alignment between Knowledge Graphs Using Attribute Embeddings》阅读笔记]]></title>
    <url>%2Fpost%2FEntity%20Alignment%20between%20Knowledge%20Graphs%20Using%20Attribute%20Embeddings%2F</url>
    <content type="text"><![CDATA[论文下载地址，知识图之间的实体对齐的任务旨在在代表相同现实世界实体的两个知识图中找到实体。本文最主要就是提出了属性字符嵌入(attribute character embeddings)的方法。 Abstract我们的模型利用知识图中存在的大量属性三元组(attribute triples)并生成属性字符嵌入。 属性字符嵌入(attribute character embeddings)通过基于实体的属性计算实体之间的相似性，将实体嵌入从两个知识图移位到同一空间中。我们使用传递规则来进一步丰富实体的属性数量以增强属性字符嵌入。 Contribution 提出了两个KG之间实体对齐的框架，它由谓词对齐模块，嵌入学习模块和实体对齐模块组成。 提出了一种新颖的嵌入模型，它将实体嵌入与属性嵌入集成在一起，以便为两个KG学习统一的嵌入空间。 我们在三个真正的KG对上评估建议的模型。结果表明，我们的模型在实体对齐任务上始终优于最先进的模型，hits@1超过50％。 模型模型总览predicate alignment, embedding learning, and entity alignment Predicate Alignment谓词对齐模块通过使用统一的命名方案重命名两个KG的谓词来合并两个KG，以便为关系嵌入提供统一的向量空间。dbp:bornIn vs. yago:wasBornIn 统一命名为 :bornIn。 为了找到部分匹配的谓词，作者计算谓词URI的最后部分的编辑距离（例如，bornIn与wasBornIn）并将0.95设置为相似性阈值。 Embedding LearningStructure Embedding作者采用TransE来学习对于实体的结构嵌入。与TransE不同的是，模型希望更关注已对齐的三元组，也就是包含对齐谓词的三元组。模型通过添加权重来实现这一目的。Structure embedding的目标函数如下： count(r)是关系r出现的数量。 Attribute Character Embedding对于属性字符嵌入，也参考TransE的思想，将谓词r解释为从头部实体h到属性a的转换。但是，相同的属性a可以在两个KG中以不同的形式出现，例如50.9989对50.9988888889作为实体的纬度; “Barack Obama”与“Barack Hussein Obama”作为人名等。因此，本文提出使用组合函数对属性值进行编码，并将属性三元组中每个元素的关系定义为h +r≈fa（a）。 这里，fa（a）是组合函数，a是属性值a = {c1，c2，c3，…，ct}的字符序列。 组合函数将属性值编码为单个向量，并将类似的属性值映射到类似的向量表示。 作者定义了三个组成函数如下。 Sum compositional function (SUM)存在问题：包含相同字符不同顺序的属性值会有相同的向量表示 LSTM-based compositional function (LSTM). N-gram-based compositional function (N-gram) 最后attribute character embedding目标函数： Joint Learning of Structure Embedding and Attribute Character Embedding作者使用属性字符嵌入通过最小化以下目标函数将结构嵌入移动到相同的向量空间： 本文整体损失函数： Entity Alignment在经过上述训练过程之后，来自不同KG的相似的实体将会有相似的向量表示，因此可通过 获得潜在实体对齐对。此外，模型设定相似度阈值来过滤潜在实体对齐对，得到最终的对齐结果。 Triple Enrichment via Transitivity Rule作者利用一阶逻辑传递关系来丰富三元组。即：存在和则可以推理出h_1+ (r_1.r_2) ≈ t_2 Database本文从 DBpedia (DBP)、LinkedGeoData (LGD)、Geonames (GEO) 和 YAGO 四个 KG 中抽取构建了三个数据集，分别是DBP-LGD、DBP-GEO和DBP-YAGO。具体的数据统计如下： ExperimentsEntity Alignment Results本文对比了三个相关的模型，分别是 TransE、MTransE 和 JAPE。试验结果表明，本文提出的模型在实体对齐任务上取得了全面的较大的提升，在三种组合函数中，N-gram函数的优势较为明显。此外，基于传递规则的三元组丰富模型对结果也有一定的提升。具体结果如下 Rule-based Entity Alignment Results为了进一步衡量 attribute character embedding 捕获实体间相似信息的能力，本文设计了基于规则的实体对齐模型。本实验对比了三种不同的模型：以label的字符串相似度作为基础模型；针对数据集特点，在基础模型的基础之上增加了坐标属性，以此作为第二个模型；第三个模型是把本文提出的模型作为附加模型，与基础模型相结合。具体结果如下： KG Completion Results本文还在KG补全任务上验证了模型的有效性。模型主要测试了链接预测和三元组分类两个标准任务，在这两个任务中，模型也取得了不错的效果。具体结果如下：]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
        <tag>知识图谱嵌入</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《RelNN A Deep Neural Model for Relational Learning》阅读笔记]]></title>
    <url>%2Fpost%2FRelNN%20A%20Deep%20Neural%20Model%20for%20Relational%20Learning%2F</url>
    <content type="text"><![CDATA[论文下载地址，这篇文章相当于结合了统计学习和深度神经网络。里面有些公式没有理解，应该是有许多先前论文需要阅读。但是本篇论文扩展了思路如何结合统计学和深度学习，并且基于其余数据来预测一个类中对象的一个属性，想法也比较好。 Introduction作者主要集中于基于其余数据来预测一个类中对象的一个属性。 Challenge当类中每个对象的属性依赖于不同数量的其他对象的属性和关系时，此问题具有挑战性。 在StarAI社区中，此问题称为聚合（aggregation）。 Relational Logistic Regression and Markov Logic NetworksStarAI模型旨在模拟对象之间关系的概率。 Relational logistic regression (RLR) (Kazemi et al. 2014)定义的概率公式如下： 上面定义的RLR模型仅适用于布尔值或多值父项。作者采用的是连续的原子（continuous atoms）（Fatemi, Kazemi, and Poole (2016)） Relational Neural Networks作者通过设计神经网络中线性层（LL），激活层（AL）和误差层（EL）的关系对应物，对具有分层架构的RLR / MLN模型进行编码。 关系神经网络（RelNN）是包含作为图形彼此连接的若干RLL和RAL的结构。 Motivations for hidden layers 使喜欢看动作电影的人数增加时，男性的概率变为[0, 1]重的任何数值，不至于直接变为0或者1。 因此，隐藏层通过使模型能够学习通用规则并相应地对对象进行分类，然后以不同方式处理不同类别的对象，从而提高了建模能力。 使用RLR表示不同类型的现有显式聚合器，然而有些情况需要使用2个RLLs和2个RALs Learning latent properties directly对象可能包含无法使用常规规则指定的潜在属性，但可以在训练期间直接从数据中学习。 考虑图2中的模型，让Latent（m）成为电影的数字潜在属性，其值将在训练期间学习。 From ConvNet Primitives to RelNNs我们解释为什么RelNN也可以被视为ConvNets的一个实例。 ConvNets的输入矩阵中的单元（例如，图像像素）具有空间相关性和空间冗余：彼此更接近的单元比更远的单元更依赖。 例如，如果M表示图像的输入通道，则M [i，j]和M [i + 1，j + 1]之间的依赖性可能远大于M [i，j]和M [i，j+20]之间的依赖性。 对于关系数据，输入矩阵中的依赖关系（关系）是不同的：同一行或列中的单元（即同一对象的关系）具有比不同行和列中的单元更高的依赖性（即不同对象的关系）。 因此，为了使ConvNets适应关系数据，我们需要矢量形状的过滤器，这些过滤器对行和列交换是不变的，并且更好地捕获关系依赖性和可交换性假设。 DatasetsMovielens 1M dataset (Harper and Konstan 2015)第一个数据集是Movielens 1M dataset (Harper and Konstan 2015)，忽略了实际的评级，只考虑电影是否被评级，只考虑动作和戏剧类型。 PAKDD15获取地址 all Chinese and Mexican restaurants in Yelp dataset challenge获取地址 Empirical Results作者提出了三个问题来进行实验： Q1：RelNN的性能与其他众所周知的关系学习算法相比如何？ Q2：基于数字和规则的潜在属性如何影响RelNN的性能?更改了RelNN中隐藏图层和数字潜在属性的数量，以查看它们如何影响性能。 请注意，添加图层只会添加一定数量的参数，但添加k个数字潜在属性会增加k * |Δm|参数。 Q3：RelNN如何推断出看不见的案例并解决指向的规模大小问题 （Poole et al.2014）?作者实施了两个实验： 我们在大量数据中训练一个RelNN，并在一小数据上进行测试：该实验可以看作每个模型受冷启动问题的严重程度 然后我们在一小数据上训练一个RelNN并在一大数据上进行测试：可以看作这些模型对更大群体的推断 Future作者将从数据中自动学习这些结构的问题留作未来的工作。]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>关系抽取</tag>
        <tag>统计学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Interaction Embeddings for Prediction and Explanation》阅读笔记]]></title>
    <url>%2Fpost%2FInteraction%20Embeddings%20for%20Prediction%20and%20Explanation%2F</url>
    <content type="text"><![CDATA[论文下载地址，此论文主要提出了实体和关系的交互作用对于知识图谱嵌入的影响，和提出了新的嵌入评估方案 - 搜索预测解释。 论文贡献 提出了CrossE，一种通过学习一个交互矩阵来给实体和关系的交互建模的新型知识图谱嵌入。 我们使用三个基准数据集评估CrossE与链接预测任务上的各种其他KGE的比较，并显示CrossE在具有适度参数大小的复杂且更具挑战性的数据集上实现最先进的结果。 我们提出了一种新的嵌入评估方案 - 搜索预测解释，并表明CrossE能够生成比其他方法更可靠的解释。 这表明交互嵌入更能在不同的三元组环境中捕捉实体和关系之间的相似性。 介绍给定知识图谱和一个要预测的三元组的头实体和关系，在预测尾实体的过程中，头实体和关系之间是有交叉交互的crossover interaction, 即关系决定了在预测的过程中哪些头实体的信息是有用的，而对预测有用的头实体的信息又决定了采用什么逻辑去推理出尾实体，文中通过一个模拟的知识图谱进行了说明如下图所示： 相关工作论文中在这部分对KGE（Knowledge graph embedding）进行了分类总结： KGEs with general embeddings KGEs with multiple embeddings. KGEs that utilize extra information. 这部分总结中对大量的方法进行描述，可以作为背景知识进行阅读。 CrossE模型基于对头实体和关系之间交叉交互的观察，本文提出了一个新的知识图谱表示学习模型CrossE. CrossE除了学习实体和关系的向量表示，同时还学习了一个交互矩阵C，C与关系相关，并且用于生成实体和关系经过交互之后的向量表示，所以在CrossE中实体和关系不仅仅有通用向量表示，同时还有很多交互向量表示。CrossE核心想法如下图： 目标函数粉四步生成： Interaction Embedding for Entities：根据头实体向量和交互矩阵（以关系确定的）来确定头实体的交互表示。 Interaction Embedding for Relations：根据头实体的交互表示和关系作用生成关系的交互表示 Combination Operator：将头实体的交互表示和关系的交互表示相结合，并进行非线性处理（tanh） Similarity Operator：计算结合后表示和尾实体表示之间的相似度。 最后分数函数： 损失函数：（这里就是一个交叉熵函数，但是写的有问题f(x)项应该在括号外） 对于预测的解释这部分作者描述了如何生成预测三元组的解释，并介绍了基于嵌入的路径搜索算法，主要步骤如下： Search for similar relations：修剪掉不合理路径 Search for paths between h and t：作者定义了6种路径（班汉一个或两个关系） Search similar entities：捕获实体之间的相似性方面越有能力，就越有可能存在（hs，r，ts） : Search for similar structures as supports：我们只将知识图中至少有一个支持的路径视为解释。 实验数据集 链接预测 从实验结果中我们可以看出，CrossE实现了较好的链接预测结果。我们去除CrossE中的头实体和关系的交叉交互，构造了模型 CrossES，CrossE 和 CrossES 的比较说明了交叉交互的有效性。 生成解释我们提出了一种基于相似结构通过知识图谱的表示学习结果生成预测结果解释的方法，并提出了两种衡量解释结果的指标，AvgSupport和Recall。Recall是指模型能给出解释的预测结果的占比，其介于0和1之间且值越大越好；AvgSupport是模型能给出解释的预测结果的平均support个数，AvgSupport是一个大于0的数且越大越好。可解释的评估结果如下： 链接预测和可解释的实验从两个不同的方面评估了知识图谱表示学习的效果，同时也说明了链接预测的准确性和可解释性没有必然联系，链接预测效果好的模型并不一定能够更好地提供解释，反之亦然。 参考链接 http://blog.openkg.cn/%E8%AE%BA%E6%96%87%E6%B5%85%E5%B0%9D-interaction-embeddings-for-prediction-and-explanation/]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
        <tag>知识图谱推理</tag>
        <tag>知识图谱嵌入</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Differentiable Learning of Logical Rules for Knowledge Base Reasoning》阅读笔记]]></title>
    <url>%2Fpost%2FDifferentiable%20Learning%20of%20Logical%20Rules%20for%20Knowledge%20Base%20Reasoning%2F</url>
    <content type="text"><![CDATA[论文下载地址，本文研究用于于知识图谱推理的学习概率一阶逻辑规则的问题，提出了Neural Logic Programming（Neural-LP）框架，它结合了端到端可微分模型中一阶逻辑规则的参数和结构学习。为了在可微分的框架中同时学习参数和结构，作者设计了一个具有注意机制和记忆的神经控制器系统，以学习顺序组成TensorLog使用的原始可微操作。作者采用的注意机制是作为逻辑规则的置信度并且有寓意含义的。 下图展示了一个使用逻辑规则进行知识图谱推理的例子 使用概率逻辑的优点是通过为逻辑规则配备概率，可以更好地模拟统计复杂和噪声数据。 statistical relational learning（统计关系学习）：学习关系规则的集合 ==inductive logic programming（归纳逻辑规划）：==当学习涉及提出新的逻辑规则时。（这应该和我正在做的方向是相关的，都是带有归纳性质的，有新的东西产生）。 FrameworkKnowledge base reasoning为了推理知识库，对于每个查询我们都有兴趣学习以下形式的加权链式逻辑规则，类似于==随机逻辑程序==： 其中$\alpha$是和规则有关的置信度，R是知识库中的关系，query(Y,X) 表示一个三元组，query 表示一个关系。 TensorLog for KB reasoning将实体转换成one-hot变量；并用一个矩阵$M_R$表示关系，该矩阵只在（i，j）处为1，i、j为第i、j个实体。 结合两个操作，逻辑规则推理$R(Y,X) \gets P(Y,X) \bigwedge Q(Z,X)$可以被表示为：$M_P \cdot M_P \cdot v_x \doteq s$，向量s中为1的位置就是Y的答案。 对于一条查询，所有的逻辑规则的右边部分被表示为以下形式： 其中，l表示所有的可能规则的个数，$\alpha_l$是规则l的置信度，$\beta_l$是某特定关系里的有序关系列表，所以在inference时，给定实体$v_x​$，实体y的score等于向量s中的对应y的位置的值。对于推理，给定实体x，实体y的score等于向量s中的对应y的位置的值。 所以总结本文关心的优化问题如下： Learning the logical rules在上式的优化问题中，算法需要学习的部分分为两个：一个是规则的结构，即一个规则是由哪些条件组合而成的；另一个是规则的置信度。由于每一条规则的置信度都是依赖于具体的规则形式，而规则结构的组成也是一个离散化的过程，因此上式整体是不可微的。因此作者对前面的式子做了以下更改： 对比与式（2）：主要交换了连乘和累加的计算顺序，对预一个关系的相关的规则，为每个关系在每个步骤都学习了一个权重，即上式的 $a_t^k$。 由于上式固定了每个规则的长度都为 T，这显然是不合适的。为了能够学习到变长的规则，Neural LP中设计了记忆向量 $u_t$,表示每个步骤输出的答案—每个实体作为答案的概率分布，还设计了两个注意力向量：一个为记忆注意力向量 $b_t$ ——表示在步骤 t 时对于之前每个步骤的注意力；一个为算子注意力向量 $a_t$ ——表示在步骤 t 时对于每个关系算子的注意力。每个步骤的输出由下面三个式子生成： 其中$b_t$和$a_t$由以下公式通过RNN获得： 推理机的整体框架是： 其中memory存的就是每步的推理结果（实体），最后的输出（例如$u_{T+1}$，目标就是最大化 $logv_y^Tu$，加log是因为非线性能让效果变好。 整个算法如下： 实验（1） 两个标准数据集上的统计关系学习相关的实验 Unified Medical Language System (UMLS)：The entities are biomedical concepts (e.g. disease, antibiotic) and relations are like treats and diagnoses. Kinship：contains kinship relationships among members of the Alyawarra tribe from Central Australia [ （2） 在$16*16$的网格上的路径寻找的实验 （3） 知识库补全实验实验所用数据集信息： FB15KSelected：这是通过从FB15K中去除近似重复和反向关系而构造的 实验结果： 为了证明Neural LP的归纳推理的能力，本文还特别设计了一个实验，在训练数据集中去掉所有涉及测试集中包含的实体的三元组，然后训练并预测，得到结果如下： （4） 知识库问答的实验 总结本文提出了一个可微的规则学习模型，并强调了知识库中的规则应该是实体无关的，对于我目前在做的方向，本体论也是与实体无关的，这种规则学习有一定的借鉴性，但是好像所区别。这个规则推理也可以看成某些关系之间的包含关系3.1中举的HasOfficeInCity(New York,Uber) and CityInCountry(USA,New York)的例子，可以看作是2对于1有包含关系。并且可以看到本篇论文中，作者设计了丰富的实验。 参考链接 https://toutiao.io/posts/wrxf4z/preview https://zhuanlan.zhihu.com/p/46024825]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
        <tag>知识图谱推理</tag>
        <tag>规则学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《OK Google, What Is Your Ontology? Or/ Exploring Freebase Classification to Understand Google’s Knowledge Graph？》阅读笔记]]></title>
    <url>%2Fpost%2FOK%20Google%2C%20What%20Is%20Your%20Ontology%3F%20Or%2F%20Exploring%20Freebase%20Classification%20to%20Understand%20Google%E2%80%99s%20Knowledge%20Graph%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[本论文详细阐述Freebase中的数据格式，并进行了重构。通过考虑整体架构的三个部分：Freebase类型系统及其缺乏继承和依赖于不兼容性，允许表示值的不确定性的实现，以及合并和拆分对象的实现。来对本体进行阐述。论文下载地址 这篇论文重构了Freebase数据转储来理解谷歌语义搜索特征背后的本体。论文将会探索Freebase本体如何由许多力量塑造的，这些力量也通过深入研究本体论和一个小的相关性研究来形成分类系统。这些发现将会提供知识图谱专有黑盒的一瞥。 The structures found in the Freebase/Knowledge Graph ontology will be analyzed in light of the findings on classification systems in a key text by Bowker and Star (2000) [5]. 术语定义 ObjectFreebase对象是一个全局唯一的标识符，它是Freebase中世界上某种东西的表示。 TypeFreebase类型用来表达类的概念。 PropertyFreebase属性是描述对象如何链接到其他值或对象的关系。 Property Detail属性详细信息指的是可以通过属性链接的对象或值的约束。 RDF triple资源描述格式（RDF）是用于“三元组”（或N = 3元组）格式的数据表示的规范[17]。 Ontology对于本文，Freebase本体是类型，属性和属性详细信息的正式结构和描述，用于指定对象如何相互关联。 Architecture在本文中，架构指的是可以在本体中找到的一般模式和关系。 ==本体是否允许类（或Freebase用语中的类型）之间的继承？ 是否有与属性相关的默认值？ 如何处理“零”或空值？ 这些类型的问题不一定关注本体（飞机，火车或汽车）中具体表达的内容，而是关于本体表达方式的更多问题应该通过检查架构来解决。== Methodology作者把数据进行切分：按照RDF中三元组的谓语进行分类，例如： Freebase Ontology and Classification As Bowker and Star note, “Information infrastructure is a tricky thing to analyze…the easier they are to use, the harder they are to see.” [5]. What does the system make sense of? What is left out? What is privileged and by extension what is ignored by Google? 虽然Freebase本体可能不会立即看起来像一个分类系统，但类型（类）和属性的结构是一个基于对各种事物进行分类的系统。 作为对世界事物表征进行排序和分类的系统，将根据Bowker和Star的分类结果讨论Freebase本体。他们将对亚里士多德和原型分类（Aristotelian and prototype classification）进行了区分。 亚里士多德的分类“按照一组二元特征进行操作，被分类的物体呈现或不呈现”，而原型分类则认为“在我们心目中对于椅子是什么的广泛描述; 我们用隐喻和类比来扩展这张图片“ 5.1. Freebase’s Type System不兼容性的概念出现在Freebase系统中，用于表示对象如何具有某些类型，而这些类型必须将其排除在其他类型之外。 没有继承（not implement inheritance）：上述不兼容性在确保数据不表达可能在Google KP中提供的令人尴尬，有害或不正确的陈述方面发挥了足够强大的作用。 缺乏继承也可能是一种允许实体具有更大灵活性的特征。这里作者举了一个狗为电影演员的例子。 5.2. Has Value or Has No Value?三元组如何表达估计值，不确定值或空值？实际处理时用“Has Value” (HV) and “Has No Value” (HNV)来分别表达不确定值和空值。 以这种方式表达未知数和空值的有趣实现可能表明Freebase / KG最初并不是为了支持这种不确定性而建立的。Google的数据编码某些不确定性的概念并未向最终用户公开，尽管它肯定以这种独特的方式实现。 5.3. Dealing with Doppelgangers and Chimeras涉及Freebase如何处理“合并”重复对象（doppelgangers）和“拆分”混合对象（嵌合体）。 the property “/dataworld/gardening hint/replaced by” is used to implement merges be- tween various objects (e.g. by saying “/m/xyz123 - Replaced By - /m/abc123”). A Small Correlational Study主要探索这个问题：域的本体的复杂性（人物，电影等领域的类型，属性等）与表达与本体相关的事实（“知识库”）的三元组数量之间是否存在关联？ 对于本研究，通过考虑与域相关的属性详细信息量（多少描述，约束等）来实现“复杂性”和“成熟度”。 对于89个域中的每一个，获得了关于每个域的本体的以下统计： ==域中的类型和属性数== ==每种类型和属性的描述数== ==每种类型和属性的属性详细信息数== 通过获取域中每种类型和属性的平均描述数和属性详细信息来计算简单的复杂性分数。 所有域的RDF三元组计数与此复杂性得分之间的Pearson相关系数与0.2824呈正相关，简单线性回归的斜率为78,424.08（见图6）。 当排除异常音乐切片时，相关性和斜率分别变为0.6680和33,899.53。 虽然需要进一步的工作来探索这个研究问题，但这个小的相关性研究为进一步的实验提供了一些有希望的初步结果 discussion考虑整体架构的三个部分：Freebase类型系统及其缺乏继承和依赖于不兼容性，允许表示值的不确定性的实现，以及合并和拆分对象的实现。此外，还进行了一项小型相关研究，以检验基于Bowker和Star推动的预感的假设。在很大程度上，分类系统中的许多特征也可以在Freebase的本体和体系结构中找到。 本文具体而言，探讨了支持整个交付流程的基础结构（本体和体系结构），而不是Freebase / KG中表示的特定事实。 conclusion 应通过探索Freebase本体和体系结构的其他方面以及对Freebase进行更全面的实验分析来进行进一步的研究。]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
        <tag>Ontology</tag>
        <tag>freebase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式]]></title>
    <url>%2Fpost%2FRegular%20expression%2F</url>
    <content type="text"><![CDATA[本文参考了一些链接，记录了一些常用正则表达式的详细使用方法。 在自然语言处理中，很多时候我们都需要从文本或字符串中抽取出想要的信息，并进一步做语义理解或其它处理。在本文中，作者由基础到高级介绍了很多正则表达式，这些表达式或规则在很多编程语言中都是通用的。 书写正则表达式网站 正则表达式（regex 或 regexp）对于从文本中抽取信息极其有用，它一般会搜索匹配特定模式的语句，而这种模式及具体的 ASCII 序列或 Unicode 字符。从解析/替代字符串、预处理数据到网页爬取，正则表达式的应用范围非常广。 其中一个比较有意思的地方是，只要我们学会了正则表达式的语句，我们几乎可以将其应用于多有的编程语言，包括 JavaScript、Python、Ruby 和 Java 等。只不过对于各编程语言所支持的最高级特征与语法有细微的区别。 下面我们可以具体讨论一些案例与解释。 基本语句锚点：^ 和 $1234^The 匹配任何以“The”开头的字符串 end$ 匹配以“end”为结尾的字符串^The end$ 抽取匹配从“The”开始到“end”结束的字符串roar 匹配任何带有文本“roar”的字符串 数量符：*、+、？和 {}**12345678abc* 匹配在“ab”后面跟着零个或多个“c”的字符串 abc+ 匹配在“ab”后面跟着一个或多个“c”的字符串abc? 匹配在“ab”后面跟着零个或一个“c”的字符串abc&#123;2&#125; 匹配在“ab”后面跟着两个“c”的字符串abc&#123;2,&#125; 匹配在“ab”后面跟着两个或更多“c”的字符串abc&#123;2,5&#125; 匹配在“ab”后面跟着2到5个“c”的字符串a(bc)* 匹配在“a”后面跟着零个或更多“bc”序列的字符串a(bc)&#123;2,5&#125; 匹配在“a”后面跟着2到5个“bc”序列的字符串 或运算符：| 、 []12a(b|c) 匹配在“a”后面跟着“b”或“c”的字符串 a[bc] 匹配在“a”后面跟着“b”或“c”的字符串 字符类：\d、\w、\s 和 .**1234\d 匹配数字型的单个字符 \w 匹配单个词字（字母加下划线） \s 匹配单个空格字符（包括制表符和换行符） . 匹配任意字符 使用「.」运算符需要非常小心，因为常见类或排除型字符类都要更快与精确。\d、\w 和\s 同样有它们各自的排除型字符类，即\D、\W 和\S。例如\D 将执行与\d 完全相反的匹配方法： 1\D 匹配单个非数字型的字符 为了正确地匹配，我们必须使用转义符反斜杠「\」定义我们需要匹配的符号「^.[$()|*+?{\」，因为我们可能认为这些符号在原文本中有特殊的含义。 1\$\d 匹配在单个数字前有符号“$”的字符串 -&gt; Try it! (https://regex101.com/r/cO8lqs/9) 注意我们同样能匹配 non-printable 字符，例如 Tab 符「\t」、换行符「\n」和回车符「\r」 Flags我们已经了解如何构建正则表达式，但仍然遗漏了一个非常基础的概念：flags。 正则表达式通常以/abc/这种形式出现，其中搜索模式由两个反斜杠「/」分离。而在模式的结尾，我们通常可以指定以下 flag 配置或它们的组合： g（global）在第一次完成匹配后并不会返回结果，它会继续搜索剩下的文本。 m（multi line）允许使用^和$匹配一行的开始和结尾，而不是整个序列。 i（insensitive）令整个表达式不区分大小写（例如/aBc/i 将匹配 AbC）。 中级语句分组和捕获：()123a(bc) 圆括弧会创建一个捕获性分组，它会捕获匹配项“bc” a(?:bc)* 使用 “?:” 会使捕获分组失效，只需要匹配前面的“a” a(?&lt;foo&gt;bc) 使用 “?&lt;foo&gt;” 会为分组配置一个名称 捕获性圆括号 () 和非捕获性圆括弧 (?:) 对于从字符串或数据中抽取信息非常重要，我们可以使用 Python 等不同的编程语言实现这一功能。从多个分组中捕获的多个匹配项将以经典的数组形式展示：我们可以使用匹配结果的索引访问它们的值。 如果需要为分组添加名称（使用 (?…)），我们就能如字典那样使用匹配结果检索分组的值，其中字典的键为分组的名称。 方括弧表达式：[]12345[abc] 匹配带有一个“a”、“ab”或“ac”的字符串 -&gt; 与 a|b|c 一样 [a-c] 匹配带有一个“a”、“ab”或“ac”的字符串 -&gt; 与 a|b|c 一样[a-fA-F0-9] 匹配一个代表16进制数字的字符串，不区分大小写 [0-9]% 匹配在%符号前面带有0到9这几个字符的字符串[^a-zA-Z] 匹配不带a到z或A到Z的字符串，其中^为否定表达式 记住在方括弧内，所有特殊字符（包括反斜杠\）都会失去它们应有的意义。 ==Greedy 和 Lazy 匹配==数量符（* + {}）是一种贪心运算符，所以它们会遍历给定的文本，并尽可能匹配。例如，&lt;.+&gt; 可以匹配文本「This is a simple div test」中的「simple div」。为了仅捕获 div 标签，我们需要使用「？」令贪心搜索变得 Lazy 一点： 1&lt;.+?&gt; 一次或多次匹配 “&lt;” 和 “&gt;” 里面的任何字符，可按需扩展 注意更好的解决方案应该需要避免使用「.」，这有利于实现更严格的正则表达式： 1&lt;[^&lt;&gt;]+&gt; 一次或多次匹配 “&lt;” 和 “&gt;” 里面的任何字符，除去 “&lt;” 或 “&gt;” 字符 高级语句 边界符：\b 和 \B 1\babc\b 执行整词匹配搜索 \b 如插入符号那样表示一个锚点（它与$和^相同）来匹配位置，其中一边是一个单词符号（如\w），另一边不是单词符号（例如它可能是字符串的起始点或空格符号）。 它同样能表达相反的非单词边界「\B」，它会匹配「\b」不会匹配的位置，如果我们希望找到被单词字符环绕的搜索模式，就可以使用它。 1\Babc\B 只要是被单词字符环绕的模式就会匹配 前向匹配和后向匹配：(?=) 和 (?&lt;=)12d(?=r) 只有在后面跟着“r”的时候才匹配“d”，但是“r”并不会成为整个正则表达式匹配的一部分 (?&lt;=r)d 只有在前面跟着“r”时才匹配“d”，但是“r”并不会成为整个正则表达式匹配的一部分 我们同样能使用否定运算子： 12d(?!r) 只有在后面不跟着“r”的时候才匹配“d”，但是“r”并不会成为整个正则表达式匹配的一部分 (?&lt;!r)d 只有在前面不跟着“r”时才匹配“d”，但是“r”并不会成为整个正则表达式匹配的一部分 结语 正如上文所示，正则表达式的应用领域非常广，很可能各位读者在开发的过程中已经遇到了它，下面是正则表达式常用的领域： 数据验证，例如检查时间字符串是否符合格式； 数据抓取，以特定顺序抓取包含特定文本或内容的网页； 数据包装，将数据从某种原格式转换为另外一种格式； 字符串解析，例如捕获所拥有 URL 的 GET 参数，或捕获一组圆括弧内的文本； 字符串替代，将字符串中的某个字符替换为其它字符。 参考链接https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650749657&amp;idx=4&amp;sn=da4852cb0c4919316d801fe19a64901d&amp;chksm=871afea7b06d77b1bda42ac134c5dddad5af24647f62a5a8e7bdcef4499f40fd4c97045a6f3d&amp;mpshare=1&amp;scene=23&amp;srcid=1009dbNFxJJahsQK6NGw4wS3%23rd python正则表达式的使用正则表达式match和search的区别]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019年计划]]></title>
    <url>%2Fpost%2F2019-plans%2F</url>
    <content type="text"><![CDATA[读个博吉他可以弹唱几首流行歌曲假期报个班，请老师指点一下 精读50篇论文平均下来，每周都需要读一篇，每一篇都要勾划，发博客。 发2篇paper上半年积累知识，做实验，暑假开始写 考托福过90上半年背单词等 买个微单学个拍照不急，现在没钱，下半年入手一个吧 自己出门旅行一次嗯，要是论文写完就去吧！目前计划去山东，顺便看一下我二姑。5天左右的旅行吧！ 学习滑板开始健身每周去三次健身房]]></content>
      <categories>
        <category>annual plans</category>
      </categories>
      <tags>
        <tag>plan</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《An Automatic Knowledge Graph Construction System for K-12 Education》阅读笔记]]></title>
    <url>%2Fpost%2FAn%20Automatic%20Knowledge%20Graph%20Construction%20System%20for%20K-12%20Education%2F</url>
    <content type="text"><![CDATA[论文原址。本篇文章主要提出了一个自动化构建数学领域知识图谱的系统，主要应用的事NER和数据挖掘技术，其中NER主要是抽取数学概念，概念间的关系是作者自己构建的（例如先修关系）。对于数据集，作者主要从the Chinese curriculum standards of mathematics上提取的概念实体，从自己的SLP平台上，通过对学生表现来提取关系（把这部分作为数据挖掘）。本篇文章实际上可以作为构建特定领域的知识图谱的一个参考。 challenges the desired educational concept entities are more abstract than real world entities like PERSON, ORGANIZATION, LOCATION the desired relations are more cognitive and implicit, so cannot be derived from the literal meanings of text like generic knowledge graphs contributions a novel but practical system entity recognition (NER) &amp; association rule mining algorithms demonstrate an exemplary case with constructing a knowledge graph for the subject of mathematics SYSTEM OVERVIEW Educational Concept Extraction Module: Implicit Relation Identification Module CONCEPT EXTRACTION 线性链式CRF模型 标签预测 RELATION IDENTIFICATION两种方法 support confidence From the perspective of prerequisite relation, if concept si is a prerequisite of concept sj, learners who do not master sivery likely do not master sj, and learners who master sjmost likely master si. EXEMPLARY CASE AND SYSTEM EVALUATIONConcept ExtractionDatasetthe Chinese curriculum standards of mathematics published by the ministry of education as the main data source Evaluation adopt precision, recall and F1- score The ground truth is manually labeled by two domain experts. Relation IdentificationDataset students’ performance data collected by our SLP platform. EvaluationThe ground truth of the prerequisite relations between selected 9 concepts are annotated manually by two domain experts.]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>NER</tag>
        <tag>知识图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《RECURRENT NEURAL NETWORK REGULARIZATION》阅读笔记]]></title>
    <url>%2Fpost%2FRECURRENT%20NEURAL%20NETWORK%20REGULARIZATION%2F</url>
    <content type="text"><![CDATA[论文链接。这篇论文提出了LSTM的dropout策略来防止过拟合，即只在非循环链接处采取dropout。在BasicLSTMCell的接口就是依据这篇论文实现的。 文章整体架构和重点 contribution present a simple regularization technique background dropout Srivastava(2013)，对于前向反馈网络最有力的正则化方法并不能很好的应用在RNNs上。这导致RNNs规模都很小，因为太大会过拟合。 Bayer et al. (2013)指出了卷积dropout不能在RNNs上很好工作的原因是循环会放大噪音。 model仿射变换（affine transform）关于仿射变换：线性变换加上平移，盗个知乎上的图（原文链接） 模型主体采用的是Graves et al. (2013) dropout策略 The main contribution of this paper is a recipe for applying dropout to LSTMs in a way that success- fully reduces overfitting. The main idea is to apply the dropout operator only to the non-recurrent connections. 观察公式，实际上就是通过在层间传递中应用dropout。如下图中虚线所示。 从上图中也可以看到，该dropout的次数只和网络深度有关（数值为网络深度+1）。 experiments实验部分作者做了4部分实验来证明自己采用的方法有效，分别为language modeling, speech recognition, machine translation, image caption generation。这部分没什么需要解释了，感兴趣可以自己看一下实验。]]></content>
      <categories>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Understanding LSTM Networks]]></title>
    <url>%2Fpost%2FUnderstanding%20LSTM%20Networks%2F</url>
    <content type="text"><![CDATA[原文链接。这篇文章很好很细的一步一步的分解讲解了LSTM，之前看过一篇翻译的博客，现在自己翻译一遍，感觉对LSTM的认识加深了许多，虽然还是对LSTM中存有一些问题，比如为什么用tanh，sigmoid，为什不采用其他的？，但是看过之后至少对LSTM没有那么畏惧，不觉得过于复杂了。 LSTMRecurrent Neural Networks 循环允许信息从一个网络传入下一个。 一个循环网络可以被认为是相同网络的多个复制，每一个网络都将信息传递给后继者。这种类似链的性质表明，递归神经网络与序列和列表密切相关。 The Problem of Long-Term DependenciesRNN的一个吸引力是他们可能能够将先前信息连接到当前任务。 有时，我们只需要查看最近的信息来执行当前任务。例如： If we are trying to predict the last word in “the clouds are in the sky,” we don’t need any further context – it’s pretty obvious the next word is going to be sky. 在这种情况下，如果相关信息与待预测地方之间的差距很小，RNN可以学习使用过去的信息。 但是，对于一些情况，我们需要更多的上下文信息。 Consider trying to predict the last word in the text “I grew up in France… I speak fluent French.” 这时，相关信息与需要变得非常大的点之间的差距完全有可能。不幸的是，随着差距的扩大，RNN无法学会连接信息。 The problem was explored in depth by Hochreiter (1991) [German] and Bengio, et al. (1994), who found some pretty fundamental reasons why it might be difficult. LSTM Networks LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn! 标准RNNs的重复模块只有一个简单的结构，比如tanh层。 LSTMs也有像这种链式结构，但是它的重复模块具有不同的结构。 有四个，而不是一个神经网络层，以一种非常特殊的方式进行交互。 基本符号如下： 在上图中，每一行都携带一个完整的向量，从一个节点的输出到其他节点的输入。 粉色圆圈表示逐点运算，如矢量加法，而黄色框表示神经网络层。 行合并表示连接，而行分叉表示其内容被复制，副本将转移到不同的位置。 The Core Idea Behind LSTMsLSTM的关键是单元状态，水平线贯穿图的顶部。 单元状态有点像传送带。 它直接沿着整个链运行，只有一些微小的线性相互作用。 信息很容易沿着它不变地流动。 LSTM确实能够移除或添加信息到细胞状态，由称为门的结构精心调节。 门是一种可选择通过信息的方式。 它们由Sigmoid神经网络层和逐点乘法运算组成。 sigmoid层输出0到1之间的数字，描述每个组件应该通过多少。 值为零意味着“不让任何东西通过”，而值为1则意味着“让一切都通过！” LSTM具有三个这样的门，用于保护和控制单元状态。 Step-by-Step LSTM Walk Through我们的第一步就是确定我们将从单元状态中丢弃的信息。这个决定是由一个称为“遗忘门层”的sigmoid层决定的。它查看$h_{t-1}$和$x_t$，并为单元状态$C_{t-1}$中的每一个数字输出一个介于0和1之间的数字。1代表“完全保留这个”，而0代表“完全舍弃这个”。 让我们回到我们的语言模型示例，试图根据以前的所有单词预测下一个单词。 在这样的问题中，单元状态可能包括当前受试者的性别，因此可以使用正确的代词。 当我们看到一个新主题时，我们想要忘记旧主题的性别。 下一步是确定我们将在单元状态中存储哪些新信息。 这有两个部分。 首先，称为“输入门层”的sigmoid层决定我们将更新哪些值。 接下来，tanh层创建可以添加到状态的新候选值$\tilde{C}_t$的向量。 在下一步中，我们将结合这两个来创建状态更新。 在我们的语言模型的例子中，我们想要将新主题的性别添加到单元格状态，以替换我们忘记的旧主题。 现在是时候将旧的单元状态$C_{T-1}$更新为新的单元状态$C_t$。 前面的步骤已经决定要做什么，我们只需要实际做到这一点。 我们将旧状态乘以$f_t$，忘记我们之前决定忘记的事情。 然后我们添加$i_t * \tilde{C}_t$。 这是新的候选值，根据我们决定更新每个状态的值来缩放。 在语言模型的情况下，我们实际上放弃了关于旧主题的性别的信息并添加新信息，正如我们在前面的步骤中所做的那样。 最后，我们需要决定我们要输出的内容。 此输出将基于我们的单元状态，但将是过滤版本。 首先，我们运行一个sigmoid层，它决定我们要输出的单元状态的哪些部分。 然后，我们将单元格状态设置为tanh（将值推到介于-1和1之间）并将其乘以sigmoid门的输出，以便我们只输出我们决定的部分。 对于语言模型示例，由于它只是看到一个主题，它可能想要输出与动词相关的信息，以防接下来会发生什么。 例如，它可能输出主语是单数还是复数，以便我们知道动词应该与什么形式共轭，如果接下来的话。 Variants on Long Short Term Memory到目前为止我所描述的是一个非常正常的LSTM。 但并非所有LSTM都与上述相同。 事实上，似乎几乎所有涉及LSTM的论文都使用略有不同的版本。 差异很小，但值得一提的是其中一些。 One popular LSTM variant, introduced by Gers &amp; Schmidhuber (2000), is adding “peephole connections.” This means that we let the gate layers look at the cell state. 上面的图表为所有门增加了窥视孔（peephole），但是许多论文会给一些窥视孔而不是其他的。 另一种变化是使用耦合的遗忘和输入门。 我们不是单独决定忘记什么以及应该添加新信息，而是一起做出这些决定。我们仅仅会当我们在当前位置将要输入时忘记。我们仅仅输入新的值到那些我们已经忘记旧的信息的那些状态 。 另一个改动较大的变体是 Gated Recurrent Unit (GRU)，这是由 Cho, et al. (2014) 提出。它将遗忘和输入门组合成一个“更新门”。它还合并了单元状态和隐藏状态，并进行了一些其他更改。 由此产生的模型比标准LSTM模型简单，并且越来越受欢迎。 这些只是最着名的LSTM变种中的一小部分。 还有很多其他的东西，如 Yao, et al. (2015) 提出的 Depth Gated RNN。 还有一些完全不同的解决长期依赖关系的方法，如 Koutnik, et al. (2014) 提出的 Clockwork RNN。 哪种变体最好？ 差异是否重要？ Greff, et al. (2015) 对流行变体进行了很好的比较，发现它们几乎完全相同。Jozefowicz, et al. (2015) 测试了超过一万个RNN架构，找到了一些在某些任务上比LSTM更好的架构。 Conclusion早些时候，我提到了人们用RNN取得的显着成果。基本上所有这些都是使用LSTM实现的。对于大多数任务来说，它们确实工作得更好！ 作为一组方程写下来，LSTM看起来非常令人生畏。希望，在这篇文章中逐步走过它们使他们更加平易近人。 LSTM是我们用RNN实现的重要一步。很自然地想知道：还有另一个重要的一步吗？研究人员的共同观点是：“是的！下一步是它的注意！“我们的想法是让RNN的每一步都从一些更大的信息集中选择信息。例如，如果您使用RNN创建描述图像的标题，则可能会选择图像的一部分来查看其输出的每个单词。 事实上， Xu, et al.(2015) 做到这一点 - 如果你想探索注意力，这可能是一个有趣的起点！使用注意力已经取得了许多非常令人兴奋的结果，似乎还有更多的事情即将来临…… 注意力不是RNN研究中唯一令人兴奋的问题。例如，Kalchbrenner, et al. (2015) 的Grid LSTMs似乎非常有希望。在生成模型中使用RNN工作 - 例如Gregor, et al. (2015), Chung, et al. (2015), 或者 Bayer &amp; Osendorfer (2015) 似乎也很有趣。过去几年对于反复出现的神经网络来说是一个激动人心的时刻，即将到来的那些承诺只会更加如此！]]></content>
      <categories>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation》阅读笔记]]></title>
    <url>%2Fpost%2FLearning_Phrase_Representations_using_RNN_Encoder%E2%80%93Decoder_for_Statistical_Machine_Translation%2F</url>
    <content type="text"><![CDATA[原文链接。该论文是Sequence to Sequence学习的最早原型，论文中提出一种崭新的RNN(GRU) Encoder-Decoder算法，虽然文章属于比较旧的文章，但作为seq2seq的基础原型，还是需要阅读了解一下的。文章写的比较详细，各部分细节都有讲解。 文章的主要结构 contribution a novel RNN Encoder–Decoder 能够处理变长序列 a novel hidden unit reset gate update gate RNN Encoder–Decoder模型结构图如下： 文中作者对齐进行总体概述为： From a probabilistic perspective, this new model is a general method to learn the conditional distribution over a variable-length sequence conditioned on yet another variable-length sequence 从概率的角度来看，这个新模型是学习在另一个可变长度序列条件下的可变长度序列上的条件分布的一般方法 Encoder这部分是一个RNN单元。每个时间步，我们向Encoder中输入一个字/词（一般为向量形式），直到我们输入这个句子的最后一个字/词$X_T$，然后输入整个句子的语义向量c。由于RNN的特带你就是把前面每一步的输入信息都考虑进来，所以理论上这个c就包含了整个句子的所有信息。我们可以把当成这个句子的一个语义表示。 DecoderDecoder是另一个RNN，其被训练出来以通过预测隐藏状态$h_t$的下一个符号$y_t$来生成输出序列。计算公式如下 h_t = f(h_{t-1},y_{t-1},c)下一个序列的计算公式如下： P(y_t|y_{t-1},y_{t-2},\dots,y_1,c)=g(h_t,y_{t-1},c)Hidden Unit该部分是对各部分具体的公式讲解，实际是GRU的具体公式算法，不再此详细叙述了。 reset gate In this formulation, when the reset gate is close to 0, the hidden state is forced to ignore the pre- vious hidden state and reset with the current input only. This effectively allows the hidden state to drop any information that is found to be irrelevant later in the future, thus, allowing a more compact representation. 这段原文主要讲解了复位门的作用：有效地允许隐藏状态丢弃在将来稍后发现不相关的任何信息，从而允许更紧凑的表示。 当捕获短期依赖时，复位门活跃 update gate the update gate controls how much information from the previous hidden state will carry over to the current hidden state. 更新门控制来自先前隐藏状态的多少信息将转移到当前隐藏状态。 当捕获长期依赖时，更新门活跃 Statistical Machine Translation(SMT) Experiments这部分作者主要做了量化分析和性质分析，主要就是说他的模型怎么厉害。。。（没有具体的数值指标，翻译的还不是中英翻译，想看的话可以去看一下，就不贴实验结果了）。 future这里作者提出了可以用decoder生成的目标短语来替换原句中短语的思路，如果没记错的话，这个想法好像对后面的机器翻译有很大的指导作用。]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>seq2seq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Differentiating Concepts and Instances for Knowledge Graph Embedding》阅读笔记]]></title>
    <url>%2Fpost%2Fread_Differentiating_Concepts_and_Instances_for_Knowledge_Graph_Embedding%2F</url>
    <content type="text"><![CDATA[论文获取地址。这篇文章最大的亮点就是把concept映射为一个球面，然后把instance映射为一个向量，通过这种空间关系来进行embedding。如果instance和concept满足InstanceOf的关系，则instance应该在球内；如果两个concept满足SubClassOf的关系，则一个球会在另一个球面内。 conceptA concept is a fundamental category of existence (Rosch, 1973) and can be reified by all of its actual or potential instances.Concepts, which represent a group of different instances sharing common properties, are essential information in knowledge representation. drawback of the previous method ignore to distinguish between concepts and instances will lead to two drawbacks: Insufficient concept representation： cannot explicitly represent the difference between concepts and instances Lack transitivity of both isA relations: instanceOf and subClassOf (generally known as isA)isA relations exhibit transitivity contributions the first to propose and formalize the problem of knowledge graph embedding which differentiates between concepts and instances a novel knowledge embedding method named TransC state-of-the-art on link prediction and triple classification Translation-based ModelsTransE triple (h, r, t) should satisfy h + r ≈ t loss function:$f_r(h,t) = ||h + r - t||^2_2$ suitable for 1-to-1 relations TransH It regards a relation vector r as a translation on a hyperplane with $w_r$ as the normal vector. loss function:$f_r(h,t) = ||h_{\bot} + r - t_{\bot}||^2_2$，其中$h_{\bot}=h-w^{\top}_r h w_r$，$t_{\bot}=t-w^{\top}_r t w_r$ suitable for 1-to-N, N-to-1, and N-to-N relations TransR/CTransR addresses the issue in TransE and TransH that some entities are similar in the entity space but comparably different in other specific aspects. loss function:$f_r(h,t) = ||M_rh +r -M_rt||^2_2$，$M_r$ for each relation r TransD considers the different types of entities and relations at the same time loss function:$f_r(h,t) = ||h_{\bot} + r - t_{\bot}||^2_2$，$h_{\bot} = M_{rh}h$和$t_{\bot} = M_{rt}t$，$M_{r,e}$ for each relation-entity pair (r, e) Bilinear ModelsRESCAL the first bilinear model It associates each entity with a vector to capture its latent semantics. Each relation is represented as a matrix which models pairwise interactions between latent factors. External Information Learning Models textual information entity descriptions Problem Formulation这部分中作者详细介绍了知识图谱的组成部分：概念和实例集、关系集（包括instanceOf、subClassOf和instance relation），三元组集（按照关系集同样分为三个部分）。为了能表达is A关系的传递性，作者将instanceOf和subClassof两种关系进行了精心的设计，也是该论文的重点。 For each concept c ∈ C, we learn a sphere s(p, m) with $p \in R^k$ and m denoting the sphere center and radius. TranCSpecifically, TransC encodes each concept in knowledge graph as a sphere and each instance as a vector in the same semantic space. InstanceOf Triple Representationloss function：$f_e(i,c) = ||i-p||_2 - m$，当该函数大于0时进行优化，使其小于零。 SubClassOf Triple Representation 如图所示，其中子图（a）为目标状态。两个概念的的圆心距离：$d = ||p_i - p_j||_2$。需要做到的就是$d-(m_j -m_i) \leq 0$并且$ (m_j &gt; m_i)$。 Relational Triple Representation这部分按照TranE的思路进行处理，$||h+r-t||^2_3$ train modelmargin based loss详解unit and bern Regarding the strategy of constructing negative labels, we use “unif” to denote the traditional way of replacing head or tail with equal probability, and use “bern.” to denote reducing false negative labels by replacing head or tail with different probabilities. the following research directions find a more expressive model instead of spheres to represent concepts A concept may have different meanings in different triples. use several typical vectors of instances as a concept’s centers to represent different meanings of a concept.]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Discourse-Level Named Entity Recognition and Relation Extraction Dataset for Chinese Literature Text 阅读笔记]]></title>
    <url>%2Fpost%2FREAD_A_Discourse-Level_Named_Entity_Recognition_and_Relation_Extraction_Dataset_for_Chinese_Literature_Text%2F</url>
    <content type="text"><![CDATA[该论文最主要的贡献就是这个数据，数据集地址。论文中提到的标标签过程也是一个创新点，运用了启发式和机器辅助标标签，这样可以提高准确度并减少标注人员工作。 contribution provide a new dataset for joint learning of NER and RE for Chinese literature text the proposed dataset is based on the discourse level which provides additional context information introduce some widely used models to conduct experiments tagging processtwo methods:one is a heuristic tagging method and another is a machine auxiliary tagging method. Step 1: First Tagging Processfind a problem of data inconsistency. Step 2: Heuristic Tagging Based on Generic disambiguating Rules For example, remove all adjective words and only tag “entity header” . re-annotate all articles and correct all inconsistency entities and relations based on the heuristic rules. Step 3: Machine Auxiliary Tagging The core idea is to train a model to learn annotation guidelines on the subset of the corpus and produce predicted tags on the rest data. CRF tagging set Annotation FormatEntityEach entity is identified by T tag, which takes several attributes. Id: a unique number identifying the entity within the document. It starts at 0, and is incremented every time a new entity is identified within the same document. Type: one of the entity tags. Begin Index: the begin index of an entity. It starts at 0, and is incremented every character. End Index: the end index of an entity. It starts at 0, and is incremented every character. Value: words being referred to an identifiable object. RelationEach relation is identified by R tag, which can take several attributes: Id: a unique number identifying the relation within the document. It starts at 0, and is incremented every time a new relation is identified within the same document. Arg1 and Arg2: two entities associated with a relation. Type: one of the relation tags.]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>NER</tag>
        <tag>RE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas的数据类型操作]]></title>
    <url>%2Fpost%2FPandas_data_type_manipulation%2F</url>
    <content type="text"><![CDATA[在原文链接中摘抄出部分信息作为记录形成本文。 数据类型 Pandas dtype Python 类型 NumPy 类型 用途 object str string_, unicode_ 文本 int64 int int_, int8, int16, int32, int64, uint8, uint16, uint32, uint64 整数 float64 float float_, float16, float32, float64 浮点数 bool bool bool_ 布尔值 datetime64 NA NA 日期时间 timedelta[ns] NA NA 时间差 category NA NA 有限长度的文本值列表 数据类型操作 使用df.dtypes可以显示数据所有列的类型 df.info（） 函数可以显示更有用的信息 使用 astype() 函数使用条件 数据是干净的，可以简单地解释为一个数字 你想要将一个数值转换为一个字符串对象 如果数据具有非数字字符或它们间不同质（homogeneous），那么 astype() 并不是类型转换的好选择。你需要进行额外的变换才能完成正确的类型转换。 使用方式为了真正修改原始 dataframe 中数据类型，记得把 astype() 函数的返回值重新赋值给 dataframe，因为 astype() 仅返回数据的副本而不原地修改。 参考链接 https://juejin.im/post/5acc36e66fb9a028d043c2a5]]></content>
      <categories>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>pandas</tag>
        <tag>python</tag>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬取中文网页时中文字符变英文的解决方法]]></title>
    <url>%2Fpost%2Fsolution_of_python_for_Chinese_characters_to_become_English_when_crawling_Chinese_web_pages%2F</url>
    <content type="text"><![CDATA[使用python的scrapy爬取网页时，源代码中的中文字符在爬取下来后变成了英文字符。 问题举例例如，原网页为： 爬取结果为： 解决方法修改请求头：在settings.py文件中找到下属代码： 12345# Override the default request headers:#DEFAULT_REQUEST_HEADERS = &#123;# 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',# 'Accept-Language': 'en',#&#125; 改为： 12345# Override the default request headers:DEFAULT_REQUEST_HEADERS = &#123; 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Language': 'zh-CN',&#125; 修改结果展示： 参考链接 https://blog.csdn.net/wuqili_1025/article/details/79690103]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python动态网页爬取之安装docker和splash]]></title>
    <url>%2Fpost%2FPython_dynamic_web_crawler_installed_docker_and_splash%2F</url>
    <content type="text"><![CDATA[利用python进行动态网页爬取时，在安装docker和splash时踩过的坑，记录了一下自己的安装过程。用的系统是mac os。 安装scrapy-splash 利用pip安装scrapy-splash库：$ pip install scrapy-splash 安装Docker==下面👇这样安不下去了== 如果是Mac的话需要使用brew安装，如下：brew install docker 报错： 1Error: Failure while executing; `git config --local --replace-all homebrew.private true` exited with 1. 解决方法： 1xcode-select --install 然后在执行： 1brew install docker 再继续： service docker start 报错： -bash: service: command not found上网上查一堆乱七八糟的解决方式，该路径啥的，真的不想改路径，怕把其他的改崩了。最后放弃这种方式，如果有兴趣也可以尝试解决。 ==尝试如下安装DOCKER方法== 去官网下载这种方法下载docker客户端需要从服务器下载，自己电脑下载12k/s，简直慢死了。 拉取镜像(pull the image)：docker pull scrapinghub/splash 用docker运行scrapinghub/splash： docker run -p 8050:8050 scrapinghub/splash 在浏览器中输入localhost:8050 ==安装成功== 参考链接 http://www.morecoder.com/article/1001249.html https://www.jianshu.com/p/e54a407c8a0a]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>splash</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用python3读写csv文档]]></title>
    <url>%2Fpost%2Fread%20the%20CSV%20document%20using%20python3%2F</url>
    <content type="text"><![CDATA[对于大多数的CSV格式的数据读写问题，都可以使用 csv 库。 例如：假设你在一个名叫stocks.csv文件中有一些股票市场数据，就像这样： 1234567Symbol,Price,Date,Time,Change,Volume&quot;AA&quot;,39.48,&quot;6/11/2007&quot;,&quot;9:36am&quot;,-0.18,181800&quot;AIG&quot;,71.38,&quot;6/11/2007&quot;,&quot;9:36am&quot;,-0.15,195500&quot;AXP&quot;,62.58,&quot;6/11/2007&quot;,&quot;9:36am&quot;,-0.46,935000&quot;BA&quot;,98.31,&quot;6/11/2007&quot;,&quot;9:36am&quot;,+0.12,104800&quot;C&quot;,53.08,&quot;6/11/2007&quot;,&quot;9:36am&quot;,-0.25,360900&quot;CAT&quot;,78.29,&quot;6/11/2007&quot;,&quot;9:36am&quot;,-0.23,225400 csv文档的读取1. 常规读取下面向你展示如何将这些数据读取为一个元组的序列： 1234567import csvwith open('stocks.csv') as f: f_csv = csv.reader(f) headers = next(f_csv) for row in f_csv: # Process row ... 在上面的代码中， row 会是一个列表。因此，为了访问某个字段，你需要使用下标，如 row[0]访问Symbol， row[4] 访问Change。==这样可以通过外建字典来存储读出的csv数据。== 2. 命名元组由于这种下标访问通常会引起混淆，你可以考虑使用==命名元组==。例如： 123456789from collections import namedtuplewith open('stock.csv') as f: f_csv = csv.reader(f) headings = next(f_csv) Row = namedtuple('Row', headings) for r in f_csv: row = Row(*r) # Process row ... 它允许你使用列名如 row.Symbol 和 row.Change 代替下标访问。 需要注意的是这个只有在列名是合法的Python标识符的时候才生效。如果不是的话， 你可能需要修改下原始的列名(如将非标识符字符替换成下划线之类的)。 3. 字典另外一个选择就是将数据读取到一个字典序列中去。可以这样做： 123456import csvwith open('stocks.csv') as f: f_csv = csv.DictReader(f) for row in f_csv: # process row ... 在这个版本中，你可以使用列名去访问每一行的数据了。比如，row[&#39;Symbol&#39;] 或者 row[&#39;Change&#39;]。 fieldnames 是dict_reader的一个属性，表示CSV文档的数据名称。可以通过f_csv.fieldnames来访问数据名称那一行。 CSV文件写入为了写入CSV数据，你仍然可以使用csv模块，不过这时候先创建一个 writer 对象。例如: 12345678910headers = ['Symbol','Price','Date','Time','Change','Volume']rows = [('AA', 39.48, '6/11/2007', '9:36am', -0.18, 181800), ('AIG', 71.38, '6/11/2007', '9:36am', -0.15, 195500), ('AXP', 62.58, '6/11/2007', '9:36am', -0.46, 935000), ]with open('stocks.csv','w') as f: f_csv = csv.writer(f) f_csv.writerow(headers) f_csv.writerows(rows) 如果你有一个字典序列的数据，可以像这样做： 12345678910111213headers = ['Symbol', 'Price', 'Date', 'Time', 'Change', 'Volume']rows = [&#123;'Symbol':'AA', 'Price':39.48, 'Date':'6/11/2007', 'Time':'9:36am', 'Change':-0.18, 'Volume':181800&#125;, &#123;'Symbol':'AIG', 'Price': 71.38, 'Date':'6/11/2007', 'Time':'9:36am', 'Change':-0.15, 'Volume': 195500&#125;, &#123;'Symbol':'AXP', 'Price': 62.58, 'Date':'6/11/2007', 'Time':'9:36am', 'Change':-0.46, 'Volume': 935000&#125;, ]with open('stocks.csv','w') as f: f_csv = csv.DictWriter(f, headers) f_csv.writeheader() f_csv.writerows(rows) 其中f_csv.writeheader()也可以替换成f_csv.writerow(dict(zip(headers, headers))) 参考链接 https://python3-cookbook.readthedocs.io/zh_CN/latest/c06/p01_read_write_csv_data.html https://blog.csdn.net/guoziqing506/article/details/52014506]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>文件读取</tag>
        <tag>csv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neo4j初始化节点显示设置]]></title>
    <url>%2Fpost%2FNeo4j_initializes_the_node_display_Settings%2F</url>
    <content type="text"><![CDATA[问题描述：neo4j中有默认的初始化节点显示设置为300个节点，如果想要显示的节点多于300个，则会只显示300个，并给予以下提示语句： Not all return nodes are being displayed due to Initial Node Display setting. Only 300 of 300 nodes are being displayed. 解决方法：在如图所示initial Node Display处可以修改，在此处修改为300000.]]></content>
      <categories>
        <category>Neo4j</category>
      </categories>
      <tags>
        <tag>Neo4j</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Bidirectional LSTM-CRF Models for Sequence Tagging》阅读笔记]]></title>
    <url>%2Fpost%2Fread_Bidirectional_LSTM-CRF_Models_for_Sequence_Tagging%2F</url>
    <content type="text"><![CDATA[这篇论文可以作为一个RNN和LSTM学习的一个例子来看，有利于新手对LSTM的理解。对于NER的处理主要是作为一个序列标注问题。但是作为经典文章还是可以读一读了解一下的。 在本篇论文中，作者提出了4种模型：LSTM、BI-LSTM、LSTM-CRF和BI-LSTM-CRF。 contribution(贡献) 作者在NLP标注数据集上系统的对比了以上四个模型； 作者是首先提出把BI-LSTM-CRF模型用于NLP序列标注，并且达到了state-of-the-art的水平； 作者展示了BI-LSTM-CRF是robust，并且极少依赖于词向量。 model(模型)LSTM首先，作者先介绍了RNN的结构和工作原理，如图： 其中输入为句子：EU rejects German call to boycott British lamb。输出为标签：B-ORG O B-MISC O O O B-MISC O O，其中B-，I-表示实体开始和中间位置。标签种类为：other (O)和四种实体标签：Person (PER), Location (LOC), Organization (ORG), and Miscellaneous (MISC). 输入层表示在时间步 t 的特征。它们可以是 one-hot-encoding 的词特征，稠密或者稀疏的向量特征。输入层与特征有相同大小的维度。输出层表示在时间步 t 的标签上的概率分布，维度与标注数量相同。相比前馈神经网络，RNN 引入前一个隐藏状态和当前隐藏状态的结合，因此可以储存历史信息。 涉及公式为： 其中，U，W，V都是权重，函数f，g分别为sigmoid和softmax函数。 接下来，作者展示了LSTM的结构和原理，如图： 公式： 其中，σ是逻辑sigmoid函数，i, f, o 和 c分别是输入门，忘记门，输出门和细胞向量，所有的大小都和向量h一样。w权重的含义如其下表所示。 LSTM序列标注模型如图所示： 其中，中间的画斜线的格子即为图2中所示部分。 Bidirectional LSTM(双向LSTM)作者展示了双向LSTM的结构，如图所示： 双向LSTM网络可以有效利用过去特征和未来特征。在作者的实现中，对于整个句子的前向和后向操作，作者只需要在每个句子开始时将隐藏状态重置为0。作者采用批处理，使得可以同时处理多个句子。 CRF使用邻居标记信息预测当前标记有两种不同的方法： 预测每个时间步长的标签分布，然后使用波束式解码来找到最优的标签序列，代表方法：MEMMs 注重句子层次而不是个体位置，代表方法：CRF，输入和输出是直接相连的；如图： 研究表明，CRFs一般能够产生更高的标签精度。 LSTM-CRF作者展示了LSTM-CRF的结构，如图： 这网络可以有效地通过 LSTM 利用过去的输入特征和通过 CRF 利用句子级的标注信息。图中CRF层由连接连续输出层的线表示。CRF层有一个状态转移矩阵作为参数。 公式为： 函数f为网络的输出分数，[x]为输入， [fθ]i,t 为带有参数θ（句子x，第i 个标签，第t个单词）的网络输出； [A]i,j为转移分数，从连续的时间步i状态到j状态的转移分数。注意，该转换矩阵是位置无关的。 BI-LSTM-CRF作者展示了BI-LSTM-CRF的结构，如图所示： 作者在实验中展示了额外的未来特征可以提高标签的准确率。 训练过程本文使用的所有模型都共享一个通用SGD前向和后向训练过程。作者展示了BI-LSTM-CRF的算法，如图 作者设置了批次大小为100。 实验data作者在以下三个数据集上测试自己的模型：Penn TreeBank (PTB) POS tagging, CoNLL 2000 chunking, and CoNLL 2003 named entity tagging.数据集信息展示如下： Features作者从三个数据集中提取出其公共特征。特征可以分为拼写特征和上下文特征。最终，作者对于POS（词性标注）、chunking（组块）和NER（命名实体识别）分别提取401K，76K和341K个特征。 spelling features（拼写特征）除了小写字母特征之外，我们提取给定单词的以下特征。 context featurs（上下文特征）对于单词特征，作者使用unigram和bi-grams特征。对于在CoNLL2000数据集中的POS特征和在CoNLL2003数据集中的 POS &amp; CHUNK特征，作者使用了unigram，bi-gram和tri-gram特征。 词向量词向量在改进序列标注任务的表现方面起着至关重要的作用，我们使用 130K 词汇并且每个词汇的词向量维度是 50 维。我们将 one-hot-encoding词表示替换每个词对应的词向量。 Features connection tricks我们可以将拼写和上下文特征与单词特征一样对待。这样网络的输入包括单词，单词的拼写和上下文特征。然而，==我们发现将拼写和上下文特征与输出直接连接可以加速训练过程，同时也能保持标注的准确率，==如下图所示： 我们注意到，这种特征的使用与使用的最大熵特征类似。区别在于采用特征三列技术可能会发生特征冲突。由于序列标注数据集中的输出标签小于语言模型（通常为数十万），所以我们可以在特征和输出之间建立完整的连接，以避免潜在的特征冲突。 结果在相同的数据集上分别训练LSTM，BI-LSTM，CRF，LSTM-CRF和BI-LSTM-CRF模型，并且采用两种方式初始化word embedding：随机和Senna方式。模型的训练速率为0.1，隐藏层数量为300.不同模型在不同word embedding下的结果如表2所示，同时列出了之前最好模型Cov-CRF。 与Cov-CRF比较 实验中设置了3个基准模型，分别为LSTM、BI-LSTM和CRF，结果中LSTM在三个数据集中效果最差，BI-LSTM跟CRF在POS和chunking中效果接近，但是在NER中后者要优于前者。有趣的是表现最好的模型BI-LSTM-CRF相对于Cov-CRF来说对Senna embedding的依赖程度更小。 (robustness)模型鲁棒性 为验证模型的鲁棒性，对不同模型只采用word feature特征进行训练，训练结果如表3，括号中数字表示相比于全部特征，模型的结果下降数值。 与其他系统的比较 这里就不贴图了，总之就是阐述作者自己模型好。 结论总之作者的模型是基于之前模型的一些改进，主要运用了IBI-LSTM和CRF的结合。 论文下载地址]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>NER</tag>
        <tag>LSTM</tag>
        <tag>BI-LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CN-DBpedia A Never-Ending Chinese Knowledge Extraction SystemCN-DBpedia System]]></title>
    <url>%2Fpost%2Fessay%2F</url>
    <content type="text"><![CDATA[前言 本篇论文为2016年的一篇论文，主要介绍了作者构建中文知识图谱所遇到的一些问题和解决方法。 challenge 如何降低人力成本？ 如何保持知识库的新鲜度？ 贡献 在构建中文知识库中降低了人力成本： 重复利用已经存在的本体论 提出了一个不用人工监督的端到端的深度学习模型 提出了一个智能主动更新策略 系统结构 提高知识库质量： Normalization： normalize the attributes and values Enrichment：reuse the ontology Correction：two steps error detection: rule-based detection based on user feedbacks error correction crowd-sourcing 降低人力成本这部分作者采用了两种方法： 重复利用已经存在在知识库的本体论和类型化的中文实体 构建一个端到端提取器 Cross-Lingual Entity Typing（跨语言的实体类型） 第一步是通过用英文DBpedia类型来类型化中文实体。为了达到这个目的，作者提出了如下系统：系统建立了监督层次分类模型，系统输入为没有标记类型的中文实体，输出为在DB中所有有效的英文类型。作者将中文实体与共享相同中文标签名称的英语实体配对，这样中文实体以及配对英语实体的类型自然是标记样本。 用上述方法得到的训练集可能出现下面一些问题： 英文DBpedia实体类型在许多情况下可能不完全； 英文DBpedia实体类型在许多情况下可能是错误的； 中英文链接可能出错； 中文实体的特征通常不完整。 为了解决以上问题，作者提出了两种方法： 完善英文DBpedia实体类型； 设计一个过滤步骤来剔除错误样本。 infobox completion Infobox completion is a task to extract object for a given pair of entity and predicate from encyclopedia articles. 作者建模了一个seq2seq模型，输入为包含tokens的自然语言句子，输出为每个token的标签。对于标签为0或1。 对于建立一个有效的提取器有以下两个关键： 如何构建训练集：作者采用远程监督方法（利用Wikipedia） 如何选取期望的提取模型：LSTM-RNN，如图所示 知识库更新作者采用动态更新：识别新实体或可能包含新事实的旧实体 作者根据以下两方面来辨别这些实体： 近期热点新闻中提及的实体 在搜索引擎的流行搜索关键字或其他流行网页中提到的实体 对于如何从新闻标题和搜素指令中提取实体名字，作者采用简单的词分割方法，从百科全书中判断其是否为实体，并提出IDF值低的分割子串。 统计数据 论文下载链接]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
        <tag>机器学习</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ontology reasoning with deep neural networks]]></title>
    <url>%2Fpost%2Fessay%2F</url>
    <content type="text"><![CDATA[Ontology reasoning with deep neural networks（基于深度神经网络的本体推理）前言 本论文属于知识图谱的下游任务，也就是知识图谱的应用的一个例子。这篇论文的方法根据作者描述RRN是第一个基于深度学习的全面本体推理方法。 目标获得一个可以在不同的场景进行有效推理的模 问题描述基于机器学习的推理文章通常假设了一个特定的应用案例：自然语言或视觉输入的推理。作者采用一个不同的方法：将正式的推理问题作为起点。对于特定的问题选择：选择一种在表现力与另一方面复杂性之间取得适当平衡的方法通常是明智的。OWL RL本体推理是指一种常见的场景，在这种场景中，用于推理的推理规则（在此上下文中称为本体）与我们寻求推理的事实信息一起指定。 本体推理是一种非常灵活的工具，它允许对大量不同的场景进行建模，因此满足了我们对适用于各种应用的系统的需求。==首先引出了什么是本质推理，然后进一步阐述为什么要用机器学习== 今天用于推理的大多数KRR形式都植根于符号逻辑,这些方法在实践中会遇到许多问题：例如处理不完整，冲突或不确定数据的困难机器学习模型通常具有高度可扩展性，更能抵抗数据中的干扰，并且即使所提供的形式是错误的也能够提供预测。 作者的目标是通过采用尖端的深度学习技术，目标是在近似于形式方法的高度期望（理论）属性和另一方面利用机器学习的稳健性之间管理平衡行为。 对于用于推理的知识图谱：作者采用的是由个体、类和二元关系组成的信息构成，其中个体对应于顶点，关系对应于被标记的有向边缘，类对应于二进制顶点标签。关系是主体和客体之间的关系或者个人和类之间的关系。这与关系学习不同：在关系学习的背景下，知识图通常通过将类视为个人以及将成员视为普通关系来简化。然而，就作者的目的而言，明确区分类和关系是很重要的，因为在用于推理的知识图谱中类和关系可能不同。 模型总览整个模型是以RRN为基础进行构建的，每个RRN都针对特定的本体进行训练。当训练模型应用于一组特定的事实时，它分为如下两个步骤： 它为所有的步骤生成矢量表示，也就是嵌入在所考虑数据中出现的个体。 它仅基于这些生成向量计算查询预测 在图中， a中它考虑一个事实三元组，并根据数据集重复多次。 b中它每读取一个事实就获取三元组中的个体潜入，并将他们的反馈送入更新层，该层产生已提供的嵌入的更新版本，然后将其存储在前一个版本的位置。 c中从随机生成的向量开始，逐步更新嵌入，以便对关于它们所代表的个体的事实和推论进行编码。 评估作者在四个不同的数据集上训练和评估了RRN，其中两个是人工生成的玩具数据集，两个是从现实世界的数据库中提取的。这样做的原因： 玩具问题具有很大的优势，即很明显某些推论是多么困难，从而为我们提供了对模型能力的相当好的印象。 在现实环境中评估方法当然是性能不可或缺的衡量标准 作者为了评估真实世界数据的RRN模型，还从从两个著名的知识库DBpedia和Claros中提取了数据集。 结果 RRN能够有效地编码提供的关于类和关系的事实 对于关系的推理，可以看到DBpedia的准确度略低于98.9％，而其他数据集中的可导出关系在所有情况中至少99.6％被正确预测。 可以预测该模型在预测可推断类别方面比在关系方面表现更好，因为大多数这些都是仅依赖于单个三元组的推论。 为了评估作者提出的KRR方法常常遇到的问题，作者进行了如下实验： 对于缺少信息的问题，作者随机删除了一个无法通过每个样本的符号推理推断出的事实，并检查模型是否能够正确地重建它。结果：对于DBpedia来说，33.8％的失踪三元组就是这种情况，而对于Claros来说，38.4％被正确预测 对于冲突的问题，作者通过在每个测试样本中随机选择一个事实来测试模型解决冲突的能力，并添加相同的否定版本作为另一个事实。对于DBpedia，RRN正确解决了88.4％的引入冲突，而对于Claros，它甚至达到了96.2％。然而，最重要的是，对于任何一个损坏的数据集，之前报告的总精度都没有下降超过0.9。 所有RRN的查询预测都完全基于它为各个数据集中的个体生成的嵌入，这就是为什么仔细研究这样一组嵌入向量是有益的。 思考本论文属于知识图谱的下游任务，也就是知识图谱的应用的一个例子。这篇论文的方法根据作者描述RRN是第一个基于深度学习的全面本体推理方法。但是具体的操作方法论文中写的比较清晰，感觉自己是理解了。重点就是对于个体的嵌入表示，如果类比的话就是词向量，作者通过不断的处理更新这个词向量，最后通过所获的词向量进行推理。并且从这篇文章中可以看到作者使用的知识图谱和我之前在弄的关系三元组有所区别。 论文下载链接]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
        <tag>深度学习</tag>
        <tag>Ontology</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初次见面，你好NYSDY！]]></title>
    <url>%2Fpost%2Fessay%2F</url>
    <content type="text"></content>
  </entry>
</search>
