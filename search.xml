<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>2016 TransG : A Generative Model for Knowledge Graph Embedding阅读笔记</title>
    <url>/post/TransG_:_A_Generative_Model_for_Knowledge_Graph_Embedding/</url>
    <content><![CDATA[<blockquote>
<p><a href="https://www.aclweb.org/anthology/P16-1219.pdf" target="_blank" rel="noopener">论文下载地址</a></p>
</blockquote>
<h1 id="解决问题"><a href="#解决问题" class="headerlink" title="解决问题"></a>解决问题</h1><p>multiple relation semantics（多重关系语义）：一个关系可能具有与对应的三元组关联的实体对揭示的多种含义。</p>
<p><img src="http://image.nysdy.com/20191015092730.png" alt></p>
<p>这里以TransE的可视化为例，表明：特定关系有不同的聚类，并且不同的聚类表示不同的潜在语义，证实了该问题的存在性。</p>
<h3 id="该现象产生原因"><a href="#该现象产生原因" class="headerlink" title="该现象产生原因"></a>该现象产生原因</h3><ul>
<li>人为简化<ul>
<li>知识库策展人不能涉及太多相似关系，因此将多个相似关系抽象为一个特定关系是一种常见的技巧</li>
</ul>
</li>
<li>知识性质<ul>
<li>语言和知识表示形式常常涉及不明确的信息。 知识的模糊性意味着语义上的混合</li>
</ul>
</li>
</ul>
<h1 id="TransG"><a href="#TransG" class="headerlink" title="TransG"></a>TransG</h1><p>利用贝叶斯非参数无限混合模型通过为关系生成多个翻译组件来处理多个关系语义</p>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>2016</tag>
        <tag>ACL</tag>
        <tag>KGE</tag>
      </tags>
  </entry>
  <entry>
    <title>Diachronic Embedding for Temporal Knowledge Graph Completion</title>
    <url>/post/Diachronic%20Embedding%20for%20Temporal%20Knowledge%20Graph%20Completion/</url>
    <content><![CDATA[<blockquote>
<p><a href="https://arxiv.org/pdf/1907.03143.pdf" target="_blank" rel="noopener">论文下载地址</a></p>
</blockquote>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><h2 id="what-your-paper-is-about"><a href="#what-your-paper-is-about" class="headerlink" title="what your paper is about?"></a>what your paper is about?</h2><p>temporal knowledge graph completion</p>
<h2 id="what-problem-it-solves"><a href="#what-problem-it-solves" class="headerlink" title="what problem it solves?"></a>what problem it solves?</h2><p>提出了一个Diachronic Embedding 能和已有的static模型结合来解决temporal knowledge graph completion，并且能生成一个unseen timestamps。</p>
<h2 id="why-the-problem-is-interesting"><a href="#why-the-problem-is-interesting" class="headerlink" title="why the problem is interesting?"></a>why the problem is interesting?</h2><p>Developing temporal KG embedding models is an increasingly important problem.</p>
<h2 id="what-is-really-new-and-what-isn’t"><a href="#what-is-really-new-and-what-isn’t" class="headerlink" title="what is really new (and what isn’t)?"></a>what is really new (and what isn’t)?</h2><ol>
<li>novel models for temporal KG completion through equipping static models with a diachronic entity embedding function which provides the characteristics of entities at any point in time. This is in contrast to the existing temporal KG embedding approaches where only static entity features are provided.</li>
<li>The proposed embedding function is model-agnostic and can be potentially combined with any static model.</li>
<li>combining it with SimplE results in a fully expressive model for temporal KG completion.（作者还提供了证明，不过我没看）</li>
</ol>
<h2 id="Diachronic-Embedding"><a href="#Diachronic-Embedding" class="headerlink" title="Diachronic Embedding"></a>Diachronic Embedding</h2><p><img src="http://image.nysdy.com/20200702191834.png" alt></p>
<p><img src="http://image.nysdy.com/20200702191909.png" alt></p>
<p>γ用来控制temporal的比例。</p>
<h1 id="experiments"><a href="#experiments" class="headerlink" title="experiments"></a>experiments</h1><p><img src="http://image.nysdy.com/20200702192000.png" alt></p>
<ul>
<li>Activation Function</li>
<li>Adding Diachronic Embedding for Relations:<ul>
<li>实验展示了关系随时间变化影响小，即使加入关系和时间的联系对结果提升也不大</li>
</ul>
</li>
<li>Generalizing to Unseen Timestamps<ul>
<li>we created a variant of the ICEWS14 dataset by including every fact except those on the 5th, 15th, and 25th day of each month in the train set. We split the excluded facts randomly into validation and test sets (removing the ones including entities not observed in the train set). This ensures that none of the timestamps in the validation or test sets has been observed by the model in the train set.</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>KGE</tag>
        <tag>KG</tag>
        <tag>temporal KG</tag>
        <tag>2019</tag>
      </tags>
  </entry>
  <entry>
    <title>2015 TransA An Adaptive Approach for Knowledge Graph Embeddin阅读笔记</title>
    <url>/post/2015_TransA_An_Adaptive_Approach_for_Knowledge_Graph_Embeddin/</url>
    <content><![CDATA[<blockquote>
<p>这篇文章具体的公式操作比较难以理解，但是对于我来说，它的每一维度加权和最后判断时也应该考虑不同维度差异和我的思路比较像。</p>
<p><a href="https://arxiv.org/pdf/1509.05490" target="_blank" rel="noopener">论文下载地址</a></p>
</blockquote>
<a id="more"></a>
<h1 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h1><p>之前的机遇翻译的方法过于简化损失度量，使得模型没有足够的能力来模拟知识库中的各种复杂实体/关系。</p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><ul>
<li>由于损失度量的不灵活性，当前基于翻译的方法应用具有不同合理性的球面等势超表面，其中三元组更靠近中心，三元组更合理。如图1（a）所示，球面等势超表面不够灵活，不足以表征拓扑结构。</li>
</ul>
<p><img src="http://image.nysdy.com/20190909156799696760406.png" alt="20190909156799696760406.png"></p>
<ul>
<li><p>其次，由于过度简化的损失度量，当前基于翻译的方法用同性的欧氏距离无法体现出各个维度的重要性，而是将各个维度的特征都等同看待。导致图2中出现的问题：<img src="http://image.nysdy.com/2019090915679977198009.png" alt="2019090915679977198009.png"></p>
<p>由于对每一维度处理相同，不正确的实体将被匹配。但是通过对不同维度进行不同加权将会避免这种由于距离相同被匹配的错误。 </p>
<h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1></li>
</ul>
<p>作者提出TransA，一种利用自适应和灵活度量的嵌入方法：</p>
<ul>
<li>TransA采用椭圆形表面而不是球形表面：通过这种方式，复杂关系引起的复杂嵌入拓扑结构可以得到更好的表现。</li>
<li>如“自适应度量方法”中所分析的那样，TransA可以被视为加权变换特征维度。 因此，来自不相关尺寸的噪声被抑制。 </li>
</ul>
<h3 id="自适应度量分数函数"><a href="#自适应度量分数函数" class="headerlink" title="自适应度量分数函数"></a>自适应度量分数函数</h3><p>TransA采用自适应Mahalanobis绝对损失距离：</p>
<script type="math/tex; mode=display">
f_{r}(h, t)=(|\mathbf{h}+\mathbf{r}-\mathbf{t}|)^{\top} \mathbf{W}_{\mathbf{r}}(|\mathbf{h}+\mathbf{r}-\mathbf{t}|)</script><p>其中</p>
<script type="math/tex; mode=display">
|\mathbf{h}+\mathbf{r}-\mathbf{t}| \doteq\left(\left|h_{1}+r_{1}-t_{1}\right|,\left|h_{2}+r_{2}-t_{2}\right|, \ldots, | h_{n}+\right.\left.\left.r_{n}-t_{n}\right\rfloor\right)</script><p>，$W_r$是对应于特定关系的对称非负权重矩阵，也是自适应权重矩阵。与传统的得分函数不同，作者取绝对值，因为想要测量（h + r）和t之间的绝对损失。原因有以下两点：</p>
<ul>
<li><p>作者将的分数函数扩展为诱导规范：</p>
<script type="math/tex; mode=display">
N_{r}(\mathbf{e})=\sqrt{f_{r}(h, t)}</script><p>这样可以用以下方式来简化运算：</p>
<script type="math/tex; mode=display">
\begin{array}{l}{\text { inequality } N_{r}\left(\mathbf{e}_{1}+\mathbf{e}_{2}\right)=\sqrt{\left|\mathbf{e}_{1}+\mathbf{e}_{2}\right|^{\top} \mathbf{W}_{\mathbf{r}}\left|\mathbf{e}_{\mathbf{1}}+\mathbf{e}_{\mathbf{2}}\right|} \leq}  {\sqrt{\left|\mathbf{e}_{\mathbf{1}}\right|^{\top} \mathbf{W}_{\mathbf{r}}\left|\mathbf{e}_{\mathbf{1}}\right|}+\sqrt{\left|\mathbf{e}_{\mathbf{2}}\right|^{\top} \mathbf{W}_{\mathbf{r}}\left|\mathbf{e}_{\mathbf{2}}\right|}=N_{r}\left(\mathbf{e}_{\mathbf{1}}\right)+N_{r}\left(\mathbf{e}_{\mathbf{2}}\right)}\end{array}</script></li>
<li><p>在几何中，负值或正值表示向下或向上的方向。而在作者的方法中，作者不考虑这个因素。 让作者看一下如图2所示的实例。 对于实体Goniff，其损耗向量的x轴分量是负的，因此扩大该分量将使整体损失更小，而这种情况应该使整体损失更大。 因此，绝对算子对作者的方法至关重要。 对于没有绝对算子的数值例子，当嵌入维数为2时，权重矩阵为[0 1; 1 0]和损失矢量（h + r  -  t）=（e1，e2），总损失为2e1e2。 如果e1≥0且e2≤0，则绝对更大的e2将减少总损耗，这是不希望的</p>
</li>
</ul>
<h3 id="从等势面的角度"><a href="#从等势面的角度" class="headerlink" title="从等势面的角度"></a>从等势面的角度</h3><p>对于其他基于翻译的方法，等势超曲面是欧几里德距离定义的球体：</p>
<script type="math/tex; mode=display">
\|(\mathbf{t}-\mathbf{h})-\mathbf{r}\|_{2}^{2}=\mathcal{C}</script><p>其中，$\mathcal{C}$表示阈值或等势值。</p>
<p>而对于TransA来说，等势面是椭圆</p>
<script type="math/tex; mode=display">
|(\mathbf{t}-\mathbf{h})-\mathbf{r}|^{\top} \mathbf{W}_{\mathbf{r}}|(\mathbf{t}-\mathbf{h})-\mathbf{r}|=\mathcal{C}</script><p>​    由于知识库是大规模且非常复杂的实际情况，嵌入的拓扑结构不能像球体那样均匀分布，如图1所示。 因此，用椭圆形替换球面等势超曲面将增强嵌入</p>
<h3 id="从特征权重角度"><a href="#从特征权重角度" class="headerlink" title="从特征权重角度"></a>从特征权重角度</h3><p>TransA可以看做是带有权重的特征变换，假设权重矩阵$W_r$是对称矩阵，那么可以通过LDL分解将权重分解为</p>
<script type="math/tex; mode=display">
\begin{array}{c}{\mathbf{W}_{\mathbf{r}}=\mathbf{L}_{\mathbf{r}}^{\top} \mathbf{D}_{\mathbf{r}} \mathbf{L}_{\mathbf{r}}} \\ {f_{r}=\left(\mathbf{L}_{\mathbf{r}}|\mathbf{h}+\mathbf{r}-\mathbf{t}|\right)^{\top} \mathbf{D}_{\mathbf{r}}\left(\mathbf{L}_{\mathbf{r}}|\mathbf{h}+\mathbf{r}-\mathbf{t}|\right)}\end{array}</script><p>相当于对loss向量通过$L_r$进行特征变换，其中，$\mathbf{D}_{r}=\operatorname{diag}\left(w_{1}, w_{2}, \ldots\right)$是一个对角矩阵，对角元素的值就是不同嵌入维度的权值。</p>
<h3 id="对比之前方法"><a href="#对比之前方法" class="headerlink" title="对比之前方法"></a>对比之前方法</h3><p>与关于旋转和缩放嵌入空间的TransR，TransA具有两个优势。</p>
<ul>
<li><p>首先，作者对特征尺寸进行加权以避免噪音。 </p>
</li>
<li><p>其次，作者放松了PSD条件以获得灵活的表示。 </p>
</li>
</ul>
<p>与使用预先计算的系数重新构造对特征尺寸进行加权的TransM，TransA具有两个优势。 </p>
<ul>
<li>首先，作者从数据中学习权重，这使得分数函数更具适应性。 </li>
<li>其次，作者应用特征转换，使嵌入更有效</li>
</ul>
<h2 id="算法训练"><a href="#算法训练" class="headerlink" title="算法训练"></a>算法训练</h2><p>loss函数为：</p>
<script type="math/tex; mode=display">
\begin{array}{c}{\mathcal{L}=\sum_{(h, l, t) \in \Delta\left(h^{\prime}, l^{\prime}, t^{\prime}\right) \in \Delta^{\prime}}\left[\gamma+f_{r}(\mathrm{h}, \mathrm{t})-f_{r^{\prime}}\left(\mathrm{h}^{\prime}, \mathrm{t}^{\prime}\right)\right]_{+}+\lambda\left(\sum_{r \in R}\|\mathbf{W} r\|_{F}^{2}\right)+C\left(\sum_{e \in E}\|\mathbf{e}\|_{2}^{2}+\sum_{r \in R}\left\|\mathbf{r}_{2}^{2}\right\|\right)} \\ {\text { s.t. }\left[\mathbf{W}_{r}\right]_{i j} \geq 0}\end{array}</script><p>保证非负性，作者将所有否定条目权重的值赋为0</p>
<script type="math/tex; mode=display">
\mathbf{W}_{r}=-\sum_{(h, r, t) \in \Delta}\left(|\mathbf{h}+\mathbf{r}-\mathbf{t} \| \mathbf{h}+\mathbf{r}-\mathbf{t}|^{\top}\right)+\sum_{\left(h^{\prime}, r^{\prime}, t^{\prime}\right) \in \Delta^{\prime}}\left(\left|\mathbf{h}^{\prime}+\mathbf{r}^{\prime}-\mathbf{t}^{\prime}\right|\left|\mathbf{h}^{\prime}+\mathbf{r}^{\prime}-\mathbf{t}^{\prime}\right|^{\top}\right)</script><h3 id="模型复杂度"><a href="#模型复杂度" class="headerlink" title="模型复杂度"></a>模型复杂度</h3><p>至于作者模型的复杂性，权重矩阵完全由现有的嵌入向量计算，这意味着TransA几乎具有与TransE相同的自由参数数。 至于作者模型的效率，权重矩阵有一个封闭的解决方案，这在很大程度上加快了培训过程</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>作者统计了一个ATPE值（Averaged Triple number Per Entity.）该数量衡量数据集的多样性和复杂性</p>
<p>作者对于TransA在WN18上表现不好进行了分析解释。</p>
<blockquote>
<p>TransA在WN18数据集上的平均排名表现不佳。 深入研究详细情况，我们发现有27个测试三元组（测试集的0.54％），其排名超过30,000，这几个案例将导致约162个平均等级损失。 所有这些三元组的尾部或头部实体从未与训练集中的相应关系共存。 训练数据不足导致权重矩阵过度扭曲，权重矩阵过度扭曲导致平均排名不良</p>
</blockquote>
<p>作者还做了一个实验</p>
<p><img src="http://image.nysdy.com/20190909156803658869046.png" alt="20190909156803658869046.png"></p>
<p>作者解释：</p>
<blockquote>
<p>精度随重量差异而变化，这意味着特征加权有利于精确度。 这证明了TransA的理论分析和有效性</p>
</blockquote>
<pre><code>但是我并不认为这个可以说明什么，那是不是调高其他关系的权重会带来该关系效果的提升？
</code></pre>]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>KG</category>
      </categories>
      <tags>
        <tag>KGE</tag>
        <tag>TransA</tag>
        <tag>2015</tag>
        <tag>arxiv</tag>
      </tags>
  </entry>
  <entry>
    <title>2016 Knowledge Graph Completion with Adaptive Sparse Transfer Matrix阅读笔记</title>
    <url>/post/2016_Knowledge_Graph_Completion_with_Adaptive_Sparse_Transfer_Matrix/</url>
    <content><![CDATA[<blockquote>
<p>为解决heterogeneity和imbalance问题，作者针对同一关系链接实体对的数量和同一关系不同头尾实体数量，分别设计了不同稀疏程度的转移矩阵。存在缺点：并没有同时解决这两个问题。</p>
<p><a href="https://link.zhihu.com/?target=https%3A//www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/11982/11693" target="_blank" rel="noopener">论文下载地址</a></p>
</blockquote>
<a id="more"></a>
<h1 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h1><ul>
<li>heterogeneity（异质性）：一些关系链接了很多实体对，另一些没有</li>
<li>imbalance（不平衡）： 在一种关系中，头实体和尾实体的数量不同</li>
</ul>
<h1 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h1><ol>
<li>作者提出了一种新颖的方法，该方法考虑了先前模型中未使用的异质性和不平衡性，以嵌入知识图来完成它们； </li>
<li>作者的方法高效且参数较少，因此很容易扩展到大规模知识图；</li>
<li>作者为转移矩阵提供了两种稀疏模式，并分析了它们的优缺点；</li>
<li>在三元组分类和链接预测任务中，作者的方法达到了最先进的性能</li>
</ol>
<h1 id="Sparse-Matrix"><a href="#Sparse-Matrix" class="headerlink" title="Sparse Matrix"></a>Sparse Matrix</h1><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>稀疏矩阵是指大多数条目（entries）为零的矩阵。 零元素占矩阵元素总数的比例称为稀疏度（sparse degree）。</p>
<h3 id="类别"><a href="#类别" class="headerlink" title="类别"></a>类别</h3><ul>
<li>结构化的</li>
<li>非结构化的</li>
</ul>
<p><img src="http://image.nysdy.com/20191011157077419915060.png" alt="20191011157077419915060.png"></p>
<h3 id="两者重要区别"><a href="#两者重要区别" class="headerlink" title="两者重要区别"></a>两者重要区别</h3><ul>
<li>结构化模式有利于矩阵向量乘积运算。</li>
<li>非结构化往往可以带来更好的实验结果：由于更加灵活地远范围线性组合。</li>
</ul>
<h1 id="Sparse-Matrix-vs-Low-Rank-Matrix"><a href="#Sparse-Matrix-vs-Low-Rank-Matrix" class="headerlink" title="Sparse Matrix vs Low-Rank Matrix"></a>Sparse Matrix vs Low-Rank Matrix</h1><h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ul>
<li>low-rank矩阵强制一些变量要满足特定的约束，因此，矩阵<strong>M</strong>无法自由地进行赋值。</li>
<li>sparse矩阵是作者令其中的部分元素值为0，并且在训练过程中不改变它的值，其他的非零值进行训练。</li>
</ul>
<h3 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h3><ul>
<li>稀疏矩阵比低秩矩阵更灵活，可以有更大的自由度：使用低秩矩阵，那么矩阵的自由度会受到严格的秩限制。然而，sparse matrix的稀疏性只是控制矩阵元素中零元素的个数。</li>
<li>稀疏矩阵比低秩矩阵更有效率：只有非零条目参与计算，极大地减少了计算量</li>
</ul>
<h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><h2 id="TranSpare-share"><a href="#TranSpare-share" class="headerlink" title="TranSpare(share)"></a>TranSpare(share)</h2><ul>
<li>解决heterogeneity问题</li>
</ul>
<h3 id="特点-1"><a href="#特点-1" class="headerlink" title="特点"></a>特点</h3><ul>
<li>转移矩阵的稀疏度由关系链接的实体对的数量确定</li>
<li>并且关系的两侧共享相同的转移矩阵</li>
<li>对于复杂关系的转移矩阵更加稀疏</li>
</ul>
<pre><code>不知道为什么复杂转移矩阵会更加稀疏？
</code></pre><h3 id="转移矩阵的稀疏程度"><a href="#转移矩阵的稀疏程度" class="headerlink" title="转移矩阵的稀疏程度"></a>转移矩阵的稀疏程度</h3><script type="math/tex; mode=display">
\theta_{r}=1-\left(1-\theta_{\min }\right) N_{r} / N_{r^{*}}</script><p>其中，$N_r$代表链接关系$r$的实体对的数量，$r^<em>$代表链接最多实体对的关系，$\theta_{\min }\left(0 \leq \theta_{\min } \leq 1\right)$是一个超参数，代表矩阵$M_{r^{</em>}}$的最小系数程度。</p>
<pre class=" language-latex"><code class="language-latex">不理解这里为什么把稀疏程度定义成这么麻烦，为什不直接定义成<span class="token equation string">$<span class="token equation-command regex">\theta</span>_{<span class="token equation-command regex">\min</span>} N_{r} / N_{r^{*}}$</span>
</code></pre>
<h3 id="映射向量"><a href="#映射向量" class="headerlink" title="映射向量"></a>映射向量</h3><script type="math/tex; mode=display">
\mathbf{h}_{p}=\mathbf{M}_{r}\left(\theta_{r}\right) \mathbf{h}, \quad \mathbf{t}_{p}=\mathbf{M}_{r}\left(\theta_{r}\right) \mathbf{t}</script><h2 id="TranSpare-separate"><a href="#TranSpare-separate" class="headerlink" title="TranSpare(separate)"></a>TranSpare(separate)</h2><ul>
<li>解决imbalance问题</li>
</ul>
<h3 id="特点-2"><a href="#特点-2" class="headerlink" title="特点"></a>特点</h3><ul>
<li>每个关系具有两个单独的稀疏传递矩阵，一个用于头实体，另一个用于尾实体</li>
<li>稀疏度取决于通过关系链接的头（尾）实体的数量</li>
</ul>
<h3 id="转移矩阵的稀疏程度-1"><a href="#转移矩阵的稀疏程度-1" class="headerlink" title="转移矩阵的稀疏程度"></a>转移矩阵的稀疏程度</h3><script type="math/tex; mode=display">
\theta_{r}^{l}=1-\left(1-\theta_{\min }\right) N_{r}^{l} / N_{r^{*}}^{l^{*}} \quad(l=h, t)</script><p>和share类似，只是头尾实体不相同，增加l来代表头尾实体数量。</p>
<h3 id="映射向量-1"><a href="#映射向量-1" class="headerlink" title="映射向量"></a>映射向量</h3><script type="math/tex; mode=display">
\theta_{r}^{l}=1-\left(1-\theta_{\min }\right) N_{r}^{l} / N_{r^{*}}^{l^{*}} \quad(l=h, t)</script><h2 id="分数函数"><a href="#分数函数" class="headerlink" title="分数函数"></a>分数函数</h2><p>两者的分数函数相同均为：</p>
<script type="math/tex; mode=display">
f_{r}(\mathbf{h}, \mathbf{t})=\left\|\mathbf{h}_{p}+\mathbf{r}-\mathbf{t}_{p}\right\|_{\ell_{1 / 2}}^{2}</script><h2 id="总loss"><a href="#总loss" class="headerlink" title="总loss"></a>总loss</h2><p>采用margin-based ranking loss</p>
<script type="math/tex; mode=display">
$L=\sum_{(h, r, t) \in \Delta\left(h^{\prime}, r, t\right) \in \Delta^{\prime}}\left[\gamma+f_{r}(\mathbf{h}, \mathbf{t})-f_{r}\left(\mathbf{h}^{\prime}, \mathbf{t}^{\prime}\right)\right]_{+}$</script><h2 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h2><p>为了加速训练时收敛以及防止过拟合，作者使用TransE算法的结果进行初始化实体和关系的embedding向量，对于转化矩阵，作者使用单位矩阵进行初始化。但是这不是非必须的</p>
<p>对于转化矩阵(假设为单位矩阵)，非零向量的个数$n z=\lfloor\theta \times n \times n\rfloor$，由于作者使用单位向量初始化，所以除了对角线上的非零元素之外，其他非零元素的个数为$n z^{\prime}=n z-n$，如果$n z \leq n$，那么作者设置$n z^{\prime}=0$。</p>
<ul>
<li>在构建结构化的转化矩阵$\mathbf{M}(\theta)$的时候，作者要让$n z^{\prime}$非零元素对称分布在对角线的两边，如果$n z^{\prime}$不能满足要求，那么作者选择另外一个整数。</li>
<li>在构建非结构化的转化矩阵$\mathbf{M}(\theta)$的时候，作者只随机散布$\mathbf{M}(\theta)$中的$n z^{\prime}$非零元素（但不在对角线上）。</li>
</ul>
<p>在训练前，作者首先设置超参数$\theta_{\min }$，然后计算每个转化矩阵的稀疏程度，然后，作者使用结构化或非结构化模式构建稀疏转化矩阵。</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>与常规不同的就是加了一个实验<img src="http://image.nysdy.com/20191011157077982280661.png" alt="20191011157077982280661.png"></p>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>2016</tag>
        <tag>KGE</tag>
        <tag>AAAI</tag>
      </tags>
  </entry>
  <entry>
    <title>2019年计划</title>
    <url>/post/2019-plans/</url>
    <content><![CDATA[<h3 id="读个博"><a href="#读个博" class="headerlink" title="读个博"></a>读个博</h3><h3 id="吉他可以弹唱几首流行歌曲"><a href="#吉他可以弹唱几首流行歌曲" class="headerlink" title="吉他可以弹唱几首流行歌曲"></a>吉他可以弹唱几首流行歌曲</h3><p>假期报个班，请老师指点一下</p>
<h3 id="精读50篇论文"><a href="#精读50篇论文" class="headerlink" title="精读50篇论文"></a>精读50篇论文</h3><p>平均下来，每周都需要读一篇，每一篇都要勾划，发博客。</p>
<h3 id="发2篇paper"><a href="#发2篇paper" class="headerlink" title="发2篇paper"></a>发2篇paper</h3><p>上半年积累知识，做实验，暑假开始写</p>
<h3 id="考托福过90"><a href="#考托福过90" class="headerlink" title="考托福过90"></a>考托福过90</h3><p>上半年背单词等</p>
<h3 id="买个微单学个拍照"><a href="#买个微单学个拍照" class="headerlink" title="买个微单学个拍照"></a>买个微单学个拍照</h3><p>不急，现在没钱，下半年入手一个吧</p>
<h3 id="自己出门旅行一次"><a href="#自己出门旅行一次" class="headerlink" title="自己出门旅行一次"></a>自己出门旅行一次</h3><p>嗯，要是论文写完就去吧！目前计划去山东，顺便看一下我二姑。5天左右的旅行吧！</p>
<h3 id="学习滑板"><a href="#学习滑板" class="headerlink" title="学习滑板"></a>学习滑板</h3><h3 id="开始健身"><a href="#开始健身" class="headerlink" title="开始健身"></a>开始健身</h3><p>每周去三次健身房</p>
]]></content>
      <categories>
        <category>annual plans</category>
      </categories>
      <tags>
        <tag>plan</tag>
      </tags>
  </entry>
  <entry>
    <title>Adaptive Graph Encoder for Attributed Graph Embedding 阅读笔记</title>
    <url>/post/Adaptive%20Graph%20Encoder%20for%20Attributed%20Graph%20Embedding/</url>
    <content><![CDATA[<h1 id="0-总览"><a href="#0-总览" class="headerlink" title="0. 总览"></a>0. 总览</h1><h2 id="0-1-文章是关于什么的？（what？）"><a href="#0-1-文章是关于什么的？（what？）" class="headerlink" title="0.1 文章是关于什么的？（what？）"></a>0.1 文章是关于什么的？（what？）</h2><p>图卷积网络，属性图嵌入，自适应学习，拉布拉斯平滑</p>
<h2 id="0-2-要解决什么问题？（why？-challenge）"><a href="#0-2-要解决什么问题？（why？-challenge）" class="headerlink" title="0.2 要解决什么问题？（why？|challenge）"></a>0.2 要解决什么问题？（why？|challenge）</h2><p>已经存在的GCN-based 模型有三个主要缺陷：</p>
<ul>
<li>作者实验显示图卷积网络的滤波器和权重矩阵的纠缠会损害性能和鲁棒性。</li>
<li>作者表明在这些方法中的图卷积滤波器是广义拉普拉斯平滑滤波器的特例，但它们并没有保留最佳的低通特性。</li>
<li>现有算法的训练目标通常是回复邻接矩阵或特征矩阵，而这些矩阵与现实应用不总是是一致的。</li>
</ul>
<h2 id="0-3-用什么方法解决？（how？）"><a href="#0-3-用什么方法解决？（how？）" class="headerlink" title="0.3 用什么方法解决？（how？）"></a>0.3 用什么方法解决？（how？）</h2><p>作者提出了一个自适应图编码 Adaptive Graph Encoder (AGE)的属性图嵌入框架：</p>
<ul>
<li>为了更好地避免节点特征中的高频噪音，作者首次应用了精心设计的拉普拉斯平滑滤波器。</li>
<li>AGE采用自适应编码器，该编码器可以迭代地增强滤波后的功能，以实现更好的节点嵌入。</li>
</ul>
<h2 id="0-4文章有什么创新？"><a href="#0-4文章有什么创新？" class="headerlink" title="0.4文章有什么创新？"></a>0.4文章有什么创新？</h2><ul>
<li>上述方法即为创新点。</li>
</ul>
<h2 id="0-5-效果如何？"><a href="#0-5-效果如何？" class="headerlink" title="0.5 效果如何？"></a>0.5 效果如何？</h2><p>AGE超过最好的图嵌入模型在节点聚类和链接预测任务上。</p>
<h2 id="0-6-还存在什么问题？"><a href="#0-6-还存在什么问题？" class="headerlink" title="0.6 还存在什么问题？"></a>0.6 还存在什么问题？</h2><h1 id="1-背景知识"><a href="#1-背景知识" class="headerlink" title="1 背景知识"></a>1 背景知识</h1><h1 id="2-模型"><a href="#2-模型" class="headerlink" title="2 模型"></a>2 模型</h1><h2 id="符号标识"><a href="#符号标识" class="headerlink" title="符号标识"></a>符号标识</h2><p>节点向量：$ \mathcal{V}=\left\{v_{1}, v_{2}, \cdots, v_{n}\right\}$</p>
<p>特征矩阵：$\mathbf{X}= \left[\mathbf{x}_{1}, \mathbf{x}_{2} \ldots, \mathbf{x}_{n}\right]^{T}$</p>
<p>邻接矩阵：$\mathbf{A}=\left\{a_{i j}\right\} \in \mathbb{R}^{n \times n}$, $a_{i j}=1 \text { if }\left(v_{i}, v_{j}\right) \in \mathcal{E}$</p>
<p>邻居矩阵$\mathbf{A}$度矩阵:$\mathbf{D}=\operatorname{diag}\left(d_{1}, d_{2}, \cdots, d_{n}\right) \in \mathbb{R}^{n \times n}$, $d_{i}=\sum_{v_{j} \in \mathcal{V}} a_{i j}$是节点$v_i$的度。</p>
<p>拉普拉斯矩阵：$\mathbf{L}=\mathbf{D}-\mathbf{A}$</p>
<p>属性图嵌入的目的是将节点映射到低维嵌入。 我们以Z为嵌入矩阵，并且嵌入应保留图G的拓扑结构和特征信息。</p>
<p>对于下游任务：</p>
<ol>
<li><strong>节点聚类任务</strong>：旨在将节点划分为𝑚个不相交的组{𝐺1，𝐺2，····，𝐺𝑚}，其中相似的节点应在同一组中。</li>
<li><strong>链接预测任务</strong>：需要模型预测两个给定节点之间是否存在潜在边缘。</li>
</ol>
<h2 id="总体框架"><a href="#总体框架" class="headerlink" title="总体框架"></a>总体框架</h2><p><img src="http://image.nysdy.com/20200902122905.png" alt></p>
<ul>
<li>拉普拉斯平滑滤波器：设计的滤波器$\mathbf{H}$用作低通滤波器，以对特征矩阵$\mathbf{X}$的高频分量进行去噪。平滑后的特征矩阵$\tilde{\mathbf{X}}$被用作自适应编码器的输入。</li>
<li>自适应编码器：为了获得更具代表性的节点嵌入，该模块通过自适应选择高度相似或不相似的节点对来构建训练集。 然后以监督的方式训练编码器。</li>
</ul>
<h2 id="2-1-拉普拉斯平滑滤波器"><a href="#2-1-拉普拉斯平滑滤波器" class="headerlink" title="2.1 拉普拉斯平滑滤波器"></a>2.1 拉普拉斯平滑滤波器</h2><p>图学习基本假设：图上的邻近节点应该相似，因此，节点特征在图流形上应该是平滑的。</p>
<h3 id="2-1-1-平滑信号"><a href="#2-1-1-平滑信号" class="headerlink" title="2.1.1 平滑信号"></a>2.1.1 平滑信号</h3><p>为了衡量信号$x$的平滑程度，计算图的Rayleigh quotient（瑞利商）：</p>
<script type="math/tex; mode=display">
R(\mathrm{L}, \mathbf{x})=\frac{\mathbf{x}^{\top} \mathbf{L} \mathbf{x}}{\mathbf{x}^{\top} \mathbf{x}}=\frac{\sum_{(i, j) \in \mathcal{E}}\left(x_{i}-x_{j}\right)^{2}}{\sum_{i \in \mathcal{V}} x_{i}^{2}}</script><ul>
<li>该商实际上是$x$的标准化方差得分。 如上所述，平滑信号应在相邻节点上分配相似的值。 因此，具有较低瑞利商的信号被假定为更平滑。</li>
</ul>
<p>考虑图的拉普拉斯特征分解：$\mathbf{L}=\mathbf{U} \mathbf{\Lambda} \mathbf{U}^{-1}$, $\mathbf{U} \in \mathbb{R}^{n \times n}$组成特征向量，$\Lambda=\operatorname{diag}\left(\lambda_{1}, \lambda_{2}, \cdots, \lambda_{n}\right)$是特征值的对角矩阵。所以特征向量$u_i$的平滑为：</p>
<script type="math/tex; mode=display">
R\left(\mathbf{L}, \mathbf{u}_{i}\right)=\frac{\mathbf{u}_{i}^{\top} \mathbf{L} \mathbf{u}_{i}}{\mathbf{u}_{i}^{\top} \mathbf{u}_{i}}=\lambda_{i}</script><ul>
<li>该等式表示更平滑的特征向量与较小的特征值相关联，这意味着频率较低。</li>
</ul>
<p>因此，作者基于上述两个等式创造出如下信号$x$:</p>
<script type="math/tex; mode=display">
\mathbf{x}=\mathbf{U p}=\sum_{i=1}^{n} p_{i} \mathbf{u}_{i}</script><ul>
<li>$p_i$是特征向量$u_i$的系数。</li>
</ul>
<p>至此，$x$的平滑实际上是：</p>
<script type="math/tex; mode=display">
R(\mathbf{L}, \mathbf{x})=\frac{\mathbf{x}^{\top} \mathbf{L} \mathbf{x}}{\mathbf{x}^{\top} \mathbf{x}}=\frac{\sum_{i=1}^{n} p_{i}^{2} \lambda_{i}}{\sum_{i=1}^{n} p_{i}^{2}}</script><ul>
<li>因此，为了获得更平滑的信号，我们的滤波器的目标是在滤除高频分量的同时保留低频分量。 由于其高计算效率和令人信服的性能，拉普拉斯平滑滤波器[28]通常用于此目的</li>
</ul>
<h3 id="2-1-2-广义拉普拉斯平滑滤波器"><a href="#2-1-2-广义拉普拉斯平滑滤波器" class="headerlink" title="2.1.2 广义拉普拉斯平滑滤波器"></a>2.1.2 广义拉普拉斯平滑滤波器</h3><p>广义拉普拉斯平滑滤波器被定义为：</p>
<script type="math/tex; mode=display">
\mathbf{H}=\mathbf{I}-k \mathbf{L}</script><ul>
<li>$k$是一个实值。</li>
</ul>
<p>使用$\mathbf{H}$作为滤波器矩阵，被过滤的信号$\tilde{\mathbf{x}}$为：</p>
<script type="math/tex; mode=display">
\tilde{\mathbf{x}}=\mathbf{H x}=\mathbf{U}(\mathbf{I}-k \mathbf{\Lambda}) \mathbf{U}^{-1} \mathbf{U} \mathbf{p}=\sum_{i=1}^{n}\left(1-k \lambda_{i}\right) p_{i} \mathbf{u}_{i}=\sum_{i=1}^{n} p^{\prime} i \mathbf{u}_{i}</script><p>因此，为了实现低通滤波，频率响应函数1-𝑘𝜆应该是一个减量和非负函数。 堆叠𝑡Laplacian平滑滤波器，我们将滤波后的特征矩阵表示为：</p>
<script type="math/tex; mode=display">
\tilde{\mathbf{X}}=\mathbf{H}^{t} \mathbf{X}</script><ul>
<li>注意到，滤波器中是没有参数的</li>
</ul>
<h3 id="2-1-3-k的选择"><a href="#2-1-3-k的选择" class="headerlink" title="2.1.3 k的选择"></a>2.1.3 k的选择</h3><p>在实际中，使用重归一化技巧：$\tilde{A}=I+A$，作者设计一个对称归一化图拉普拉斯：</p>
<script type="math/tex; mode=display">
\tilde{\mathbf{L}}_{s y m}=\tilde{\mathbf{D}}^{-\frac{1}{2}} \tilde{\mathbf{L}} \tilde{\mathbf{D}}^{-\frac{1}{2}}</script><ul>
<li>其中$\tilde{\mathbf{D}}$和$\tilde{\mathbf{L}}$分别是与$\tilde{\mathbf{A}}$相对应的度矩阵和拉普拉斯矩阵</li>
</ul>
<p>然后滤波器变成：</p>
<script type="math/tex; mode=display">
\mathbf{H}=\mathbf{I}-k \tilde{\mathbf{L}}_{s y m}</script><ul>
<li>注意到，这里如果取$k=1$，则滤波器变成GCN滤波器。</li>
</ul>
<p>为了选择最佳$k$，应该仔细地发现特征值$\tilde{\Lambda}$的分布（从$\tilde{\mathbf{L}}_{s y m}=\tilde{\mathbf{U}} \tilde{\Lambda} \tilde{\mathbf{U}}^{-1}$的分解获得）。</p>
<p>$\tilde{x}$的平滑为：</p>
<script type="math/tex; mode=display">
R(\mathrm{L}, \tilde{\mathrm{x}})=\frac{\tilde{\mathbf{x}}^{\top} \mathrm{L} \tilde{\mathbf{x}}}{\tilde{\mathbf{x}}^{\top} \tilde{\mathbf{x}}}=\frac{\sum_{i=1}^{n} p_{i}^{2} \lambda_{i}}{\sum_{i=1}^{n} p_{i}^{2}}</script><ul>
<li>因此，$p \prime^2$随着$\lambda_i$增加应当减少。作者表示最大特征值为$\lambda_{max}$。</li>
<li>$k&lt;1 / \lambda_{\max }$就不能滤除所有高频部分，</li>
<li>$k&gt;1 / \lambda_{\max }$，滤波器不是一个低通滤波器在$\left(1 / k, \lambda_{\max }\right]$上，因为这段区间中，$p \prime^2$随着$\lambda_i$增加而增加。</li>
</ul>
<h2 id="2-2-自适应编码"><a href="#2-2-自适应编码" class="headerlink" title="2.2 自适应编码"></a>2.2 自适应编码</h2><p>对于属性图嵌入任务，两个节点之间的关系至关重要，这要求训练目标是合适的相似性度量。作者认为GAE方法选用邻接矩阵作为节点对的真实标签仅仅记录了一跳的结构信息，这远远不够。同时，平滑特征或经过训练的嵌入的相似度更加准确，因为它们将结构和特征融合在一起。为此，作者自适应地选择相似度高的节点对作为正训练样本，而相似度低的节点对作为负训练样本。</p>
<p>节点嵌入通过被过滤的节点特征线性编码为：</p>
<script type="math/tex; mode=display">
\mathbf{Z}=f(\tilde{\mathbf{X}} ; \mathbf{W})=\tilde{\mathbf{X}} \mathbf{W}</script><ul>
<li>$\mathbf{W}$是一个权重矩阵。</li>
<li>作者用了min-max scaler来讲嵌入缩减到[0,1]</li>
</ul>
<p>作者使用余弦相似度来计算节点对的相似程度：</p>
<script type="math/tex; mode=display">
\mathrm{s}=\frac{\mathrm{ZZ}^{\mathrm{T}}}{\|\mathrm{Z}\|_{2}^{2}}</script><h3 id="2-2-1训练负样例选择："><a href="#2-2-1训练负样例选择：" class="headerlink" title="2.2.1训练负样例选择："></a>2.2.1训练负样例选择：</h3><p>$r_{ij}$是节点对$\left(v_{i}, v_{j}\right)$的相似度将排序rank。节点对$\left(v_{i}, v_{j}\right)$的标签为：</p>
<script type="math/tex; mode=display">
l_{i j}=\left\{\begin{array}{ll}1 & r_{i j} \leq r_{p o s} \\ 0 & r_{i j}>r_{n e g} \\ \text { None } & \text { otherwise }\end{array}\right.</script><ul>
<li>然后作者设置了正样例的最大rank值$r_{pos}$，和负样例的最小rank值$r_{neg}$。</li>
</ul>
<p>用这种方法作者的训练集由$r_{pos}$个正样例和$n^2-r_{neg}$个负样例（因为负样例实际是远远多于正阳里的，所以这里作者生成的负样例比较多）</p>
<p>对于首次的训练集构建，由于encoder部分还未训练，直接应用平滑特征来计算初始的相似度矩阵：</p>
<script type="math/tex; mode=display">
\mathrm{S}=\frac{\tilde{\mathbf{X}} \tilde{\mathbf{X}}^{\top}}{\|\tilde{\mathbf{X}}\|_{2}^{2}}</script><p>在每个epoch中，作者随机采样$r_{pos}$个负样例来平和正负样例的数量。</p>
<p>作者采用交叉熵损失：</p>
<script type="math/tex; mode=display">
\mathcal{L}=\sum_{\left(v_{i}, v_{j}\right) \in O}-l_{i j} \log \left(s_{i j}\right)-\left(1-l_{i j}\right) \log \left(1-s_{i j}\right)</script><h3 id="2-2-2-阈值更新"><a href="#2-2-2-阈值更新" class="headerlink" title="2.2.2 阈值更新"></a>2.2.2 阈值更新</h3><p>受课程学习(curriculum learning)理念的启发，作者针对$r_{pos}$和$r_{neg}$设计了一种特定的更新策略，以控制训练集的大小。</p>
<p>在训练过程开始时，会为编码器选择更多样本以找到粗糙的聚类模式。 之后，将保留较高置信度的样本以进行训练，从而迫使编码器捕获精炼的模式。 实际上，随着训练过程的进行，$r_{pos}$减小而$r_{neg}$线性增加。 所以作者设计了不同的开始和结束时的阈值，并随着更新的次数$T$来不断变化，以达到动态调整。</p>
<script type="math/tex; mode=display">
\begin{array}{l}r_{p o s}^{\prime}=r_{p o s}+\frac{r_{p o s}^{e d}-r_{p o s}^{s t}}{T} \\ r_{n e g}^{\prime}=r_{n e g}+\frac{r_{n e g}^{e d}-r_{n e g}^{s t}}{T}\end{array}</script><p>整个算法流程如下：</p>
<p><img src="http://image.nysdy.com/20200902162217.png" alt></p>
<h1 id="3-实验"><a href="#3-实验" class="headerlink" title="3 实验"></a>3 实验</h1><h2 id="3-1-数据集"><a href="#3-1-数据集" class="headerlink" title="3.1 数据集"></a>3.1 数据集</h2><p><img src="http://image.nysdy.com/20200901165315.png" alt></p>
<blockquote>
<p>Features in Cora and Citeseer are binary word vectors, while in Wiki and Pubmed, nodes are associated with tf-idf weighted word vectors.</p>
</blockquote>
<h2 id="3-2-Baseline"><a href="#3-2-Baseline" class="headerlink" title="3.2 Baseline"></a>3.2 Baseline</h2><ul>
<li>GAE and VGAE</li>
<li>ARGA and ATVGA</li>
<li>GALA</li>
</ul>
<p>在节点聚类任务中可以分为以下三组：</p>
<ul>
<li>只用特征模型：Kmeans 和 Spectral-Clustering</li>
<li>只用图结构模型:Spectral-G, DeepWalk</li>
<li>特征和图都用模型:TADW, MGAE, AGC, DAEGC</li>
</ul>
<h2 id="3-3-评价标准"><a href="#3-3-评价标准" class="headerlink" title="3.3 评价标准"></a>3.3 评价标准</h2><p>节点聚类</p>
<ul>
<li>Accuracy (ACC), Normalized Mutual Information (NMI), and Adjusted Rand Index (ARI)</li>
</ul>
<p>链接预测</p>
<ul>
<li>Area Under Curve (AUC) and Average Precision (AP) scores</li>
</ul>
<h2 id="3-4-实验结果"><a href="#3-4-实验结果" class="headerlink" title="3.4 实验结果"></a>3.4 实验结果</h2><p>作者设计了辅助实验去回答以下三个假设：</p>
<ol>
<li>滤波器和权重矩阵的纠缠对嵌入质量没有提升；</li>
<li>对比于重构损失，作者的自适应学习策略是有效的，并且每个机制有它自己的贡献；</li>
<li>对于拉普拉斯平滑滤波器$k=1 / \lambda_{\max }$是最佳选择。</li>
</ol>
<h3 id="3-4-1-节点分类"><a href="#3-4-1-节点分类" class="headerlink" title="3.4.1 节点分类"></a>3.4.1 节点分类</h3><p><img src="http://image.nysdy.com/20200901204347.png" alt></p>
<h3 id="3-4-2-链接预测结果"><a href="#3-4-2-链接预测结果" class="headerlink" title="3.4.2 链接预测结果"></a>3.4.2 链接预测结果</h3><p>作者使用简单的内积来得到预测的邻接矩阵：</p>
<script type="math/tex; mode=display">
\hat{\mathbf{A}}=\sigma\left(\mathbf{Z} \mathbf{Z}^{\top}\right)</script><ul>
<li>其中$\sigma$是sigmoid函数</li>
</ul>
<p><img src="http://image.nysdy.com/20200902100141.png" alt></p>
<p>GALA还为链接预测任务增加了重建损失，而AGE不使用显式链接进行监督。</p>
<h3 id="3-4-3-GAE-v-s-LS-RA"><a href="#3-4-3-GAE-v-s-LS-RA" class="headerlink" title="3.4.3 GAE v.s. LS+RA"></a>3.4.3 GAE v.s. LS+RA</h3><p>回答假设1.滤波器和权重矩阵的纠缠对嵌入质量没有提升；</p>
<p>GAE在每一层中结合力权重矩阵和滤波器，如图一所示；</p>
<p>LS+RA把权重矩阵移动到滤波器之后。</p>
<p>实验结果：<img src="http://image.nysdy.com/20200902102829.png" alt></p>
<p><img src="http://image.nysdy.com/20200902100844.png" alt></p>
<h3 id="3-4-4-消融实验"><a href="#3-4-4-消融实验" class="headerlink" title="3.4.4 消融实验"></a>3.4.4 消融实验</h3><p>为了验证假设2：对比于重构损失，作者的自适应学习策略是有效的，并且每个机制有它自己的贡献；</p>
<p>LS+RA and LS+RX 展现出了可以和基线模型相比的表现，AGE模型要好于这两个模型说明自适应目标是更好的。</p>
<p>LS+RA 和 LS+RX分别在Cora, Wiki and Pubmed 和 Citeseer上表现效果更好。这说明了对于不同的数据集结构信息和特征信息的重要程度不同。此外，在Citeseer和Pubmed上，重建损失对平滑特征产生负面影响。</p>
<p>对Cora进行消融研究，以证明AGE中四种机制的功效。所有五个变体都通过对节点特征或嵌入的余弦相似度矩阵执行谱聚类来对节点进行聚类。</p>
<ul>
<li>“raw features”仅对原始节点特征执行频谱聚类；</li>
<li>“ +Filter”使用平滑节点特征对节点进行聚类；</li>
<li>“ +Encoder”从平滑节点特征的相似度矩阵初始化训练集，并通过固定的训练集学习节点嵌入；</li>
<li>“ +Adaptive”以固定的阈值自适应地选择训练样本；</li>
<li>“ +Thresholds Update”进一步添加了阈值更新策略，并且完全是完整模型。</li>
</ul>
<p><img src="http://image.nysdy.com/20200902104404.png" alt></p>
<h3 id="3-4-5-k-的选择"><a href="#3-4-5-k-的选择" class="headerlink" title="3.4.5 $k$的选择"></a>3.4.5 $k$的选择</h3><p>验证假设3：对于拉普拉斯平滑滤波器$k=1 / \lambda_{\max }$是最佳选择。</p>
<p><img src="http://image.nysdy.com/20200902104911.png" alt></p>
<h3 id="3-4-6-可视化"><a href="#3-4-6-可视化" class="headerlink" title="3.4.6 可视化"></a>3.4.6 可视化</h3><p><img src="http://image.nysdy.com/20200902113935.png" alt></p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul>
<li><a href="https://arxiv.org/pdf/2007.01594" target="_blank" rel="noopener">论文原文下载地址</a></li>
</ul>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>GCN</tag>
        <tag>adaptive learning</tag>
        <tag>attributed graph embedding</tag>
      </tags>
  </entry>
  <entry>
    <title>AM-GCN: Adaptive Multi-channel Graph Convolutional Networks 阅读笔记</title>
    <url>/post/Adaptive%20Multi-channel%20Graph%20Convolutional%20Networks/</url>
    <content><![CDATA[<h1 id="0-导读"><a href="#0-导读" class="headerlink" title="0. 导读"></a>0. 导读</h1><h2 id="0-1-文章是关于什么的？（what？）"><a href="#0-1-文章是关于什么的？（what？）" class="headerlink" title="0.1 文章是关于什么的？（what？）"></a>0.1 文章是关于什么的？（what？）</h2><p>图卷积网络</p>
<h2 id="0-2-要解决什么问题？（why？-challenge）"><a href="#0-2-要解决什么问题？（why？-challenge）" class="headerlink" title="0.2 要解决什么问题？（why？|challenge）"></a>0.2 要解决什么问题？（why？|challenge）</h2><ul>
<li>目前最好的GCN模型在融合节点特征和拓扑结构的能力上不能使人满意；</li>
<li>GCN真正从拓扑结构和节点特征中学习并融合了哪些信息？</li>
</ul>
<h2 id="0-3-用什么方法解决？（how？）"><a href="#0-3-用什么方法解决？（how？）" class="headerlink" title="0.3 用什么方法解决？（how？）"></a>0.3 用什么方法解决？（how？）</h2><ul>
<li>作者从节点特征，拓扑结构及其组合中同时抽取了特定和常见的嵌入，并使用注意力机制学习嵌入的自适应重要性权重。</li>
</ul>
<h2 id="0-4文章有什么创新？"><a href="#0-4文章有什么创新？" class="headerlink" title="0.4文章有什么创新？"></a>0.4文章有什么创新？</h2><ul>
<li>研究了如何实现GCN的拓扑结构和节点特征的融合；</li>
<li>提出了用注意力机制来自适应融合拓扑结构和节点特征；</li>
<li></li>
</ul>
<h2 id="0-5-效果如何？"><a href="#0-5-效果如何？" class="headerlink" title="0.5 效果如何？"></a>0.5 效果如何？</h2><ul>
<li>在benchmark datasets 上超过了sota GCN</li>
</ul>
<h2 id="0-6-还存在什么问题？"><a href="#0-6-还存在什么问题？" class="headerlink" title="0.6 还存在什么问题？"></a>0.6 还存在什么问题？</h2><h1 id="1-背景知识"><a href="#1-背景知识" class="headerlink" title="1 背景知识"></a>1 背景知识</h1><ol>
<li><a href="https://blog.csdn.net/lilong117194/article/details/78202637" target="_blank" rel="noopener">Gram矩阵</a></li>
</ol>
<h2 id="GCN融合能力实验"><a href="#GCN融合能力实验" class="headerlink" title="GCN融合能力实验"></a>GCN融合能力实验</h2><p>猜想：如果GCN可以自适应学习节点特征和拓补结构的话，那么调整网络和节点特征或者拓扑结构的相关性，GCN应该不会出现剧烈的效果差异变化。</p>
<p>作者设计了以下两个case来验证上述猜想：</p>
<h3 id="随机拓扑和相关节点特征"><a href="#随机拓扑和相关节点特征" class="headerlink" title="随机拓扑和相关节点特征"></a>随机拓扑和相关节点特征</h3><p>这里作者建立了一个节点标签和节点特征高度相关，但是和拓扑结构无关。</p>
<p>对比模型：GCN和MLP</p>
<h3 id="相关拓扑和随机节点特征"><a href="#相关拓扑和随机节点特征" class="headerlink" title="相关拓扑和随机节点特征"></a>相关拓扑和随机节点特征</h3><p>这里作者把节点分为3个团体，节点标签由团体决定。</p>
<p>对比模型：GCN和DeepWalk</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>这些情况表明，目前的GCN融合机制[14]远未达到理想甚至令人满意的水平。</p>
<h1 id="2-模型"><a href="#2-模型" class="headerlink" title="2 模型"></a>2 模型</h1><p>AM-GCN整体模型框架如下图所示：<img src="http://image.nysdy.com/20200916114215.png" alt></p>
<h2 id="2-1-特定卷积模型"><a href="#2-1-特定卷积模型" class="headerlink" title="2.1 特定卷积模型"></a>2.1 特定卷积模型</h2><p><strong>特征空间：</strong></p>
<p>作者首先构建一个$k$最近邻居（kNN)图： $G_{f}=\left(\mathbf{A}_{f}, \mathbf{X}\right)$依据节点特征矩阵$\mathbf{X}$，其中$\mathbf{A}_{f}$是kNN图的邻接矩阵。特别的，作者还计算量一个相似度矩阵$\mathbf{S} \in \mathbb{R}^{n \times n}$，并介绍了两种方式：</p>
<ol>
<li><p>余弦相似度：</p>
<script type="math/tex; mode=display">
\mathbf{S}_{i j}=\frac{\mathbf{x}_{i} \cdot \mathbf{x}_{j}}{\left|\mathbf{x}_{i}\right|\left|\mathbf{x}_{j}\right|}</script></li>
<li><p>Heat Kernel:</p>
<script type="math/tex; mode=display">
\mathbf{S}_{i j}=e^{-\frac{\left\|\mathbf{x}_{i}-\mathbf{x}_{j}\right\|^{2}}{t}}</script><p>作者取：$t=2$</p>
</li>
</ol>
<p>作者选择使用余弦相似度来计算，并且选择前k个相似度的节点建立边，最终得到$\mathbf{A}$。</p>
<p>至此， 作者可以得到第l层输出：</p>
<script type="math/tex; mode=display">
\mathbf{Z}_{f}^{(l)}=\operatorname{ReLU}\left(\tilde{\mathbf{D}}_{f}^{-\frac{1}{2}} \tilde{\mathbf{A}}_{f} \tilde{\mathbf{D}}_{f}^{-\frac{1}{2}} \mathbf{Z}_{f}^{(l-1)} \mathbf{W}_{f}^{(l)}\right)</script><ul>
<li>$ \mathbf{Z}_{f}^{(0)}=\mathbf{X}$, $\mathbf{w}_{f}^{(l)}$是GCN中第l层的权重矩阵</li>
<li>$\tilde{\mathbf{A}}_{f}=\mathbf{A}_{f}+\mathbf{I}_{f}$， $\tilde{\mathbf{D}} f$是对角度矩阵。</li>
</ul>
<p><strong>拓扑空间：</strong></p>
<p>整体流程和特征空间一样，只不过其中的一些矩阵替换如下：</p>
<ul>
<li>$G_{t}=\left(\mathbf{A}_{t}, \mathbf{X}_{t}\right) \text { where } \mathbf{A}_{t}=\mathbf{A} \text { and } \mathbf{X}_{t}=\mathbf{X}$</li>
</ul>
<h2 id="2-2-公共卷积模型"><a href="#2-2-公共卷积模型" class="headerlink" title="2.2 公共卷积模型"></a>2.2 公共卷积模型</h2><p>因为特征空间和拓扑空间不是完全不相关的，所以作者设计了一个Common-GCN来来提取被两个空间共享的公共信息。</p>
<p>从拓扑图中提取节点嵌入：</p>
<script type="math/tex; mode=display">
\mathbf{Z}_{c t}^{(l)}=\operatorname{ReLU}\left(\tilde{\mathbf{D}}_{t}^{-\frac{1}{2}} \tilde{\mathbf{A}}_{t} \tilde{\mathbf{D}}_{t}^{-\frac{1}{2}} \mathbf{Z}_{c t}^{(l-1)} \mathbf{W}_{c}^{(l)}\right)</script><p>从特征图中提取节点嵌入：</p>
<script type="math/tex; mode=display">
\mathbf{Z}_{c f}^{(l)}=\operatorname{Re} L U\left(\tilde{\mathbf{D}}_{f}^{-\frac{1}{2}} \tilde{\mathbf{A}}_{f} \tilde{\mathbf{D}}_{f}^{-\frac{1}{2}} \mathbf{Z}_{c f}^{(l-1)} \mathbf{W}_{c}^{(l)}\right)</script><p>合并两个输出嵌入变为公共嵌入：</p>
<script type="math/tex; mode=display">
\mathbf{Z}_{C}=\left(\mathbf{Z}_{C T}+\mathbf{Z}_{C F}\right) / 2</script><ul>
<li>其中字母对应于2.1中字母，$\mathbf{W}_{c}^{(l)}$是一个共享的权重矩阵。</li>
</ul>
<h3 id="2-3-注意力机制"><a href="#2-3-注意力机制" class="headerlink" title="2.3 注意力机制"></a>2.3 注意力机制</h3><p>现在作者得到了3个向量，采用一个注意力机制来自适应学习三者的重要程度：</p>
<script type="math/tex; mode=display">
\left(\alpha_{t}, \alpha_{c}, \alpha_{f}\right)=a t t\left(\mathbf{Z}_{T}, \mathbf{Z}_{C}, \mathbf{Z}_{F}\right)</script><p>对于节点$i$的向量$ \mathbf{z}_{T}^{i} \in \mathbb{R}^{1 \times h}$在矩阵$\mathbf{Z}_{T} $中。为了获得注意力的值，作者首先通过一个非线性变换转换嵌入，然后使用一个共享的注意力向量$\mathbf{q} \in \mathbb{R}^{h^{\prime} \times 1}$去得到注意力值：</p>
<script type="math/tex; mode=display">
\omega_{T}^{i}=\mathbf{q}^{T} \cdot \tanh \left(\mathbf{W}_{T} \cdot\left(\mathbf{z}_{T}^{i}\right)^{T}+\mathbf{b}_{T}\right)</script><ul>
<li>其中$\mathbf{e} \mathbf{W}_{T} \in \mathbb{R}^{h^{\prime} \times h}$和$\mathbf{b}_{T} \in \mathbb{R}^{h^{\prime} \times 1}$分别是罪域矩阵$\mathbf{z}_T$的权重矩阵和偏移向量；</li>
<li>注意，上角标的$T$应该是转置的意思。</li>
</ul>
<p>同理可得其他两种向量的注意力值，最后权重为：</p>
<script type="math/tex; mode=display">
\alpha_{T}^{i}=\operatorname{softmax}\left(\omega_{T}^{i}\right)=\frac{\exp \left(\omega_{T}^{i}\right)}{\exp \left(\omega_{T}^{i}\right)+\exp \left(\omega_{C}^{i}\right)+\exp \left(\omega_{F}^{i}\right)}</script><ul>
<li>该值越大说明对应的嵌入越重要。</li>
</ul>
<p>最终获得的最终向量$\mathbf{Z}$表示为：</p>
<script type="math/tex; mode=display">
\mathbf{Z}=\boldsymbol{\alpha}_{T} \cdot \mathbf{Z}_{T}+\boldsymbol{\alpha}_{C} \cdot \mathbf{Z}_{C}+\boldsymbol{\alpha}_{F} \cdot \mathbf{Z}_{F}</script><ul>
<li>$\alpha_{T}=\operatorname{diag}\left(\alpha_{t}\right)$</li>
<li>$\boldsymbol{\alpha}_{t}=\left[\alpha_{T}^{i}\right], \boldsymbol{\alpha}_{c}=\left[\alpha_{C}^{i}\right], \boldsymbol{\alpha}_{f}=\left[\alpha_{F}^{i}\right] \in \mathbb{R}^{n \times 1}$</li>
</ul>
<h2 id="2-4目标函数"><a href="#2-4目标函数" class="headerlink" title="2.4目标函数"></a>2.4目标函数</h2><h3 id="2-4-1-Consistency-Constraint-一致性限制"><a href="#2-4-1-Consistency-Constraint-一致性限制" class="headerlink" title="2.4.1 Consistency Constraint 一致性限制"></a>2.4.1 Consistency Constraint 一致性限制</h3><p>对于Common-GCN输出的两个向量$\mathbf{Z}_{C T} \text { and } \mathbf{Z}_{C F}$，作者谁记录一个一致性限制来进一步增强他们的通用性。</p>
<p>作者使用L2方式去归一化矩阵为$\mathbf{Z}_{C T n o r}, \mathbf{Z}_{C F n o r}$，并计算节点的相似度：</p>
<script type="math/tex; mode=display">
\begin{array}{l}\mathbf{s}_{T}=\mathbf{Z}_{C T n o r} \cdot \mathbf{Z}_{C T n o r}^{T} \\ \mathbf{s}_{F}=\mathbf{Z}_{C F n o r} \cdot \mathbf{Z}_{C F n o r}^{T}\end{array}</script><p>一致性意味着两个相似性矩阵应该相似，这引起以下约束：</p>
<script type="math/tex; mode=display">
\mathcal{L}_{c}=\left\|\mathbf{S}_{T}-\mathbf{S}_{F}\right\|_{F}^{2}</script><h3 id="2-4-2-Disparity-Constraint-差异限制"><a href="#2-4-2-Disparity-Constraint-差异限制" class="headerlink" title="2.4.2 Disparity Constraint 差异限制"></a>2.4.2 Disparity Constraint 差异限制</h3><p>在这里，由于从同一图$G_{t}=\left(\mathbf{A}_{t}, \mathbf{X}_{t}\right)$学习嵌入<script type="math/tex">\mathbf{Z}_{T} \text { and } \mathbf{Z}_{C T}</script>，为确保它们可以捕获不同的信息，我们采用了希尔伯特-施密特独立性准则（HSIC），这很简单，但是有效的独立性措施，以扩大这两个嵌入之间的差距。</p>
<p>作者给予以下定义：</p>
<script type="math/tex; mode=display">
H S I C\left(\mathbf{Z}_{T}, \mathbf{Z}_{C T}\right)=(n-1)^{-2} \operatorname{tr}\left(\mathbf{R} \mathbf{K}_{T} \mathbf{R} \mathbf{K}_{C T}\right)</script><ul>
<li>$\mathbf{K}_{T} \text { and } \mathbf{K}_{C T}$是格拉姆矩阵(Gram matrices)，$k_{T, i j}=k_{T}\left(\mathbf{z}_{T}^{i}, \mathbf{z}_{T}^{j}\right)$and $k_{C T, i j}=k_{C T}\left(\mathbf{z}_{C T}^{i}, \mathbf{z}_{C T}^{j}\right)$</li>
<li>$\mathbf{R}=\mathbf{I}-\frac{1}{n} e e^{T}$，$\mathbf{I}$是单位矩阵，$e$是一个全1列向量。</li>
</ul>
<p>同理，得到$\mathbf{Z}_{F} \text { and } \mathbf{Z}_{C F}$的HSIC：</p>
<script type="math/tex; mode=display">
H S I C\left(\mathbf{Z}_{F}, \mathbf{Z}_{C F}\right)=(n-1)^{-2} \operatorname{tr}\left(\mathbf{R} \mathbf{K}_{F} \mathbf{R} \mathbf{K}_{C F}\right)</script><p>所以，差异化限制：</p>
<script type="math/tex; mode=display">
\mathcal{L}_{d}=H S I C\left(\mathbf{Z}_{T}, \mathbf{Z}_{C T}\right)+H S I C\left(\mathbf{Z}_{F}, \mathbf{Z}_{C F}\right)</script><h3 id="2-4-3-优化目标"><a href="#2-4-3-优化目标" class="headerlink" title="2.4.3 优化目标"></a>2.4.3 优化目标</h3><p>作者在下面等式中使用输出嵌入$\mathbf{Z}$用于具有线性变换和softmax函数的半监督多类分类:</p>
<script type="math/tex; mode=display">
\hat{\mathbf{Y}}=\operatorname{softmax}(\mathbf{W} \cdot \mathbf{Z}+\mathbf{b})</script><ul>
<li>$\hat{\mathbf{Y}}=\left[\hat{y}_{i c}\right] \in \mathbb{R}^{n \times C}$， $\hat{y}_{i c}$表示节点$i$属于类$c$的概率</li>
</ul>
<p>对于节点分类任务，作者采用交叉熵损失函数：</p>
<script type="math/tex; mode=display">
\mathcal{L}_{t}=-\sum_{l \in L} \sum_{i=1}^{C} \mathbf{Y}_{l} \ln \hat{\mathbf{Y}}_{l}</script><p><strong>至此，</strong>整个任务的目标函数如下：</p>
<script type="math/tex; mode=display">
\mathcal{L}=\mathcal{L}_{t}+\gamma \mathcal{L}_{c}+\beta \mathcal{L}_{d}</script><ul>
<li>$\gamma \text { and } \beta$分别是consistency and disparity constraint的超参数。</li>
</ul>
<h1 id="3-实验"><a href="#3-实验" class="headerlink" title="3 实验"></a>3 实验</h1><h2 id="3-1-数据集"><a href="#3-1-数据集" class="headerlink" title="3.1 数据集"></a>3.1 数据集</h2><p><img src="http://image.nysdy.com/20200917102036.png" alt></p>
<h2 id="3-2-baseline"><a href="#3-2-baseline" class="headerlink" title="3.2 baseline"></a>3.2 baseline</h2><p>DeepWalk，LINE，Chebyshev，GCN，kNN-GCN，GAT， DEMO-Net，MixHop</p>
<h2 id="3-3-节点分类"><a href="#3-3-节点分类" class="headerlink" title="3.3 节点分类"></a>3.3 节点分类</h2><p><img src="http://image.nysdy.com/20200917102456.png" alt></p>
<ul>
<li>与GCN和kNN-GCN相比，我们可以了解到拓扑图和特征图之间确实存在结构差异，并且在传统拓扑图上执行GCN并不总是比在特征图上显示更好的结果。 例如，在BlogCatalog，Flickr和UAI2010中，功能图的性能优于拓扑。 这进一步证实了在GCN中引入特征图的必要性。</li>
</ul>
<h3 id="3-4-变体分析"><a href="#3-4-变体分析" class="headerlink" title="3.4 变体分析"></a>3.4 变体分析</h3><p><img src="http://image.nysdy.com/20200917102800.png" alt></p>
<p><img src="http://image.nysdy.com/20200917102855.png" alt></p>
<ul>
<li>比较图2和表2的结果，可以发现AM-GCN-w/o尽管没有任何限制，但在基准方面仍然具有非常好的竞争性能，这表明作者的框架是稳定且具有竞争力的。</li>
</ul>
<h3 id="3-5-可视化展示"><a href="#3-5-可视化展示" class="headerlink" title="3.5 可视化展示"></a>3.5 可视化展示</h3><p><img src="http://image.nysdy.com/20200917103107.png" alt></p>
<h3 id="3-6-注意力机制分析"><a href="#3-6-注意力机制分析" class="headerlink" title="3.6 注意力机制分析"></a>3.6 注意力机制分析</h3><p><strong>分析注意力的分布</strong>：分析数据集的构成，并且结合表2做分析。发现注意力的分布符合预期。</p>
<p><img src="http://image.nysdy.com/20200917103448.png" alt></p>
<p><strong>注意力趋势分析</strong></p>
<p><img src="http://image.nysdy.com/20200917103649.png" alt></p>
<h3 id="3-7-调参分析"><a href="#3-7-调参分析" class="headerlink" title="3.7 调参分析"></a>3.7 调参分析</h3><p><img src="http://image.nysdy.com/20200917103736.png" alt></p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul>
<li><a href="https://blog.csdn.net/lilong117194/article/details/78202637" target="_blank" rel="noopener">https://blog.csdn.net/lilong117194/article/details/78202637</a></li>
<li><a href="https://dl.acm.org/doi/pdf/10.1145/3394486.3403177" target="_blank" rel="noopener">论文下载地址</a></li>
</ul>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>GCN</tag>
      </tags>
  </entry>
  <entry>
    <title>A Discourse-Level Named Entity Recognition and Relation Extraction Dataset for Chinese Literature Text 阅读笔记</title>
    <url>/post/READ_A_Discourse-Level_Named_Entity_Recognition_and_Relation_Extraction_Dataset_for_Chinese_Literature_Text/</url>
    <content><![CDATA[<blockquote>
<p>该论文最主要的贡献就是这个数据，<a href="https://github.com/lancopku/Chinese-Literature-NER-RE-Dataset" target="_blank" rel="noopener">数据集地址</a>。论文中提到的标标签过程也是一个创新点，运用了启发式和机器辅助标标签，这样可以提高准确度并减少标注人员工作。</p>
</blockquote>
<a id="more"></a>
<h2 id="contribution"><a href="#contribution" class="headerlink" title="contribution"></a>contribution</h2><ul>
<li>provide a new dataset for joint learning of NER and RE for <strong>Chinese literature text</strong></li>
<li>the proposed dataset is based on the <strong>discourse level</strong> which provides additional context information</li>
<li>introduce some widely used models to conduct experiments </li>
</ul>
<h2 id="tagging-process"><a href="#tagging-process" class="headerlink" title="tagging process"></a>tagging process</h2><p><strong>two methods:</strong>one is a <strong>heuristic tagging</strong> method and another is a <strong>machine auxiliary tagging</strong> method. </p>
<h3 id="Step-1-First-Tagging-Process"><a href="#Step-1-First-Tagging-Process" class="headerlink" title="Step 1: First Tagging Process"></a>Step 1: First Tagging Process</h3><p>find a problem of data inconsistency.</p>
<h3 id="Step-2-Heuristic-Tagging-Based-on-Generic-disambiguating-Rules"><a href="#Step-2-Heuristic-Tagging-Based-on-Generic-disambiguating-Rules" class="headerlink" title="Step 2: Heuristic Tagging Based on Generic disambiguating Rules"></a>Step 2: Heuristic Tagging Based on Generic disambiguating Rules</h3><ul>
<li>For example, remove all adjective words and only tag “entity header” .</li>
<li><strong>re-annotate</strong> all articles and <strong>correct</strong> all inconsistency entities and relations based on the heuristic rules.</li>
</ul>
<h3 id="Step-3-Machine-Auxiliary-Tagging"><a href="#Step-3-Machine-Auxiliary-Tagging" class="headerlink" title="Step 3: Machine Auxiliary Tagging"></a>Step 3: Machine Auxiliary Tagging</h3><ul>
<li>The core idea is to train a model to <strong>learn annotation guidelines</strong> on the subset of the corpus and produce predicted tags on the rest data. </li>
<li>CRF</li>
</ul>
<h2 id="tagging-set"><a href="#tagging-set" class="headerlink" title="tagging set"></a><img src="https://i.loli.net/2018/12/10/5c0dd578c91a0.jpg" alt>tagging set</h2><p><img src="https://i.loli.net/2018/12/10/5c0dd6230f1c3.jpg" alt></p>
<h2 id="Annotation-Format"><a href="#Annotation-Format" class="headerlink" title="Annotation Format"></a>Annotation Format</h2><h3 id="Entity"><a href="#Entity" class="headerlink" title="Entity"></a>Entity</h3><p>Each entity is identified by T tag, which takes several attributes.</p>
<ul>
<li>Id: a unique number identifying the entity within the document. It starts at 0, and is incremented every time a new entity is identified within the same document.</li>
<li>Type: one of the entity tags.</li>
<li>Begin Index: the begin index of an entity. It starts at 0, and is incremented every character.</li>
<li>End Index: the end index of an entity. It starts at 0, and is incremented every character.</li>
<li>Value: words being referred to an identifiable object.</li>
</ul>
<h3 id="Relation"><a href="#Relation" class="headerlink" title="Relation"></a>Relation</h3><p>Each relation is identified by R tag, which can take several attributes:</p>
<ul>
<li>Id: a unique number identifying the relation within the document. It starts at 0, and is incremented every time a new relation is identified within the same document.</li>
<li>Arg1 and Arg2: two entities associated with a relation.</li>
<li>Type: one of the relation tags.</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>NER</tag>
        <tag>RE</tag>
      </tags>
  </entry>
  <entry>
    <title>Attention Is All You Need阅读笔记</title>
    <url>/post/Attention%20Is%20All%20You%20Need/</url>
    <content><![CDATA[<blockquote>
<p>transformer 是一个完全由注意力机制组成的搭建的模型，模型复杂度低，并可以进行并行计算，使得计算速度快。在翻译模型上取得了较好的效果。本篇论文属于经典必读论文，阅读笔记中对一些不清楚的地方进行了汉语解释，读完论文后阅读参考链接以加深理解。</p>
<p><a href="https://arxiv.org/pdf/1706.03762" target="_blank" rel="noopener">论文下载地址</a></p>
</blockquote>
<a id="more"></a>
<h1 id="research-objective"><a href="#research-objective" class="headerlink" title="research objective"></a>research objective</h1><p>based solely on attention mechanisms, increase parallezable computation and decrease train time</p>
<h1 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h1><p>recurrent models hidden states depended on previous hidden state and the input for position precludes parallelization</p>
<h1 id="contribution"><a href="#contribution" class="headerlink" title="contribution"></a>contribution</h1><ul>
<li>Transformer,<ul>
<li>eschewing recurrence and instead relying entirely on an attention mechanism, solve the long dependency problem.</li>
<li>draw global dependecies between input and output </li>
<li>allow for significantly more parallelization</li>
</ul>
</li>
</ul>
<h1 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h1><p>The Transformer uses stacked self-attention and point-wise, fully connected layers for both the encoder and decoder.<img src="http://image.nysdy.com/2019052515587656321218.jpg" alt="2019052515587656321218.jpg"></p>
<h2 id="Encoder-and-Decoder-Stacks"><a href="#Encoder-and-Decoder-Stacks" class="headerlink" title="Encoder and Decoder Stacks"></a>Encoder and Decoder Stacks</h2><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><ul>
<li>compose of a stack of N identical layers</li>
<li>each layers has two sub-layers<ol>
<li>multi-head self-attention mechanism</li>
<li>position-wise fully connected feed forward network</li>
</ol>
</li>
<li>employ a residual connection around each of the two sub-layers, followed by layer normalization</li>
<li>the output of each sub-layer is $\text { LayerNorm }(x+\text { Sublayer }(x))$</li>
<li>encoder中的Q，K，V都是学出来的</li>
</ul>
<h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><ul>
<li>composed of a stack of N identical layers</li>
<li>has the same two sub-layers as the encoder</li>
<li>the third sub-layer between the two sub-layers<ul>
<li>perform multi-head attention over the output of the encoder stack</li>
</ul>
</li>
<li>add a mask to modify the self-attention sub-layer to ensure that the predictions for position $i$ can depend only the known outputs at positions less than $i$</li>
<li>除了第一子层中Q，K，V是自己学出来的，第二个子层利用了encoder中的K，V。</li>
</ul>
<h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p><img src="http://image.nysdy.com/20190526155885488441950.jpg" alt="20190526155885488441950.jpg"></p>
<h3 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h3><p>the calculation process as the left at the figure 2. <strong>formula：</strong></p>
<script type="math/tex; mode=display">
\text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V</script><ul>
<li>where $\sqrt{d_{k}}$ is to  prevent value from getting too large, which will push the softmax function into regions where it has extremely small gradients. 因为量级太大，softmax后就非0即1了，不够“soft”了。也会导致softmax的梯度非常小。也就是让softmax结果<strong>不稀疏</strong>(问号脸，通常人们希望得到更稀疏的attention吧)。</li>
<li>$Q, K,V$ is a matrix needed to learn from input.</li>
</ul>
<h3 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h3><p><strong>helps the encoder look at other words in the input sentence as it encodes a specific word</strong></p>
<p>in the figure 2 right. </p>
<ul>
<li>it’s beneficial to <strong>lineraly project</strong> the quries, keys and values $h$ times with different, learned projections to $d_k, d_k, d_v$ dimensions, respectively</li>
<li>concatenate the output </li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned} \text { MultiHead }(Q, K, V) &=\text { Concat (head }_{1}, \ldots, \text { head }_{h} ) W^{O} \\ \text { where head }_{i} &=\text { Attention }\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right) \end{aligned}</script><p>where $W_{i}^{Q} \in \mathbb{R}^{d_{\text { model }} \times d_{k}}, W_{i}^{K} \in \mathbb{R}^{d_{\text { model }} \times d_{k}}, W_{i}^{V} \in \mathbb{R}^{d_{\text { model }} \times d_{v}}, W^{O} \in \mathbb{R}^{h d_{v} \times d_{\mathrm{model}}}$</p>
<h3 id="Applications-of-Attention-in-our-Model"><a href="#Applications-of-Attention-in-our-Model" class="headerlink" title="Applications of Attention in our Model"></a>Applications of Attention in our Model</h3><ul>
<li>the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. mimicing the seq-to-seq</li>
<li>self -attention can make that each position in the encoder can attend to all positions in the previous layer of the encoder</li>
<li><strong>We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. See Figure 2</strong>。即我们只能attend到前面已经翻译过的输出的词语，因为翻译过程我们当前还并不知道下一个输出词语，这是我们之后才会推测到的。即将$QK^T$中每行该单词之后的数值做处理，使得前面的单词看不到后面单词所占的重要性程度。</li>
</ul>
<h2 id="Position-wise-Feed-Forward-Networks"><a href="#Position-wise-Feed-Forward-Networks" class="headerlink" title="Position-wise Feed-Forward Networks"></a>Position-wise Feed-Forward Networks</h2><ul>
<li>applied to each position separately and identically</li>
<li>feed-forward network consists of tow linear transformations with a ReLU activation. formula:</li>
</ul>
<script type="math/tex; mode=display">
\mathrm{FFN}(x)=\max \left(0, x W_{1}+b_{1}\right) W_{2}+b_{2}</script><blockquote>
<p><strong>小结</strong></p>
<ol>
<li>为什么叫强调<code>position-wise</code>?<ul>
<li>解释一: 这里FFN层是每个position进行相同且独立的操作，所以叫position-wise。对每个position独立做FFN。</li>
<li>解释二：从卷积的角度解释，这里的FFN等价于kernel_size=1的卷积，这样每个position都是独立运算的。如果kernel_size=2，或者其他，position之间就具有依赖性了，貌似就不能叫做position-wise了</li>
</ul>
</li>
<li>为什么要采用全连接层？<ul>
<li>目的: 增加非线性变换</li>
<li>如果不采用FFN呢？有什么替代的设计？</li>
</ul>
</li>
<li>为什么采用2层全连接，而且中间升维？<ul>
<li>这也是所谓的bottle neck，只不过低维在IO上，中间采用high rank</li>
</ul>
</li>
</ol>
</blockquote>
<h2 id="Embeddings-and-Softmax"><a href="#Embeddings-and-Softmax" class="headerlink" title="Embeddings and Softmax"></a>Embeddings and Softmax</h2><p>Sharing the same weight maatrix between the two embedding layers and the pre-softmax linear transformation</p>
<h2 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h2><p>Using sine and xosine functions of different frequencies:</p>
<script type="math/tex; mode=display">
P E_{(p o s, 2 i)}=\sin \left(p o s / 10000^{2 i / d_{\text { model }}}\right)
\\
P E_{(p o s, 2 i+1)}=\cos \left(p o s / 10000^{2 i / d_{\mathrm{model}}}\right)</script><ul>
<li><p>where $pos$ is the postiiton and $i$ is the dimension</p>
</li>
<li><p><strong>Authors hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$, $PE_{pos+k}$can be represented as a linear function of $PE_{pos}$</strong></p>
<p>但在语言中，<code>相对位置</code>也很重要，Google选择前述的位置向量公式的一个重要原因是：由于我们有$\sin (\alpha+\beta)=\sin \alpha \cos \beta+\cos \alpha \sin \beta$以及$\cos (\alpha+\beta)=\cos \alpha \cos \beta-\sin \alpha \sin \beta$，这表明位置$p+k$的向量可以表示成位置$p$的向量的线性变换，这提供了表达相对位置信息的可能性。</p>
</li>
<li><p>Compared with using learned positional embeddings, the sinusoidal version may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.</p>
</li>
</ul>
<p>注意由于该模型没有recurrence或convolution操作，所以没有明确的关于单词在源句子中位置的相对或绝对的信息，为了更好的让模型学习位置信息，所以添加了position encoding并将其叠加在word embedding上。</p>
<h1 id="Why-Self-Attention"><a href="#Why-Self-Attention" class="headerlink" title="Why Self-Attention"></a>Why Self-Attention</h1><ul>
<li>total computational complexity per layer</li>
<li>the amount of computation that can be parallelized</li>
<li>the path between long-range dependencies in the network</li>
</ul>
<p><img src="http://image.nysdy.com/20190527155892126287777.jpg" alt="20190527155892126287777.jpg"></p>
<p><img src="http://image.nysdy.com/20190528155902465937774.jpg" alt="20190528155902465937774.jpg"></p>
<p>self-attention|：</p>
<ul>
<li>$QK^TV$相乘，根据矩阵大小（分别为$n<em>d, n</em>d, n<em>d$需要的复杂度为$O(n^2d</em>2)$（忽略softmax）</li>
<li>maximum path length：图说明了， 对于self-attention, target node (生成的那个点) 实际上和 输入中的任意一点的距离是相同的</li>
</ul>
<p>convolutional:  </p>
<ul>
<li><p>每层有k个卷积和，对于input matix（$n<em>d$)矩阵执行卷积需要运算复杂度是$n</em>d*(d-m)$, m为卷积和宽度是一个比较小的常数，所以总复杂度为$O\left(k \cdot n \cdot d^{2}\right)$,作者提到可分离的卷基层暂时还不了解，可以以后查阅。</p>
</li>
<li><p>maximum path length: 正常卷积和的距离是$O(n/k)$, 但如果是堆叠卷积如图：　　<img src="http://image.nysdy.com/2019052815590251436862.jpg" alt="2019052815590251436862.jpg"></p>
<p>就可以减小到$O\left(\log _{k}(n)\right)$</p>
</li>
</ul>
<p>recurrent:</p>
<ul>
<li>计算是每个词向量乘隐藏权重($d*d$)，所以易得计算复杂度：$O\left(n \cdot d^{2}\right)$</li>
<li>maximum path length: 长度就是n。</li>
<li>操作步骤要从第一个到第n个为n步，是有顺序的。其他的都没有顺序要求</li>
</ul>
<p>self-attentin(restricted)</p>
<ul>
<li>相当于只输入r邻近的句子长度，自然可以得到如图结果</li>
</ul>
<h1 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h1><h2 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h2><script type="math/tex; mode=display">
\text { lrate }=d_{\text { model }}^{-0.5} \cdot \min \left(\text {step}_{-} n u m^{-0.5}, \text { step }_{-} n u m \cdot \text { warmup steps }^{-1.5}\right)</script><ul>
<li>increasing the learning rate linearly for the first warmup_steps training steps</li>
<li>decreasing it thereafter proportionally to the inverse square root of the step number</li>
</ul>
<h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><h3 id="Residual-Dropout"><a href="#Residual-Dropout" class="headerlink" title="Residual Dropout"></a>Residual Dropout</h3><ul>
<li>apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized</li>
<li>apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks</li>
</ul>
<h3 id="Label-Smoothing"><a href="#Label-Smoothing" class="headerlink" title="Label Smoothing"></a>Label Smoothing</h3><ul>
<li>This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score</li>
</ul>
<h1 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h1><h2 id="machine-Translation"><a href="#machine-Translation" class="headerlink" title="machine Translation"></a>machine Translation</h2><p>Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models</p>
<p><img src="http://image.nysdy.com/2019052715589406009133.jpg" alt="2019052715589406009133.jpg"></p>
<h2 id="Model-Variations"><a href="#Model-Variations" class="headerlink" title="Model Variations"></a>Model Variations</h2><p><img src="http://image.nysdy.com/20190527155894062794865.jpg" alt="20190527155894062794865.jpg"></p>
<h1 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a><strong>缺点</strong></h1><p>缺点在原文中没有提到，是后来在Universal Transformers中指出的，在这里加一下吧，主要是两点：</p>
<ol>
<li>实践上：有些rnn轻易可以解决的问题transformer没做到，比如复制string，尤其是碰到比训练时的sequence更长的时</li>
<li>理论上：transformers非computationally universal（<a href="https://www.zhihu.com/question/20115374/answer/288346717" target="_blank" rel="noopener">图灵完备</a>），（我认为）因为无法实现“while”循环</li>
</ol>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h1><p>Transformer是第一个用纯attention搭建的模型，不仅计算速度更快，在翻译任务上也获得了更好的结果。</p>
<p>Google现在的翻译应该是在此基础上做的，但是数据量大可能用transformer好一些，小的话还是继续用rnn-based model。</p>
<p>花了不少时间，算是理解了attention和transformer，对其中不是很清楚的点如attention的内部中Q，K，V具体是什么在self-attention和multi-head attention中大小是不同的，如何mask，如何计算复杂，等进行查阅资料弄懂了。总体来说还是收获很大的。准备在看一些代码讲解。</p>
<h1 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h1><ul>
<li>Attention机制详解（二）——Self-Attention与Transformer - 川陀学者的文章 - 知乎<br><a href="https://zhuanlan.zhihu.com/p/47282410" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/47282410</a></li>
<li><strong><a href="https://jalammar.github.io/illustrated-transformer/（这个讲的比较详细，建议看完论文后再看一遍这个会加深理解）" target="_blank" rel="noopener">https://jalammar.github.io/illustrated-transformer/（这个讲的比较详细，建议看完论文后再看一遍这个会加深理解）</a></strong></li>
<li>【NLP】Transformer详解 - 李如的文章 - 知乎<br><a href="https://zhuanlan.zhihu.com/p/44121378" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/44121378</a></li>
<li><a href="https://blog.eson.org/pub/664e9bad/" target="_blank" rel="noopener">https://blog.eson.org/pub/664e9bad/</a></li>
<li><a href="https://mp.weixin.qq.com/s/J-anyCuwLd5UYjTsUFNT1g" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/J-anyCuwLd5UYjTsUFNT1g</a></li>
</ul>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>classical</category>
      </categories>
      <tags>
        <tag>attention</tag>
        <tag>transformer</tag>
        <tag>translation</tag>
        <tag>classical</tag>
      </tags>
  </entry>
  <entry>
    <title>CN-DBpedia A Never-Ending Chinese Knowledge Extraction SystemCN-DBpedia System</title>
    <url>/post/essay/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><blockquote>
<p>本篇论文为2016年的一篇论文，主要介绍了作者构建中文知识图谱所遇到的一些问题和解决方法。</p>
</blockquote>
<a id="more"></a>
<h3 id="challenge"><a href="#challenge" class="headerlink" title="challenge"></a>challenge</h3><ol>
<li>如何降低人力成本？</li>
<li>如何保持知识库的新鲜度？</li>
</ol>
<h3 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h3><ol>
<li>在构建中文知识库中降低了人力成本：<ul>
<li>重复利用已经存在的本体论</li>
<li>提出了一个不用人工监督的端到端的深度学习模型</li>
</ul>
</li>
<li>提出了一个智能主动更新策略</li>
</ol>
<h3 id="系统结构"><a href="#系统结构" class="headerlink" title="系统结构"></a>系统结构</h3><p><img src="https://i.loli.net/2018/09/26/5baaf081b2644.jpg" alt></p>
<p>提高知识库质量：</p>
<ol>
<li><strong>Normalization</strong>： normalize the attributes and values</li>
<li><strong>Enrichment</strong>：reuse the ontology</li>
<li><strong>Correction</strong>：two steps<ol>
<li>error detection:<ul>
<li>rule-based detection</li>
<li>based on user feedbacks</li>
</ul>
</li>
<li>error correction<ul>
<li>crowd-sourcing</li>
</ul>
</li>
</ol>
</li>
</ol>
<h3 id="降低人力成本"><a href="#降低人力成本" class="headerlink" title="降低人力成本"></a>降低人力成本</h3><p>这部分作者采用了两种方法：</p>
<ol>
<li>重复利用已经存在在知识库的本体论和类型化的中文实体</li>
<li>构建一个端到端提取器</li>
</ol>
<h4 id="Cross-Lingual-Entity-Typing（跨语言的实体类型）"><a href="#Cross-Lingual-Entity-Typing（跨语言的实体类型）" class="headerlink" title="Cross-Lingual Entity Typing（跨语言的实体类型）"></a>Cross-Lingual Entity Typing（跨语言的实体类型）</h4><ul>
<li>第一步是通过用英文DBpedia类型来类型化中文实体。为了达到这个目的，作者提出了如下系统：<img src="https://i.loli.net/2018/10/01/5bb2273b35ab0.jpg" alt>系统建立了监督层次分类模型，系统输入为没有标记类型的中文实体，输出为在DB中所有有效的英文类型。作者将中文实体与共享相同中文标签名称的英语实体配对，这样中文实体以及配对英语实体的类型自然是标记样本。</li>
<li>用上述方法得到的训练集可能出现下面一些问题：<ul>
<li>英文DBpedia实体类型在许多情况下可能不完全；</li>
<li>英文DBpedia实体类型在许多情况下可能是错误的；</li>
<li>中英文链接可能出错；</li>
<li>中文实体的特征通常不完整。</li>
</ul>
</li>
<li>为了解决以上问题，作者提出了两种方法：<ul>
<li>完善英文DBpedia实体类型；</li>
<li>设计一个过滤步骤来剔除错误样本。</li>
</ul>
</li>
</ul>
<h4 id="infobox-completion"><a href="#infobox-completion" class="headerlink" title="infobox completion"></a>infobox completion</h4><blockquote>
<p>Infobox completion is a task to extract object for a given pair of entity and predicate from encyclopedia articles.</p>
</blockquote>
<p>作者建模了一个seq2seq模型，输入为包含tokens的自然语言句子，输出为每个token的标签。对于标签为0或1。</p>
<p>对于建立一个有效的提取器有以下两个关键：</p>
<ol>
<li>如何构建训练集：作者采用远程监督方法（利用Wikipedia）</li>
<li>如何选取期望的提取模型：LSTM-RNN，如图所示<img src="https://i.loli.net/2018/10/01/5bb22b72af582.jpg" alt></li>
</ol>
<h3 id="知识库更新"><a href="#知识库更新" class="headerlink" title="知识库更新"></a>知识库更新</h3><p>作者采用动态更新：识别新实体或可能包含新事实的旧实体</p>
<p>作者根据以下两方面来辨别这些实体：</p>
<ul>
<li>近期热点新闻中提及的实体</li>
<li>在搜索引擎的流行搜索关键字或其他流行网页中提到的实体</li>
</ul>
<p>对于如何从新闻标题和搜素指令中提取实体名字，作者采用简单的词分割方法，从百科全书中判断其是否为实体，并提出IDF值低的分割子串。</p>
<h3 id="统计数据"><a href="#统计数据" class="headerlink" title="统计数据"></a>统计数据</h3><p><img src="https://i.loli.net/2018/10/01/5bb22d57ab3ec.jpg" alt></p>
<h3 id="论文下载链接"><a href="#论文下载链接" class="headerlink" title="论文下载链接"></a><a href="https://www.researchgate.net/publication/318144300_CN-DBpedia_A_Never-Ending_Chinese_Knowledge_Extraction_System" target="_blank" rel="noopener">论文下载链接</a></h3>]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
        <tag>机器学习</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title>Deep contextualized word representations 阅读笔记</title>
    <url>/post/Deep%20contextualized%20word%20representations/</url>
    <content><![CDATA[<blockquote>
<p><a href="https://arxiv.org/pdf/1802.05365.pdf" target="_blank" rel="noopener">论文下载地址</a>，ELMo事先用语言模型学好一个单词的 Word Embedding，此时多义词无法区分，不过这没关系。在我实际使用 Word Embedding 的时候，单词已经具备了特定的上下文了，这个时候我可以根据上下文单词的语义去调整单词的 Word Embedding 表示，这样经过调整后的 Word Embedding 更能表达在这个上下文中的具体含义，自然也就解决了多义词的问题了。<strong>所以 ELMO 本身是个根据当前上下文对 Word Embedding 动态调整的思路。</strong></p>
</blockquote>
<a id="more"></a>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><h2 id="ELMo-Embedddings-from-Language-Models"><a href="#ELMo-Embedddings-from-Language-Models" class="headerlink" title="ELMo(Embedddings from Language Models):"></a>ELMo(Embedddings from Language Models):</h2><h3 id="why-call-ELMo"><a href="#why-call-ELMo" class="headerlink" title="why call ELMo:"></a>why call ELMo:</h3><p>Using vectors derived from a bidirectional LSTM that is trained with a coupled language model(LM) objective on a large text corups.</p>
<h3 id="characteristics"><a href="#characteristics" class="headerlink" title="characteristics"></a>characteristics</h3><ul>
<li><p>ELMo representations are a function of all of the internal layers of the biLM.</p>
</li>
<li><p>learn a linear combination of the vectors stacked above each input word for each end task</p>
</li>
<li><p>the higher-level LSTM states capture context-dependent aspects of word meaning</p>
<p>the lower-level states model aspects of syntax</p>
</li>
</ul>
<h3 id="Extensive-experiments"><a href="#Extensive-experiments" class="headerlink" title="Extensive experiments"></a>Extensive experiments</h3><ul>
<li>EMLo representations can be easily added to existing models</li>
<li>improve the state of art in every case</li>
<li>ELMo outperform those derived from just the top layer of a LSTM</li>
</ul>
<h1 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h1><ul>
<li><p>Some approaches for learning word vectors only allow a single context-independent representation for each word.</p>
</li>
<li><p>to overcome some shortcomings of traditional word vectors:</p>
<ul>
<li>enriching them with subword information</li>
<li>learning separate vectors for each word sense</li>
</ul>
<p>Authors uses subword units through the use of character convolutions, seamlessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes.</p>
</li>
<li><p>context-depends representations</p>
<p> Authors take full advantage of access to plentiful monolingual data</p>
</li>
<li><p>Previous work also shown that different layers of deep biRNNs encode different types of information</p>
<ul>
<li>introducing multi-task syntactic supervision at the lower levels of a deep LSTM can improve overall performance of higher level tasks</li>
<li>the top layer of an LSTM for encoding word context (Melamud et al., 2016) has been shown to learn representations of word sense.</li>
</ul>
<p>ELMo representations can also induce similar signals.</p>
</li>
</ul>
<h1 id="ELMo-Embeddings-from-Language-Models"><a href="#ELMo-Embeddings-from-Language-Models" class="headerlink" title="ELMo: Embeddings from Language Models"></a>ELMo: Embeddings from Language Models</h1><h2 id="Bidirectional-language-models"><a href="#Bidirectional-language-models" class="headerlink" title="Bidirectional language models"></a>Bidirectional language models</h2><ul>
<li><p>model the probability of token $t_k$ given the history($t_1, … , t_{k-1}$):</p>
<p><img src="http://image.nysdy.com/20190512155766627486478.png" alt="20190512155766627486478.png"></p>
</li>
<li><p>a backward LM:<img src="http://image.nysdy.com/2019051215576663543534.png" alt="2019051215576663543534.png"></p>
</li>
</ul>
<p>Authors’ formulation jointly maximizes the log likelihood of the forward and backward directions:</p>
<p><img src="http://image.nysdy.com/20190512155766643539454.png" alt="20190512155766643539454.png"></p>
<h2 id="ELMo"><a href="#ELMo" class="headerlink" title="ELMo"></a>ELMo</h2><ul>
<li><p>For each token $t_k$, a L-layer biLM computes a set of 2L + 1 representations:<img src="http://image.nysdy.com/20190512155767113638451.png" alt="20190512155767113638451.png"></p>
</li>
<li><p>For a downstream model, ELMo collapses all layers in R into a single vector.</p>
<p>In the simplest case, ELMo just selects the top layer.</p>
</li>
<li><p>For a task specific weighting of all biLM layers:<img src="http://image.nysdy.com/20190512155767132777658.png" alt="20190512155767132777658.png"></p>
<p>$s^{task}$ are softmax-normalized weithts and the scalar parameter $γ^{task}$ allows the task model to scale the entire ELMo vector</p>
</li>
</ul>
<h2 id="Using-biLMs-for-supervised-NLP-tasks"><a href="#Using-biLMs-for-supervised-NLP-tasks" class="headerlink" title="Using biLMs for supervised NLP tasks"></a>Using biLMs for supervised NLP tasks</h2><ul>
<li>Given a pre-trained biLM and a supervised architecture for a target NLP task</li>
<li>let the end task model learn a linear combination of these representations<ol>
<li>consider the lowest layers of th supervised model without the biLM</li>
<li>add ELMo to the supervised model<ul>
<li>freeze the weights of the biLM</li>
<li>concatenate the ELMo vector $ELMo^{task}_k$ with $x_k$ and pass the ELMo enhanced representation $[x_k,;ELMo^{task}_k ]$ into the task RNN.</li>
<li>for some tasks, authors also include ELMo ar the output of task RNN by introducing another set of out put specific linear weights and replacing $h_k$ with $[h_k,;ELMo^{task}_k ]$</li>
<li>add a moderate amount of dropout to ELMo and in some case to regularize the ELMo weights</li>
</ul>
</li>
</ol>
</li>
</ul>
<h2 id="Pre-trained-bidirectional-language-model-architecture"><a href="#Pre-trained-bidirectional-language-model-architecture" class="headerlink" title="Pre-trained bidirectional language model architecture"></a>Pre-trained bidirectional language model architecture</h2><ul>
<li>the biLM provides three layers of representations for each input token, both directions and a residual connection between LSTM layers </li>
<li>fine tuning the biLM on domain specific data</li>
</ul>
<h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><p>the following picture shows the performance of ELMo in Question answering, Textual entailment, Semantic role labeling, Corefrence resolution, Named entity extraction, Sentiment analysis.</p>
<p><img src="http://image.nysdy.com/2019051315577106394943.png" alt="2019051315577106394943.png"></p>
<p>In every task considered, simply adding ELMo establishes a new state-of-the-art result.</p>
<h1 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h1><h2 id="Alternate-layer-weighting-schemes"><a href="#Alternate-layer-weighting-schemes" class="headerlink" title="Alternate layer weighting schemes"></a>Alternate layer weighting schemes</h2><p><img src="http://image.nysdy.com/20190512155767132777658.png" alt="20190512155767132777658.png"></p>
<p>the following picture compares these alternatives.</p>
<p><img src="http://image.nysdy.com/20190513155771140936480.png" alt="20190513155771140936480.png"></p>
<p>Including representations from all layers improves overall performance over just using the last layer, and including contextual representations from the last layer improves performace over the baseline.</p>
<p>Also shows the $\lambda$ is important.</p>
<h2 id="Where-to-include-ELMo"><a href="#Where-to-include-ELMo" class="headerlink" title="Where to include ELMo?"></a>Where to include ELMo?</h2><p>The ELMo can be included in both the input and output.</p>
<p><img src="http://image.nysdy.com/20190513155771190646517.png" alt="20190513155771190646517.png"></p>
<p>the results show including the ELMo in both input and output can preform better.</p>
<h2 id="What-information-is-captured-by-the-biLM’s-representations"><a href="#What-information-is-captured-by-the-biLM’s-representations" class="headerlink" title="What information is captured by the biLM’s representations?"></a>What information is captured by the biLM’s representations?</h2><p>Intuitively, the biLM must be disambiguating the meaning of words using their context.<img src="http://image.nysdy.com/20190513155771262634271.png" alt="20190513155771262634271.png"></p>
<p>The GloVe can only capure the speech. but the biLM is able to disambiguate both the part of speech and word sense in the source sentence.</p>
<h3 id="Word-sense-disambiguation"><a href="#Word-sense-disambiguation" class="headerlink" title="Word sense disambiguation"></a>Word sense disambiguation</h3><p>given a sentence, predicting  the sense of a target word using a simple 1-nearst negihbor approach</p>
<p><img src="http://image.nysdy.com/20190513155771312514655.png" alt="20190513155771312514655.png"></p>
<h3 id="POS-tagging"><a href="#POS-tagging" class="headerlink" title="POS tagging"></a>POS tagging</h3><p>to examine whether the biLM captures basic syntax.</p>
<p><img src="http://image.nysdy.com/20190513155771328944169.png" alt="20190513155771328944169.png"></p>
<h2 id="Sample-efficiency"><a href="#Sample-efficiency" class="headerlink" title="Sample efficiency"></a>Sample efficiency</h2><p>Adding ELMo to a model increases the sample efficiency considerably, both in terms of number of parameter updates to reach state-of-the-art performance and the overall training set size.<img src="http://image.nysdy.com/20190513155771349825964.png" alt="20190513155771349825964.png"></p>
<h2 id="Visualization-of-learned-weights"><a href="#Visualization-of-learned-weights" class="headerlink" title="Visualization of learned weights"></a>Visualization of learned weights</h2><p><img src="http://image.nysdy.com/20190513155771355211483.png" alt="20190513155771355211483.png"></p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul>
<li><a href="https://zhuanlan.zhihu.com/p/63115885" target="_blank" rel="noopener">NAACL2018:高级词向量(ELMo)详解(超详细) 经典</a>，这篇文章中阐述了一些使用的细节，并用图来表示，更加清晰。</li>
<li><a href="https://blog.csdn.net/triplemeng/article/details/82380202" target="_blank" rel="noopener">ELMo算法介绍</a>，这篇博客中自己对整个论文的概述和总结和好，需要学习。</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>embedding</tag>
      </tags>
  </entry>
  <entry>
    <title>COMPOSITION-BASED MULTI-RELATIONAL GRAPH CONVOLUTIONAL NETWORKS 阅读笔记</title>
    <url>/post/COMPOSITION-BASED%20MULTI-RELATIONAL%20GRAPH%20CONVOLUTIONAL%20NETWORKS/</url>
    <content><![CDATA[<h1 id="0-导读"><a href="#0-导读" class="headerlink" title="0. 导读"></a>0. 导读</h1><h2 id="0-1-文章是关于什么的？（what？）"><a href="#0-1-文章是关于什么的？（what？）" class="headerlink" title="0.1 文章是关于什么的？（what？）"></a>0.1 文章是关于什么的？（what？）</h2><p>图卷积网络，</p>
<h2 id="0-2-要解决什么问题？（why？-challenge）"><a href="#0-2-要解决什么问题？（why？-challenge）" class="headerlink" title="0.2 要解决什么问题？（why？|challenge）"></a>0.2 要解决什么问题？（why？|challenge）</h2><ul>
<li>以前的方法主要集中于处理简单无向图</li>
<li>大多数现存模型在处理多关系图示遭受参数过大或者只学习节点表示</li>
</ul>
<h2 id="0-3-用什么方法解决？（how？）"><a href="#0-3-用什么方法解决？（how？）" class="headerlink" title="0.3 用什么方法解决？（how？）"></a>0.3 用什么方法解决？（how？）</h2><ul>
<li>在关系图中联合嵌入节点和关系的图卷积框架</li>
</ul>
<h2 id="0-4文章有什么创新？"><a href="#0-4文章有什么创新？" class="headerlink" title="0.4文章有什么创新？"></a>0.4文章有什么创新？</h2><ul>
<li>在关系图中联合嵌入节点和关系的图卷积框架</li>
<li>该框架可以推广到几个现存的多关系GCN模型</li>
</ul>
<h2 id="0-5-效果如何？"><a href="#0-5-效果如何？" class="headerlink" title="0.5 效果如何？"></a>0.5 效果如何？</h2><p>在节点分类，链接预测，图分类上都取得了明显的提升。</p>
<h2 id="0-6-还存在什么问题？"><a href="#0-6-还存在什么问题？" class="headerlink" title="0.6 还存在什么问题？"></a>0.6 还存在什么问题？</h2><h1 id="1-背景知识"><a href="#1-背景知识" class="headerlink" title="1 背景知识"></a>1 背景知识</h1><p>暂无</p>
<h1 id="2-模型"><a href="#2-模型" class="headerlink" title="2 模型"></a>2 模型</h1><p><strong>模型图结构：</strong><img src="http://image.nysdy.com/20200903213538.png" alt></p>
<p>作者在图中添加了反关系。</p>
<blockquote>
<p>The GCN formulation as devised by Marcheggiani &amp; Titov (2017) is based on the assumption that information in a directed edge ﬂows along both directions.</p>
</blockquote>
<p>多关系图表示为：$\mathcal{G}=(\mathcal{V}, \mathcal{R}, \mathcal{E}, \mathcal{X}, \mathcal{Z})$， 其中$\boldsymbol{\mathcal { Z }} \in \mathbb{R}|\mathcal{R}| \times d_{0}$表示初始关系特征。作者拓展了自反边和关系：</p>
<script type="math/tex; mode=display">
\left.\mathcal{E}^{\prime}=\mathcal{E} \cup\left\{\left(v, u, r^{-1}\right) \mid(u, v, r) \in \mathcal{E}\right\} \cup\{(u, u, \top) \mid u \in \mathcal{V})\right\}</script><script type="math/tex; mode=display">
\mathcal{R}^{\prime}=\mathcal{R} \cup \mathcal{R}_{\text {inv}} \cup\{\top\}</script><ul>
<li>其中$ \mathcal{R}_{\text {inv}}=\left\{r^{-1} \mid r \in \mathcal{R}\right\}$代表自反关系，$\top$代表自循环。</li>
</ul>
<h2 id="基于关系的合成"><a href="#基于关系的合成" class="headerlink" title="基于关系的合成"></a>基于关系的合成</h2><p>CompGCN学习了一个关系表示$\boldsymbol{h}_{r} \in \mathbb{R}^{d}, \forall r \in \mathcal{R}$， 节点嵌入：$\boldsymbol{h}_{v} \in \mathbb{R}^{d}, \forall v \in \mathcal{V}$把关系表示成向量可以避免过渡参数化问题，同时作者利用可以得到的关系特征作为初始表示。作者利用(Bordes et al., 2013; Nickel et al., 2016)使用的关系实体结合操作得到如下形式：</p>
<script type="math/tex; mode=display">
\boldsymbol{e}_{o}=\phi\left(\boldsymbol{e}_{s}, \boldsymbol{e}_{r}\right)</script><ul>
<li>$\phi: \mathbb{R}^{d} \times \mathbb{R}^{d} \rightarrow \mathbb{R}^{d}$是一个结合操作（作者使用了transE， multiplication， circular-correlation—方式）。$s,r,o$分别表示主语，关系和宾语，$\boldsymbol{e}_{(\cdot)} \in \mathbb{R}^{d}$表示他们的相关嵌入。</li>
</ul>
<h2 id="CompGCN"><a href="#CompGCN" class="headerlink" title="CompGCN"></a>CompGCN</h2><p>节点更新公式：</p>
<script type="math/tex; mode=display">
\boldsymbol{h}_{v}=f\left(\sum_{(u, r) \in \mathcal{N}(v)} \boldsymbol{W}_{\lambda(r)} \phi\left(\boldsymbol{x}_{u}, \boldsymbol{z}_{r}\right)\right)</script><ul>
<li>其中$\boldsymbol{x}_{u}, \boldsymbol{Z}_{r}$分别代表节点$u$和关系$r$，$h_v$代表节点$v$被更新表示。 </li>
<li>$\boldsymbol{W}_{\lambda(r)} \in \mathbb{R}^{d_{1} \times d_{0}}$代表关系类型的特定参数。</li>
</ul>
<p>其中作者使用了方向特定的权重，$\lambda(r)=\operatorname{dir}(r)$，被给与以下形式：</p>
<script type="math/tex; mode=display">
\boldsymbol{W}_{\operatorname{dir}(r)}=\left\{\begin{array}{ll}\boldsymbol{W}_{O}, & r \in \mathcal{R} \\ \boldsymbol{W}_{I}, & r \in \mathcal{R}_{i n v} \\ \boldsymbol{W}_{S}, & r=\top(\text {self-loop})\end{array}\right.</script><p>在节点更细以后，关系嵌入被转换为以下形式：</p>
<script type="math/tex; mode=display">
\boldsymbol{h}_{r}=\boldsymbol{W}_{\mathrm{rel}} \boldsymbol{z}_{r}</script><ul>
<li>$\boldsymbol{W}_{\mathrm{rel}} \in \mathbb{R}^{d_{1} \times d_{0}}$是一个可学习的转换矩阵，把所有的关系映射到与节点相同的嵌入空间，并再下一层可以被利用。</li>
</ul>
<p>作者展示了各种GCN每层的参数对比</p>
<p><img src="http://image.nysdy.com/20200904151209.png" alt></p>
<h3 id="随着关系数量的增加而缩放"><a href="#随着关系数量的增加而缩放" class="headerlink" title="随着关系数量的增加而缩放"></a>随着关系数量的增加而缩放</h3><p>作者使用了Schlichtkrull等人（2017）提出的基础公式的一种变体。 它们不是独立定义每个关系的嵌入，而是表示为一组基本向量的线性组合。 正式地，让$\left\{\boldsymbol{v}_{1}, \boldsymbol{v}_{2}, \ldots, \boldsymbol{v}_{\mathcal{B}}\right\}$为一组可学习的基础向量。 然后，初始关系表示为：</p>
<script type="math/tex; mode=display">
\boldsymbol{z}_{r}=\sum_{b=1}^{\mathcal{B}} \alpha_{b r} \boldsymbol{v}_{b}</script><ul>
<li>$\alpha_{b r} \in \mathbb{R}$是关系和基础特定的可学习标量权重。</li>
</ul>
<h3 id="关于与Relational-GCN的比较注意"><a href="#关于与Relational-GCN的比较注意" class="headerlink" title="关于与Relational-GCN的比较注意"></a>关于与Relational-GCN的比较注意</h3><p>这与Schlichtkrull等人的基础公式不同，其中为每个GCN层定义了一组单独的基础矩阵。 相比之下，COMPGCN使用嵌入矢量代替矩阵，并且仅在第一层定义基本矢量。 后面的层根据等式4通过转换共享关系。这使作者的模型比Relational-GCN更有效的参数。</p>
<p>通过堆叠COMPGCN层，可以得到：</p>
<script type="math/tex; mode=display">
\boldsymbol{h}_{v}^{k+1}=f\left(\sum_{(u, r) \in \mathcal{N}(v)} \boldsymbol{W}_{\lambda(r)}^{k} \phi\left(\boldsymbol{h}_{u}^{k}, \boldsymbol{h}_{r}^{k}\right)\right)</script><p>相似的，可以得到堆叠的关系表示：</p>
<script type="math/tex; mode=display">
\boldsymbol{h}_{r}^{k+1}=\boldsymbol{W}_{\mathrm{rel}}^{k} \boldsymbol{h}_{r}^{k}</script><p>这里$\boldsymbol{h}_{v}^{0} \text { and } \boldsymbol{h}_{r}^{0}$代表初始的节点和关系特征。</p>
<h3 id="Proposition-4-1"><a href="#Proposition-4-1" class="headerlink" title="Proposition 4.1."></a>Proposition 4.1.</h3><blockquote>
<p>COMPGCN generalizes the following Graph Convolutional based methods: KipfGCN (Kipf &amp; Welling, 2016), Relational GCN (Schlichtkrull et al., 2017), Directed GCN (Marcheggiani &amp; Titov, 2017), and Weighted GCN (Shang et al., 2019</p>
</blockquote>
<p>证明：例如对于Kipf-GCN，这可以通过使权重$\left(\boldsymbol{W}_{\lambda(r)}\right)$和合成函数$(\phi)$与关系无关来简单地获得，即$\boldsymbol{W}_{\lambda(r)}=\boldsymbol{W} \text { and } \phi\left(\boldsymbol{h}_{u}, \boldsymbol{h}_{r}\right)=\boldsymbol{h}_{u}$。 对于其他方法，也可以得到类似方法获得，如表2所示。</p>
<h1 id="3-实验"><a href="#3-实验" class="headerlink" title="3 实验"></a>3 实验</h1><p>任务：链接预测、节点分类、图分类</p>
<p>基线：</p>
<ul>
<li>Relational-GCN (R-GCN)</li>
<li>Directed-GCN (D-GCN)</li>
<li>Directed-GCN (D-GCN)</li>
</ul>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p><img src="http://image.nysdy.com/20200904153012.png" alt></p>
<h2 id="链接预测"><a href="#链接预测" class="headerlink" title="链接预测"></a>链接预测</h2><p><img src="http://image.nysdy.com/20200904155302.png" alt></p>
<h2 id="与其他GCN作对比"><a href="#与其他GCN作对比" class="headerlink" title="与其他GCN作对比"></a>与其他GCN作对比</h2><p><img src="http://image.nysdy.com/20200904155509.png" alt></p>
<script type="math/tex; mode=display">
\begin{array}{l}\text { Subtraction }(\mathbf{S u b}): \phi\left(e_{s}, e_{r}\right)=e_{s}-e_{r} \\ \text { Multiplication (Mult): } \phi\left(e_{s}, e_{r}\right)=e_{s} * e_{r} \\ \text { Circular-correlation (Corr): } \phi\left(e_{s}, e_{r}\right)=e_{s} \star e_{r}\end{array}</script><ul>
<li>这里有个疑问，对于那些没有关系嵌入的模型要怎么处理，是随机生成，然后通过分数函数来进行更新嘛。</li>
</ul>
<p><img src="http://image.nysdy.com/20200904153745.png" alt></p>
<p>总的来说，作者观察到观察到像循环相关这样的更复杂的算子的性能要好于或比简单的算子（如减法）要好。</p>
<h2 id="CompGCN的缩放能力"><a href="#CompGCN的缩放能力" class="headerlink" title="CompGCN的缩放能力"></a>CompGCN的缩放能力</h2><p>我们分析了具有不同数量的关系和基向量的COMPGCN的scalability。</p>
<h3 id="变化关系基向量的影响"><a href="#变化关系基向量的影响" class="headerlink" title="变化关系基向量的影响"></a>变化关系基向量的影响</h3><p><img src="http://image.nysdy.com/20200904160518.png" alt></p>
<p>模型性能随着基础数量的增加而提高。作者注意到注意到在B = 100的情况下，模型的性能变得与所有关系都有其各自嵌入的情况相当。</p>
<h3 id="关系数量的影响"><a href="#关系数量的影响" class="headerlink" title="关系数量的影响"></a>关系数量的影响</h3><p><img src="http://image.nysdy.com/20200904161031.png" alt></p>
<p>作者报告使用5个关联基向量（B = 5）的COMPGCN相对于每个关系都使用单独的向量的相对性能。结果表明，随着关系数量的增加，COMPGCN的参数有效变体也随之缩放。</p>
<h3 id="与R-GCN相比"><a href="#与R-GCN相比" class="headerlink" title="与R-GCN相比"></a>与R-GCN相比</h3><p><img src="http://image.nysdy.com/20200904160211.png" alt></p>
<h2 id="节点分类和图分类"><a href="#节点分类和图分类" class="headerlink" title="节点分类和图分类"></a>节点分类和图分类</h2><p><img src="http://image.nysdy.com/20200904161536.png" alt></p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul>
<li><a href="https://arxiv.org/pdf/1911.03082" target="_blank" rel="noopener">论文下载地址</a></li>
</ul>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>KG</tag>
        <tag>GCN</tag>
      </tags>
  </entry>
  <entry>
    <title>Embedding Edge-attributed Relational Hierarchies阅读笔记</title>
    <url>/post/Embedding_Edge-attributed_Relational_Hierarchies/</url>
    <content><![CDATA[<blockquote>
<p><a href="http://yellowstone.cs.ucla.edu/~muhao/articles/_SIGIR__hre.pdf" target="_blank" rel="noopener">论文下载地址</a></p>
</blockquote>
<a id="more"></a>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
  </entry>
  <entry>
    <title>DocRED A Large-Scale Document-Level Relation Extraction Dataset阅读笔记</title>
    <url>/post/DocRED_A_Large-Scale_Document-Level_Relation_Extraction_Dataset/</url>
    <content><![CDATA[<blockquote>
<p>这是一个介绍数据集的论文，主要是文档级别的关系抽取数据集。</p>
<p><a href="http://arxiv.org/abs/1906.06127" target="_blank" rel="noopener">论文下载地址</a></p>
</blockquote>
<h1 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h1><p>existing datasets for document-level RE </p>
<ul>
<li>either only have a small number of manually-annotated relations and entities, </li>
<li>or exhibit noisy annotations from distant supervision, </li>
<li>or serve specific domains or approaches.</li>
</ul>
<h1 id="Contribution-DocRED"><a href="#Contribution-DocRED" class="headerlink" title="Contribution (DocRED)"></a>Contribution (DocRED)</h1><ul>
<li>constructed from Wikipedia and Wikidata</li>
<li>DocRED contains 132, 375 entities and 56, 354 relational facts annotated on 5, 053 Wikipedia documents</li>
<li>As at least 40.7% of the relational facts in DocRED can only be extracted from multiple sentences</li>
<li><p>also provide large-scale distantly supervised data to support weakly supervised RE research</p>
</li>
<li><p>indicate the existing methods deal with the taks document level RE is  more difficult sentence-level RE.</p>
</li>
</ul>
<h1 id="data"><a href="#data" class="headerlink" title="data"></a>data</h1><p><img src="http://image.nysdy.com/20190701156194521958350.jpg" alt="20190701156194521958350.jpg"></p>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>RE</tag>
        <tag>dataset</tag>
      </tags>
  </entry>
  <entry>
    <title>ERNIE Enhanced Language Representation with Informative Entities阅读笔记</title>
    <url>/post/ERNIE_Enhanced_Language_Representation_with_Informative_Entities/</url>
    <content><![CDATA[<blockquote>
<p>该篇论文借鉴BERT，试图将实体信息（TransE）融入token(singal word)中，通过类似实体对齐的方法将实体与token对齐（并采取mask方式进行预训练），通过infromation fusion 将token与实体融合映射入相关联的两个向量空间。</p>
<p><a href="https://arxiv.org/pdf/1905.07129" target="_blank" rel="noopener">论文下载地址</a></p>
</blockquote>
<a id="more"></a>
<h1 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h1><p>the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding.</p>
<h1 id="Challenge"><a href="#Challenge" class="headerlink" title="Challenge"></a>Challenge</h1><p>For incorporating external knowledge into language representation models</p>
<ul>
<li>Structured Knowledge Encoding<ul>
<li>regarding to the given text, how to effectively extract and encode its related informative facts in KGs for language representation models</li>
</ul>
</li>
<li>Heterogeneous Information Fusion<ul>
<li>how to design a special pre-training objective to fuse the lexical, syntactic, and knowledge information is another challenge.</li>
</ul>
</li>
</ul>
<h1 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h1><p><img src="http://image.nysdy.com/20190625156142554544546.jpg" alt="20190625156142554544546.jpg"></p>
<h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p>ERNIE</p>
<ul>
<li><p>the underlying textual encoder (T-Encoder)负责从文本中捕获基本的词法和语法信息</p>
<ul>
<li><script type="math/tex; mode=display">
\left\{\boldsymbol{w}_{1}, \ldots, \boldsymbol{w}_{n}\right\}=\mathrm{T}-\operatorname{Encoder}\left(\left\{w_{1}, \ldots, w_{n}\right\}\right)</script><p>T-Encoder(·) is a multi-layer bidirectional Transformer encoder</p>
</li>
</ul>
</li>
<li><p>the upper knowledgeable encoder (K-Encoder)</p>
<ul>
<li>entity embeddings are pre-trained by TransE负责将知识图谱集成到底层的文本信息中</li>
</ul>
</li>
</ul>
<h2 id="Knowledgeable-Encoder"><a href="#Knowledgeable-Encoder" class="headerlink" title="Knowledgeable Encoder"></a>Knowledgeable Encoder</h2><ul>
<li>the knowledgeable encoder K-Encoder consists of stacked aggregators</li>
<li>designed for encoding both tokens and entities as well as fusing their heterogeneous features.</li>
</ul>
<p>In the i-th aggregator</p>
<ul>
<li><p>the input:</p>
<ul>
<li>token embeddings: $\left\{\boldsymbol{w}_{1}^{(i-1)}, \ldots, \boldsymbol{w}_{n}^{(i-1)}\right\}$</li>
<li>entity embeddings :$\left\{\boldsymbol{e}_{1}^{(i-1)}, \ldots, \boldsymbol{e}_{m}^{(i-1)}\right\}$</li>
</ul>
</li>
<li><p>fed into two multi-head self-attentions(MH-ATTs)</p>
<ul>
<li>$\left\{\tilde{\boldsymbol{w}}_{1}^{(i)}, \ldots, \tilde{\boldsymbol{w}}_{n}^{(i)}\right\}=\mathrm{MH}-\operatorname{ATT}\left(\left\{\boldsymbol{w}_{1}^{(i-1)}, \ldots, \boldsymbol{w}_{n}^{(i-1)}\right\}\right)$</li>
<li>$\left\{\tilde{\boldsymbol{e}}_{1}^{(i)}, \ldots, \tilde{\boldsymbol{e}}_{m}^{(i)}\right\}=\mathrm{MH}-\operatorname{ATT}\left(\left\{\boldsymbol{e}_{1}^{(i-1)}, \ldots, \boldsymbol{e}_{m}^{(i-1)}\right\}\right)$</li>
</ul>
</li>
<li><p>an information fusion layer</p>
<ul>
<li><script type="math/tex; mode=display">
\begin{aligned} \boldsymbol{h}_{j} &=\sigma\left(\tilde{\boldsymbol{W}}_{t}^{(i)} \tilde{\boldsymbol{w}}_{j}^{(i)}+\tilde{\boldsymbol{W}}_{e}^{(i)} \tilde{\boldsymbol{e}}_{k}^{(i)}+\tilde{\boldsymbol{b}}^{(i)}\right) \\ \boldsymbol{w}_{j}^{(i)} &=\sigma\left(\boldsymbol{W}_{t}^{(i)} \boldsymbol{h}_{j}+\boldsymbol{b}_{t}^{(i)}\right) \\ \boldsymbol{e}_{k}^{(i)} &=\sigma\left(\boldsymbol{W}_{e}^{(i)} \boldsymbol{h}_{j}+\boldsymbol{b}_{e}^{(i)}\right) \end{aligned}</script></li>
</ul>
<p>$h_j$ is the inner hidden state</p>
</li>
</ul>
<p>For the tokens without corresponding entities</p>
<script type="math/tex; mode=display">
\begin{aligned} \boldsymbol{h}_{j} &=\sigma\left(\tilde{\boldsymbol{W}}_{t}^{(i)} \tilde{\boldsymbol{w}}_{j}^{(i)}+\tilde{\boldsymbol{b}}^{(i)}\right) \\ \boldsymbol{w}_{j}^{(i)} &=\sigma\left(\boldsymbol{W}_{t}^{(i)} \boldsymbol{h}_{j}+\boldsymbol{b}_{t}^{(i)}\right) \end{aligned}</script><h2 id="Pre-training-for-Injecting-Knowledge"><a href="#Pre-training-for-Injecting-Knowledge" class="headerlink" title="Pre-training for Injecting Knowledge"></a>Pre-training for Injecting Knowledge</h2><p>In order to inject knowledge into language rep- resentation by informative entities.</p>
<p>Randomly masks some token-entity alignments and then requires the system to predict all corresponding entities based on aligned tokens.</p>
<ul>
<li><p>denoising entity auto-encoder (dEA)</p>
</li>
<li><p>define the aligned entity distribution for the token $w_i$ as follows:</p>
<script type="math/tex; mode=display">
p\left(e_{j} | w_{i}\right)=\frac{\exp \left(\text { linear }\left(\boldsymbol{w}_{i}^{o}\right) \cdot \boldsymbol{e}_{j}\right)}{\sum_{k=1}^{m} \exp \left(\text { linear }\left(\boldsymbol{w}_{i}^{o}\right) \cdot \boldsymbol{e}_{k}\right)}</script></li>
</ul>
<p>  linear(·) is a linear layer</p>
<p>For dEA, perform the following operations:</p>
<ul>
<li>in 5% of the time, replace the entity with another random<ul>
<li>aims to train model to correct the errors that the token is aligned with a wrong entity;</li>
</ul>
</li>
<li>In 15% of the time, mask token-entity alignments<ul>
<li>aims to train model to correct the errors that entity alignment system doesn’t extract all existing alignments;</li>
</ul>
</li>
<li>in the rest of the time, keep token-entity alignments unchanged <ul>
<li>aims to encourage our model to integrate the entity information into token representations for better language understanding.</li>
</ul>
</li>
</ul>
<h2 id="Fine-tuning-for-Specific-Tasks"><a href="#Fine-tuning-for-Specific-Tasks" class="headerlink" title="Fine-tuning for Specific Tasks"></a>Fine-tuning for Specific Tasks</h2><p><img src="http://image.nysdy.com/20190625156143137852366.jpg" alt="20190625156143137852366.jpg"></p>
<p>We can take the final output embedding for the first token, which corresponds to the special [CLS] token, as the representation of the input sequence for specific tasks.</p>
<p>For some knowledge-driven tasks, we design special fine-tuning procedure:</p>
<ul>
<li>relation classification<ul>
<li>design different tokens [HD] and [TL] for head entities and tail entities respectively</li>
<li>a similar role like position embeddings in the conventional relation classification models (Zeng et al., 2015)</li>
</ul>
</li>
<li>entity typing<ul>
<li>the mention mark token [ENT]</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>这里的CLS不知道有什么作用，所有的任务都有，是不同的任务重CLS的embedding有所不同吗？个人目前觉得是这样的。</li>
<li>作者这里采用的mark token的方法代替position embedding，不知道两个对比那种效果会更好一些。直观觉得都是标记位置信息。</li>
</ul>
</blockquote>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h3 id="Pre-training-Dataset"><a href="#Pre-training-Dataset" class="headerlink" title="Pre-training Dataset"></a>Pre-training Dataset</h3><ul>
<li>we use English Wikipedia as our pre-training corpus and align text to Wiki-data<ul>
<li>4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities</li>
</ul>
</li>
<li>before pre-training ERINE, entity embeddings by TransE<ul>
<li>sample part of Wikidata which contains 5, 040, 986 entities and 24, 267, 796 fact triples</li>
</ul>
</li>
</ul>
<h3 id="Training-Details"><a href="#Training-Details" class="headerlink" title="Training Details"></a>Training Details</h3><ul>
<li>We also fine-tune ERNIE on the distant supervised dataset, i.e., FIGER (Ling et al., 2015)</li>
<li>we use TAGME (Ferragina and Scaiella, 2010) to extract the entity mentions in the sentences and link them to their corresponding entities in KGs</li>
</ul>
<h2 id="Entity-Typing"><a href="#Entity-Typing" class="headerlink" title="Entity Typing"></a>Entity Typing</h2><h3 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h3><p>two well-established datasets FIGER (Ling et al., 2015) and Open Entity (Choi et al., 2018).</p>
<ul>
<li>The training set of FIGER is labeled with distant supervision, and its test set is annotated by human.</li>
<li>Open Entity is a completely manually-annotated dataset.</li>
</ul>
<p><img src="http://image.nysdy.com/20190625156143360958681.jpg" alt="20190625156143360958681.jpg"></p>
<h3 id="Comparble-model"><a href="#Comparble-model" class="headerlink" title="Comparble model"></a>Comparble model</h3><ul>
<li>NFGEC<ul>
<li>NFGEC is a hybrid model proposed by Shimaoka et al. (2016)</li>
</ul>
</li>
<li>UFET<ul>
<li>(Choi et al., 2018)</li>
</ul>
</li>
</ul>
<h4 id="The-results-on-FIGER"><a href="#The-results-on-FIGER" class="headerlink" title="The results on FIGER:"></a>The results on FIGER:</h4><p>However, BERT has lower accuracy than the best NFGEC model. As strict accuracy is the ratio of instances whose predictions are identical to human annotations, it illustrates <strong>some wrong labels from distant supervision are learned by BERT</strong> due to its powerful fitting ability.</p>
<h2 id="Relation-Classification"><a href="#Relation-Classification" class="headerlink" title="Relation Classification"></a>Relation Classification</h2><h3 id="dataset-1"><a href="#dataset-1" class="headerlink" title="dataset"></a>dataset</h3><p>two well-established datasets FewRel (Han et al., 2018b) and TACRED (Zhang et al., 2017).</p>
<ul>
<li>FewRel<ul>
<li>As FewRel does not have any null instance where there isn’t any relation between entities, we adopt macro averaged metrics to present the model performances. Since FewRel is built by checking whether the sentences contain facts in Wiki-data, we drop the related facts in KGs before pretraining for fair comparison</li>
</ul>
</li>
<li>TACRED<ul>
<li>In TACRED, there are nearly 80% null instances so that we follow the previous work (Zhang et al., 2017) to adopt micro averaged metrics to represent the model performances instead of the macro</li>
</ul>
</li>
</ul>
<p><img src="http://image.nysdy.com/20190625156143680247028.jpg" alt="20190625156143680247028.jpg"></p>
<h3 id="Comparble-model-1"><a href="#Comparble-model-1" class="headerlink" title="Comparble model"></a>Comparble model</h3><ul>
<li>CNN:(Zeng et al., 2015).</li>
<li>PA-LSTM</li>
<li>C-GCN :Zhang et al. (2018) adopt the graph convolution operations to model dependency trees for relation classification.<graph convolution over pruned dependency trees improves relation extraction.></graph></li>
</ul>
<h2 id="GLUE"><a href="#GLUE" class="headerlink" title="GLUE"></a>GLUE</h2><p>The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is a collection of diverse natural language understanding tasks</p>
<h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><p>explore the effects of the informative entities and the knowledgeable pretraining task (dEA) for ERNIE using FewRel dataset</p>
<blockquote>
<p>实验部分做的很丰富，既有两个任务的对比实验，也有对自身模块的对比实验，并且还对比了bert来检测自己模型是否对GLUE任务效果有降低。</p>
</blockquote>
<h1 id="future-research"><a href="#future-research" class="headerlink" title="future research"></a>future research</h1><p>1) inject knowledge into feature-based pre-training models such as ELMo (Peters et al., 2018); </p>
<p>(2) introduce diverse structured knowledge into language representation models such as ConceptNet (Speer and Havasi, 2012) which is different from world knowledge database Wikidata; </p>
<p>(3) annotate more real-world corpora heuristically for larger pre-training data</p>
<blockquote>
</blockquote>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul>
<li><a href="https://blog.csdn.net/summerhmh/article/details/91042273" target="_blank" rel="noopener">https://blog.csdn.net/summerhmh/article/details/91042273</a></li>
</ul>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>KG</category>
      </categories>
      <tags>
        <tag>KG</tag>
        <tag>KGR</tag>
        <tag>BERT</tag>
      </tags>
  </entry>
  <entry>
    <title>End-to-End Structure-Aware Convolutional Networks for Knowledge Base Completion--阅读笔记</title>
    <url>/post/End-to-End%20Structure-Aware%20Convolutional%20Networks%20for%20Knowledge%20Base%20Completion%20reading%20note/</url>
    <content><![CDATA[<h1 id="0-总览"><a href="#0-总览" class="headerlink" title="0. 总览"></a>0. 总览</h1><h2 id="0-1-要解决什么问题？（what？）"><a href="#0-1-要解决什么问题？（what？）" class="headerlink" title="0.1 要解决什么问题？（what？）"></a>0.1 要解决什么问题？（what？）</h2><p>知识图谱补全中：</p>
<ol>
<li>以前的方法只建模关系三元组，忽略了大量图节点相关属性；</li>
<li>以前的方法并没有增强在嵌入空间中大规模连接结构，完全忽略了图结构。</li>
</ol>
<h2 id="0-2-用什么方法解决？（how？）"><a href="#0-2-用什么方法解决？（how？）" class="headerlink" title="0.2 用什么方法解决？（how？）"></a>0.2 用什么方法解决？（how？）</h2><p>用GCN和ConvErt相结合的方法组成一个端到端的学习方法。</p>
<ol>
<li>encoder部分利用一个带权重的GCN的方法（利用图的结构并保留节点的属性）。</li>
<li>decoder部分利用Conv-TransE，利用啊convE的方法，但是去掉了实体和关系的矩阵重组部分。（为了保留TransE的 <code>h+r=t</code> 的特性）。</li>
</ol>
<h2 id="0-3文章有什么创新？"><a href="#0-3文章有什么创新？" class="headerlink" title="0.3文章有什么创新？"></a>0.3文章有什么创新？</h2><p>见上述方法</p>
<h2 id="0-4-效果如何？"><a href="#0-4-效果如何？" class="headerlink" title="0.4 效果如何？"></a>0.4 效果如何？</h2><p>FB15k-237 and WN18RR数据机上，较ConvE提升了10%。</p>
<h2 id="0-5-还存在什么问题？"><a href="#0-5-还存在什么问题？" class="headerlink" title="0.5 还存在什么问题？"></a>0.5 还存在什么问题？</h2><h1 id="1-背景知识"><a href="#1-背景知识" class="headerlink" title="1 背景知识"></a>1 背景知识</h1><h1 id="2-模型"><a href="#2-模型" class="headerlink" title="2 模型"></a>2 模型</h1><p>end-to-end SACN<img src="http://image.nysdy.com/20200825165457.png" alt></p>
<h2 id="2-1-权重图卷积层（WGCN）"><a href="#2-1-权重图卷积层（WGCN）" class="headerlink" title="2.1 权重图卷积层（WGCN）"></a>2.1 权重图卷积层（WGCN）</h2><p>节点$v_i$的第$l$层输出为：</p>
<script type="math/tex; mode=display">
h_{i}^{l+1}=\sigma\left(\sum_{j \in \mathbf{N}_{i}} \alpha_{t}^{l} g\left(h_{i}^{l}, h_{j}^{l}\right)\right)</script><ul>
<li>其中$h_{j}^{l} \in \mathbb{R}^{F^{l}}$是节点$v_j$的输入，$v_j$是节点$v_i$的邻居节点（一共$N_i$个）。$\alpha_{t}^{l} $是第$l$层，第$t$种边（关系）类型的权重参数。</li>
</ul>
<p>$g$函数是整合邻居信息函数，在这里坐着采用以下函数：</p>
<script type="math/tex; mode=display">
g\left(h_{i}^{l}, h_{j}^{l}\right)=h_{j}^{l} W^{l}</script><ul>
<li>其中$W^{l} \in \mathbb{R}^{F^{l} \times F^{l+1}}$</li>
</ul>
<p>作者再加入$v_i$的自连接，则节点$v_i$的传播过程可以被定义为：</p>
<script type="math/tex; mode=display">
h_{i}^{l+1}=\sigma\left(\sum_{j \in \mathbf{N}_{i}} \alpha_{t}^{l} h_{j}^{l} W^{l}+h_{i}^{l} W^{l}\right)</script><ul>
<li>节点$h_i^{l+1}$是节点第$l$层输出的节点特征矩阵$H^{l+1} \in \mathbb{R}^{N \times F^{l+1}}$的第$l+1$行，代表$v_i$节点在第$l+1$层的特征。</li>
</ul>
<p><img src="http://image.nysdy.com/20200825171648.png" alt></p>
<p>以上的为不同类型的边加权重的过程可以看做一个矩阵乘法：通过一个邻接矩阵同时为所有节点计算embeddings，见上图：</p>
<p>邻接矩阵可以被写作：</p>
<script type="math/tex; mode=display">
A^{l}=\sum_{t=1}^{T}\left(\alpha_{t}^{l} A_{t}\right)+I</script><ul>
<li>其中$I$是一个单位矩阵。$A^t$是一个二进制矩阵，当$v_i$和$v_j$节点间有边连接则其中第$ij$项值为1，否则为0。</li>
</ul>
<p>故，由此可以等到每一层线性变换的所有一节邻居：</p>
<script type="math/tex; mode=display">
H^{l+1}=\sigma\left(A^{l} H^{l} W^{l}\right)</script><h2 id="2-2-属性节点"><a href="#2-2-属性节点" class="headerlink" title="2.2 属性节点"></a>2.2 属性节点</h2><p>在知识图谱中，属性节点的形式为(entity, relation, attribute)，例如：(s = Tom, r = people.person.gender, a = male)。但是如果用向量表达节点属性会出现两个问题：</p>
<ol>
<li>由于每个节点的属性数量少，会产生属性向量稀疏的问题；</li>
<li>属性向量的0值会产生歧义：该节点没有该属性或者该属性缺失值。</li>
</ol>
<p>本文还使用了节点的属性作为图的节点，如属性（Tom，gender，male）。这样做的目的是将属性也作为节点，起到“桥”的作用，相同属性的节点可以共享信息。还有作者为了减少过多的属性节点，对节点进行了合并， 将gender也作为了图中的节点，而不是建立male和female两个属性，理由是gender已经能够确定实体的person，而不必过多区分性别。（即作者把属性三元组变为了两元组（实体，属性名）</p>
<h2 id="2-3-Conv-TransE"><a href="#2-3-Conv-TransE" class="headerlink" title="2.3 Conv-TransE"></a>2.3 Conv-TransE</h2><p>作者仅将节点向量$e_s$和关系向量$e_r$进行堆叠，然后进行卷积操作：</p>
<script type="math/tex; mode=display">
\begin{aligned} m_{c}\left(e_{s}, e_{r}, n\right)=& \sum_{\tau=0}^{K-1} \omega_{c}(\tau, 0) \hat{e}_{s}(n+\tau) +\omega_{c}(\tau, 1) \hat{e}_{r}(n+\tau) \end{aligned}</script><ul>
<li>其中$K$是卷积核的宽度，$n$索引输出向量中的条目,$\omega_{c}$是卷积核参数。</li>
</ul>
<p>分数函数被定义为：</p>
<script type="math/tex; mode=display">
\psi\left(e_{s}, e_{o}\right)=f\left(\operatorname{vec}\left(\mathbf{M}\left(e_{s}, e_{r}\right)\right) W\right) e_{o}</script><ul>
<li>$W \in \mathbb{R}^{C F^{L} \times F^{L}}$是一个线性变换，$\operatorname{vec}(\mathbf{M}) \in \mathbb{R}^{C F^{L}}$reshape成一个向量.</li>
</ul>
<p>然后应用了一个逻辑sigmod函数：</p>
<script type="math/tex; mode=display">
p\left(e_{s}, e_{r}, e_{o}\right)=\sigma\left(\psi\left(e_{s}, e_{o}\right)\right)</script><h1 id="3-实验"><a href="#3-实验" class="headerlink" title="3 实验"></a>3 实验</h1><p><img src="http://image.nysdy.com/20200826205435.png" alt></p>
<blockquote>
<p>数据集：FB15k-237, WN18RR and FB15k-237-Attr</p>
<p>baseline:DistMult，ComplEx，R-GCN，ConvE</p>
<p>评价准则：MRR(mean reciprocal rank)（Raw，Filtered），Hits @（1，3，10）</p>
</blockquote>
<h2 id="3-1-链接预测实验结果："><a href="#3-1-链接预测实验结果：" class="headerlink" title="3.1 链接预测实验结果："></a>3.1 链接预测实验结果：<img src="http://image.nysdy.com/20200826205515.png" alt></h2><h2 id="3-2-卷积核大小分析"><a href="#3-2-卷积核大小分析" class="headerlink" title="3.2 卷积核大小分析"></a>3.2 卷积核大小分析</h2><p><img src="http://image.nysdy.com/20200826210216.png" alt></p>
<p>内核“ 2×1”表示在实体向量的一个属性和关系向量的对应属性之间转换的知识或信息。 如果将内核大小增加到“ 2×k”，其中k = {3，5}，则会在实体向量的s属性组合和关系向量的k属性组合之间转换信息。</p>
<h2 id="3-3-节点度分析"><a href="#3-3-节点度分析" class="headerlink" title="3.3 节点度分析"></a>3.3 节点度分析</h2><p><img src="http://image.nysdy.com/20200826210546.png" alt></p>
<p>知识图中节点的度数是连接到该节点的边数。</p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul>
<li><a href="https://www.cnblogs.com/jws-2018/p/11519383.html" target="_blank" rel="noopener">https://www.cnblogs.com/jws-2018/p/11519383.html</a></li>
</ul>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>2019</tag>
        <tag>AAAI</tag>
        <tag>GCN</tag>
        <tag>KGC</tag>
      </tags>
  </entry>
  <entry>
    <title>Graph Neural Networks with Generated Parameters for Relation Extraction阅读笔记</title>
    <url>/post/Graph_Neural_Networks_with_Generated_Parameters_for_Relation/</url>
    <content><![CDATA[<blockquote>
<p>本文将GNNs应用到处理非结构化文本的（多跳）关系推理任务来进行关系抽取。采用从句子序列中获取的实体构建全链接图，应用编码（sequence model），传播（节点间信息）和分类（预测）三个模块来处理关系推理。本文提供了三个数据集。</p>
</blockquote>
<a id="more"></a>
<h1 id="problem-statement"><a href="#problem-statement" class="headerlink" title="problem statement"></a>problem statement</h1><ul>
<li>existing relation extraction models fail to infer the relationship without multi-hop relational reasoning.</li>
<li>existing GNNs can’t process multi-hop relational reasoning in natural language relational reasoning </li>
</ul>
<h1 id="research-objective"><a href="#research-objective" class="headerlink" title="research objective"></a>research objective</h1><p>enable GNNs to porcess relational reasoning on unstructed text inputs</p>
<h1 id="contribution"><a href="#contribution" class="headerlink" title="contribution"></a>contribution</h1><ul>
<li>extend a GNN with generated parameters, which could be applied to process relational reasoning on unstructured inputs</li>
<li>verify GP-GNNs in the taks of relation extraction from text; present three datasets</li>
</ul>
<h1 id="GP-GNNs"><a href="#GP-GNNs" class="headerlink" title="GP-GNNs"></a>GP-GNNs</h1><ul>
<li>construct a fully-connected graph with the entities in the sequence of text</li>
<li>employs three models to process relational reasoning<ul>
<li>an encoding modul: enable edges to encode rich information from natural languages </li>
<li>a propagation modul: propagates realtional information among various nodes </li>
<li>a classification modul: make prediction with node representations </li>
</ul>
</li>
</ul>
<p>As compared to tradtional GNNs, GP-GNNs could learn edges’ parameters from natural lanuages</p>
<h1 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h1><h2 id="Graph-Neural-Networks-GNNs"><a href="#Graph-Neural-Networks-GNNs" class="headerlink" title="Graph Neural Networks(GNNs)"></a>Graph Neural Networks(GNNs)</h2><ul>
<li>existing models still perfom message-passing on predefined graphs</li>
<li><em>Learning Graphical State Transitions</em> is most related<ul>
<li>introduecs a nove lnerual architecture to generate a graph based on the textal input</li>
<li>dynamically update the relationship during the learning process</li>
</ul>
</li>
</ul>
<h2 id="relational-reasoning"><a href="#relational-reasoning" class="headerlink" title="relational reasoning"></a>relational reasoning</h2><ul>
<li>existing models could not make full use of the multi-hop inference patterns among multiple entity pair and their relaitons within the sentence </li>
<li><em>LEARNING GRAPHICAL STATE TRANSITIONS</em> is the most related work<ul>
<li>the proposed model incorporates contextual relations with attention mechanism when predicting the relation of a target entity pair</li>
</ul>
</li>
</ul>
<h1 id="Graph-Neural-Network-with-Grenerated-Parameters-GP-GNNs"><a href="#Graph-Neural-Network-with-Grenerated-Parameters-GP-GNNs" class="headerlink" title="Graph Neural Network with Grenerated Parameters(GP-GNNs)"></a>Graph Neural Network with Grenerated Parameters(GP-GNNs)</h1><p>The picture is overall architecture: encoding module, propagation module and classification module</p>
<p><img src="http://image.nysdy.com/20190523155858968594021.jpg" alt="20190523155858968594021.jpg"></p>
<h2 id="Encoding-Module"><a href="#Encoding-Module" class="headerlink" title="Encoding Module"></a>Encoding Module</h2><p>formula:</p>
<script type="math/tex; mode=display">
\mathcal{A}_{i, j}^{(n)}=f\left(E\left(x_{0}^{i, j}\right), E\left(x_{1}^{i, j}\right), \cdots, E\left(x_{l-1}^{i, j}\right) ; \theta_{e}^{n}\right)</script><p>where $f(\cdot)$ could be any model that could sequential(such as LSTMs); $E(\cdot)$ indicates an embedding function. $x^{i, j}$ is the word in sentence labeled( $i,j$)</p>
<h2 id="Porpagation-Module"><a href="#Porpagation-Module" class="headerlink" title="Porpagation Module"></a>Porpagation Module</h2><p>the representations of layer n + 1 are calculated by:</p>
<script type="math/tex; mode=display">
\mathbf{h}_{i}^{(n+1)}=\sum_{v_{j} \in \mathcal{N}\left(v_{i}\right)} \sigma\left(\mathcal{A}_{i, j}^{(n)} \mathbf{h}_{j}^{(n)}\right)</script><p>where $\mathcal{N}\left(v_{i}\right)$ denotes the neighbors of node $v_i$</p>
<h2 id="Classification-Module"><a href="#Classification-Module" class="headerlink" title="Classification Module"></a>Classification Module</h2><p>the loss of GP-GNNs:</p>
<script type="math/tex; mode=display">
\mathcal{L}=g\left(\mathbf{h}_{0 :|\mathcal{V}|-1}^{0}, \mathbf{h}_{0 :|\mathcal{V}|-1}^{1}, \ldots, \mathbf{h}_{0 :|\mathcal{V}|-1}^{K}, Y ; \theta_{c}\right)</script><h1 id="Relation-Extraction-with-GP-GNNs"><a href="#Relation-Extraction-with-GP-GNNs" class="headerlink" title="Relation Extraction with GP-GNNs"></a>Relation Extraction with GP-GNNs</h1><p>Authors introduce how to apply GP-GNNs to relation extraction</p>
<h2 id="Encoding-Module-1"><a href="#Encoding-Module-1" class="headerlink" title="Encoding Module"></a>Encoding Module</h2><p>encoding then context of entity pairs (or edges in the graph)</p>
<script type="math/tex; mode=display">
E\left(x_{t}^{i, j}\right)=\left[\boldsymbol{x}_{t} ; \boldsymbol{p}_{t}^{i, j}\right]</script><p>where $x_t$ denotes the word embedding; $\boldsymbol{p}_{t}^{i, j}$denotes the position embedding of word posistion t relative to the entity pair’s position $i, j$.</p>
<h3 id="position-embedding"><a href="#position-embedding" class="headerlink" title="position embedding"></a>position embedding</h3><p>we mark each token in the sentence as either belonging to the first entity $v_i$, the second entity $v_j$ or to neither of those</p>
<h2 id="Propagation-Module"><a href="#Propagation-Module" class="headerlink" title="Propagation Module"></a>Propagation Module</h2><p> the formula is the same as the front</p>
<h3 id="The-Initial-Embeddings-of-Nodes"><a href="#The-Initial-Embeddings-of-Nodes" class="headerlink" title="The Initial Embeddings of Nodes"></a>The Initial Embeddings of Nodes</h3><ul>
<li>when extracting the relationship between entity $v_i$ and entity $v_j$, the initial embeddings of them are annotated as $\mathbf{h}_{v_{i}}^{(0)}=a_{\text { subject }}$, and $h_{v_{j}}^{(0)}=a_{\text { object }}$, while the intial embeddings of other entities are set to all zeros.</li>
<li>In our experiments, we generalize the idea of Gated Graph Neural Networks (Li et al., 2016) by setting $a_{\text { subject }}=[1 ; 0]^{\top}$and $a_{\text { object }}=[0 ; 1]^{\top}$.</li>
</ul>
<h2 id="classification-Module"><a href="#classification-Module" class="headerlink" title="classification Module"></a>classification Module</h2><p><strong>As the  target entity pair $(v_i, v_j)$:</strong></p>
<script type="math/tex; mode=display">
\boldsymbol{r}_{v_{i}, v_{j}}=\left[\left[\boldsymbol{h}_{v_{i}}^{(1)} \odot \boldsymbol{h}_{v_{j}}^{(1)}\right]^{\top} ;\left[\boldsymbol{h}_{v_{i}}^{(2)} \odot \boldsymbol{h}_{v_{j}}^{(2)}\right]^{\top} ; \ldots ;\left[\boldsymbol{h}_{v_{i}}^{(K)} \odot \boldsymbol{h}_{v_{j}}^{(K)}\right]^{\top}\right]</script><p>where $\odot$ represents element-wise multiplication</p>
<p><strong>classification:</strong></p>
<script type="math/tex; mode=display">
\mathbb{P}\left(r_{v_{i}, v_{j}} | h, t, s\right)=\operatorname{softmax}\left(M L P\left(\boldsymbol{r}_{v_{i}, v_{j}}\right)\right)</script><p><strong>loss:</strong></p>
<script type="math/tex; mode=display">
\mathcal{L}=\sum_{s \in S} \sum_{i \neq j} \log \mathbb{P}\left(r_{v_{i}, v_{j}} | i, j, s\right)</script><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="aim"><a href="#aim" class="headerlink" title="aim"></a>aim</h2><ul>
<li>showing their best models could improve the performance of relation extraction under a variety of settings</li>
<li>illlustrating that how the number of layers affect the performance of their model</li>
<li>performing a qualitiative investigation to highlight the diference between their models and baseline models</li>
</ul>
<h2 id="design"><a href="#design" class="headerlink" title="design"></a>design</h2><p>as the first and second aim</p>
<ul>
<li>show that our models could improve instance-level relation extraction on a human annotated test set</li>
<li>we will show that our models could also help enhance the performance of bag-level relation extraction on a distantly labeled test set</li>
<li>split a subset of distantly labeled test set, where the number of entities and edges is large</li>
</ul>
<h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><h3 id="distantly-label-set"><a href="#distantly-label-set" class="headerlink" title="distantly label set"></a>distantly label set</h3><ul>
<li>Sorokin and Gurevych (2017) proposed </li>
<li>modify their dataset<ul>
<li>added reversed edges</li>
<li>for all of the entity pairs with no relations, added “NA” labels to them</li>
</ul>
</li>
</ul>
<h3 id="Human-annotated-test-set"><a href="#Human-annotated-test-set" class="headerlink" title="Human annotated test set"></a>Human annotated test set</h3><ul>
<li>Sorokin and Gurevych (2017)</li>
<li>select the distantly lablel pairs which all 5 annotaters are accepted.</li>
<li>There are 350 sentences and 1,230 triples in this test set </li>
</ul>
<h3 id="Dense-distantly-labeled-test-set"><a href="#Dense-distantly-labeled-test-set" class="headerlink" title="Dense distantly labeled test set"></a>Dense distantly labeled test set</h3><ul>
<li>criteria<ul>
<li>the number of entities should be strictly larger than 2</li>
<li>there must be at least one circle (with at least three entities) in the ground-truth label of the sentence</li>
</ul>
</li>
<li>There are 1,350 sentences and more than 17,915 triples and 7,906 relational facts in this test set.</li>
</ul>
<h2 id="Models-for-comparison"><a href="#Models-for-comparison" class="headerlink" title="Models for comparison"></a>Models for comparison</h2><ul>
<li>Context-aware RE</li>
<li>Multi-Window CNN</li>
<li>PCNN</li>
<li>LSTM or GP-GNN with K = 1 layer</li>
<li>GP-GNN with K = 2 or K = 3 layerss</li>
</ul>
<h2 id="Evaluation-Details"><a href="#Evaluation-Details" class="headerlink" title="Evaluation Details"></a>Evaluation Details</h2><p><strong>To evaluation models in bag-level:</strong></p>
<script type="math/tex; mode=display">
E\left(r | v_{i}, v_{j}, S\right)=\max _{s \in S} \mathbb{P}\left(r_{v_{i}, v_{j}} | i, j, s\right)</script><p><strong>result</strong>:</p>
<p><img src="http://image.nysdy.com/20190523155859369893411.jpg" alt="20190523155859369893411.jpg"></p>
<h2 id="Effectiveness-of-Reasoning-Mechanism"><a href="#Effectiveness-of-Reasoning-Mechanism" class="headerlink" title="Effectiveness of Reasoning Mechanism"></a>Effectiveness of Reasoning Mechanism</h2><p><img src="http://image.nysdy.com/2019052315585938129506.jpg" alt="2019052315585938129506.jpg"></p>
<ul>
<li>Context-Aware RE may <strong>introduce more noise,</strong> for it may mistakenly increase the probability of a relation with the similar topic with the context relations</li>
<li>sentences from Wikipedia corpus are always complex, which may be hard to model for CNN and PCNN</li>
</ul>
<h2 id="The-Effectiveness-of-the-Number-of-Layers"><a href="#The-Effectiveness-of-the-Number-of-Layers" class="headerlink" title="The Effectiveness of the Number of Layers"></a>The Effectiveness of the Number of Layers</h2><p><img src="http://image.nysdy.com/20190523155859455443298.jpg" alt="20190523155859455443298.jpg"></p>
<ul>
<li>the improvement of the third layer is much smaller on the overall distantly supervised test set than the one on the dense subset<ul>
<li>This observation reveals that the reasoning mechanism could help us identify relations especially on sentences where there are more entities</li>
</ul>
</li>
<li>as the number of layers grows, the curves get higher and higher precision, <ul>
<li>indicating considering more hops in reasoning leads to better performance</li>
</ul>
</li>
</ul>
<h2 id="Qualitative-Results-Case-Study"><a href="#Qualitative-Results-Case-Study" class="headerlink" title="Qualitative Results: Case Study"></a>Qualitative Results: Case Study</h2><p><img src="http://image.nysdy.com/20190523155859457710223.jpg" alt="20190523155859457710223.jpg"></p>
<p>Context-Aware RE makes a mistake by predicting (Kentucky, share boarder with, Ohio). As we have discussed before, this is due to its mechanism to model co-occurrence of multiple relations</p>
<h1 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h1><p>文章是刘知远组的论文，针对的方向是关系抽取，在其中结合了关系推理，最近许多任务都在结合推理的思想。文章整体的结构，逻辑十分清晰，论述的也比较详细，属于标准论文。感觉文章中GP-GNNs结构图还可以画的更好一点，展现一下encoding module的层，可以更好理解。文章的精髓应该是这个propagation module的部分，还需要消化一下，不过这部分可能是有先前的知识支撑的。</p>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>GNNs</tag>
        <tag>relation extraction</tag>
        <tag>relation reasoning</tag>
      </tags>
  </entry>
  <entry>
    <title>From Knowledge Graph Embedding to Ontology Embedding  An Analysis of the Compatibility between Vector Space Representations and Rules阅读笔记</title>
    <url>/post/From_Knowledge_Graph_Embedding_to_Ontology_Embedding_An_Analysis_of_the_Compatibility_between_Vector_Space_Representations_and_Rules/</url>
    <content><![CDATA[<blockquote>
<p>论文下载地址</p>
</blockquote>
<a id="more"></a>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
  </entry>
  <entry>
    <title>Incorporating Literals into Knowledge Graph Embeddings阅读笔记</title>
    <url>/post/Incorporating_Literals_into_Knowledge_Graph_Embeddings/</url>
    <content><![CDATA[<blockquote>
<p>读完了前两章，简单的看了一下作者提出的模型，感觉并没有太大价值，就是给实体输入多加入了一个literal的信息（加入方法可以采用线性、非线性或者神经网络）。</p>
<p>读论文前需要先熟悉DistMult、ComlLEx和ConvE模型，此论文方法是添加在这些方法上的。</p>
</blockquote>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>KG</category>
      </categories>
      <tags>
        <tag>KGE</tag>
        <tag>link prediction</tag>
      </tags>
  </entry>
  <entry>
    <title>Knowledge Graph Embedding via Dynamic Mapping Matrix阅读笔记</title>
    <url>/post/Knowledge_Graph_Embedding_via_Dynamic_Mapping_Matrix/</url>
    <content><![CDATA[<blockquote>
<p>论文下载地址</p>
</blockquote>
<a id="more"></a>
<h1 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h1><ol>
<li>对于特定的关系$r$，所有的实体都共享相同的映射矩阵$M_r$。然而，由关系链接的实体总是包含各种类型和属性。</li>
<li>投影是实体和关系之间的交互过程，映射矩阵只能由关系决定是不合理的。</li>
<li>矩阵向量乘法使其具有大量计算，并且当关系数大时，它还具有比TransE和TransH更多的参数。 由于复杂性，TransR / CTransR难以应用于大规模知识图</li>
</ol>
<h1 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h1><ol>
<li>作者构建了一个新颖的模型TransD，通过同时考虑实体和关系的多样性，为每一个实体-关系构建动态映射矩阵。它为实体表示映射到关系向量空间提供灵活的样式。</li>
<li>与TransR / CTransR相比，TransD具有更少的参数并且没有矩阵向量乘法</li>
<li>在实验中，作者的方法优于之前的模型。</li>
</ol>
<h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><p>模型在TransD中，每个命名的符号对象（实体和关系）由两个向量表示。 第一个捕获实体（关系）的含义，另一个用于构造映射矩阵。</p>
<p><img src="http://image.nysdy.com/20190826156680488849411.png" alt="20190826156680488849411.png"></p>
<p>对于一个三元组$(h,r,t)$,向量一共有$h, h_p, r, r_p, t, t_p$,其中带$p$的为映射向量，则有</p>
<script type="math/tex; mode=display">
\begin{aligned} \mathbf{M}_{r h} &=\mathbf{r}_{p} \mathbf{h}_{p}^{\top}+\mathbf{I}^{m \times n} \\ \mathbf{M}_{r t} &=\mathbf{r}_{p} \mathbf{t}_{p}^{\top}+\mathbf{I}^{m \times n} \end{aligned}</script><p>故</p>
<script type="math/tex; mode=display">
\mathbf{h}_{\perp}=\mathbf{M}_{r h} \mathbf{h}, \quad \mathbf{t}_{\perp}=\mathbf{M}_{r t} \mathbf{t}</script><p>可以综合为：</p>
<script type="math/tex; mode=display">
\begin{aligned} \mathbf{h}_{\perp} &=\mathbf{M}_{r h} \mathbf{h}=\mathbf{h}+\mathbf{h}_{p}^{\top} \mathbf{h} \mathbf{r}_{p} \\ \mathbf{t}_{\perp} &=\mathbf{M}_{r t} \mathbf{t}=\mathbf{t}+\mathbf{t}_{p}^{\top} \mathbf{t} \mathbf{r}_{p} \end{aligned}</script><p>这样就没有矩阵和向量间的乘法运算，变成向量间运算，提升计算速度。</p>
<h1 id="Experiments-and-Results-Analysis"><a href="#Experiments-and-Results-Analysis" class="headerlink" title="Experiments and Results Analysis"></a>Experiments and Results Analysis</h1><p>常规实验：triplets classification and link prediction不再赘述。</p>
<p>作者在实验过程中关注了一些具有更低accuracy的关系。</p>
<p><img src="http://image.nysdy.com/20190826156680558013943.png" alt="20190826156680558013943.png"></p>
<p>分析：</p>
<pre><code>1. 对于$similar_to$关系主要因为训练数据不充足，只占了1.5%。
 2. 对于最右侧的图说明了bern方法的效果要好于unif
</code></pre><h3 id="Properties-of-Projection-Vectors"><a href="#Properties-of-Projection-Vectors" class="headerlink" title="Properties of Projection Vectors"></a>Properties of Projection Vectors</h3><p>作者还做了case study，通过不同类型实体和关系的投影向量的相似性表明了作者方法的合理性</p>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>KGE</tag>
        <tag>TransD</tag>
      </tags>
  </entry>
  <entry>
    <title>Knowledge Graph Embedding by Translating on Hyperplanes阅读笔记</title>
    <url>/post/Knowledge%20Graph%20Embedding%20by%20Translating%20on%20Hyperplanes/</url>
    <content><![CDATA[<blockquote>
<p>作为trans系列经典文献，必读。文章主要精华在于这种超平面想法的由来解决了同一实体的多关系问题。</p>
<p>Authors proposed TransH which models a relation as a hyperplane together with a translation operation on it. It solves the problem of multi-relation and makes a good trade-off between model capacity and efficiency.</p>
</blockquote>
<a id="more"></a>
<h1 id="推测transH的想法来源"><a href="#推测transH的想法来源" class="headerlink" title="推测transH的想法来源"></a>推测transH的想法来源</h1><blockquote>
<p>既然实际是表达同一关系不同实体最后通过TransE后会趋于一致，那么我直接通过一个中介来进行映射将同一表示映射成不同向量表示，那么这些向量表示就可以代表不同的实体，就达到了不同实体拥有不同表示的目的。因为关系是不变的所以想到了将关系作为映射平面，让实体向量向其中映射。</p>
</blockquote>
<h1 id="research-objective"><a href="#research-objective" class="headerlink" title="research objective"></a>research objective</h1><ul>
<li>solves the problem of multi-relation </li>
<li>makes a good trade-off between model capacity and efficiency</li>
</ul>
<h1 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h1><ul>
<li>TransE can’t deal with reflexive, one-to-many, many-to-many and many -to-one relations</li>
<li>some complex model sacrifice efficiency in the process(although can deal with transE’s problem)</li>
</ul>
<h1 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h1><ul>
<li>proposing a method named <em>translation on hyperplanes</em>(TransH)<ul>
<li>interpreting a relation as a translating operation on a hyperplane</li>
</ul>
</li>
<li>proposing a simple trick to reduce the chance of false negative labeling</li>
</ul>
<h1 id="Embedding-by-Translating-on-Hyperplanes"><a href="#Embedding-by-Translating-on-Hyperplanes" class="headerlink" title="Embedding by Translating on Hyperplanes"></a>Embedding by Translating on Hyperplanes</h1><h2 id="Relations’-Mapping-Properties-in-Embedding"><a href="#Relations’-Mapping-Properties-in-Embedding" class="headerlink" title="Relations’ Mapping Properties in Embedding"></a>Relations’ Mapping Properties in Embedding</h2><p>transE</p>
<ul>
<li>the representation of an  entity is the same when involved in any relations, ignoring <strong>distributed representations of entities when invovled in different relaions</strong></li>
</ul>
<h2 id="Translating-on-Hyperplanes-TransH"><a href="#Translating-on-Hyperplanes-TransH" class="headerlink" title="Translating on Hyperplanes (TransH)"></a>Translating on Hyperplanes (TransH)</h2><p><strong>同一个实体在不同关系中的意义不同</strong>，同时<strong>不同实体，在同一关系中的意义，也可以相同</strong>。</p>
<blockquote>
<p>将每个关系定义在一个独特的平面呢，在该平面内有符合该关系的transE的表示（h,r,t)，多加入的代表该平面的法向量完成了将不同实体向平面内和h，t转化的任务，使得同一关系的不同实体拥有不同的表示，但是在关系平面内的投影相同；同一实体可以在不同的关系平面内拥有不同的含义（平面内的投影）</p>
</blockquote>
<p><img src="http://image.nysdy.com/20190601155935483248827.jpg" alt="20190601155935483248827.jpg"></p>
<p>如图所示，对于正确的三元组来说$(h, r, t) \in \Delta$，所需满足的关系如图所示。那么对于一个实体$h’’$如果满足$\left(h^{\prime \prime}, r, t\right) \in \Delta    $，在transE中是需要$h’’=h$，而在transH中则将约束放宽到$h,h’’$在$W_r$上的投影相同就可以了，也可以实现将$h,h’’$区分开并且具有不同的表示。</p>
<h4 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h4><p>scoring function：</p>
<script type="math/tex; mode=display">
d(h+r, t)=f_{r}(h, t)=\left\|h_{\perp}+d_{r}-t_{\perp}\right\|_{2}^{2}</script><p>As the hyperplane $W_r$, the $w_r$ is the normal vector of it, and $\left|w_{r}\right|_{2}^{2}=1$, so the projection $h$ in $w_r$ is:</p>
<script type="math/tex; mode=display">
h_{w_{r}}=w_r^{T} h w_r</script><p>其中，$w_r^{T} h=|w_r||h| \cos \theta$可以表示$h$在$w_r$上的投影的长度和$w_r$长度的乘积，因为$\left|w_{r}\right|_{2}^{2}=1$,所以可以代表投影的长度，再乘上单位向量即可表示投影向量。所以：</p>
<script type="math/tex; mode=display">
\mathbf{h}_{\perp}=\mathbf{h}-\mathbf{w}_{r}^{\top} \mathbf{h w}_{r}, \quad \mathbf{t}_{\perp}=\mathbf{t}-\mathbf{w}_{r}^{\top} \mathbf{t} \mathbf{w}_{r}</script><p>如图所示：<img src="http://image.nysdy.com/2019060115593616504994.jpg" alt="2019060115593616504994.jpg"></p>
<p>the score function is:</p>
<script type="math/tex; mode=display">
f_{r}(\mathbf{h}, \mathbf{t})=\left\|\left(\mathbf{h}-\mathbf{w}_{r}^{\top} \mathbf{h w}_{r}\right)+\mathbf{d}_{r}-\left(\mathbf{t}-\mathbf{w}_{r}^{\top} \mathbf{t} \mathbf{w}_{r}\right)\right\|_{2}^{2}</script><h4 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h4><p>loss function consists of margin-based ranking loss and some constraints:</p>
<script type="math/tex; mode=display">
\begin{aligned} \mathcal{L} &=\sum_{(h, r, t) \in \Delta\left(h^{\prime}, r^{\prime}, t^{\prime}\right) \in \Delta_{(h, r, t)}}\left[f_{r}(\mathbf{h}, \mathbf{t})+\gamma-f_{r^{\prime}}\left(\mathbf{h}^{\prime}, \mathbf{t}^{\prime}\right)\right]_{+} \\ &+C\left\{\sum_{e \in E}\left[\|\mathbf{e}\|_{2}^{2}-1\right]_{+}+\sum_{r \in R}\left[\frac{\left(\mathbf{w}_{r}^{\top} \mathbf{d}_{r}\right)^{2}}{\left\|\mathbf{d}_{r}\right\|_{2}^{2}}-\epsilon^{2}\right]_{+}\right\}, \text { (4) } \end{aligned}</script><p>the constraints:</p>
<script type="math/tex; mode=display">
\forall e \in E,\|\mathrm{e}\|_{2} \leq 1, // \text { scale }\\
\forall r \in R,\left|\mathbf{w}_{r}^{\top} \mathbf{d}_{r}\right| 
/\left\|\mathbf{d}_{r}\right\|_{2} \leq \epsilon, / / \text { orthogonal }\\
\forall r \in R,\left\|\mathbf{w}_{r}\right\|_{2}=1, / / \text { unit normal vector }</script><ul>
<li><strong>the second grantees the translation vectot $d_r$ is in the hyperplane</strong></li>
<li>they project each $w_r$ to unit $l_2$-ball before visiting each mini-batch</li>
</ul>
<blockquote>
<p>既然transH可以完成将同一实体映射到不同的关系平面来获得不同的含义，那么我觉得</p>
<ul>
<li>是不是不同代表同一含义的投影表示应该相同或者相似</li>
<li>这样是不是可以解决同一个实体的多义性问题。</li>
</ul>
</blockquote>
<h2 id="Reducing-Ralse-Negative-Labels"><a href="#Reducing-Ralse-Negative-Labels" class="headerlink" title="Reducing Ralse Negative Labels"></a>Reducing Ralse Negative Labels</h2><p>Authors set different probabilities for replacing the head or tail entity depending on the mapping property of the relation (one-to-many, many-to-one, many-to-many)</p>
<ul>
<li><p>give more chance to replacing the head entity if the relation is one-to-many</p>
<ul>
<li>分别统计每个头实体对应尾实体的数量（反之亦然），按占比进行生成负样例</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>通过这样的方式，例如one-many关系，替换头实体显然更不容易得到正样例（因为只有一种头实体是对的，然而替换尾实体因为对于头实体对应该关系的尾实体更多，说不定就有其他不在此many中的尾实体符合这个关系。</li>
<li>相比之下我认为在《Bootstrapping-Entity-Alignment-with-Knowledge-Graph-Embedding》采用的均匀截断负采样效果会更好一些</li>
</ul>
</blockquote>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>the detail can be seen in the paper</p>
<h2 id="Link-prediction"><a href="#Link-prediction" class="headerlink" title="Link prediction"></a>Link prediction</h2><h3 id="outperform-TransE-in-one-to-one"><a href="#outperform-TransE-in-one-to-one" class="headerlink" title="outperform TransE in one-to-one"></a>outperform TransE in one-to-one</h3><p>Authors explain:</p>
<ul>
<li>entities are connected with relations so that better embeddings of some parts lead to better results on the whole.</li>
</ul>
<blockquote>
<p>我是觉得有些牵强，不过要是硬理解也是可以，毕竟通过投影相当于把实体和关系进行了一个联系，可能这个增强了效果。</p>
</blockquote>
<h2 id="Triplets-Classification"><a href="#Triplets-Classification" class="headerlink" title="Triplets Classification"></a>Triplets Classification</h2><p>This means FB13 is a very dense subgraph where strong correlations exist between entities</p>
<h2 id="Relational-Fact-Extraction-from-Text"><a href="#Relational-Fact-Extraction-from-Text" class="headerlink" title="Relational Fact Extraction from Text"></a>Relational Fact Extraction from Text</h2><ul>
<li>Actually, knowledge graph embedding is able to score a candidate fact, without observing any evidence from ex- ternal text corpus</li>
</ul>
<blockquote>
<p>可以看到从14年开始就有利用知识图谱来从文本抽取关系，最近这个应用好像又有起色，这个也可作为自己实验的一部分。</p>
</blockquote>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li><a href="https://blog.csdn.net/MonkeyDSummer/article/details/85273843" target="_blank" rel="noopener">https://blog.csdn.net/MonkeyDSummer/article/details/85273843</a></li>
</ul>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>KG</category>
      </categories>
      <tags>
        <tag>KGE</tag>
        <tag>transH</tag>
      </tags>
  </entry>
  <entry>
    <title>Convolutional 2D Knowledge Graph Embeddings 阅读笔记</title>
    <url>/post/Convolutional%202D%20Knowledge%20Graph%20Embeddings/</url>
    <content><![CDATA[<h1 id="0-导读"><a href="#0-导读" class="headerlink" title="0. 导读"></a>0. 导读</h1><h2 id="0-1-文章是关于什么的？（what？）"><a href="#0-1-文章是关于什么的？（what？）" class="headerlink" title="0.1 文章是关于什么的？（what？）"></a>0.1 文章是关于什么的？（what？）</h2><p>用知识表示来做链接预测，其中使用了2D的卷积操作</p>
<h2 id="0-2-要解决什么问题？（why？-challenge）"><a href="#0-2-要解决什么问题？（why？-challenge）" class="headerlink" title="0.2 要解决什么问题？（why？|challenge）"></a>0.2 要解决什么问题？（why？|challenge）</h2><ul>
<li>计算速度</li>
<li>对于复杂网络的适应性</li>
</ul>
<h2 id="0-3-用什么方法解决？（how？）"><a href="#0-3-用什么方法解决？（how？）" class="headerlink" title="0.3 用什么方法解决？（how？）"></a>0.3 用什么方法解决？（how？）</h2><ul>
<li>用了2D的卷积操作</li>
</ul>
<h2 id="0-4文章有什么创新？"><a href="#0-4文章有什么创新？" class="headerlink" title="0.4文章有什么创新？"></a>0.4文章有什么创新？</h2><ul>
<li>首次引入一个简单的2D卷积操作来进行链接预测任务；</li>
<li>展示了一个1-N分数过程来加速训练；</li>
<li>提出的模型可以用于复杂图</li>
</ul>
<h2 id="0-5-效果如何？"><a href="#0-5-效果如何？" class="headerlink" title="0.5 效果如何？"></a>0.5 效果如何？</h2><h2 id="0-6-还存在什么问题？"><a href="#0-6-还存在什么问题？" class="headerlink" title="0.6 还存在什么问题？"></a>0.6 还存在什么问题？</h2><h1 id="1-背景知识"><a href="#1-背景知识" class="headerlink" title="1 背景知识"></a>1 背景知识</h1><p>通过使用2D而不是1D卷积，可以通过嵌入之间的其他交互点来提高模型的表达能力。作者考虑一个两行的1D 嵌入：</p>
<script type="math/tex; mode=display">
\left(\left[\begin{array}{lll}a & a & a\end{array}\right] ;\left[\begin{array}{lll}b & b & b\end{array}\right]\right)=\left[\begin{array}{llllll}a & a & a & b & b & b\end{array}\right]</script><ul>
<li>过滤器大小为$k = 3$的填充一维卷积将能够在连接点周围模拟这两个嵌入之间的相互作用（相互作用的数量与$k$成正比）。</li>
</ul>
<p>如果作者将尺寸为$m×n$的两行2D嵌入进行串联（即堆叠），其中$m = 2$和$n = 3$，则将获得以下结果：</p>
<script type="math/tex; mode=display">
\left(\left[\begin{array}{lll}a & a & a \\ a & a & a\end{array}\right] ;\left[\begin{array}{lll}b & b & b \\ b & b & b\end{array}\right]\right)=\left[\begin{array}{lll}a & a & a \\ a & a & a \\ b & b & b \\ b & b & b\end{array}\right]</script><ul>
<li>过滤器大小为3×3的填充2D卷积将能够对整个串联线之间的相互作用进行建模（相互作用的数量与n和k成正比）。</li>
</ul>
<p>作者可以将此原理扩展为交替模式，例如：</p>
<script type="math/tex; mode=display">
\left[\begin{array}{lll}a & a & a \\ b & b & b \\ a & a & a \\ b & b & b\end{array}\right]</script><ul>
<li>在这种情况下，二维卷积运算能够建模a和b之间的更多交互（交互数量与m，n和k成比例）。 因此，与1D卷积相比，2D卷积能够提取两个嵌入之间的更多特征交互。</li>
</ul>
<h1 id="2-模型ConvE"><a href="#2-模型ConvE" class="headerlink" title="2 模型ConvE"></a>2 模型ConvE</h1><h2 id="模型结构图"><a href="#模型结构图" class="headerlink" title="模型结构图"></a>模型结构图</h2><p><img src="http://image.nysdy.com/20200923134436.png" alt></p>
<p>作者模型的主要特征是分数由2D形状嵌入上的卷积定义，数学公式如下所示：</p>
<script type="math/tex; mode=display">
\psi_{r}\left(\mathbf{e}_{s}, \mathbf{e}_{o}\right)=f\left(\operatorname{vec}\left(f\left(\left[\overline{\mathbf{e}}_{s} ; \overline{\mathbf{r}}_{r}\right] * \omega\right)\right) \mathbf{W}\right) \mathbf{e}_{o}</script><ul>
<li>其中$\mathbf{r}_{r} \in \mathbb{R}^{k}$是一个依赖关系$r$的关系参数，$\overline{\mathbf{e}_{s}}，\overline{\mathbf{r}_{r}}$分别代表$\mathrm{e}_{s}$ and $\mathbf{r}_{r}$的2D重塑：（如果$\mathbf{e}_{s}, \mathbf{r}_{r} \in \mathbb{R}^{k}$，则$\overline{\mathbf{e}_{s}}, \overline{\mathbf{r}_{r}} \in \mathbb{R}^{k_{w} \times k_{h}}$，其中$k=k_{w} k_{h}$</li>
</ul>
<p>整个ConvE的模型流程如下：</p>
<ol>
<li>在前馈过程中，模型对两个嵌入矩阵执行行矢量查找操作，其中一个用于实体，表示为$\mathbf{E}|\mathcal{E}| \times k$，另一用于关系式，表示为$\mathbf{R}^{|\mathcal{R}| \times k^{\prime}}$。其中$k$和$k^{\prime}$是实体和关系嵌入维度，$|\mathcal{E}|$ and $|\mathcal{R}|$代表实体和关系的数量。</li>
<li>然后，模型将$\overline{\mathbf{e}_{s}}$ and $\overline{\mathbf{r}_{r}}$进行串联，并将其用作带有滤波器$\omega$的2D卷积层的输入。这样一层返回一个特征图张量$\mathcal{T} \in \mathbb{R}^{c \times m \times n}$，其中$c$是维数为$m$和$n$的特征图的数量。</li>
<li>再将张量$\mathcal{T}$整形为一个向量<script type="math/tex">\operatorname{vec}(\mathcal{T}) \in \mathbb{R}^{c m n}</script></li>
<li>然后使用矩阵$\mathbf{W} \in \mathbb{R}^{c m n \times k}$设置的线性变化将其投影到$k$维空间中，并通过内积与嵌入$e_o$的对象匹配。</li>
</ol>
<ul>
<li>卷积滤波器和矩阵$W$的参数与实体$s$和$o$以及关系$r$的参数无关。</li>
</ul>
<p>作者用一个非线性函数sigmoid来计算得分：</p>
<script type="math/tex; mode=display">
p = \sigma\left(\psi_{r}\left(\mathbf{e}_{s}, \mathbf{e}_{o}\right)\right)</script><p>最终的损失函数为：</p>
<script type="math/tex; mode=display">
\mathcal{L}(p, t)=-\frac{1}{N} \sum_{i}\left(t_{i} \cdot \log \left(p_{i}\right)+\left(1-t_{i}\right) \cdot \log \left(1-p_{i}\right)\right)</script><ul>
<li>其中对于1-1的分数$t$是维度为$\mathbb{R}^{1 x 1}$的标签向量，对于1-N分数来说是$\mathbb{R}^{1 x N}$向量。当关系存在是$t=1$否则，$t=0$</li>
</ul>
<h2 id="模型的空间复杂度"><a href="#模型的空间复杂度" class="headerlink" title="模型的空间复杂度"></a>模型的空间复杂度</h2><p><img src="http://image.nysdy.com/20200923221323.png" alt></p>
<h1 id="3-实验"><a href="#3-实验" class="headerlink" title="3 实验"></a>3 实验</h1><h2 id="3-1-数据集"><a href="#3-1-数据集" class="headerlink" title="3.1 数据集"></a>3.1 数据集</h2><p>WN18， FB15K，YAGO3-10，Countires</p>
<p>作者证明了FB15k和WN18存在test leakage through inverse relations的问题：简单地通过反转训练集中的三元组就可以获得大量的测试三元组。</p>
<p>所以作者使用了FB15K-237和WN18RR，并且建议以后不要使用FB15K和WN18.</p>
<h2 id="3-2-实验结果"><a href="#3-2-实验结果" class="headerlink" title="3.2 实验结果"></a>3.2 实验结果</h2><h3 id="3-2-1-参数效率"><a href="#3-2-1-参数效率" class="headerlink" title="3.2.1 参数效率"></a>3.2.1 参数效率</h3><p><img src="http://image.nysdy.com/20200923221659.png" alt></p>
<ul>
<li>对比参数规模和实验结果，发现ConvE模型可以在使用较少参数的时候达到较好的实验效果。</li>
</ul>
<h3 id="3-2-2-链接预测结果"><a href="#3-2-2-链接预测结果" class="headerlink" title="3.2.2 链接预测结果"></a>3.2.2 链接预测结果</h3><p><img src="http://image.nysdy.com/20200923223630.png" alt></p>
<ul>
<li>链接预测任务上各种模型的表现，ConvE模型在各个性能指标上都取得了很好的效果。</li>
</ul>
<p><img src="http://image.nysdy.com/20200923224353.png" alt></p>
<p><img src="http://image.nysdy.com/20200923224303.png" alt></p>
<ul>
<li>Table 4是将逆关系去掉之后的各个模型在数据集上的表现，逆模型法处理YAGO310和FB15k-237的逆关系</li>
<li>Toutanova和Chen（2015）用来推导FB15k-237的过程并未消除某些对称关系，例如“类似于”。 这些关系的存在解释了作者在WN18RR上的逆模型的良好评分，该评分是使用相同的过程得出的。</li>
</ul>
<h3 id="3-2-3-自反模型（inverse-Model）"><a href="#3-2-3-自反模型（inverse-Model）" class="headerlink" title="3.2.3 自反模型（inverse Model）"></a>3.2.3 自反模型（inverse Model）</h3><p>有统计数据指出在数据集WN18 和FB15K中，训练集中有94% 和81% 中三元组的逆关系在测试集中出现，可能会导致在数据集上表现的好，却只学习了某关系是其他的逆关系，而不是真正的知识图谱。</p>
<p>为了衡量此问题的严重性，作者构建了一个简单的基于规则的模型，该模型仅对逆关系建模。作者将此模型称为逆模型。该模型自动从训练集中提取逆关系：给定两个关系对$r_{1}, r_{2} \in \mathcal{R}$，作者检查$\left(s, r_{1}, o\right)$是否隐含$\left(o, r_{2}, s\right)$，反之亦然。</p>
<p>作者假设逆关系在训练，验证和测试集之间随机分布，因此，作者期望逆关系的数量与训练集的大小（与总数据集大小相比）成比例。因此，如果$\left(\mathcal{S}, \boldsymbol{r}_{1}, \boldsymbol{O}\right)$的存在与$\left(o, r_{2}, s\right)$的存在以至少$0.99-\left(f_{v}+f_{t}\right)$的频率同时出现，则作者检测到逆关系。 $f_{v},f_{t}$是验证和测试集相对于数据集总大小的分数。假定符合此标准的关系是彼此相反的。</p>
<p>在测试时，作者检查测试三元组是否在测试集之外具有逆匹配项：如果找到k个匹配项，则对这些匹配项的前k个排名进行抽样；如果找不到匹配项，则为测试三元组选择一个随机排名。</p>
<h3 id="3-2-3-Indegree和PageRank分析"><a href="#3-2-3-Indegree和PageRank分析" class="headerlink" title="3.2.3 Indegree和PageRank分析"></a>3.2.3 Indegree和PageRank分析</h3><p><img src="http://image.nysdy.com/20200923222702.png" alt></p>
<p>对于indegree，作者通过构建不同indegree的数据集发现：较深的模型（例如ConvE）具有对较复杂的图形进行建模的优势（例如FB15k和FB15k-237），而较浅的模型（例如DistMult）具有对较复杂的图形进行建模的优势（例如WN18 WN18RR）。</p>
<p>PageRank，它是节点中心性的度量。 PageRank也可以看作是节点的递归度的度量：节点的PageRank值与该节点的度数，其邻居度数，其邻居-邻居度数等相对于网络中所有其他节点成比例。</p>
<p>与标准链接预测变量DistMult相比，我们模型的性能提高可以部分解释，这是因为我们能够以较高的度数对节点进行高精度建模，这可能与其深度有关。</p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul>
<li></li>
</ul>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>Link Prediction</tag>
        <tag>CNN</tag>
        <tag>Knowledge Representation</tag>
      </tags>
  </entry>
  <entry>
    <title>GloVe: Global Vectors for Word Representation阅读笔记</title>
    <url>/post/GloVe:Global%20Vectors%20for%20Word%20Representation/</url>
    <content><![CDATA[<blockquote>
<p>论文<a href="https://www.aclweb.org/anthology/D14-1162" target="_blank" rel="noopener">下载地址</a>，GloVe是一个新的全球对数双线性回归模型，属于经典的词向量表示方法之一。</p>
</blockquote>
<a id="more"></a>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>evaluate the intrinsic quality</p>
<ul>
<li>Most word vector methods rely on the distance or angle between pairs of word vectors </li>
<li>Mikolov et al. (2013c) introduced word analogies that examines word vector’s various dimensions of difference.</li>
</ul>
<p>two main model families for learning vectors:</p>
<ul>
<li>global matrix factorization methods</li>
<li>local context window methods</li>
</ul>
<p>Authors propose a specific weighted least squares model that trains on globla word-word co-occurrence counts and thus makes efficient use of statistics.</p>
<h1 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h1><h2 id="Matix-Facroization-Methods"><a href="#Matix-Facroization-Methods" class="headerlink" title="Matix Facroization Methods"></a>Matix Facroization Methods</h2><p>These methods utilize low-rank approximations to decompose large matrices that capture statistical information about a corpus.</p>
<h3 id="shortcoming"><a href="#shortcoming" class="headerlink" title="shortcoming"></a>shortcoming</h3><p>the most frequent words contribute a dispropoertionate amount to the similarity measure.</p>
<h2 id="Shallow-Window-Based-Methods"><a href="#Shallow-Window-Based-Methods" class="headerlink" title="Shallow Window-Based Methods"></a>Shallow Window-Based Methods</h2><p>Another approach is to learn word representations that aid in making predictins within local context windows.</p>
<h3 id="shortcoming-1"><a href="#shortcoming-1" class="headerlink" title="shortcoming"></a>shortcoming</h3><p>do not operate directly on the co-occurrence statistics of the corpus and fails to take advantage of the vast amount of repetition in the data.</p>
<h1 id="The-GloVe-Model"><a href="#The-GloVe-Model" class="headerlink" title="The GloVe Model"></a>The GloVe Model</h1><h3 id="GloVe-Global-Vectors"><a href="#GloVe-Global-Vectors" class="headerlink" title="GloVe: Global Vectors"></a>GloVe: Global Vectors</h3><p>the global corpus statistics are captured directly by the model</p>
<h3 id="the-question-about-the-model-using-the-statistics-of-word-occurrences-in-a-corpus"><a href="#the-question-about-the-model-using-the-statistics-of-word-occurrences-in-a-corpus" class="headerlink" title="the question about the model using the statistics of word occurrences in a corpus"></a>the question about the model using the statistics of word occurrences in a corpus</h3><ul>
<li>how meaning is generated from these statistics</li>
<li>how the resulting word vectors might represent that meaning</li>
</ul>
<h2 id="some-notation"><a href="#some-notation" class="headerlink" title="some notation"></a>some notation</h2><p>$X_{ij}$ : the number of times word j occurs in the context of word i</p>
<p>$X_i = \sum_{k} X_{i k}$ : the number of times any word appears in the context of word i</p>
<p>$P_{i j}=P(j | i)=X_{i j} / X_{i}$: the probability that word j appear in the context of word i</p>
<p><img src="http://image.nysdy.com/20190515155788329289127.jpg" alt="20190515155788329289127.jpg"></p>
<p>above that, werd vector learning should be with ratios of co-occurrence probabilities:</p>
<p><img src="http://image.nysdy.com/20190515155788352871603.jpg" alt="20190515155788352871603.jpg"></p>
<p>$w \in \mathbb{R}^{d}$are word vectors and $\tilde{w} \in \mathbb{R}^{d}$are separate context word vectors</p>
<p>For F, we should select a unique choice by enforcing a few desiderata.</p>
<ul>
<li><p>encoding the information present the ratio $P_{i k} / P_{j k}$ in the word vector space. </p>
<p>Since vector spaces are inherently linear structures</p>
<p><img src="http://image.nysdy.com/20190515155788379647745.jpg" alt="20190515155788379647745.jpg"></p>
</li>
<li><p>put F to be a compicated function parameterized, and avoiding bofuscating the linear structure<img src="http://image.nysdy.com/20190515155788397136355.jpg" alt="20190515155788397136355.jpg"></p>
</li>
<li><p>the word-word co-occurrence matrices, we can exchange a word and a context word(because a word can also be a context word)</p>
<ol>
<li><p>F should be a homomorphism<img src="http://image.nysdy.com/2019051515578842869345.jpg" alt="2019051515578842869345.jpg"></p>
<p>by Eqn.(3)<img src="http://image.nysdy.com/20190515155788435674239.png" alt="20190515155788435674239.png"></p>
<p>F = exp or <img src="http://image.nysdy.com/20190515155788448759173.jpg" alt="20190515155788448759173.jpg"></p>
</li>
<li><p>the Eqn(6) would have the exchange symmetry if not $\log \left(X_{i}\right)$ and $\log \left(X_{i}\right)$ is independent of k, so it can be absorbed into a bias $b_i$<img src="http://image.nysdy.com/20190515155788566687599.jpg" alt="20190515155788566687599.jpg"></p>
</li>
<li><p>for avoiding diverge, $\log \left(X_{i k}\right) \rightarrow \log \left(1+X_{i k}\right)$</p>
</li>
<li><p>a new weighted least squares regression model to address the problem that LSA wirhts all co-occuttences equally.</p>
<p>cost function:<img src="http://image.nysdy.com/20190515155788560186237.jpg" alt="20190515155788560186237.jpg"></p>
</li>
<li><p><img src="http://image.nysdy.com/20190515155788571777804.jpg" alt="20190515155788571777804.jpg"></p>
</li>
</ol>
</li>
</ul>
<h2 id="Relationship-to-Other-Models"><a href="#Relationship-to-Other-Models" class="headerlink" title="Relationship to Other Models"></a>Relationship to Other Models</h2><p>In this subsection authors show how these models are related to their proposed model.</p>
<h4 id="the-defect-of-cross-entropy"><a href="#the-defect-of-cross-entropy" class="headerlink" title="the defect of cross entropy"></a>the defect of cross entropy</h4><ul>
<li>it has the unfortunate property that distributions with long tails are often modeled poorly with too much wieght given to the unlikely events.</li>
</ul>
<h2 id="Complexity-of-the-model"><a href="#Complexity-of-the-model" class="headerlink" title="Complexity of the model"></a>Complexity of the model</h2><p>the computational complexity of the model depends on the number of nonzero elects in the matrix $X$</p>
<h4 id="some-assumptions-about-the-distribution-of-word-co-occurrences"><a href="#some-assumptions-about-the-distribution-of-word-co-occurrences" class="headerlink" title="some assumptions about the distribution of word co-occurrences"></a>some assumptions about the distribution of word co-occurrences</h4><ul>
<li><p>the number of co-occurrences of word $i$ with word $j$, $X_{ij}$, can be modeled as a power-law function of the frequency rank of that word pair, $r_{ij}$:</p>
<p>$X_{i j}=\frac{k}{\left(r_{i j}\right)^{\alpha}}$</p>
</li>
</ul>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Evaluation-methods"><a href="#Evaluation-methods" class="headerlink" title="Evaluation methods"></a>Evaluation methods</h2><p>authors conduct experiments on the word analogy taks of Mikolov et al. (2013a)</p>
<h3 id="Word-analogies"><a href="#Word-analogies" class="headerlink" title="Word analogies"></a>Word analogies</h3><p>The word analogy task consists of questions like, “a is to b as c is to ?”</p>
<h3 id="Word-similarity"><a href="#Word-similarity" class="headerlink" title="Word similarity"></a>Word similarity</h3><p><img src="http://image.nysdy.com/20190520155833277478435.jpg" alt="20190520155833277478435.jpg"></p>
<h3 id="Named-entity-recognition"><a href="#Named-entity-recognition" class="headerlink" title="Named entity recognition"></a>Named entity recognition</h3><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>Table 2 shows the CloVe model performs significantly better than the other baslines, often with smaller vector sizes and smaller corpora.</p>
<p><img src="http://image.nysdy.com/20190520155833353468570.jpg" alt="20190520155833353468570.jpg"></p>
<p>Table 3 shows results on five different word similarity datasets.</p>
<p>Table 4 shows results on the NER task with the CRF-based model.</p>
<p><img src="http://image.nysdy.com/20190520155833377540169.jpg" alt="20190520155833377540169.jpg"></p>
<h2 id="Model-Analysis-Vector-Length-and-Context-Size"><a href="#Model-Analysis-Vector-Length-and-Context-Size" class="headerlink" title="Model Analysis: Vector Length and Context Size"></a>Model Analysis: Vector Length and Context Size</h2><p><img src="http://image.nysdy.com/2019052015583339399087.jpg" alt="2019052015583339399087.jpg"></p>
<h3 id="Model-Analysis-Corpus-Size"><a href="#Model-Analysis-Corpus-Size" class="headerlink" title="Model Analysis: Corpus Size"></a>Model Analysis: Corpus Size</h3><p><img src="http://image.nysdy.com/20190520155833403240640.jpg" alt="20190520155833403240640.jpg"></p>
<ul>
<li>On the syntactic subtask, larger corpora typically produce better statistics so that there is a monotonic increase in performance as the cor- pus size increases.</li>
<li>But the same trend is not true for the semantic subtask, which is probably because of analogy dataset</li>
</ul>
<h2 id="Model-Analysis-Run-time"><a href="#Model-Analysis-Run-time" class="headerlink" title="Model Analysis: Run-time"></a>Model Analysis: Run-time</h2><p><img src="http://image.nysdy.com/20190520155833432462881.jpg" alt="20190520155833432462881.jpg"></p>
<h2 id="Model-Analysis-Comparison-with-word2vec"><a href="#Model-Analysis-Comparison-with-word2vec" class="headerlink" title="Model Analysis: Comparison with word2vec"></a>Model Analysis: Comparison with word2vec</h2><p>For the same corpus, vocabulary, window size, and training time, GloVe consistently outperforms word2vec</p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul>
<li><a href="https://blog.csdn.net/coderTC/article/details/73864097" target="_blank" rel="noopener">https://blog.csdn.net/coderTC/article/details/73864097</a></li>
</ul>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>word vector</tag>
        <tag>GloVe</tag>
      </tags>
  </entry>
  <entry>
    <title>DeepWalk: Online Learning of Social Representations 阅读笔记</title>
    <url>/post/DeepWalk%20Online%20Learning%20of%20Social%20Representations/</url>
    <content><![CDATA[<h1 id="0-导读"><a href="#0-导读" class="headerlink" title="0. 导读"></a>0. 导读</h1><h2 id="0-1-文章是关于什么的？（what？）"><a href="#0-1-文章是关于什么的？（what？）" class="headerlink" title="0.1 文章是关于什么的？（what？）"></a>0.1 文章是关于什么的？（what？）</h2><h2 id="0-2-要解决什么问题？（why？-challenge）"><a href="#0-2-要解决什么问题？（why？-challenge）" class="headerlink" title="0.2 要解决什么问题？（why？|challenge）"></a>0.2 要解决什么问题？（why？|challenge）</h2><ul>
<li><h2 id="0-3-用什么方法解决？（how？）"><a href="#0-3-用什么方法解决？（how？）" class="headerlink" title="0.3 用什么方法解决？（how？）"></a>0.3 用什么方法解决？（how？）</h2></li>
<li></li>
</ul>
<h2 id="0-4文章有什么创新？"><a href="#0-4文章有什么创新？" class="headerlink" title="0.4文章有什么创新？"></a>0.4文章有什么创新？</h2><ul>
<li><h2 id="0-5-效果如何？"><a href="#0-5-效果如何？" class="headerlink" title="0.5 效果如何？"></a>0.5 效果如何？</h2></li>
</ul>
<h2 id="0-6-还存在什么问题？"><a href="#0-6-还存在什么问题？" class="headerlink" title="0.6 还存在什么问题？"></a>0.6 还存在什么问题？</h2><h1 id="1-背景知识"><a href="#1-背景知识" class="headerlink" title="1 背景知识"></a>1 背景知识</h1><h1 id="2-模型"><a href="#2-模型" class="headerlink" title="2 模型"></a>2 模型</h1><p>DeepWalk由两部分组成：</p>
<ol>
<li>随机游走发生器</li>
<li>更新过程</li>
</ol>
<p><img src="http://image.nysdy.com/20200915151832.png" alt></p>
<h1 id="3-实验"><a href="#3-实验" class="headerlink" title="3 实验"></a>3 实验</h1><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p>- </p>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
  </entry>
  <entry>
    <title>Knowledge graph embedding with concepts阅读笔记</title>
    <url>/post/Knowledge_graph_embedding_with_concepts/</url>
    <content><![CDATA[<blockquote>
<p>这篇论文，运用skip-gram方法，将实体对应相关概念引入实体向量表示，以增强表示效果。实体和概念在同一空间中，但是概念是空间中的一个超平面（类似于transH）。文中举例很多例子来辅助说明，使得文章可读性大幅提升。文中实验最后俩个比较有意思。本文值得思考借鉴的东西不少，值得再好好回顾。</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0950705118304945/pdfft?md5=b9bad12f2bc771990ad0feaefa7402d4&amp;pid=1-s2.0-S0950705118304945-main.pdf" target="_blank" rel="noopener">论文下载地址</a></p>
</blockquote>
<a id="more"></a>
<h1 id="problem-statement"><a href="#problem-statement" class="headerlink" title="problem statement"></a>problem statement</h1><ul>
<li>已经存在的KGE模型主要集中于实体-关系-实体三元组或者文本语料交互。<ul>
<li>三元组是缺少信息的，并且域内文本不总是可以获得的——导致嵌入结果偏离实际</li>
</ul>
</li>
<li>常识概念知识发挥很重要的作用。</li>
</ul>
<h1 id="background"><a href="#background" class="headerlink" title="background"></a>background</h1><blockquote>
<p>For example, for two triplets (Apple, Developer, IPhone) and (Apple, Developer, Samsung Mobile), it is quite difficult to distinguish which is the true triplet that contains fact triplets only, because ‘‘IPhone’’ and ‘‘Samsung Mobile’’ both belong to mobile phones. However, in the concept graph, ‘‘IPhone’’ has a concept ‘‘apple device’’, but ‘‘Samsung Mobile’’ does not. Thus, it is easy to infer the correct triplet by mapping ‘‘IPhone’’ to the ‘‘apple device’’concept</p>
</blockquote>
<pre><code>很好的一个举例关于如何运用concept来辅助关系识别
</code></pre><blockquote>
<p>Specifically, when a corpus about technology is provided, embedding methods with technical textual descriptions could easily infer the fact (Apple, Developer, IPhone), because the keywords ‘‘hardware products’’ and ‘‘iPhone smartphone’’ occur frequently in the textual description of ‘‘Apple’’. However, it is difficult to infer the fact (Apple, Taste, Sweet), which is irrelevant to textual descriptions of ‘‘Apple’’ about the specific topic of ‘‘technology company</p>
</blockquote>
<pre><code>这里作者举例说明：与具有文本信息的嵌入方法相比，具有概念信息的嵌入方法在其任务中更加通用，并且它不依赖于语料库的主题。
</code></pre><p>作者把KGE分成了三类，如下：</p>
<ul>
<li>Embedding with symbolic triplets：trans系列都放到了这部分中</li>
<li>Embedding with textual information</li>
<li>Embedding with category information</li>
</ul>
<h1 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h1><h2 id="concept-graph-embedding"><a href="#concept-graph-embedding" class="headerlink" title="concept graph embedding"></a>concept graph embedding</h2><p>作者采用skip-gram来学习可以捕获其语义相关性的概念和实体的表示。</p>
<p><img src="http://image.nysdy.com/20190725156403562012591.png" alt="20190725156403562012591.png"></p>
<p>其中，每个实体对应多个概念，每个概念又包含多个实体（这些实体作为实体的上下文）。</p>
<p>则，skip-gram函数可以写为：</p>
<script type="math/tex; mode=display">
\begin{array}{l}{P\left(e_{c} | e_{t}\right)=\frac{\exp \left(e_{c} \cdot e_{t}\right)}{\sum_{e \in E} \exp \left(e \cdot e_{t}\right)}} \\ {P\left(e_{c} | c_{i}\right)=\frac{\exp \left(e_{c} \cdot c_{i}\right)}{\sum_{e \in E} \exp \left(e \cdot c_{i}\right)}}\end{array}</script><p>故损失函数为：</p>
<script type="math/tex; mode=display">
L=\frac{1}{|D|} \sum_{\left(e_{c}, e_{t}\right) \in D}\left[\log P\left(e_{c} | e_{t}\right)+\sum_{c_{i} \in C\left(e_{t}\right)} \log P\left(e_{c} | c_{i}\right)\right]</script><p>学习率设为：</p>
<p>α = starting_alpha×(1−count_actual/(real)(iter × total_size+1))</p>
<pre><code>这里作者说为了避免过拟合，对优化目标采用“负抽样”方法。&quot;负抽样&quot;方法还可以避免过拟合？
</code></pre><h2 id="knowledge-graph-embedding"><a href="#knowledge-graph-embedding" class="headerlink" title="knowledge graph embedding"></a>knowledge graph embedding</h2><p>将特定三元组嵌入到概念子空间中，首先构建一个超平面，其中法向量$c$为概念子空间：</p>
<script type="math/tex; mode=display">
c=C\left(e_{h}, e_{t}\right)=\frac{e_{h}-e_{t}}{\left\|e_{h}-e_{t}\right\|_{2}^{2}}</script><p>根据TransE三元组的嵌入损失为：</p>
<script type="math/tex; mode=display">
l=h+r-t</script><p>所以，可以计算出法向量方向上的损失分量是：</p>
<script type="math/tex; mode=display">
\left(c^{T} l c\right)</script><p>然后，投影到超平面上的另一个正交分量是：</p>
<script type="math/tex; mode=display">
\left(l-c^{T} l c\right)</script><p><img src="http://image.nysdy.com/20190725156403899071000.png" alt="20190725156403899071000.png"></p>
<p>定义总损失函数：</p>
<script type="math/tex; mode=display">
f_{r}(h, t)=-\lambda\left\|l-c^{T} l c\right\|_{2}^{2}+\|l\|_{2}^{2}</script><h2 id="Model-interpretation"><a href="#Model-interpretation" class="headerlink" title="Model interpretation"></a>Model interpretation</h2><ol>
<li>可以通过概念来辅助三元组识别，文中以(Christopher Plummer, /people/person/nationality, Canada)举例</li>
<li>可以解决在当两个候选实体在KGE，中计算loss相等时辨别这两个哪个是真实的。文中以“which the director made the film ‘‘WALL-E’’”为例来进行说明</li>
</ol>
<pre><code>都是通过查询实体对应概念来进行辅助
</code></pre><h1 id="Objectives-and-training"><a href="#Objectives-and-training" class="headerlink" title="Objectives and training"></a>Objectives and training</h1><p>margin-based loss function：</p>
<script type="math/tex; mode=display">
L=\sum_{(h, r, t) \in S} \sum_{\left(h^{\prime}, r, t^{\prime}\right) \in S_{(h, r, t)}^{\prime}}\left[\gamma+f_{r}(h, t)-f_{r^{\prime}}\left(h^{\prime}, t^{\prime}\right)\right]_{+}</script><h2 id="train"><a href="#train" class="headerlink" title="train"></a>train</h2><ol>
<li>先预训练概念图模型嵌入，获得在概念空间中的实体向量</li>
<li>利用1中获得的实体向量进行更新。</li>
</ol>
<h1 id="datasets"><a href="#datasets" class="headerlink" title="datasets"></a>datasets</h1><ul>
<li><p>WN18 and FB15K</p>
</li>
<li><p>Microsoft Concept Graph<img src="http://image.nysdy.com/20190725156404326996082.png" alt="20190725156404326996082.png"></p>
<p>其中，relations表示频率</p>
<pre><code>真的有统计频率的这种
</code></pre></li>
</ul>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h3 id="Knowledge-graph-completion"><a href="#Knowledge-graph-completion" class="headerlink" title="Knowledge graph completion"></a>Knowledge graph completion</h3><h3 id="Entity-classification"><a href="#Entity-classification" class="headerlink" title="Entity classification"></a>Entity classification</h3><h3 id="Concept-relevance-analysis"><a href="#Concept-relevance-analysis" class="headerlink" title="Concept relevance analysis"></a>Concept relevance analysis</h3><p><img src="http://image.nysdy.com/20190725156404278694395.png" alt="20190725156404278694395.png"></p>
<p>这个实验比较有意思：每个单元格中的数字表示在TransE中排名大于m且在我们的模型中小于n的三元组的数量。</p>
<h3 id="Precise-semantic-expression-analysis"><a href="#Precise-semantic-expression-analysis" class="headerlink" title="Precise semantic expression analysis"></a>Precise semantic expression analysis</h3><p>我们在链接预测（换句话说，这些是TransE的难以证明的例子）中收集那些得分略高于真实三元组作为负三元组</p>
<p>然后在KEC中对比两者的分数差值。</p>
<p><img src="http://image.nysdy.com/20190725156404301070773.png" alt="20190725156404301070773.png"></p>
<p>右边条表示KEC在TransE失败时作出正确决定，左边条表示KEC和TransE都失败。</p>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>KG</category>
      </categories>
      <tags>
        <tag>KGE</tag>
        <tag>ontology</tag>
        <tag>concept</tag>
      </tags>
  </entry>
  <entry>
    <title>Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs 阅读笔记</title>
    <url>/post/Learning%20Attention-based%20Embeddings%20for%20Relation%20Prediction%20in%20Knowledge%20Graphs/</url>
    <content><![CDATA[<h1 id="0-导读"><a href="#0-导读" class="headerlink" title="0. 导读"></a>0. 导读</h1><h2 id="0-1-文章是关于什么的？（what？）"><a href="#0-1-文章是关于什么的？（what？）" class="headerlink" title="0.1 文章是关于什么的？（what？）"></a>0.1 文章是关于什么的？（what？）</h2><p>知识图谱，图神经网络，关系预测，链接预测，图注意力模型</p>
<h2 id="0-2-要解决什么问题？（why？）"><a href="#0-2-要解决什么问题？（why？）" class="headerlink" title="0.2 要解决什么问题？（why？）"></a>0.2 要解决什么问题？（why？）</h2><ul>
<li>基于CNN的嵌入模型独立的处理三元组，导致无法覆盖在三元组周围的本地邻居中固有隐含的复杂和隐藏信息。</li>
<li>随着模型深度的增加，远方实体的贡献呈指数下降。</li>
</ul>
<h2 id="0-3-用什么方法解决？（how？）"><a href="#0-3-用什么方法解决？（how？）" class="headerlink" title="0.3 用什么方法解决？（how？）"></a>0.3 用什么方法解决？（how？）</h2><ul>
<li>将不同的权重（注意力）分配给附近的节点，并通过迭代方式通过层传播注意力。</li>
<li>提出的关系组合在n跳邻居之间引入辅助边，这样就很容易允许实体之间的知识流。</li>
<li>作者设计一个encoder-decoder模型（坐着的生成图注意力模型和ConvKB的组合）</li>
</ul>
<h2 id="0-4文章有什么创新？"><a href="#0-4文章有什么创新？" class="headerlink" title="0.4文章有什么创新？"></a>0.4文章有什么创新？</h2><ul>
<li>作者是第一个学习新的基于图注意力的嵌入，这些嵌入专门针对KG的关系预测。</li>
</ul>
<h2 id="0-5-效果如何？"><a href="#0-5-效果如何？" class="headerlink" title="0.5 效果如何？"></a>0.5 效果如何？</h2><p>基于流行的Freebase（FB15K-237）数据集上Hits @ 1指标的最新方法，作者的方法实现了104％的改进。</p>
<h2 id="0-6-还存在什么问题？"><a href="#0-6-还存在什么问题？" class="headerlink" title="0.6 还存在什么问题？"></a>0.6 还存在什么问题？</h2><ol>
<li>分步训练编解码器，容易产生错误传播</li>
<li>需要transE作为初始嵌入。</li>
<li>这明明是链接预测，咋整个关系预测，这里可能有笔误</li>
<li>该方法无法以自上而下的递归方式捕获信息</li>
</ol>
<h1 id="1-背景知识"><a href="#1-背景知识" class="headerlink" title="1 背景知识"></a>1 背景知识</h1><h1 id="2-模型"><a href="#2-模型" class="headerlink" title="2 模型"></a>2 模型</h1><h2 id="2-1-Graph-Attention-Networks-GATs-图注意力网络"><a href="#2-1-Graph-Attention-Networks-GATs-图注意力网络" class="headerlink" title="2.1 Graph Attention Networks (GATs) 图注意力网络"></a>2.1 Graph Attention Networks (GATs) 图注意力网络</h2><p>一个单一的GAT层可以表示为：</p>
<script type="math/tex; mode=display">
e_{i j}=a\left(\mathbf{W} \overrightarrow{x_{i}}, \mathbf{W} \overrightarrow{x_{j}}\right)</script><ul>
<li>其中$e_{ij}$是边$(e_i, e_j)$的注意力值，$\mathbf{W}$是线性变换矩阵（把输入特征映射到高维输出特征空间），$a$是注意力函数</li>
</ul>
<p>节点$e_i$的的GAT输出特征向量为：</p>
<script type="math/tex; mode=display">
\overrightarrow{x_{i}^{\prime}}=\sigma\left(\sum_{j \in \mathcal{N}_{i}} \alpha_{i j} \mathbf{W} \overrightarrow{x_{j}}\right)</script><ul>
<li>其中$\alpha_{ij}$是$e_{ij}$，其中$a$使用了softmax函数</li>
</ul>
<p>作者使用了多头注意力的方式去加固学习过程：</p>
<script type="math/tex; mode=display">
\overrightarrow{x_{i}^{\prime}}=\|_{k=1}^{K} \sigma\left(\sum_{j \in \mathcal{N}_{i}} \alpha_{i j}^{k} \mathbf{W}^{k} \overrightarrow{x_{j}}\right)</script><ul>
<li>其中$k$代表第$k$个头，$|$代表一个串联。（这是第一种连接方式）</li>
</ul>
<p>最后的输出结果取得是多头注意力的平均值，即</p>
<script type="math/tex; mode=display">
\overrightarrow{x_{i}^{\prime}}=\sigma\left(\frac{1}{K} \sum_{k=1}^{K} \sum_{j \in \mathcal{N}_{i}} \alpha_{i j}^{k} \mathbf{W}^{k} \overrightarrow{x_{j}}\right)</script><ul>
<li>这是第二种连接方式。</li>
</ul>
<h2 id="2-2-引入关系的重要性"><a href="#2-2-引入关系的重要性" class="headerlink" title="2.2 引入关系的重要性"></a>2.2 引入关系的重要性</h2><p>GAT在处理过程中其实忽略了知识图谱中的关系，节点的关系十分重要。作者提出了一个整合节点特征和关系的注意力机制。（所以其实上面写的是GAT的流程，下面写的才是作者实际采用的方式，所以其实可以忽略上面写的直接看下面的也可以）</p>
<p>作者把尸体嵌入表示为一个矩阵$\mathbf{H} \in \mathbb{R}^{N_{e} \times T}$，相似的关系嵌入表示为$\mathbf{G} \in \mathbb{R}^{N_{r} \times P}$，$T$和$P$分别为实体特征和关系特征的维度。</p>
<p>作者把三元组$t_{i j}^{k}=\left(e_{i}, r_{k}, e_{j}\right)$表示为：</p>
<script type="math/tex; mode=display">
c_{i j k}^{\rightarrow}=\mathbf{W}_{1}\left[\vec{h}_{i}\left\|\vec{h}_{j}\right\| \vec{g}_{k}\right]</script><ul>
<li>其中，$\vec{h}_{i}, \vec{h}_{j}, \text { and } \vec{g}_{k}$分别代表实体$e_{i}, e_{j}, r_{k}$的嵌入表示。</li>
</ul>
<p>作者参考<code>Veliˇckovi´c et al., 2018)</code>的方式是，为三元组设计了一个值来表示三元组的重要性：</p>
<script type="math/tex; mode=display">
b_{i j k}=\text { LeakyReLU }\left(\mathbf{W}_{2} c_{i j k}\right)</script><p>同样，作者给予了不同关系一个注意力值：</p>
<script type="math/tex; mode=display">
\begin{aligned} \alpha_{i j k} &=\operatorname{softmax}_{j k}\left(b_{i j k}\right) \\ &=\frac{\exp \left(b_{i j k}\right)}{\sum_{n \in \mathcal{N}_{i}} \sum_{r \in \mathcal{R}_{i n}} \exp \left(b_{i n r}\right)} \end{aligned}</script><p>图3展示了$\alpha_{i j k}$的机制：<img src="http://image.nysdy.com/20200829203733.png" alt></p>
<p>所以一个实体$e_i$的新嵌入表示为：</p>
<script type="math/tex; mode=display">
\overrightarrow{h_{i}^{\prime}}=\sigma\left(\sum_{j \in \mathcal{N}_{i}} \sum_{k \in \mathcal{R}_{i j}} \alpha_{i j k} c_{i j k}\right)</script><p>同样，作者采用多头注意力机制：</p>
<script type="math/tex; mode=display">
\overrightarrow{h_{i}^{\prime}}=\|_{m=1}^{M} \sigma\left(\sum_{j \in \mathcal{N}_{i}} \alpha_{i j k}^{m} c_{i j k}^{m}\right)</script><ul>
<li>这里实际是图4中1所示部分。</li>
</ul>
<p>这就是图4中展示的图注意力层：<img src="http://image.nysdy.com/20200829210354.png" alt></p>
<p><strong>作者采用一个线性变化来获得关系嵌入矩阵</strong>：</p>
<script type="math/tex; mode=display">
G^{\prime}=G . \mathbf{W}^{R}</script><ul>
<li>其中，$\mathbf{W}^{R} \in \mathbb{R}^{T \times T^{\prime}}$。</li>
<li>这里是图4中2所示的部分。</li>
<li>感觉这里是因为关系嵌入在传播的过程中没有发生变化，强行线性变化使得发生变化。</li>
</ul>
<p>在最后一层（Graph Attention Layer 2），作者作者将多头注意力进行取平均操作：</p>
<script type="math/tex; mode=display">
\overrightarrow{h_{i}^{\prime}}=\sigma\left(\frac{1}{M} \sum_{m=1}^{M} \sum_{j \in \mathcal{N}_{i}} \sum_{k \in \mathcal{R}_{i j}} \alpha_{i j k}^{m} c_{i j k}^{m}\right)</script><ul>
<li>这里是图4中4所示的部分</li>
</ul>
<p>作者把初始的实体表示矩阵做了一个线性变换，如图4中3所示。即$\mathbf{W}^{E} \mathbf{H}^{t}$操作。</p>
<p>作者把图4中3和4连接在一起以一下灯饰表示：</p>
<script type="math/tex; mode=display">
\mathbf{H}^{\prime \prime}=\mathbf{W}^{E} \mathbf{H}^{t}+\mathbf{H}^{f}</script><h3 id="作者引入辅助关系路径"><a href="#作者引入辅助关系路径" class="headerlink" title="作者引入辅助关系路径"></a>作者引入辅助关系路径</h3><p><img src="http://image.nysdy.com/20200829211155.png" alt></p>
<p>如图所示，作者强行建立了多跳关系实体的直接连接。</p>
<p>辅助关系的嵌入为有向路径中所有关系的嵌入和。</p>
<blockquote>
<p>这里作者这个辅助边描述的不够清晰，弄得搞不懂这个辅助边在作者的数据集中是不是已经手动建立了，还是不存在。但是通过图中分配了权重说明作者是建立这个辅助边的。</p>
</blockquote>
<h2 id="2-3-训练目标"><a href="#2-3-训练目标" class="headerlink" title="2.3 训练目标"></a>2.3 训练目标</h2><p>作者借鉴transE的思想$\vec{h}_{i}+\vec{g}_{k} \approx \vec{h}_{j}$，使用L1距离计算方式得到：</p>
<script type="math/tex; mode=display">
d_{t_{i j}}=\left\|\overrightarrow{h_{i}}+\overrightarrow{g_{k}}-\overrightarrow{h_{j}}\right\|_{1}</script><ul>
<li>其中$t_{i j}^{k}=\left(e_{i}, r_{k}, e_{j}\right)$</li>
</ul>
<p>作者采用hinge-loss：</p>
<script type="math/tex; mode=display">
L(\Omega)=\sum_{t_{i j} \in S} \sum_{t_{i j}^{\prime} \in S^{\prime}} \max \left\{d_{t_{i j}^{\prime}}-d_{t_{i j}}+\gamma, 0\right\}</script><ul>
<li>其中，$\gamma &gt; 0$是超参数，</li>
<li>$S^{\prime}=\underbrace{\left\{t_{i^{\prime} j}^{k} \mid e_{i}^{\prime} \in \mathcal{E} \backslash e_{i}\right\}}_{\text {replace head entity }} \cup \underbrace{\left\{t_{i j^{\prime}}^{k} \mid e_{j}^{\prime} \in \mathcal{E} \backslash e_{j}\right\}}_{\text {replace tail entity }}$</li>
</ul>
<h2 id="2-4-解码器"><a href="#2-4-解码器" class="headerlink" title="2.4 解码器"></a>2.4 解码器</h2><p>作者使用ConvKB作为一个解码器。卷积层的目的是分析每个维度上的三重$t_{i j}^{k}$的全局嵌入特性，并概括模型中的过渡特性.</p>
<p>多特征图的分数函数写成：</p>
<script type="math/tex; mode=display">
f\left(t_{i j}^{k}\right)=\left(\|_{m=1}^{\Omega} \operatorname{ReLU}\left(\left[\vec{h}_{i}, \vec{g}_{k}, \vec{h}_{j}\right] * \omega^{m}\right)\right) . \mathbf{W}</script><ul>
<li>其中，$\omega^{m}$是第$m$个卷积核，$*$是卷积操作。$\mathbf{W} \in \mathbb{R}^{\Omega k \times 1}$是线性变换来计算三元组最终分数。</li>
</ul>
<p>最后模型被训练的损失函数为：</p>
<script type="math/tex; mode=display">
\begin{array}{l}\mathcal{L}=\sum_{t_{i j}^{k} \in\left\{S \cup S^{\prime}\right\}} \log \left(1+\exp \left(l_{t_{i j}^{k} . f}\left(t_{i j}^{k}\right)\right)\right)+\frac{\lambda}{2}\|\mathbf{W}\|_{2}^{2} \\ \text { where } l_{t_{i j}^{k}}=\left\{\begin{array}{ll}1 & \text { for } t_{i j}^{k} \in S \\ -1 & \text { for } t_{i j}^{k} \in S^{\prime}\end{array}\right.\end{array}</script><h1 id="3-实验"><a href="#3-实验" class="headerlink" title="3 实验"></a>3 实验</h1><h2 id="3-1-数据集"><a href="#3-1-数据集" class="headerlink" title="3.1 数据集"></a>3.1 数据集</h2><p><img src="http://image.nysdy.com/20200829214116.png" alt></p>
<h2 id="3-2-训练协议"><a href="#3-2-训练协议" class="headerlink" title="3.2 训练协议"></a>3.2 训练协议</h2><p>TransE产生的实体和关系嵌入（Bordes等，2013； Nguyen等，2018）用于初始化我们的嵌入。（用transE作为初始化嵌入，这个模型越来越有取巧的嫌疑了）</p>
<p>作者遵循两步训练过程：</p>
<ol>
<li>即我们首先训练广义GAT以编码有关图实体和关系的信息，</li>
<li>然后训练诸如ConvKB（Nguyen等人，2018）的解码器模型以执行关系预测任务。</li>
</ol>
<h2 id="3-3-评估标准"><a href="#3-3-评估标准" class="headerlink" title="3.3 评估标准"></a>3.3 评估标准</h2><p>这明明是链接预测，咋整个关系预测，我这里怀疑作者是否有好好的研究知识表示相关内容</p>
<h2 id="3-4-实验结果"><a href="#3-4-实验结果" class="headerlink" title="3.4 实验结果"></a>3.4 实验结果</h2><p><img src="http://image.nysdy.com/20200829214539.png" alt></p>
<p><img src="http://image.nysdy.com/20200829214911.png" alt></p>
<p>这图根本看不出来啥呀。</p>
<h3 id="pagerank"><a href="#pagerank" class="headerlink" title="pagerank"></a>pagerank</h3><p>这个接触过，不知道作用，</p>
<p><img src="http://image.nysdy.com/20200829215051.png" alt></p>
<h3 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h3><p><img src="http://image.nysdy.com/20200829215300.png" alt></p>
<p>这里作者对于两个消融实验也不多描述清楚一下：</p>
<ol>
<li>-PG这个不清楚是移除了啥，那个辅助关系本来也没搞懂。但这个我觉得本来传播模型的时候就带入了多跳传输。</li>
<li>-Relation，这个应该是直接采用GAT的方式，没有考虑关系</li>
</ol>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p>- </p>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>KG</tag>
      </tags>
  </entry>
  <entry>
    <title>Learning as the Unsupervised Alignment of Conceptual Systems阅读笔记</title>
    <url>/post/Learning_as_the_Unsupervised_Alignment_of_Conceptual_Systems/</url>
    <content><![CDATA[<blockquote>
<p>这篇文章没怎么看懂，主要思想应该是代表同时概念的不同形式（文本，图像，语音等）应该具有相似的分布，以此来进行无监督的概念对齐。这种思路挺不错的，不过还没有深入的想法，算是拓展视野吧！</p>
</blockquote>
<a id="more"></a>
<h1 id="KEY"><a href="#KEY" class="headerlink" title="KEY"></a>KEY</h1><p>The key insight is that each concept has a unique signature within one conceptual system (e.g., images) that is recapitulated in other systems (e.g., text or audio)</p>
<h1 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h1><ul>
<li>For supervised approaches, as the number of concepts grows, so does the number of required training examples</li>
<li>V. W. Quine argued, even supervised instruction contains a substantial amount of ambiguity (Quine, 1960).Quine suggested that meaning may derive from something’s place within a conceptual system.</li>
</ul>
<h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><p>In order to solve Quinne’s problem, we align a system of word labels and a system of visual semantics that both refer to the same underlying reality and therefore have related structure that can be discovered by unsupervised means (Figure 1）</p>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
  </entry>
  <entry>
    <title>Learning Entity and Relation Embeddings for Knowledge Graph Completion阅读笔记</title>
    <url>/post/Learning_Entity_and_Relation_Embeddings_for_Knowledge_Graph_Completion/</url>
    <content><![CDATA[<blockquote>
<p>TransR embeds entities and relations in distinct entity space and relation space, and learns embeddings via translation between projected entities.CTransR models internal complicated correlations within each relation type.</p>
<p>论文下载地址</p>
</blockquote>
<a id="more"></a>
<h1 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h1><p>In fact, an entity may have multiple aspects and various relaitons may focus on different aspects of entities, which makes a common space insurficient for modeling.</p>
<h1 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h1><ul>
<li>propose a TransR model which models entities and relations in distinct spaces</li>
<li>CTransR models internal complicated correlations within each relation type.</li>
<li>experiment on benchmark datasets of WordNet and Freebase and gain consistent improvements compared to state-of-the-art models</li>
</ul>
<h1 id="Future-work"><a href="#Future-work" class="headerlink" title="Future work"></a>Future work</h1><ul>
<li>Existing models including TransR consider each relational fact separately.<ul>
<li>relation transitive</li>
</ul>
</li>
<li>explore a unified embedding model of both text side and knowledge graph</li>
<li>modeling internal correlations within each relation type</li>
</ul>
<h1 id="TransR"><a href="#TransR" class="headerlink" title="TransR"></a>TransR</h1><p><img src="http://image.nysdy.com/20190627156159912894954.jpg" alt="20190627156159912894954.jpg"></p>
<ol>
<li><p>for each triple$(h, r, t)$, entities embeddings are set as $\mathbf{h}, \mathbf{t} \in \mathbb{R}^{k}$ and relation embedding is set as $\mathbf{r} \in \mathbb{R}^{d}$, $k \neq d$</p>
</li>
<li><p>for each relation $r$, set a projection matrix $\mathbf{M}_{r} \in\mathbb{R}^{k \times d}$</p>
<ul>
<li>projects entities from entity space to relation space</li>
</ul>
</li>
<li><p>projected vectors of entities as </p>
<script type="math/tex; mode=display">
\mathbf{h}_{r}=\mathbf{h} \mathbf{M}_{r}, \quad \mathbf{t}_{r}=\mathbf{t} \mathbf{M}_{r}</script></li>
<li><p>score function:</p>
<script type="math/tex; mode=display">
f_{r}(h, t)=\left\|\mathbf{h}_{r}+\mathbf{r}-\mathbf{t}_{r}\right\|_{2}^{2}</script></li>
</ol>
<h1 id="Cluster-based-TransR-CTransR"><a href="#Cluster-based-TransR-CTransR" class="headerlink" title="Cluster-based TransR (CTransR)"></a>Cluster-based TransR (CTransR)</h1><h3 id="why-propose-CTransR"><a href="#why-propose-CTransR" class="headerlink" title="why propose CTransR"></a>why propose CTransR</h3><p>TransE, TransH and TransR, learn a unique vector for each relation, which may be under-representative to fit all entity pairs under this relation, because these relations are usually rather diverse.</p>
<h3 id="basic-idea"><a href="#basic-idea" class="headerlink" title="basic idea"></a>basic idea</h3><ul>
<li>incorporate the idea of piecewise linear regression Ritzema and others 1994</li>
<li>segment input instances into several groups</li>
</ul>
<h3 id="process"><a href="#process" class="headerlink" title="process"></a>process</h3><ol>
<li><p>for a specific relation r, all entity pairs (h, t) in the training data are clustered into multiple groups, and entity pairs in each group are expected to exhibit similar r relation.</p>
<ul>
<li>All entity pairs (h, t) are represented with their vector offsets (h − t) for clustering, where h and t are obtained with TransE.</li>
</ul>
</li>
<li><p>learn a separate relation vector $r_c$for each cluster and matrix $M_r$ for each relation, respectively</p>
</li>
<li><p>projected vectors of entities as $\mathbf{h}_{r, c}=\mathbf{h} \mathbf{M}_{r} \text { and } \mathbf{t}_{r, c}=\mathbf{t} \mathbf{M}_{r}$</p>
</li>
<li><p>sorce fuction</p>
<script type="math/tex; mode=display">
f_{r}(h, t)=\left\|\mathbf{h}_{r, c}+\mathbf{r}_{c}-\mathbf{t}_{r, c}\right\|_{2}^{2}+\alpha\left\|\mathbf{r}_{c}-\mathbf{r}\right\|_{2}^{2}</script><p>the later item aims to ensure cluster-specific relation vector rcnot too far away from the original relation vector r</p>
</li>
</ol>
<h1 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h1><p>采用和前人所用一样的数据集</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Dataset</th>
<th>#Rel</th>
<th>#Ent</th>
<th>#Train</th>
<th>#Valid</th>
<th># Test</th>
</tr>
</thead>
<tbody>
<tr>
<td>WN18</td>
<td>18</td>
<td>40,943</td>
<td>141,442</td>
<td>5,000</td>
<td>5,000</td>
</tr>
<tr>
<td>FB15K</td>
<td>1,345</td>
<td>14,951</td>
<td>483,142</td>
<td>50,000</td>
<td>59,071</td>
</tr>
<tr>
<td>WN11</td>
<td>11</td>
<td>38,696</td>
<td>112,581</td>
<td>2,609</td>
<td>10,544</td>
</tr>
<tr>
<td>FB13</td>
<td>13</td>
<td>75,043</td>
<td>316,232</td>
<td>5,908</td>
<td>23,733</td>
</tr>
<tr>
<td>FB40K</td>
<td>1,336</td>
<td>39528</td>
<td>370,648</td>
<td>67,946</td>
<td>96,678</td>
</tr>
</tbody>
</table>
</div>
<h1 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h1><p>作者采取常规实验</p>
<h3 id="Link-Prediction"><a href="#Link-Prediction" class="headerlink" title="Link Prediction"></a>Link Prediction</h3><p>这里作者对关系中聚类进行了展示： <img src="http://image.nysdy.com/2019062715616031534723.jpg" alt="2019062715616031534723.jpg"></p>
<blockquote>
<p>我觉得这种方式是值得尝试的。</p>
</blockquote>
<h3 id="Triple-classification"><a href="#Triple-classification" class="headerlink" title="Triple classification"></a>Triple classification</h3><p>Moreover, the “bern” sampling technique improves the performance of TransE, TransH and TransR on all three data sets.</p>
<blockquote>
<p>bern采样方法需要掌握。</p>
</blockquote>
<h3 id="Relation-Extraction-from-Text"><a href="#Relation-Extraction-from-Text" class="headerlink" title="Relation Extraction from Text"></a>Relation Extraction from Text</h3>]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>KGE</tag>
        <tag>KGR</tag>
        <tag>TransR</tag>
      </tags>
  </entry>
  <entry>
    <title>Learning Knowledge Embeddings by Combining Limit-based Scoring Loss阅读笔记</title>
    <url>/post/Learning_Knowledge_Embeddings_by_Combining_Limit-based_Scoring_Loss/</url>
    <content><![CDATA[<blockquote>
<p>此篇文章最为重要的就是作者设计的 margin-based ranking loss 的改进，对两个超参数$\lambda$和$\gamma$的实验，对于实验结果有很多值得分析与思考的地方。</p>
<p><a href="https://dl.acm.org/ft_gateway.cfm?id=3132939&amp;ftid=1920664&amp;dwn=1&amp;CFID=135630312&amp;CFTOKEN=71536805165d7c9d-10D2074A-AD9B-A596-5CA31DB63C36A322" target="_blank" rel="noopener">论文下载地址</a></p>
</blockquote>
<a id="more"></a>
<h1 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h1><p>The margin-based ranking loss function cannot ensure the fact that the scoring of correct triplets must be low enough to fulfill the translation.</p>
<h1 id="research-objective"><a href="#research-objective" class="headerlink" title="research objective"></a>research objective</h1><p>reduce the scoring of correct triplets to fulfill the translation by mending the margin-based ranking loss function</p>
<h1 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h1><ul>
<li>proposing a limit-based ranking loss item combined with margin-based ranking loss </li>
<li>extending TransE and TransH to TransE-RS and TransH-RS</li>
</ul>
<h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><h2 id="Margin-based-Tanking-Loss"><a href="#Margin-based-Tanking-Loss" class="headerlink" title="Margin-based Tanking Loss"></a>Margin-based Tanking Loss</h2><p>formula:</p>
<script type="math/tex; mode=display">
L_{R}=\sum_{(h, r, t) \in \Delta} \sum_{\left(h^{\prime}, r, t^{\prime}\right) \in \Delta^{\prime}}\left[\gamma_{1}+f_{r}(h, t)-f_{r}\left(h^{\prime}, t^{\prime}\right)\right) ]_{+}</script><ul>
<li><p>The margin-based ranking loss function aims to make the score $f_{r}\left(h^{\prime}, t^{\prime}\right)$ of corrupted triplet higher by at least $\gamma_{1}$ than  of positive triplet.</p>
</li>
<li><p>cannot be proved $f_{r}(h, t)&lt;\varepsilon$ </p>
</li>
</ul>
<h2 id="Limit-based-Scoring-Loss"><a href="#Limit-based-Scoring-Loss" class="headerlink" title="Limit-based Scoring Loss"></a>Limit-based Scoring Loss</h2><p>formula:</p>
<script type="math/tex; mode=display">
L_{S}=\sum_{(h, r, t) \in \Delta}\left[f_{r}(h, t)-\gamma_{2}\right]_{+}</script><h2 id="Finally-loss"><a href="#Finally-loss" class="headerlink" title="Finally loss"></a>Finally loss</h2><p>formula:</p>
<script type="math/tex; mode=display">
L_{R S}=L_{R}+\lambda L_{S}, \quad(\lambda>0)</script><p>detail is :</p>
<script type="math/tex; mode=display">
\begin{array}{c}{L_{R S}=\sum_{(h, r, t) \in \Delta} \sum_{\left(h^{\prime}, r, t^{\prime}\right) \in \Delta^{\prime}}\left\{\left[\gamma_{1}+f_{r}(h, t)-f_{r}\left(h^{\prime}, t^{\prime}\right)\right]_{+}\right.} \\ {+\lambda\left[f_{r}(h, t)-\gamma_{2}\right]_{+} \}}\end{array}</script><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h2><p><img src="http://image.nysdy.com/20190603155952634332494.jpg" alt="20190603155952634332494.jpg"></p>
<h2 id="Link-prediction"><a href="#Link-prediction" class="headerlink" title="Link prediction"></a>Link prediction</h2><blockquote>
<p>思考</p>
<p>作者只是对表格的数据进行了陈述，有一些问题并没有进行分析解释</p>
<ul>
<li>并没有分析比如说为什么改进loss后的transE为什么会比TransH（R、D）效果要好？</li>
<li>为什么在n-to-1中的表现效果没有达到最好（其他的都达到了最好）？</li>
<li>通过这种改进可以发现，transH相比于TransE并没有显著提升，原因是什么？</li>
</ul>
</blockquote>
<h2 id="Triple-Classification"><a href="#Triple-Classification" class="headerlink" title="Triple Classification"></a>Triple Classification</h2><ul>
<li>TransE-RS and TransH-RS have same parameter and operation complexities as TransE and TransH, which is lower than TransR and TransD.</li>
<li>Our models randomly initial the entities, not use the learned embeddings by TransE as TransR and TransD.<ul>
<li>It means that our models have much better ability to overcome the problem of overfitting</li>
</ul>
</li>
</ul>
<h2 id="Distributions-of-Triplets’-Scores"><a href="#Distributions-of-Triplets’-Scores" class="headerlink" title="Distributions of Triplets’ Scores"></a>Distributions of Triplets’ Scores</h2><h3 id="aim"><a href="#aim" class="headerlink" title="aim"></a>aim</h3><p>analyze the difference between $L_R$ Loss and our $L_RS$ Loss</p>
<h3 id="Parameters"><a href="#Parameters" class="headerlink" title="Parameters"></a>Parameters</h3><p><img src="http://image.nysdy.com/20190603155952784132260.jpg" alt="20190603155952784132260.jpg"></p>
<blockquote>
<p>思考：</p>
<ul>
<li>对于我自己正在做的实验：是不是我自己用的间隔太小了</li>
</ul>
</blockquote>
<h3 id="result"><a href="#result" class="headerlink" title="result"></a>result</h3><p><img src="http://image.nysdy.com/20190603155952821850690.jpg" alt="20190603155952821850690.jpg"></p>
<p><img src="http://image.nysdy.com/20190603155952848841252.jpg" alt="20190603155952848841252.jpg"></p>
<blockquote>
<p>思考</p>
<ul>
<li>这部分的实验值得借鉴，它可以相对于直观的可以展示出为什么效果会好。</li>
<li>比如对于上述为什么改进后的transE的效果会更好<ul>
<li>看到最后的分数分布transE-RS的分布效果和Trans-H的十分接近，</li>
<li>而transE的模型较为简单，可能最终loss最小化会使得模型充分表达，而其他模型引入了更多的假设可能会带来更多的噪声</li>
<li>也可能当loss很小时，其他的假设条件发挥作用的很小（至少从实验结果来看是的，但是还有待于进一步设计实验验证）</li>
</ul>
</li>
</ul>
</blockquote>
<h2 id="Discussion-of-Parameters"><a href="#Discussion-of-Parameters" class="headerlink" title="Discussion of Parameters"></a>Discussion of Parameters</h2><h3 id="Discussion-on-γ1-and-γ2"><a href="#Discussion-on-γ1-and-γ2" class="headerlink" title="Discussion on γ1 and γ2."></a>Discussion on γ1 and γ2.</h3><p><img src="http://image.nysdy.com/20190603155952911595962.jpg" alt="20190603155952911595962.jpg"></p>
<ul>
<li>We find that γ2 = 3γ1 or γ2 = 4γ1 is better for link prediction, but for triplet classification there are not obvious characteristics on γ1 and γ2.</li>
<li>a lower γ2 is expected to ensure the golden condition $\mathbf{h}+\mathbf{r} \approx \mathbf{t}$ for positive triplets, but an entity needs to satisfy many golden coditions at the same time.</li>
</ul>
<blockquote>
<p>思考</p>
<ul>
<li>既然如作者说，那么理论上transH的效果应该很好才对，但是结果并不是这样的，这又产生矛盾。</li>
</ul>
</blockquote>
<h3 id="Discussion-on-λ"><a href="#Discussion-on-λ" class="headerlink" title="Discussion on λ"></a>Discussion on λ</h3><p><img src="http://image.nysdy.com/20190603155953002076189.jpg" alt="20190603155953002076189.jpg"></p>
<blockquote>
<p>思考</p>
<ul>
<li>看到λ并没有对模型影响并没很大</li>
<li>λ在1左右是效果会比较好</li>
<li>λ和margin会不会产生关联？</li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>KG</category>
      </categories>
      <tags>
        <tag>KG</tag>
        <tag>transH</tag>
        <tag>margin loss</tag>
        <tag>transE</tag>
      </tags>
  </entry>
  <entry>
    <title>NASE Learning Knowledge Graph Embedding for Link Prediction via Neural Architecture Search 阅读笔记</title>
    <url>/post/Learning%20Knowledge%20Graph%20Embedding%20for%20Link%20Prediction%20via%20Neural%20Architecture%20Search/</url>
    <content><![CDATA[<h1 id="0-导读"><a href="#0-导读" class="headerlink" title="0. 导读"></a>0. 导读</h1><h2 id="0-1-文章是关于什么的？（what？）"><a href="#0-1-文章是关于什么的？（what？）" class="headerlink" title="0.1 文章是关于什么的？（what？）"></a>0.1 文章是关于什么的？（what？）</h2><p>知识图谱表示，</p>
<h2 id="0-2-要解决什么问题？（why？-challenge）"><a href="#0-2-要解决什么问题？（why？-challenge）" class="headerlink" title="0.2 要解决什么问题？（why？|challenge）"></a>0.2 要解决什么问题？（why？|challenge）</h2><ul>
<li>AutoML只在双线性语义匹配的方法中搜索适合KG的表示方法，这显然不够全面；</li>
<li></li>
</ul>
<h2 id="0-3-用什么方法解决？（how？）"><a href="#0-3-用什么方法解决？（how？）" class="headerlink" title="0.3 用什么方法解决？（how？）"></a>0.3 用什么方法解决？（how？）</h2><ul>
<li>提出了一种搜索方法，但是感觉具体的如何搜索并没有讲明白。</li>
</ul>
<h2 id="0-4文章有什么创新？"><a href="#0-4文章有什么创新？" class="headerlink" title="0.4文章有什么创新？"></a>0.4文章有什么创新？</h2><ul>
<li>把搜索空间变为连续空间提高算法效率；</li>
<li></li>
</ul>
<h2 id="0-5-效果如何？"><a href="#0-5-效果如何？" class="headerlink" title="0.5 效果如何？"></a>0.5 效果如何？</h2><h2 id="0-6-还存在什么问题？"><a href="#0-6-还存在什么问题？" class="headerlink" title="0.6 还存在什么问题？"></a>0.6 还存在什么问题？</h2><ul>
<li>感觉没读懂，文中好多细节描述不够清晰</li>
<li>每一层是怎么选择用哪一种方法？</li>
<li>g函数分解候选操作还有权重又是什么？</li>
</ul>
<h1 id="1-背景知识"><a href="#1-背景知识" class="headerlink" title="1 背景知识"></a>1 背景知识</h1><p>我觉得在读这篇之前可能需要读一下两篇论文：</p>
<ul>
<li><p>Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2018. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055 (2018).</p>
</li>
<li><p>Yongqi Zhang, Quanming Yao, Wenyuan Dai, and Lei Chen. 2020. AutoSF:</p>
<p>Searching Scoring Functions for Knowledge Graph Embedding. In ICDE. IEEE.</p>
</li>
</ul>
<h1 id="2-模型"><a href="#2-模型" class="headerlink" title="2 模型"></a>2 模型</h1><p><img src="http://image.nysdy.com/20200921153104.png" alt></p>
<p>如图1所示，作者的的NAS框架包含两个搜索模块。 <strong>表示搜索模块</strong>旨在通过多个表示层分别优化头部，关系和尾部的嵌入$e_{h,} e_{r}, e_{t}$。 <strong>得分函数搜索模块</strong>负责选择浅层架构，以计算输入三元组的合理性得分。</p>
<h2 id="2-1-表示搜索模块"><a href="#2-1-表示搜索模块" class="headerlink" title="2.1 表示搜索模块"></a>2.1 表示搜索模块</h2><p>该模块中，作者包含了卷积运算符和翻译运算符，分别对应了两种主流的reconstruction-based 链接预测模型。</p>
<p>图1中所示，每个操作都是一个融合步骤，作者以头实体嵌入为例定义了融合步骤如下：</p>
<script type="math/tex; mode=display">
\begin{aligned} \boldsymbol{e}_{\boldsymbol{h}}^{l+1} &=\beta_{\boldsymbol{h}}^{l} \boldsymbol{e}_{\boldsymbol{h}}^{\boldsymbol{l}}+\left(1-\beta_{h}^{l}\right) \operatorname{opt}\left(\boldsymbol{e}_{r}^{l}, \boldsymbol{e}_{t}^{l}\right) \\ \beta_{h}^{l} &=\sigma\left(\boldsymbol{W}_{h}^{l}\left[\boldsymbol{e}_{h}^{l} ; \operatorname{opt}\left(\boldsymbol{e}_{r}^{l}, \boldsymbol{e}_{t}^{l}\right)\right]+b_{h}^{l}\right) \end{aligned}</script><ul>
<li>其中$e_{h}^{l}$是第l层的头嵌入输出，$\operatorname{opt}(\cdot, \cdot)$表示可搜索的运算符，$W_{h}^{l} \text { and } b_{h}^{l}$是可训练的参数。</li>
</ul>
<p>引入融合步骤的目的是迫使新的嵌入集中在有关头部的信息上，而不是应该通过关系或尾部嵌入进行建模的事物。同样，该方法可以应用到关系和尾实体。</p>
<p>作者把候选运算符分为3类：</p>
<ul>
<li><p>卷积运算符</p>
<script type="math/tex; mode=display">
\boldsymbol{e}_{h}^{l+1}=\operatorname{ReLU}\left(\operatorname{Conv} 1 \mathrm{d}\left(\left[\boldsymbol{e}_{r}^{l} ; \boldsymbol{e}_{t}^{l}\right]\right)\right), \boldsymbol{e}_{h}^{l+1}=\operatorname{ReLU}\left(\operatorname{Conv} 2 \mathrm{d}\left(\left[\overline{\boldsymbol{e}}_{r}^{l} ; \overline{\boldsymbol{e}}_{t}^{l}\right]\right)\right)</script><ul>
<li>$[\cdot ; \cdot]$代表row-wise 连接，$\overline{\boldsymbol{e}}$表示$e$的2D重建形状。</li>
</ul>
</li>
<li><p>翻译运算符</p>
<script type="math/tex; mode=display">
g_{r, 1}\left(e_{h}\right)+e_{r}-g_{r, 2}\left(e_{t}\right)=0</script><ul>
<li>$g_{r,\cdot} (x)=\mathbf{W} \mathbf{x}$, 其中$\mathbf{W}$可以是单位矩阵或者无限制矩阵，这样就分别对应了TransE和TransR的方法。</li>
</ul>
</li>
<li><p>本身运算符</p>
<p>身份运算符无需任何转换即可直接将输入映射到输出。 引入这种运算符的目的是为了防止生成的体系结构过于复杂，并它能够使最终模型在得分函数搜索空间中退化为基本模型。</p>
</li>
</ul>
<h2 id="2-2-分数函数搜索模块"><a href="#2-2-分数函数搜索模块" class="headerlink" title="2.2 分数函数搜索模块"></a>2.2 分数函数搜索模块</h2><p>在此模块中，作者在几个基于语义匹配的模型中搜索以产生最终的合理性得分，该得分用于预测三元组是否有效。</p>
<ul>
<li><p>基于卷积的分数函数：ConvKB</p>
<ul>
<li><script type="math/tex; mode=display">
f(h, r, t)=W\left(\operatorname{ReLU}\left(\operatorname{Conv}\left(\left[e_{h} ; e_{r} ; e_{t}\right]\right)\right)\right)</script></li>
</ul>
</li>
<li><p>基于翻译的分数函数：TransE</p>
<script type="math/tex; mode=display">
f(h, r, t)=\left\|e_{h}+e_{r}-e_{t}\right\|_{p}</script></li>
<li><p>双线性分数函数：DistMult，SimplE</p>
<script type="math/tex; mode=display">
\begin{array}{c}f(h, r, t)=\boldsymbol{e}_{h}^{T} \boldsymbol{M}_{\boldsymbol{r}} \boldsymbol{e}_{\boldsymbol{t}} \\ f(h, r, t)=1 / 2\left(\boldsymbol{e}_{h}^{T} \boldsymbol{M}_{r}, \boldsymbol{e}_{t}+\boldsymbol{e}_{h}^{T}, \boldsymbol{M}_{r}^{\prime}, \boldsymbol{e}_{t}\right)\end{array}</script></li>
<li><p>MLP分数函数</p>
<p>使用一个单一的隐藏层来处理三元组。</p>
</li>
</ul>
<h2 id="2-3-NASE搜索过程"><a href="#2-3-NASE搜索过程" class="headerlink" title="2.3 NASE搜索过程"></a>2.3 NASE搜索过程</h2><p>作者把$k$个候选操作组合起来，形成一个函数如下：</p>
<script type="math/tex; mode=display">
g=\sum_{i=1}^{k} a_{i} \varphi_{i}(x), \quad a_{i}=\frac{\exp \left(\alpha_{i}\right)}{\sum_{i=1}^{k} \exp \left(\alpha_{i}\right)}</script><ul>
<li>这里$g$代表了在表示搜索模块，每一层中应用的运算操作，也表示每层中的运算可以由多种操作同时进行然后给以不同权重作为选择过程。</li>
<li>g还可以代表在分数函数搜索时的函数选择。</li>
</ul>
<blockquote>
<p>但是这里就又问题了，要是按照上面来说这是组合也不是选择了呀。</p>
</blockquote>
<p>整个损失函数：</p>
<script type="math/tex; mode=display">
\mathcal{L}=-\frac{1}{N} \sum_{i=1}^{N}\left(y_{i} \log (f(\cdot))+\left(1-y_{i}\right)(1-\log (f(\cdot)))\right.</script><h1 id="3-实验"><a href="#3-实验" class="headerlink" title="3 实验"></a>3 实验</h1><p>常规实验，感觉没啥可以细讲的。</p>
<p><img src="http://image.nysdy.com/20200921202107.png" alt></p>
<p><img src="http://image.nysdy.com/20200921202135.png" alt></p>
<p>这个图就没咋懂，为啥三个颜色用的是不同的函数，这个是怎么选择出来的，论文中并没有解释。</p>
<p><img src="http://image.nysdy.com/20200921202312.png" alt></p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul>
<li></li>
</ul>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>Link Prediction</tag>
        <tag>Knowledge Representation</tag>
        <tag>没读懂</tag>
      </tags>
  </entry>
  <entry>
    <title>Neural Relation Extraction with Selective Attention over Instances阅读笔记</title>
    <url>/post/Neural_Relation_Extraction_with_Selective_Attention_over_Instances/</url>
    <content><![CDATA[<blockquote>
<p>这篇文章之前看过😂。</p>
<p>论下载地址</p>
</blockquote>
<a id="more"></a>
<h1 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h1><p>​    Distant supervision inevitably accompanies with the wrong labelling problem, and thse noisy data will substantially hurt the performance of relation extraction.</p>
<h1 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h1><ul>
<li>As compared to existing neural relation extraction model, our model can make full use of all informative sentences of each entity pair.</li>
<li>To address the wrong labelling problem in distant supervision, we propose selective attention to de-emphasize those noisy instances.</li>
<li>In the experiments, we show that selective attention is beneficial to two kinds of CNN models in the task of relation extraction.</li>
</ul>
<h1 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h1><p>模型整体架构如下所示：</p>
<p><img src="http://image.nysdy.com/20190626156153649268323.jpg" alt="20190626156153649268323.jpg"></p>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>RE</tag>
      </tags>
  </entry>
  <entry>
    <title>Neo4j初始化节点显示设置</title>
    <url>/post/Neo4j_initializes_the_node_display_Settings/</url>
    <content><![CDATA[<h3 id="问题描述："><a href="#问题描述：" class="headerlink" title="问题描述："></a>问题描述：</h3><p>neo4j中有默认的初始化节点显示设置为300个节点，如果想要显示的节点多于300个，则会只显示300个，并给予以下提示语句：</p>
<p><code>Not all return nodes are being displayed due to Initial Node Display setting. Only 300 of 300 nodes are being displayed.</code></p>
<h3 id="解决方法："><a href="#解决方法：" class="headerlink" title="解决方法："></a>解决方法：</h3><p>在如图所示<code>initial Node Display</code>处可以修改，在此处修改为300000.<img src="https://i.loli.net/2018/10/10/5bbdb388a368b.jpg" alt></p>
]]></content>
      <categories>
        <category>Neo4j</category>
      </categories>
      <tags>
        <tag>Neo4j</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>Modeling Relational Data with Graph Convolutional Networks 阅读笔记</title>
    <url>/post/Modeling%20Relational%20Data%20with%20Graph%20Convolutional%20Networks/</url>
    <content><![CDATA[<h1 id="1-总览"><a href="#1-总览" class="headerlink" title="1. 总览"></a>1. 总览</h1><h2 id="要解决什么问题？（what？）"><a href="#要解决什么问题？（what？）" class="headerlink" title="要解决什么问题？（what？）"></a>要解决什么问题？（what？）</h2><p>链接预测和实体分类</p>
<h2 id="为什么要解决这个问题？（why？）"><a href="#为什么要解决这个问题？（why？）" class="headerlink" title="为什么要解决这个问题？（why？）"></a>为什么要解决这个问题？（why？）</h2><p>因为虽然知识图谱用途很多，而现有的知识图谱都存在不完整的问题。</p>
<h2 id="用什么方法解决？（how？）"><a href="#用什么方法解决？（how？）" class="headerlink" title="用什么方法解决？（how？）"></a>用什么方法解决？（how？）</h2><ol>
<li>用图卷积网络和因式分解相结合来解决链接预测问题；</li>
<li>用图卷积网络单独解决实体分类问题。</li>
<li>用权重矩阵的基本分解和块分解来解决参数随着关系数量增加而迅速增长的问题。</li>
</ol>
<h2 id="文章有什么创新？"><a href="#文章有什么创新？" class="headerlink" title="文章有什么创新？"></a>文章有什么创新？</h2><ol>
<li>首次把GCN引入关系数据建模；</li>
<li>提出了一种参数共享和增强稀疏限制的方法——权重矩阵的基本分解和块分解</li>
<li>用GCN与因式分解组成auto-encoder的方法，可以提高因式分解模型在链接预测上的效果。</li>
</ol>
<h2 id="效果如何？"><a href="#效果如何？" class="headerlink" title="效果如何？"></a>效果如何？</h2><p>在FB15k-237上高出baseline 29.8%</p>
<h2 id="还存在什么问题？"><a href="#还存在什么问题？" class="headerlink" title="还存在什么问题？"></a>还存在什么问题？</h2><p>模型并没有优于基于CNN的模型</p>
<h1 id="2-模型R-GCN"><a href="#2-模型R-GCN" class="headerlink" title="2 模型R-GCN"></a>2 模型R-GCN</h1><p><img src="http://image.nysdy.com/20200823150715.png" alt></p>
<h2 id="2-1-Relational-graph-convolutional-networks"><a href="#2-1-Relational-graph-convolutional-networks" class="headerlink" title="2.1 Relational graph convolutional networks"></a>2.1 Relational graph convolutional networks</h2><p>作者基于消息传递框架：</p>
<script type="math/tex; mode=display">
h_{i}^{(l+1)}=\sigma\left(\sum_{m \in \mathcal{M}_{i}} g_{m}\left(h_{i}^{(l)}, h_{j}^{(l)}\right)\right)</script><ul>
<li>其中$h_{i}^{(l)} \in \mathbb{R}^{d^{(l)}}$是节点$v_i$在第$l$层神经网络的隐藏状态；$d^{(l)}$是当前层表示的维度；$g_m$是传入消息的累积形式。</li>
</ul>
<p>设计了一个在关系多图中的传播策略：</p>
<script type="math/tex; mode=display">
h_{i}^{(l+1)}=\sigma\left(\sum_{r \in \mathcal{R}} \sum_{j \in \mathcal{N}_{i}^{r}} \frac{1}{c_{i, r}} W_{r}^{(l)} h_{j}^{(l)}+W_{0}^{(l)} h_{i}^{(l)}\right)</script><ul>
<li>其中$\mathcal{N}_{i}^{r}$代表在关系$r \in \mathcal{R}$下节点$i$的邻居索引集合。其中$c_{i, r}$是一个特定于问题的归一化常数，可以预先学习或选择$c_{i, r}=\left|\mathcal{N}_{i}^{r}\right|$。$W_{0}^{(l)} h_{i}^{(l)}$是作者添加的对于每个节点的一个特定关系的自连接。</li>
<li>此处采取简单的线性消息转换，其实<strong>可以选择更灵活的函数，如多层神经网络(以牺牲计算效率为代价)</strong>。</li>
</ul>
<h2 id="2-2-Regularization-正则化"><a href="#2-2-Regularization-正则化" class="headerlink" title="2.2 Regularization | 正则化"></a>2.2 Regularization | 正则化</h2><p><strong>核心问题：</strong>在处理多元关系数据时，图中参数数量和关系数量快速增长可能会导致对稀有关系的过拟合和模型规模过大。</p>
<p>为了解决这个问题作者提出了两种调整R-GCN层的权重的方式：</p>
<p><strong>basis-decomposition</strong>:</p>
<script type="math/tex; mode=display">
W_{r}^{(l)}=\sum_{b=1}^{B} a_{r b}^{(l)} V_{b}^{(l)}</script><ul>
<li>其中$V_{b}^{(l)} \in \mathbb{R}^{d^{(l+1)} \times d^{(l)}}$是一个作为基础变换的线性组合，$a_{r b}^{(l)}$是一个只依赖于$r$的系数。</li>
<li>基函数分解可以看作是不同关系类型之间有效权重共享的一种形式</li>
</ul>
<p><strong>block-diagonal-decomposition:</strong></p>
<script type="math/tex; mode=display">
W_{r}^{(l)}=\bigoplus_{b=1}^{B} Q_{b r}^{(l)}</script><ul>
<li>其中$Q_{b, r}^{(l)} \in \mathbb{R}^{\frac{d(l+1)}{B} \times \frac{d^{(l)}}{B}}$,$W_{r}^{(l)}$是对角块矩阵：$\operatorname{diag}\left(Q_{1 r}^{(l)}, \ldots, Q_{B r}^{(l)}\right)$。</li>
<li>而块分解可以看做是每种关系类型对权重矩阵的系数约束。</li>
<li>块分解结构编码一种直觉，即可以将潜在特征分组为变量集，这些变量集在组内比组间更加紧密地联系。</li>
</ul>
<p>同时，作者期望基本参数化可以减轻稀疏关系的过度拟合，因为稀疏关系和更频繁关系之间共享参数更新。</p>
<p>对所有的R-GCN模型都采取以下形式：</p>
<ul>
<li>按照作者提出的传播模型进行堆叠$L$层</li>
<li>如果不存在其他特征，则可以将第一层的输入选择为图中每个节点的唯一one-hot向量；</li>
<li>对于块表示，作者将one-hot向量通过一个单一的线性转换为一个dense表示；</li>
<li>作者只考虑用一个featureless的方法，不同于GCN模型。</li>
</ul>
<h1 id="3-实体分类"><a href="#3-实体分类" class="headerlink" title="3 实体分类"></a>3 实体分类</h1><p><img src="http://image.nysdy.com/20200823152704.png" alt></p>
<p>作者通过堆叠R-GCN的传播函数，最后一层通过softmax函数输入，在所有节点上采用交叉熵损失（忽略无标签节点）：</p>
<script type="math/tex; mode=display">
\mathcal{L}=-\sum_{i \in \mathcal{Y}} \sum_{k=1}^{K} t_{i k} \ln h_{i k}^{(L)}</script><ul>
<li>其中 $\mathcal{Y}$代表有标签的节点索引集，$h_{i k}^{(L)}$代表第$i$个标记节点的网络输出的第$k$个条目，$t_{ik}$表示真实标签。</li>
</ul>
<h1 id="4-链接预测"><a href="#4-链接预测" class="headerlink" title="4 链接预测"></a>4 链接预测</h1><p><img src="http://image.nysdy.com/20200823152727.png" alt></p>
<p>作者引入一个auto-encoder模型：</p>
<ul>
<li>encoder：将每个实体$v_{i} \in \mathcal{V}$ 映射到一个是指向量 $\vec{e}_{i} \in \mathbb{R}^{d}$ ；</li>
<li>decoder：根据顶点表示重建图的边：通过一个scorce函数 $s: \mathbb{R}^{d} \times \mathcal{R} \times \mathbb{R}^{d} \rightarrow \mathbb{R}$ 来将三元组(subject, relation, object)映射为一个实数分数。</li>
</ul>
<p>在本文中，作者采用的是DistMult函数作为解码器，每一个关系$r$被映射为一个对角矩阵 $R_{r} \in \mathbb{R}^{d \times d}$，一个三元组(s,r,o)的分数为：</p>
<script type="math/tex; mode=display">
f(s, r, o)=e_{s}^{T} R_{r} e_{o}</script><p>最后作者得出自己的损失函数：</p>
<script type="math/tex; mode=display">
\mathcal{L}=-\frac{1}{(1+w)|\hat{\mathcal{E}}|} \sum_{(s, r, o, y) \in \mathcal{T}} y \log l(f(s, r, o))+(1-y) \log (1-l(f(s, r, o)))</script><ul>
<li>其中 $\mathcal{T}$ 是真实三元组和破坏得到的负样例三元组的总样本。</li>
</ul>
<h1 id="5-实验"><a href="#5-实验" class="headerlink" title="5 实验"></a>5 实验</h1><p><img src="http://image.nysdy.com/20200823154310.png" alt="dataset"></p>
<h2 id="5-1-实体分类实验"><a href="#5-1-实体分类实验" class="headerlink" title="5.1 实体分类实验"></a>5.1 实体分类实验</h2><blockquote>
<p>数据集：AIFB、MUTAG、BGS、AM</p>
<p>baseline：Feat、WL、RDF2Vec</p>
<p>评价准则：准确率</p>
</blockquote>
<p>结果：<img src="http://image.nysdy.com/20200823154454.png" alt></p>
<h2 id="5-2-链接预测"><a href="#5-2-链接预测" class="headerlink" title="5.2 链接预测"></a>5.2 链接预测</h2><p><img src="http://image.nysdy.com/20200823154557.png" alt="dataset"></p>
<blockquote>
<p><strong>关系抽取实验</strong>：</p>
<p>数据集：WordNet(WN18)，Freebase(FB15K)</p>
<p>baseline：LinkFeat,DistMult,CP,TransE,HolE,ComplEx</p>
<p>评价准则：MRR(mean reciprocal rank)（Raw，Filtered），Hits @（1，3，10）</p>
</blockquote>
<p>实验结果：</p>
<p><img src="http://image.nysdy.com/20200823154643.png" alt></p>
<ul>
<li>其中 $f(s, r, t)_{\mathrm{R}-\mathrm{GCN+}} = \alpha f(s, r, t)_{\mathrm{R}-\mathrm{GCN}}+(1-\alpha) f(s, r, t)_{\text {DistMult }}$,R-GCN 和 DistMult都是各自训练好的。</li>
</ul>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul>
<li><p><a href="https://blog.csdn.net/yyl424525/article/details/102764903" target="_blank" rel="noopener">https://blog.csdn.net/yyl424525/article/details/102764903</a></p>
</li>
<li><p><a href="https://blog.csdn.net/weixin_35505731/article/details/104581783" target="_blank" rel="noopener">https://blog.csdn.net/weixin_35505731/article/details/104581783</a></p>
</li>
<li><p>论文下载地址：<a href="https://arxiv.org/pdf/1703.06103v4.pdf" target="_blank" rel="noopener">Modeling Relational Data with Graph Convolutional Networks</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>GCN</tag>
        <tag>Link Prediction</tag>
        <tag>ESWC</tag>
        <tag>2018</tag>
        <tag>Entity Classification</tag>
      </tags>
  </entry>
  <entry>
    <title>Ontology reasoning with deep neural networks</title>
    <url>/post/essay/</url>
    <content><![CDATA[<h1 id="Ontology-reasoning-with-deep-neural-networks（基于深度神经网络的本体推理）"><a href="#Ontology-reasoning-with-deep-neural-networks（基于深度神经网络的本体推理）" class="headerlink" title="Ontology reasoning with deep neural networks（基于深度神经网络的本体推理）"></a>Ontology reasoning with deep neural networks（基于深度神经网络的本体推理）</h1><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><blockquote>
<p>本论文属于知识图谱的下游任务，也就是知识图谱的应用的一个例子。这篇论文的方法根据作者描述RRN是第一个基于深度学习的全面本体推理方法。</p>
</blockquote>
<a id="more"></a>
<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><p>获得一个可以在不同的场景进行有效推理的模</p>
<h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>基于机器学习的推理文章通常假设了一个特定的应用案例：自然语言或视觉输入的推理。<br>作者采用一个不同的方法：将正式的推理问题作为起点。<br>对于特定的问题选择：选择一种在表现力与另一方面复杂性之间取得适当平衡的方法通常是明智的。<br>OWL RL<br>本体推理是指一种常见的场景，在这种场景中，用于推理的推理规则（在此上下文中称为本体）与我们寻求推理的事实信息一起指定。  </p>
<p>本体推理是一种非常灵活的工具，它允许对大量不同的场景进行建模，因此满足了我们对适用于各种应用的系统的需求。<br>==首先引出了什么是本质推理，然后进一步阐述为什么要用机器学习==</p>
<p>今天用于推理的大多数KRR形式都植根于符号逻辑,这些方法在实践中会遇到许多问题：例如处理不完整，冲突或不确定数据的困难<br>机器学习模型通常具有高度可扩展性，更能抵抗数据中的干扰，并且即使所提供的形式是错误的也能够提供预测。</p>
<p><strong>作者的目标是通过采用尖端的深度学习技术，目标是在近似于形式方法的高度期望（理论）属性和另一方面利用机器学习的稳健性之间管理平衡行为。</strong>  </p>
<p>对于用于推理的知识图谱：作者采用的是由个体、类和二元关系组成的信息构成，其中个体对应于顶点，关系对应于被标记的有向边缘，类对应于二进制顶点标签。关系是主体和客体之间的关系或者个人和类之间的关系。这与关系学习不同：在关系学习的背景下，知识图通常通过将类视为个人以及将成员视为普通关系来简化。然而，就作者的目的而言，明确区分类和关系是很重要的，因为在用于推理的知识图谱中类和关系可能不同。  </p>
<h3 id="模型总览"><a href="#模型总览" class="headerlink" title="模型总览"></a>模型总览</h3><p><img src="https://i.loli.net/2018/09/04/5b8e474079995.jpg" alt><br>整个模型是以RRN为基础进行构建的，每个RRN都针对特定的本体进行训练。当训练模型应用于一组特定的事实时，它分为如下两个步骤：</p>
<ol>
<li>它为所有的步骤生成矢量表示，也就是嵌入在所考虑数据中出现的个体。</li>
<li>它仅基于这些生成向量计算查询预测</li>
</ol>
<p>在图中，</p>
<ul>
<li>a中它考虑一个事实三元组，并根据数据集重复多次。</li>
<li>b中它每读取一个事实就获取三元组中的个体潜入，并将他们的反馈送入更新层，该层产生已提供的嵌入的更新版本，然后将其存储在前一个版本的位置。 </li>
<li>c中从随机生成的向量开始，逐步更新嵌入，以便对关于它们所代表的个体的事实和推论进行编码。</li>
</ul>
<h3 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h3><p>作者在四个不同的数据集上训练和评估了RRN，其中两个是人工生成的玩具数据集，两个是从现实世界的数据库中提取的。这样做的原因：</p>
<ol>
<li>玩具问题具有很大的优势，即很明显某些推论是多么困难，从而为我们提供了对模型能力的相当好的印象。</li>
<li>在现实环境中评估方法当然是性能不可或缺的衡量标准</li>
</ol>
<p>作者为了评估真实世界数据的RRN模型，还从从两个著名的知识库DBpedia和Claros中提取了数据集。</p>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p><img src="https://i.loli.net/2018/09/04/5b8e6d1edbfdc.jpg" alt></p>
<ol>
<li>RRN能够有效地编码提供的关于类和关系的事实</li>
<li>对于关系的推理，可以看到DBpedia的准确度略低于98.9％，而其他数据集中的可导出关系在所有情况中至少99.6％被正确预测。</li>
<li>可以预测该模型在预测可推断类别方面比在关系方面表现更好，因为大多数这些都是仅依赖于单个三元组的推论。  </li>
</ol>
<p>为了评估作者提出的KRR方法常常遇到的问题，作者进行了如下实验：</p>
<ol>
<li>对于缺少信息的问题，作者随机删除了一个无法通过每个样本的符号推理推断出的事实，并检查模型是否能够正确地重建它。结果：对于DBpedia来说，33.8％的失踪三元组就是这种情况，而对于Claros来说，38.4％被正确预测</li>
<li>对于冲突的问题，作者通过在每个测试样本中随机选择一个事实来测试模型解决冲突的能力，并添加相同的否定版本作为另一个事实。对于DBpedia，RRN正确解决了88.4％的引入冲突，而对于Claros，它甚至达到了96.2％。然而，最重要的是，对于任何一个损坏的数据集，之前报告的总精度都没有下降超过0.9。</li>
</ol>
<p>所有RRN的查询预测都完全基于它为各个数据集中的个体生成的嵌入，这就是为什么仔细研究这样一组嵌入向量是有益的。</p>
<h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><p>本论文属于知识图谱的下游任务，也就是知识图谱的应用的一个例子。这篇论文的方法根据作者描述RRN是第一个基于深度学习的全面本体推理方法。但是具体的操作方法论文中写的比较清晰，感觉自己是理解了。重点就是对于个体的嵌入表示，如果类比的话就是词向量，作者通过不断的处理更新这个词向量，最后通过所获的词向量进行推理。并且从这篇文章中可以看到作者使用的知识图谱和我之前在弄的关系三元组有所区别。</p>
<h3 id="论文下载链接"><a href="#论文下载链接" class="headerlink" title="论文下载链接"></a><a href="https://arxiv.org/abs/1808.07980" target="_blank" rel="noopener">论文下载链接</a></h3>]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
        <tag>深度学习</tag>
        <tag>Ontology</tag>
      </tags>
  </entry>
  <entry>
    <title>Transition-based Knowledge Graph Embedding with Relational Mapping Properties阅读笔记</title>
    <url>/post/Transition-based_Knowledge_Graph_Embedding_with_Relational_Mapping_Properties/</url>
    <content><![CDATA[<blockquote>
<p><a href="https://link.zhihu.com/?target=https%3A//pdfs.semanticscholar.org/0ddd/f37145689e5f2899f8081d9971882e6ff1e9.pdf" target="_blank" rel="noopener">论文下载地址</a></p>
</blockquote>
<a id="more"></a>
<h1 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h1><h3 id="把知识图谱映射到低维向量的原因："><a href="#把知识图谱映射到低维向量的原因：" class="headerlink" title="把知识图谱映射到低维向量的原因："></a>把知识图谱映射到低维向量的原因：</h3><p>符号和离散的存储结构使我们很难利用这些知识来增强其他智能获取的应用程序（例如问答系统），因为许多与AI相关的算法更倾向于进行关于连续数据计算。</p>
<pre><code>文中用ONE-TO-ONE (husband-to-wife), MANY-TO-ONE (children-to-father), ONE-TO- MANY (mother-to-children), MANY-TO-MANY (parents-to-children)
来进行非单一关系的例子觉得很好。
</code></pre><h3 id="以前算法的目标函数和参数复杂度："><a href="#以前算法的目标函数和参数复杂度：" class="headerlink" title="以前算法的目标函数和参数复杂度："></a>以前算法的目标函数和参数复杂度：</h3><p><img src="http://image.nysdy.com/20190901156734544015276.png" alt="20190901156734544015276.png"></p>
<h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><pre><code>理解不了是如何区分1对多关系的？？？？
</code></pre><p>作者将最优函数将通过对应于该关系的预先计算的权重给出每个训练三元组的不同方面。</p>
<p><img src="http://image.nysdy.com/20190902156739285578897.png" alt="20190902156739285578897.png"></p>
<p>实际上，大约只有26.2％的ONE-TO-ONE三元组适合由TransE建模。 另一方面，其余三元组（73.8％）受到影响，如图1左侧所示。</p>
<h3 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h3><p>根据作者的观察，三元组的映射属性在很大程度上取决于它的关系。</p>
<h3 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h3><p>权重是关系特定的，作者为三元组（h，r，t）提出的新评分函数是：</p>
<script type="math/tex; mode=display">
f_{r}(h, t)=w_{\mathbf{r}}\|\mathbf{h}+\mathbf{r}-\mathbf{t}\|_{L_{1} / L_{2}}</script><script type="math/tex; mode=display">
w_{r}=\frac{1}{\log \left(h_{r} p t_{r}+t_{r} p h_{r}\right)}</script><p><img src="http://image.nysdy.com/20190902156739214061518.png" alt="20190902156739214061518.png"></p>
<p>测量关系的映射属性程度的一种简单方法是计算每个不同头部实体的尾部实体的平均数量，反之亦然。</p>
<pre><code>这里还是不理解这个权重的意义在哪里？🧐
</code></pre><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><script type="math/tex; mode=display">
\begin{array}{l}{\mathcal{L}=\min \sum_{(h, r, t) \in \Delta\left(h^{\prime}, r, t^{\prime}\right) \in \Delta_{(h, r, t)}^{\prime}}\left[\gamma+f_{r}(h, t)-f_{r}\left(h^{\prime}, t^{\prime}\right)\right]_{+}} \\ {\text {s.t. } \quad \forall e \in E,\|e\|_{2}=1}\end{array}</script><h4 id="对于-e-2-1-的解释："><a href="#对于-e-2-1-的解释：" class="headerlink" title="对于$|e|_{2}=1$的解释："></a>对于$|e|_{2}=1$的解释：</h4><p>约束位于单位球上的每个实体的原因是为了保证它们可以以<strong>相同的比例更新</strong>，而不是太大或太小而不能满足最佳目标。</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>见论文，没什么好阐述的。</p>
<p>All the codes for the related models can be downloaded from <a href="https://github.com/glorotxa/SME" target="_blank" rel="noopener">https://github.com/glorotxa/SME</a></p>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>KG</category>
      </categories>
      <tags>
        <tag>KGE</tag>
        <tag>2014</tag>
      </tags>
  </entry>
  <entry>
    <title>Triple Trustworthiness Measurement for Knowledge Graph阅读笔记</title>
    <url>/post/Triple%20Trustworthiness%20Measurement%20for%20Knowledge%20Graph/</url>
    <content><![CDATA[<blockquote>
<p>本文提出了一种通过计算triple trustworthiness来评估知识图谱的准确程度的方法。模型利用神经网络综合来自实体（借鉴Resource allocation）、关系（借鉴翻译模型的思想，如TransE）和KG全局（借鉴关系路径，RNN）三个层面的语义和全局信息，输出最后的 trustworthiness作为判断依据。</p>
<p><a href="http://delivery.acm.org/10.1145/3320000/3313586/p2865-jia.pdf?ip=59.64.129.22&amp;id=3313586&amp;acc=OPEN&amp;key=BF85BBA5741FDC6E%2E66A15327C2E204FC%2E4D4702B0C3E38B35%2E6D218144511F3437&amp;__acm__=1558515578_57e0bf75d529cf4656975ffe7da506b9" target="_blank" rel="noopener">下载地址</a></p>
</blockquote>
<a id="more"></a>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>This paper proposed a method for estimating the accuracy of a knowledge graph by computing triple trustworthiness. The model uses neural network to synthesize semantic and global information from three levels: entity(resource allocation), relationship(translation model ideas, such as TransE)m and KG global(relationship path, RNN) and outputting the final trustworthiness as the basis for judgment.</p>
<h1 id="Problem-statement"><a href="#Problem-statement" class="headerlink" title="Problem statement"></a>Problem statement</h1><p>possible noises and conflicts are inevitably intoduced in the process of constructing the KG</p>
<h1 id="research-objective"><a href="#research-objective" class="headerlink" title="research objective"></a>research objective</h1><p>quantify the KG’s semantic correctness and the true degree of the facts expressed</p>
<h1 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h1><ul>
<li>Knowledge graph triple trustworthiness measurement<ul>
<li>use the  triple semantic information and globally inferring information</li>
<li>three levels measurement and an intergration of confidence value</li>
</ul>
</li>
<li>experiment result verified the model valid on large-scale KG Freebase</li>
<li>the KGTtm could be utilized in knowledge graph construction or improvement</li>
</ul>
<h1 id="THE-TRIPLE-TRUSTWORTHINESS-MEASUREMENT-MODEL"><a href="#THE-TRIPLE-TRUSTWORTHINESS-MEASUREMENT-MODEL" class="headerlink" title="THE TRIPLE TRUSTWORTHINESS MEASUREMENT MODEL"></a>THE TRIPLE TRUSTWORTHINESS MEASUREMENT MODEL</h1><p><img src="http://image.nysdy.com/20190521155842605796518.jpg" alt="20190521155842605796518.jpg"></p>
<ul>
<li>Longitudinally, the model can be divided into two level.<ul>
<li>the upper is a pool of multiple trustworthiness estimate cells(estimator)</li>
<li>the output of these Estimator forms the input of lower-level fusion device(Fusioner)</li>
</ul>
</li>
<li>Viewed laterally, three progressive levels   are be considered, as following.</li>
</ul>
<h2 id="Is-there-a-possible-relationship-between-the-entity-pairs"><a href="#Is-there-a-possible-relationship-between-the-entity-pairs" class="headerlink" title="Is there a possible relationship between the entity pairs?"></a>Is there a possible relationship between the entity pairs?</h2><p><img src="http://image.nysdy.com/20190521155842810227816.jpg" alt="20190521155842810227816.jpg"></p>
<p>ResourceRank:</p>
<ul>
<li>The algorithm assumes that the association between entity paires $(h,t)$ will be stronger, and more resource is passed from the  head $h$ through all associated paths to the tail $t$ in a graph</li>
<li>The amount of resource aggregated into $t$ ingeniously indicateds the association strength from $h$ to $t$.</li>
</ul>
<p>As pair $(e_1,e_2)$, there only one directed edge from $e_1$ to $e_2$ in the graph, where the different bandwidth of the edge indicates the number of the multiple relations.</p>
<p>output:</p>
<script type="math/tex; mode=display">
\left\{\begin{array}{c}{u=\alpha\left(W_{1} V+b_{1}\right)} \\ {R R(h, t)=W_{2} u+b_{2}}\end{array}\right.</script><p>Authors constructed a $V$ vector by combining six characteristics.</p>
<ol>
<li>R (t | h); </li>
<li>In-degree of head node ID(h); </li>
<li>Out-degree of head node OD(h); </li>
<li>In-degree of tail node ID(t);</li>
<li>Out-degree of tail node OD(t);</li>
<li>The depth from head node to tail node Dep</li>
</ol>
<p>As for 1. the formula:</p>
<script type="math/tex; mode=display">
R(t | h)=(1-\theta) \sum_{e_{i} \in M_{t}} \frac{R\left(e_{i} | h\right) \cdot B W_{e_{i} t}}{O D\left(e_{i}\right)}+\frac{\theta}{N}</script><ul>
<li>$M_t$is the set of all nodes that have outgoing links to the node $t$, $OD (e_i)$ is the out-degree of the node eiand the $BW_{e_it}$ is the bandwidth from the $e_i$ to $t$.</li>
<li>In order to improve the model fault-tolerance, we assume that the resource fow from each node may directly jump to a random node with the same probability θ</li>
</ul>
<h2 id="Can-the-determined-relationship-r-occur-between-the-entity-pair-h-t"><a href="#Can-the-determined-relationship-r-occur-between-the-entity-pair-h-t" class="headerlink" title="Can the determined relationship $r$ occur between the entity pair $(h,t)$ ?"></a>Can the determined relationship $r$ occur between the entity pair $(h,t)$ ?</h2><p><img src="http://image.nysdy.com/20190522155850919251079.jpg" alt="20190522155850919251079.jpg"></p>
<p>Translation-based energy function (TEF)：depended on TransE</p>
<p>$E(h, r, t)=|\mathbf{h}+\mathbf{r}-\mathbf{t}|$</p>
<p>output:</p>
<script type="math/tex; mode=display">
P(E(h, r, t))=\frac{1}{1+e^{-\lambda\left(\delta_{r}-E(h, r, t)\right)}}</script><h2 id="Can-the-relevant-triples-in-the-KG-infer-that-the-triple-is-trustworthy"><a href="#Can-the-relevant-triples-in-the-KG-infer-that-the-triple-is-trustworthy" class="headerlink" title="Can the relevant triples in the KG infer that the triple is trustworthy?"></a>Can the relevant triples in the KG infer that the triple is trustworthy?</h2><p><img src="http://image.nysdy.com/20190522155851013726520.jpg" alt="20190522155851013726520.jpg"></p>
<p>Reachable paths inference (RPI):</p>
<p>There two challenges to exploit the reachable paths for inferring triple trustworthiness:</p>
<h3 id="reachable-paths-selection"><a href="#reachable-paths-selection" class="headerlink" title="reachable paths selection"></a>reachable paths selection</h3><p>Semantic distance-based path selection<img src="http://image.nysdy.com/2019052215585105592950.jpg" alt="2019052215585105592950.jpg"></p>
<h3 id="Reachable-Paths-Representation"><a href="#Reachable-Paths-Representation" class="headerlink" title="Reachable Paths Representation"></a>Reachable Paths Representation</h3><p>using a RNN to deal with the embeddings of the three elements of each triple in the selected path</p>
<h2 id="Fusing-the-Estimators"><a href="#Fusing-the-Estimators" class="headerlink" title="Fusing the Estimators"></a>Fusing the Estimators</h2><p>a classifer based on a multi-layer perceptron </p>
<h1 id="EXPERIMENTS"><a href="#EXPERIMENTS" class="headerlink" title="EXPERIMENTS"></a>EXPERIMENTS</h1><h2 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h2><p>FB15K</p>
<h2 id="Interpreting-the-Validity-of-the-Trustworthiness"><a href="#Interpreting-the-Validity-of-the-Trustworthiness" class="headerlink" title="Interpreting the Validity of the Trustworthiness"></a>Interpreting the Validity of the Trustworthiness</h2><p><img src="http://image.nysdy.com/20190522155851139148623.jpg" alt="20190522155851139148623.jpg"></p>
<ul>
<li>The left picture shows that the positives examples are mainly concentrated in the upper region, vice versa.</li>
<li>As for the right picture<ul>
<li>only if the value of a triple is higher than the threshold can it be considered trustworthy</li>
<li>shows that the positive examples universally have higher confidence values</li>
</ul>
</li>
</ul>
<h2 id="Comparing-With-Other-Models-on-The-Knowledge-Graph-Error-Detection-Task"><a href="#Comparing-With-Other-Models-on-The-Knowledge-Graph-Error-Detection-Task" class="headerlink" title="Comparing With Other Models on The Knowledge Graph Error Detection Task"></a>Comparing With Other Models on The Knowledge Graph Error Detection Task</h2><p><img src="http://image.nysdy.com/20190522155851269267340.jpg" alt="20190522155851269267340.jpg"></p>
<p>Authors’ model has beter results in terms of accuracy and the F1-score than the other models.</p>
<h2 id="Analyzing-the-ability-of-models-to-tackle-the-three-type-noises"><a href="#Analyzing-the-ability-of-models-to-tackle-the-three-type-noises" class="headerlink" title="Analyzing the ability of models to tackle the three type noises."></a>Analyzing the ability of models to tackle the three type noises.</h2><p><img src="http://image.nysdy.com/20190522155851290149973.jpg" alt="20190522155851290149973.jpg"></p>
<ul>
<li>a higher recall shows that authors’ model can more accurately find the right from noisy triples</li>
<li>higher average trustworthiness values show that authors’ model can better identify the correct instances and with high confidence </li>
<li>the worst among the $(h, ?, t)$, because the various relations between a certain entity  increase the difficulty of model judgment.</li>
</ul>
<h2 id="Analyzing-the-Efects-of-Single-Estimators"><a href="#Analyzing-the-Efects-of-Single-Estimators" class="headerlink" title="Analyzing the Efects of Single Estimators"></a>Analyzing the Efects of Single Estimators</h2><p><img src="http://image.nysdy.com/20190522155851337652153.jpg" alt="20190522155851337652153.jpg"></p>
<p>It can be found that the accuracy obtained by each model is above 0.8, which proves the effectiveness of each Estimator</p>
<h1 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h1><p>本文在方法上几乎没有什么创新，本质上就是一个老方法的多个组合。最大亮点就是作者能提出trustworthiness来把这个评价知识图谱准确度的问题进行了量化。这种能力比提出方法上的创新更加厉害，也是需要学习的地方。</p>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>KG</tag>
        <tag>Knowledge Graph</tag>
        <tag>Triple</tag>
      </tags>
  </entry>
  <entry>
    <title>Shared Embedding Based Neural Networks for Knowledge Graph Completion阅读笔记</title>
    <url>/post/Shared%20Embedding%20Based%20Neural%20Networks%20for%20Knowledge%20Graph%20Completion/</url>
    <content><![CDATA[<blockquote>
<p><a href="http://delivery.acm.org/10.1145/3280000/3271704/p247-guan.pdf?ip=59.64.129.243&amp;id=3271704&amp;acc=ACTIVE%20SERVICE&amp;key=BF85BBA5741FDC6E%2E66A15327C2E204FC%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1555657159_db1582f1a6ea923a16011064e5cc7955" target="_blank" rel="noopener">原文下载链接</a>，知识图谱补全（KGC，Knowledge Graph Completion)是一种自动建立图谱内部知识关联的工作。目标是补全知识图谱中三元组的缺失部分。主要方法为基于张量（或者矩阵）和基于翻译两类。在本文中，作者提出了一种基于共享嵌入的神经网络的模型（SENN）来处理KGC。</p>
</blockquote>
<a id="more"></a>
<h2 id="Contribulation"><a href="#Contribulation" class="headerlink" title="Contribulation"></a>Contribulation</h2><ul>
<li>提出了SENN模型，该模型明确区分头实体、关系和为实体预测任务，并把它们整合到一个基于全连接神经网络框架中，该框架共享的实体和关系嵌入。</li>
<li>SENN提出了一个自适应全中损失机制，该方法可以很好的处理具有不同映射属性的三元组，并处理不同的预测任务。</li>
<li>由于关系预测通常比头尾实体预测具有更好的性能，我们把SENN应用到头尾实体预测，从而将SENN扩展到SENN+。</li>
</ul>
<h2 id="Related-works"><a href="#Related-works" class="headerlink" title="Related works"></a>Related works</h2><h3 id="Tensor-Matrix-Based-Methods"><a href="#Tensor-Matrix-Based-Methods" class="headerlink" title="Tensor/Matrix Based Methods"></a>Tensor/Matrix Based Methods</h3><p>RESCAL是一个典型的方法，该方法基于三向张量因子分解的方法。</p>
<p>目标函数为：<img src="http://image.nysdy.com/20190419155565817094503.png" alt="20190419155565817094503.png"></p>
<p>$M_r$是r的关系矩阵，大小为k x k。</p>
<p>ComlEx是最近提出的方法，该方法基于矩阵分解，并且它使用复数值来定义实体和关系的嵌入。</p>
<p>目标函数为：<img src="http://image.nysdy.com/20190419155565837140789.png" alt="20190419155565837140789.png"></p>
<p>Re(x)返回x的实部。</p>
<h3 id="Translation-Based-Methods"><a href="#Translation-Based-Methods" class="headerlink" title="Translation Based Methods"></a>Translation Based Methods</h3><p>代表模型为经典的TransE模型（这里不再赘述）</p>
<h3 id="Translation-Based-Methods-1"><a href="#Translation-Based-Methods-1" class="headerlink" title="Translation Based Methods"></a>Translation Based Methods</h3><p>ER-MLP使用多层感知器来捕获头实体，关系和尾实体之间的隐式交互。</p>
<p>目标函数为：<img src="http://image.nysdy.com/2019041915556586361053.png" alt="2019041915556586361053.png"></p>
<p>ProjE使用具有组合层和投影层的神经网络来对头尾实体预测建模。</p>
<h2 id="THE-SENN-METHOD"><a href="#THE-SENN-METHOD" class="headerlink" title="THE SENN METHOD"></a>THE SENN METHOD</h2><p>模型结构如图所示：<img src="http://image.nysdy.com/20190419155565880062506.png" alt="20190419155565880062506.png"></p>
<p>作者将框架划分为以下四个部分：</p>
<ol>
<li>三元组的批量预处理</li>
<li>知识图谱的Shared embeddings表示学习</li>
<li>独立的头尾实体及关系预测子模型训练与融合</li>
<li>联合损失函数构成</li>
</ol>
<p>整个KGC的流程可以描述如下：</p>
<ol>
<li>将训练数据中的完整三元组（知识图谱）划分批量后作为模型的输入</li>
<li>对于输入的三元组，分别训练得到实体（包括头尾实体）嵌入矩阵与关系嵌入矩阵（embeddings）</li>
<li>将头尾实体及关系embeddings分别输入到三个预测模型中（头实体预测（?, r, t），关系预测(h, ?, t)，尾实体预测(h, r, ?)）</li>
</ol>
<h3 id="The-Three-Substructures"><a href="#The-Three-Substructures" class="headerlink" title="The Three Substructures"></a>The Three Substructures</h3><p>预测子模型具有相似的结构如下图，模型输入关系向量与实体向量后，进入n层全连接层，得到预测向量，再经过一个sigmoid（或者softmax）层，输出预测标签向量。<img src="http://image.nysdy.com/20190419155565925349707.png" alt="20190419155565925349707.png"></p>
<p>头实体预测目标函数：<img src="http://image.nysdy.com/20190419155565929066802.png" alt="20190419155565929066802.png"></p>
<p>f(x)= max(0,x).</p>
<p>预测标签：<img src="http://image.nysdy.com/20190419155565936981620.png" alt="20190419155565936981620.png"></p>
<p>其它两种与此头实体类似。</p>
<h3 id="Model-Training"><a href="#Model-Training" class="headerlink" title="Model Training"></a>Model Training</h3><h4 id="The-General-Loss-Function"><a href="#The-General-Loss-Function" class="headerlink" title="The General Loss Function"></a>The General Loss Function</h4><p>模型目标标签向量表示为：<img src="http://image.nysdy.com/20190419155565949172586.png" alt="20190419155565949172586.png"></p>
<p>$I_h$是在训练集中给定r和t的所有有效头实体集。</p>
<p>三者的平滑向量表示为：<img src="http://image.nysdy.com/20190419155565967448577.png" alt="20190419155565967448577.png"></p>
<p>三个预测任务的损失函数为：<img src="http://image.nysdy.com/20190419155565972110097.png" alt="20190419155565972110097.png"></p>
<p>总损失函数为：<img src="http://image.nysdy.com/20190419155565974814392.png" alt="20190419155565974814392.png"></p>
<h4 id="The-Adaptively-Weighted-Loss-Mechanism"><a href="#The-Adaptively-Weighted-Loss-Mechanism" class="headerlink" title="The Adaptively Weighted Loss Mechanism."></a>The Adaptively Weighted Loss Mechanism.</h4><p>该方法的动机：</p>
<ul>
<li>在知识图谱中的三元组有4种类型：1-TO-1, 1-TO-M, M-TO-1 and M-TO-M。所以预测在训练集中具有的有效实体/关系越多，它就越不确定。所以作者将对应于头部实体预测，关系预测和尾部实体预测的损失的权重与有效实体的数量相关联。</li>
<li>因为关系预测比实体预测更加容易。所以作者加大对头尾实体的错误预测的惩罚。</li>
</ul>
<p>所以作者得到新的损失函数：<img src="http://image.nysdy.com/20190419155566013038361.png" alt="20190419155566013038361.png"></p>
<p>总损失函数变为：<img src="http://image.nysdy.com/20190419155566016395908.png" alt="20190419155566016395908.png"></p>
<h2 id="THE-SENN-METHOD-1"><a href="#THE-SENN-METHOD-1" class="headerlink" title="THE SENN+METHOD"></a>THE SENN+METHOD</h2><p>作者相信可以进一步利用关系预测的相当好的性能来辅助测试过程中的头部和尾部实体预测。</p>
<p>给定头部预测任务（？，r，t）并假设h是有效的头部实体。 如果我们采用SENN方法来预测h和t之间的关系，即执行关系预测任务（h，？，t），则关系r最有可能具有 预测标签高于其他关系，因此应排名高于其他关系。</p>
<p><img src="http://image.nysdy.com/20190419155566073220975.png" alt="20190419155566073220975.png"></p>
<p>其中Value（x，r）返回对应于关系r的向量x的条目; Rank（x，r）以降序返回对应于关系r的向量x的条目的等级。</p>
<p>最后SENN+种预测标签为：<img src="http://image.nysdy.com/20190419155566089424123.png" alt="20190419155566089424123.png"></p>
<p>其中<img src="http://image.nysdy.com/20190419155566090898589.png" alt="20190419155566090898589.png"></p>
<h2 id="EXPERIMENTS"><a href="#EXPERIMENTS" class="headerlink" title="EXPERIMENTS"></a>EXPERIMENTS</h2><h3 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h3><p><img src="http://image.nysdy.com/20190419155566095994824.png" alt="20190419155566095994824.png"></p>
<h3 id="Entity-Prediction"><a href="#Entity-Prediction" class="headerlink" title="Entity Prediction"></a>Entity Prediction</h3><p><img src="http://image.nysdy.com/20190419155566101939979.png" alt="20190419155566101939979.png"></p>
<p><img src="http://image.nysdy.com/20190419155566104394441.png" alt="20190419155566104394441.png"></p>
<h3 id="Relation-Prediction"><a href="#Relation-Prediction" class="headerlink" title="Relation Prediction"></a>Relation Prediction</h3><p><img src="http://image.nysdy.com/20190419155566106385514.png" alt="20190419155566106385514.png"></p>
<p>论文还进行了共享嵌入和自适应权重损失机制有效性的验证。</p>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul>
<li><a href="http://blog.openkg.cn/%E8%AE%BA%E6%96%87%E6%B5%85%E5%B0%9D-%E9%9D%A2%E5%90%91%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E8%A1%A5%E5%85%A8%E7%9A%84%E5%85%B1%E4%BA%AB%E5%B5%8C%E5%85%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">http://blog.openkg.cn/%E8%AE%BA%E6%96%87%E6%B5%85%E5%B0%9D-%E9%9D%A2%E5%90%91%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E8%A1%A5%E5%85%A8%E7%9A%84%E5%85%B1%E4%BA%AB%E5%B5%8C%E5%85%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</a></li>
</ul>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
        <tag>神经网络</tag>
        <tag>知识图谱补全</tag>
      </tags>
  </entry>
  <entry>
    <title>Universal Representation Learning of Knowledge Bases by Jointly Embedding Instances and Ontological Concepts阅读笔记</title>
    <url>/post/Universal_Representation_Learning_of_Knowledge_Bases_by_Jointly_Embedding_Instances_and_Ontological_Concepts/</url>
    <content><![CDATA[<blockquote>
<p></p>
<p><a href="http://web.cs.ucla.edu/~yzsun/papers/2019_KDD_JOIE.pdf" target="_blank" rel="noopener">论文下载地址</a></p>
</blockquote>
<a id="more"></a>
<h1 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h1><p>Existing KG embedding models merely focus on representing of an ontology view for abstract and commonsense concepts or an instance view for special entities that are instantiated from ontological concepts. </p>
<h1 id="Challenge"><a href="#Challenge" class="headerlink" title="Challenge"></a>Challenge</h1><ul>
<li><strong>mappings difficult</strong> :the semantic mappings from entities to concepts and from relations to meta-relations are complicated and difficult to be precisely captured by any current embedding models</li>
<li><strong>inadequate cross-view links:</strong> the known cross-view links inadequately cover a vast number of entities, which leads to insufficient information to align both views of the KB, and curtails discovering new cross-view links</li>
<li>the scales and topological structures are different in ontological views and instance views</li>
</ul>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p><img src="http://image.nysdy.com/20190722156378173495213.jpg" alt="20190722156378173495213.jpg"></p>
<p>从两种视图来学习表示有以下两点好处：</p>
<ul>
<li>instance embeddings provide detailed and rich information for their corresponding ontological concepts.</li>
<li>a concept embedding provides a high-level summary of its instances, which is extremely helpful when an instance is rarely observed.</li>
</ul>
<h1 id="contribution"><a href="#contribution" class="headerlink" title="contribution"></a>contribution</h1><ul>
<li>a novel KG embedding model named JOIE, which jointly encodes both the ontology and instance views of a KB<ul>
<li>cross-view association model : a novel KG embedding model named JOIE, which jointly encodes both the ontology and instance views of a KB<ul>
<li>cross-view grouping technique : assumes that the two views can be forced into the same embedding space</li>
<li>cross-view transformation technique : enables non-linear transformations from the instance embedding space to the ontology embedding space</li>
</ul>
</li>
<li>intra-view embedding model : characterizes the relational facts of ontology and instance views in two separate embedding spaces<ul>
<li>three state-of-the-art translational or similarity-based relational embedding techniques</li>
<li>hierarchy-aware embedding: based on intra-view non- linear transformations to preserve ontologies hierarchical substructures.</li>
</ul>
</li>
</ul>
</li>
<li>implement two experiments:<ul>
<li>the triple completion task : confirm the effectiveness of JOIE for populating knowledge in both ontology and instance-view KGs, which has significantly outperformed various baseline models.</li>
<li>the entity typing task :  show that JOIE is competent in discovering cross-view links to align the ontology-view and the instance-view KGs.</li>
</ul>
</li>
</ul>
<h2 id="Modeling"><a href="#Modeling" class="headerlink" title="Modeling"></a>Modeling</h2><p><img src="http://image.nysdy.com/2019072215637837731893.jpg" alt="2019072215637837731893.jpg"></p>
<h2 id="Cross-view-Association-Model"><a href="#Cross-view-Association-Model" class="headerlink" title="Cross-view Association Model"></a>Cross-view Association Model</h2><p><img src="http://image.nysdy.com/20190722156378594486796.png" alt="20190722156378594486796.png"></p>
<h3 id="Cross-view-Grouping-CG"><a href="#Cross-view-Grouping-CG" class="headerlink" title="Cross-view Grouping (CG)"></a>Cross-view Grouping (CG)</h3><p>该模型可以视为grouping-based regularization， 假设本体视图KG和实例视图KG可以被嵌入到同一空间中，并强制使实例向量靠近与它相关联的概念向量，如图3(a)所示</p>
<p>定义损失函数如下：</p>
<script type="math/tex; mode=display">
J_{\text { Cross }}^{\mathrm{CG}}=\frac{1}{|\mathcal{S}|} \sum_{(e, c) \in \mathcal{S}}\left[\|\mathbf{c}-\mathbf{e}\|_{2}-\gamma^{\mathrm{CG}}\right]_{+}</script><h3 id="Cross-view-Transformation-CT"><a href="#Cross-view-Transformation-CT" class="headerlink" title="Cross-view Transformation (CT)"></a>Cross-view Transformation (CT)</h3><p>试图在实体嵌入空间和概念空间之间转换信息，如图3(b)所示</p>
<p>定义映射函数，将实例映射到本体视图空间，该映射后向量应该靠近它的相关联概念：</p>
<script type="math/tex; mode=display">
\mathbf{c} \leftarrow f_{\mathrm{CT}}(\mathbf{e}), \forall(e, c) \in \mathcal{S}</script><p>其中，</p>
<script type="math/tex; mode=display">
f_{\mathrm{CT}}(\mathbf{e})=\sigma\left(\mathbf{W}_{\mathrm{ct}} \cdot \mathbf{e}+\mathbf{b}_{\mathrm{ct}}\right)</script><p>整个CT的损失函数为：</p>
<script type="math/tex; mode=display">
J_{\text { Cross }}^{\mathrm{CT}}=\frac{1}{|\mathcal{S}|} \sum_{(e, c) \in \mathcal{S} \atop \wedge\left(e, c^{\prime}\right) \in \mathcal{S}}\left[\gamma^{\mathrm{CT}}+\left\|\mathbf{c}-f_{\mathrm{CT}}(\mathbf{e})\right\|_{2}-\left\|\mathbf{c}^{\prime}-f_{\mathrm{CT}}(\mathbf{e})\right\|_{2}\right]_{+}</script><h2 id="Intra-view-Model"><a href="#Intra-view-Model" class="headerlink" title="Intra-view Model"></a>Intra-view Model</h2><p>该模型的目的：在两个嵌入空间中分别保留KB的每个视图中的原始结构信息。</p>
<h3 id="Default-Intra-view-Model"><a href="#Default-Intra-view-Model" class="headerlink" title="Default Intra-view Model"></a>Default Intra-view Model</h3><p>作者采用三种方式：</p>
<script type="math/tex; mode=display">
\begin{aligned} f_{\text { TransE }}(\mathbf{h}, \mathbf{r}, \mathbf{t}) &=-\|\mathbf{h}+\mathbf{r}-\mathbf{t}\|_{2} \\ f_{\text { Mult }}(\mathbf{h}, \mathbf{r}, \mathbf{t}) &=(\mathbf{h} \circ \mathbf{t}) \cdot \mathbf{r} \\ f_{\text { HolE }}(\mathbf{h}, \mathbf{r}, \mathbf{t}) &=(\mathbf{h} \star \mathbf{t}) \cdot \mathbf{r} \end{aligned}</script><p>损失函数：</p>
<script type="math/tex; mode=display">
J_{\text { Intra }}^{G}=\frac{1}{|\mathcal{G}|} \sum_{(h, r, t) \in \mathcal{G}}\left[\gamma^{\mathcal{G}}+f\left(\mathbf{h}^{\prime}, \mathbf{r}, \mathbf{t}^{\prime}\right)-f(\mathbf{h}, \mathbf{r}, \mathbf{t})\right]_{+}</script><p>intra损失函数：</p>
<script type="math/tex; mode=display">
J_{\text { Intra }}=J_{\text { Intra }}^{\mathcal{G}_{I}}+\alpha_{1} \cdot J_{\text { Intra }}^{\mathcal{G}_{O}}</script><h3 id="Hierarchy-Aware-Intra-view-Model-for-the-Ontology"><a href="#Hierarchy-Aware-Intra-view-Model-for-the-Ontology" class="headerlink" title="Hierarchy-Aware Intra-view Model for the Ontology"></a>Hierarchy-Aware Intra-view Model for the Ontology</h3><p>进一步区分了构成本体层次结构的元关系和视图内模型中的规则语义关系(如“related_to”)。</p>
<p>给定概念对（cl，ch），我们将这种层次结构建模为粗略概念和相关更精细概念之间的非线性变换:</p>
<script type="math/tex; mode=display">
g_{\mathrm{HA}}\left(\mathbf{c}_{h}\right)=\sigma\left(\mathbf{W}_{\mathrm{HA}} \cdot \mathbf{c}_{l}+\mathbf{b}_{\mathrm{HA}}\right)</script><p>损失函数为：</p>
<script type="math/tex; mode=display">
J_{\text { Intra }}^{\mathrm{HA}}=\frac{1}{|\mathcal{T}|} \sum_{\left(c_{l}, c_{h}\right) \in \mathcal{T}}\left[\gamma^{\mathrm{HA}}+\left\|\mathbf{c}_{h}-g\left(\mathbf{c}_{l}\right)\right\|_{2}-\left\|\mathbf{c}_{\mathrm{h}}^{\prime}-g\left(\mathbf{c}_{1}\right)\right\|_{2}\right]_{+}</script><p>故，该部分损失函数为：</p>
<script type="math/tex; mode=display">
J_{\text { Intra }}=J_{\text { Intra }}^{G_{I}}+\alpha_{1} \cdot J_{\text { Intra }}^{\mathcal{G} o \backslash \mathcal{T}}+\alpha_{2} \cdot J_{\text { Intra }}^{\mathrm{HA}}</script><ul>
<li>$J_{\text { Intra }}^{\mathcal{G} o} \backslash \mathcal{T}$: 默认的视图内模型的丢失，该模型仅在具有规则语义关系的三元组上训练</li>
<li>$J_{\text { Intra }}^{\mathrm{HA}}$明确训练三元组与形成本体层次结构的元关系</li>
</ul>
<blockquote>
<p>感觉这部分就是传递关系，类似推理性质的。</p>
<p>没明白两种ontology关系的区分点</p>
</blockquote>
<h2 id="Joint-Training-on-Two-View-KBs"><a href="#Joint-Training-on-Two-View-KBs" class="headerlink" title="Joint Training on Two-View KBs"></a>Joint Training on Two-View KBs</h2><p>联合损失函数：</p>
<script type="math/tex; mode=display">
J=J_{\text { Intra }}+\omega \cdot J_{\text { Cross }}</script><p>作者并不直接更新$J$，而是交替更新$J_{\text { Intra }}^{\mathcal{G}_{I}}, J_{\text { Intra }}^{\mathcal{G} O} \text { and } J_{\text { Cross }}$.</p>
<h1 id="EXPERIMENTS"><a href="#EXPERIMENTS" class="headerlink" title="EXPERIMENTS"></a>EXPERIMENTS</h1><p>具体细节直接见论文</p>
<h2 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h2><p><img src="http://image.nysdy.com/20190722156379991044398.png" alt="20190722156379991044398.png">数据集是作者自己构建的，信息如上图所示。</p>
<h2 id="Case-Study"><a href="#Case-Study" class="headerlink" title="Case Study"></a>Case Study</h2><h3 id="Ontology-Population"><a href="#Ontology-Population" class="headerlink" title="Ontology Population"></a>Ontology Population</h3><p>作者想预测在元关系词表中并不存在的元关系，例如：预测(“Office Holder”, ?r, “Country”)</p>
<p>这里，作者采取的方式是将概念通过之前提到的实体空间到概念空间的映射来进行反映射。然后按照$f_{\mathrm{CT}}^{\mathrm{inv}}\left(\mathbf{c}_{\text { country }}\right)-f_{\mathrm{CT}}^{\mathrm{inv}}\left(\mathbf{c}_{\text { office }}\right)$来在实体嵌入空间进行搜索与之相近的实体间关系。</p>
<p><img src="http://image.nysdy.com/20190722156380029962081.png" alt="20190722156380029962081.png"></p>
<h3 id="Long-tail-entity-typing"><a href="#Long-tail-entity-typing" class="headerlink" title="Long-tail entity typing"></a>Long-tail entity typing</h3><p><img src="http://image.nysdy.com/2019072215638007155223.png" alt="2019072215638007155223.png"></p>
<p>In KGs, the frequency of entities and relations often follow a long-tail distribution (Zipf’s law)</p>
<p>作者抽取了低频次实体进行了训练，发现JOIE模型的效果虽然有下降，但尚在可以接受的程度内。</p>
<h1 id="FUTURE-WORK"><a href="#FUTURE-WORK" class="headerlink" title="FUTURE WORK"></a>FUTURE WORK</h1><ul>
<li>Particularly, instead of optimizing structure loss with triples (first-order neighborhood) locally, we plan to adopt more complex embedding models which leverage information from higher order neighborhood, logic paths or even global knowledge graph structures. </li>
<li>We also plan to explore the alignment on relations and meta-relations like entity-concept.</li>
<li>exploring different triple encoding techniques</li>
<li>Note that we are also aware of the fact that there are more comprehensive properties of relations and meta-relations in the two views such as logical rules of relations and entity types. Incorporating such properties into the learning process is left as future work.</li>
</ul>
<h1 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h1><p>这篇论文和之前跟张老师定的我的论文的思路基本一致，额，有点感觉有点受打击。这篇文章也是该作者博士毕业论文中的一部分，所以应该是这个作者早就有这个思路了，所以也没什么好纠结的。这篇文章也是走的trans的路线，和刘的论文又不一样的思路，但是都是概念本体这类的。其中有一点不一样，就是is_a关系可能两篇论文用的不一样。这篇论文中提到了数据集开源，可是github的链接中并没有数据集。虽然轮文中说他结合了概念和实例的视图，但是其实像刘的论文就已经提出结合了概念和实例的角度了。</p>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>KG</category>
      </categories>
      <tags>
        <tag>KGE</tag>
        <tag>ontology</tag>
      </tags>
  </entry>
  <entry>
    <title>python动态网页爬取之安装docker和splash</title>
    <url>/post/Python_dynamic_web_crawler_installed_docker_and_splash/</url>
    <content><![CDATA[<blockquote>
<p>利用python进行动态网页爬取时，在安装docker和splash时踩过的坑，记录了一下自己的安装过程。用的系统是mac os。</p>
</blockquote>
<a id="more"></a>
<h3 id="安装scrapy-splash"><a href="#安装scrapy-splash" class="headerlink" title="安装scrapy-splash"></a>安装scrapy-splash</h3><ul>
<li>利用pip安装scrapy-splash库：<br><code>$ pip install scrapy-splash</code></li>
</ul>
<h3 id="安装Docker"><a href="#安装Docker" class="headerlink" title="安装Docker"></a>安装Docker</h3><p>==下面👇这样安不下去了==</p>
<ul>
<li><p>如果是Mac的话需要使用brew安装，如下：<code>brew install docker</code>  </p>
<p>报错：</p>
<pre class=" language-shell"><code class="language-shell">Error: Failure while executing; `git config --local --replace-all homebrew.private true` exited with 1.
</code></pre>
<p>解决方法：</p>
<pre class=" language-sh"><code class="language-sh">xcode-select --install
</code></pre>
<p>然后在执行：</p>
<pre class=" language-shell"><code class="language-shell">brew install docker
</code></pre>
<p>再继续：</p>
<p><code>service docker start</code></p>
<p>报错：</p>
<p><code>-bash: service: command not found</code>上网上查一堆乱七八糟的解决方式，该路径啥的，真的不想改路径，怕把其他的改崩了。最后放弃这种方式，如果有兴趣也可以尝试解决。</p>
</li>
</ul>
<p>==尝试如下安装DOCKER方法==</p>
<ol>
<li><p><a href="https://www.docker.com/get-started" target="_blank" rel="noopener">去官网下载</a><img src="https://i.loli.net/2018/10/23/5bcf2ef4c4994.jpg" alt>这种方法下载docker客户端需要从服务器下载，自己电脑下载12k/s，简直慢死了。</p>
</li>
<li><p>拉取镜像(pull the image)：<code>docker pull scrapinghub/splash</code></p>
<p><img src="https://i.loli.net/2018/10/23/5bcf34a4265a2.jpg" alt></p>
</li>
<li><p>用docker运行scrapinghub/splash：</p>
<p><code>docker run -p 8050:8050 scrapinghub/splash</code></p>
<p><img src="https://i.loli.net/2018/10/23/5bcf3578aa04a.jpg" alt></p>
<p>在浏览器中输入<code>localhost:8050</code><img src="https://i.loli.net/2018/10/23/5bcf363bc08d8.jpg" alt></p>
<p>==安装成功==</p>
</li>
</ol>
<h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ul>
<li><a href="http://www.morecoder.com/article/1001249.html" target="_blank" rel="noopener">http://www.morecoder.com/article/1001249.html</a></li>
<li><a href="https://www.jianshu.com/p/e54a407c8a0a" target="_blank" rel="noopener">https://www.jianshu.com/p/e54a407c8a0a</a></li>
</ul>
]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>splash</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Understanding LSTM Networks</title>
    <url>/post/Understanding%20LSTM%20Networks/</url>
    <content><![CDATA[<blockquote>
<p><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">原文链接</a>。这篇文章很好很细的一步一步的分解讲解了LSTM，之前看过一篇翻译的博客，现在自己翻译一遍，感觉对LSTM的认识加深了许多，虽然还是对LSTM中存有一些问题，比如为什么用tanh，sigmoid，为什不采用其他的？，但是看过之后至少对LSTM没有那么畏惧，不觉得过于复杂了。</p>
</blockquote>
<a id="more"></a>
<h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><h2 id="Recurrent-Neural-Networks"><a href="#Recurrent-Neural-Networks" class="headerlink" title="Recurrent Neural Networks"></a>Recurrent Neural Networks</h2><p><img src="https://i.loli.net/2019/01/06/5c31826c20c5f.jpg" alt></p>
<p>循环允许信息从一个网络传入下一个。</p>
<p><img src="https://i.loli.net/2019/01/06/5c3189b3b43e8.jpg" alt></p>
<p>一个循环网络可以被认为是相同网络的多个复制，每一个网络都将信息传递给后继者。这种类似链的性质表明，递归神经网络与序列和列表密切相关。</p>
<h2 id="The-Problem-of-Long-Term-Dependencies"><a href="#The-Problem-of-Long-Term-Dependencies" class="headerlink" title="The Problem of Long-Term Dependencies"></a>The Problem of Long-Term Dependencies</h2><p>RNN的一个吸引力是他们可能能够将先前信息连接到当前任务。</p>
<ul>
<li><p>有时，我们只需要查看最近的信息来执行当前任务。例如：</p>
<blockquote>
<p>If we are trying to predict the last word in “the clouds are in the <em>sky</em>,” we don’t need any further context – it’s pretty obvious the next word is going to be sky.</p>
</blockquote>
<p> 在这种情况下，如果相关信息与待预测地方之间的差距很小，RNN可以学习使用过去的信息。</p>
</li>
<li><p>但是，对于一些情况，我们需要更多的上下文信息。</p>
<blockquote>
<p>Consider trying to predict the last word in the text “I grew up in France… I speak fluent <em>French</em>.”</p>
</blockquote>
<p>这时，相关信息与需要变得非常大的点之间的差距完全有可能。不幸的是，随着差距的扩大，RNN无法学会连接信息。</p>
<blockquote>
<p>The problem was explored in depth by <a href="http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf" target="_blank" rel="noopener">Hochreiter (1991) [German]</a> and <a href="http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf" target="_blank" rel="noopener">Bengio, et al. (1994)</a>, who found some pretty fundamental reasons why it might be difficult.</p>
</blockquote>
</li>
</ul>
<h2 id="LSTM-Networks"><a href="#LSTM-Networks" class="headerlink" title="LSTM Networks"></a>LSTM Networks</h2><blockquote>
<p>LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!</p>
</blockquote>
<p>标准RNNs的重复模块只有一个简单的结构，比如tanh层。</p>
<p><img src="https://i.loli.net/2019/01/06/5c31942fa0852.jpg" alt></p>
<p>LSTMs也有像这种链式结构，但是它的重复模块具有不同的结构。 有四个，而不是一个神经网络层，以一种非常特殊的方式进行交互。</p>
<p><img src="https://i.loli.net/2019/01/06/5c3195013e06a.jpg" alt></p>
<p>基本符号如下：</p>
<p><img src="https://i.loli.net/2019/01/06/5c319541ec43b.jpg" alt></p>
<p>在上图中，每一行都携带一个完整的向量，从一个节点的输出到其他节点的输入。 粉色圆圈表示逐点运算，如矢量加法，而黄色框表示神经网络层。 行合并表示连接，而行分叉表示其内容被复制，副本将转移到不同的位置。</p>
<h2 id="The-Core-Idea-Behind-LSTMs"><a href="#The-Core-Idea-Behind-LSTMs" class="headerlink" title="The Core Idea Behind LSTMs"></a>The Core Idea Behind LSTMs</h2><p><strong>LSTM的关键是单元状态</strong>，水平线贯穿图的顶部。</p>
<p>单元状态有点像传送带。 它直接沿着整个链运行，只有一些微小的线性相互作用。 信息很容易沿着它不变地流动。</p>
<p><img src="https://i.loli.net/2019/01/06/5c31965a2ea97.jpg" alt></p>
<p>LSTM确实能够移除或添加信息到细胞状态，由称为门的结构精心调节。</p>
<p>门是一种可选择通过信息的方式。 它们由Sigmoid神经网络层和逐点乘法运算组成。</p>
<p><img src="https://i.loli.net/2019/01/06/5c3196e0eb9e6.jpg" alt></p>
<p>sigmoid层输出0到1之间的数字，描述每个组件应该通过多少。 值为零意味着“不让任何东西通过”，而值为1则意味着“让一切都通过！”</p>
<p>LSTM具有三个这样的门，用于保护和控制单元状态。</p>
<h2 id="Step-by-Step-LSTM-Walk-Through"><a href="#Step-by-Step-LSTM-Walk-Through" class="headerlink" title="Step-by-Step LSTM Walk Through"></a>Step-by-Step LSTM Walk Through</h2><p>我们的第一步就是确定我们将从单元状态中丢弃的信息。这个决定是由一个称为“遗忘门层”的sigmoid层决定的。它查看$h_{t-1}$和$x_t$，并为单元状态$C_{t-1}$中的每一个数字输出一个介于0和1之间的数字。1代表“完全保留这个”，而0代表“完全舍弃这个”。</p>
<p>让我们回到我们的语言模型示例，试图根据以前的所有单词预测下一个单词。 在这样的问题中，单元状态可能包括当前受试者的性别，因此可以使用正确的代词。 当我们看到一个新主题时，我们想要忘记旧主题的性别。</p>
<p><img src="https://i.loli.net/2019/01/06/5c31997a2ecec.jpg" alt></p>
<p>下一步是确定我们将在单元状态中存储哪些新信息。 这有两个部分。 首先，称为“输入门层”的sigmoid层决定我们将更新哪些值。 接下来，tanh层创建可以添加到状态的新候选值$\tilde{C}_t$的向量。 在下一步中，我们将结合这两个来创建状态更新。</p>
<p>在我们的语言模型的例子中，我们想要将新主题的性别添加到单元格状态，以替换我们忘记的旧主题。</p>
<p><img src="https://i.loli.net/2019/01/06/5c319afb31868.jpg" alt></p>
<p>现在是时候将旧的单元状态$C_{T-1}$更新为新的单元状态$C_t$。 前面的步骤已经决定要做什么，我们只需要实际做到这一点。</p>
<p>我们将旧状态乘以$f_t$，忘记我们之前决定忘记的事情。 然后我们添加$i_t * \tilde{C}_t$。 这是新的候选值，根据我们决定更新每个状态的值来缩放。</p>
<p>在语言模型的情况下，我们实际上放弃了关于旧主题的性别的信息并添加新信息，正如我们在前面的步骤中所做的那样。</p>
<p><img src="https://i.loli.net/2019/01/06/5c319c9f37f63.jpg" alt></p>
<p>最后，我们需要决定我们要输出的内容。 此输出将基于我们的单元状态，但将是过滤版本。 首先，我们运行一个sigmoid层，它决定我们要输出的单元状态的哪些部分。 然后，我们将单元格状态设置为tanh（将值推到介于-1和1之间）并将其乘以sigmoid门的输出，以便我们只输出我们决定的部分。</p>
<p>对于语言模型示例，由于它只是看到一个主题，它可能想要输出与动词相关的信息，以防接下来会发生什么。 例如，它可能输出主语是单数还是复数，以便我们知道动词应该与什么形式共轭，如果接下来的话。</p>
<p><img src="https://i.loli.net/2019/01/06/5c319da831938.jpg" alt></p>
<h2 id="Variants-on-Long-Short-Term-Memory"><a href="#Variants-on-Long-Short-Term-Memory" class="headerlink" title="Variants on Long Short Term Memory"></a>Variants on Long Short Term Memory</h2><p>到目前为止我所描述的是一个非常正常的LSTM。 但并非所有LSTM都与上述相同。 事实上，似乎几乎所有涉及LSTM的论文都使用略有不同的版本。 差异很小，但值得一提的是其中一些。</p>
<blockquote>
<p>One popular LSTM variant, introduced by <a href="ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf" target="_blank" rel="noopener">Gers &amp; Schmidhuber (2000)</a>, is adding “peephole connections.” This means that we let the gate layers look at the cell state.</p>
</blockquote>
<p><img src="https://i.loli.net/2019/01/06/5c319ec053e96.jpg" alt></p>
<p>上面的图表为所有门增加了窥视孔（peephole），但是许多论文会给一些窥视孔而不是其他的。</p>
<p>另一种变化是使用耦合的遗忘和输入门。 我们不是单独决定忘记什么以及应该添加新信息，而是一起做出这些决定。<strong>我们仅仅会当我们在当前位置将要输入时忘记。我们仅仅输入新的值到那些我们已经忘记旧的信息的那些状态</strong> 。</p>
<p><img src="https://i.loli.net/2019/01/06/5c31a06e327f5.jpg" alt></p>
<p>另一个改动较大的变体是 Gated Recurrent Unit (GRU)，这是由 <a href="http://arxiv.org/pdf/1406.1078v3.pdf" target="_blank" rel="noopener">Cho, et al. (2014)</a> 提出。它将遗忘和输入门组合成一个“更新门”。它还合并了单元状态和隐藏状态，并进行了一些其他更改。 由此产生的模型比标准LSTM模型简单，并且越来越受欢迎。</p>
<p><img src="https://i.loli.net/2019/01/06/5c31a1285f207.jpg" alt></p>
<p>这些只是最着名的LSTM变种中的一小部分。 还有很多其他的东西，如 <a href="http://arxiv.org/pdf/1508.03790v2.pdf" target="_blank" rel="noopener">Yao, et al. (2015)</a> 提出的 Depth Gated RNN。 还有一些完全不同的解决长期依赖关系的方法，如 <a href="http://arxiv.org/pdf/1402.3511v1.pdf" target="_blank" rel="noopener">Koutnik, et al. (2014)</a> 提出的 Clockwork RNN。</p>
<p>哪种变体最好？ 差异是否重要？  <a href="http://arxiv.org/pdf/1503.04069.pdf" target="_blank" rel="noopener">Greff, et al. (2015)</a> 对流行变体进行了很好的比较，发现它们几乎完全相同。<a href="http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf" target="_blank" rel="noopener">Jozefowicz, et al. (2015)</a> 测试了超过一万个RNN架构，找到了一些在某些任务上比LSTM更好的架构。</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>早些时候，我提到了人们用RNN取得的显着成果。基本上所有这些都是使用LSTM实现的。对于大多数任务来说，它们确实工作得更好！</p>
<p>作为一组方程写下来，LSTM看起来非常令人生畏。希望，在这篇文章中逐步走过它们使他们更加平易近人。</p>
<p>LSTM是我们用RNN实现的重要一步。很自然地想知道：还有另一个重要的一步吗？研究人员的共同观点是：“是的！下一步是它的注意！“我们的想法是让RNN的每一步都从一些更大的信息集中选择信息。例如，如果您使用RNN创建描述图像的标题，则可能会选择图像的一部分来查看其输出的每个单词。</p>
<p>事实上， <a href="http://arxiv.org/pdf/1502.03044v2.pdf" target="_blank" rel="noopener">Xu, <em>et al.</em>(2015)</a> 做到这一点 - 如果你想探索注意力，这可能是一个有趣的起点！使用注意力已经取得了许多非常令人兴奋的结果，似乎还有更多的事情即将来临……</p>
<p>注意力不是RNN研究中唯一令人兴奋的问题。例如，<a href="http://arxiv.org/pdf/1507.01526v1.pdf" target="_blank" rel="noopener">Kalchbrenner, <em>et al.</em> (2015)</a> 的Grid LSTMs似乎非常有希望。在生成模型中使用RNN工作 - 例如<a href="http://arxiv.org/pdf/1502.04623.pdf" target="_blank" rel="noopener">Gregor, <em>et al.</em> (2015)</a>, <a href="http://arxiv.org/pdf/1506.02216v3.pdf" target="_blank" rel="noopener">Chung, <em>et al.</em> (2015)</a>, 或者 <a href="http://arxiv.org/pdf/1411.7610v3.pdf" target="_blank" rel="noopener">Bayer &amp; Osendorfer (2015)</a>  似乎也很有趣。过去几年对于反复出现的神经网络来说是一个激动人心的时刻，即将到来的那些承诺只会更加如此！</p>
]]></content>
      <categories>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title>allennlp安装踩坑</title>
    <url>/post/allennlp_install/</url>
    <content><![CDATA[<blockquote>
<p>安装allennlp的踩坑之路，踩了不少坑最后选择’Installing from source’的安装方法，排坑后下面方法亲测可用</p>
</blockquote>
<a id="more"></a>
<h2 id="Installing-from-source"><a href="#Installing-from-source" class="headerlink" title="Installing from source"></a>Installing from source</h2><p>安装步骤：</p>
<h3 id="1-下载GitHub文件"><a href="#1-下载GitHub文件" class="headerlink" title="1.下载GitHub文件"></a>1.下载GitHub文件</h3><pre class=" language-shell"><code class="language-shell">git clone https://github.com/allenai/allennlp.git
</code></pre>
<h3 id="2-创建conda环境"><a href="#2-创建conda环境" class="headerlink" title="2.创建conda环境"></a>2.创建conda环境</h3><pre class=" language-shell"><code class="language-shell">conda create -n allennlp python=3.6
</code></pre>
<h3 id="3-激活环境下载依赖文件"><a href="#3-激活环境下载依赖文件" class="headerlink" title="3.激活环境下载依赖文件"></a>3.激活环境下载依赖文件</h3><ul>
<li><p>激活环境</p>
<pre class=" language-shell"><code class="language-shell">source activate allennlp
</code></pre>
</li>
<li><p>进入github上下载的文件夹</p>
</li>
<li><p>下载依赖文件</p>
<pre class=" language-shell"><code class="language-shell">pip install -r requirements.txt
</code></pre>
<p>遇到报错问题，参考下一小节，所欲问题解决。</p>
</li>
</ul>
<h3 id="4-安装allennlp"><a href="#4-安装allennlp" class="headerlink" title="4.安装allennlp"></a>4.安装allennlp</h3><pre class=" language-shell"><code class="language-shell">pip install --editable .
</code></pre>
<h3 id="5-测试"><a href="#5-测试" class="headerlink" title="5.测试"></a>5.测试</h3><pre class=" language-shell"><code class="language-shell">allennlp
</code></pre>
<p>成功后效果如下：</p>
<pre class=" language-shell"><code class="language-shell">$ allennlp
2019-05-22 21:58:42,297 - INFO - pytorch_pretrained_bert.modeling - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
usage: allennlp

Run AllenNLP

optional arguments:
  -h, --help     show this help message and exit
  --version      show program's version number and exit

Commands:

    configure    Run the configuration wizard.
    train        Train a model.
    evaluate     Evaluate the specified model + dataset.
    predict      Use a trained model to make predictions.
    make-vocab   Create a vocabulary.
    elmo         Create word vectors using a pretrained ELMo model.
    fine-tune    Continue training a model on a new dataset.
    dry-run      Create a vocabulary, compute dataset statistics and other
                 training utilities.
    test-install
                 Run the unit tests.
    find-lr      Find a learning rate range.
    print-results
                 Print results from allennlp serialization directories to the
                 console.
</code></pre>
<h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><h3 id="问题1"><a href="#问题1" class="headerlink" title="问题1"></a>问题1</h3><h4 id="报错信息："><a href="#报错信息：" class="headerlink" title="报错信息："></a>报错信息：</h4><p>ERROR: Failed building wheel for jsonnet</p>
<p><img src="http://image.nysdy.com/20190522155853293350470.jpg" alt="20190522155853293350470.jpg"></p>
<h4 id="解决方法："><a href="#解决方法：" class="headerlink" title="解决方法："></a>解决方法：</h4><pre class=" language-shell"><code class="language-shell">conda install -c conda-forge jsonnet
</code></pre>
<h3 id="问题2"><a href="#问题2" class="headerlink" title="问题2"></a>问题2</h3><h4 id="报错信息：-1"><a href="#报错信息：-1" class="headerlink" title="报错信息："></a>报错信息：</h4><p>报的都是某些包的版本问题</p>
<pre class=" language-shell"><code class="language-shell">ERROR: botocore 1.12.152 has requirement urllib3<1.25,>=1.20; python_version >= "3.4", but you'll have urllib3 1.25.2 which is incompatible.
ERROR: aws-sam-translator 1.11.0 has requirement jsonschema~=2.6, but you'll have jsonschema 3.0.1 which is incompatible.
ERROR: cfn-lint 0.20.3 has requirement jsonschema~=2.6, but you'll have jsonschema 3.0.1 which is incompatible.
ERROR: cfn-lint 0.20.3 has requirement requests<=2.21.0,>=2.15.0, but you'll have requests 2.22.0 which is incompatible
</code></pre>
<h4 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h4><p>根据报错信息下载相应安装包即可</p>
<h2 id="问题3"><a href="#问题3" class="headerlink" title="问题3"></a>问题3</h2><h4 id="报错信息：-2"><a href="#报错信息：-2" class="headerlink" title="报错信息："></a>报错信息：</h4><pre class=" language-shell"><code class="language-shell">ImportError: dlopen: cannot load any more object with static TLS
___________________________________________________________________________
Contents of /home/minelab/anaconda3/envs/allennlp/lib/python3.6/site-packages/sklearn/__check_build:
__init__.py               setup.py                  _check_build.cpython-36m-x86_64-linux-gnu.so
__pycache__
___________________________________________________________________________
It seems that scikit-learn has not been built correctly.

If you have installed scikit-learn from source, please do not forget
to build the package before using it: run `python setup.py install` or
`make` in the source directory.

If you have used an installer, please check that it is suited for your
Python version, your operating system and your platform.
</code></pre>
<h4 id="解决方法：-1"><a href="#解决方法：-1" class="headerlink" title="解决方法："></a>解决方法：</h4><p>下载更低版本的scikit-learn,例如</p>
<pre class=" language-shell"><code class="language-shell">pip install scikit-learn=0.20.3
</code></pre>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul>
<li><a href="https://github.com/pytorch/pytorch/issues/10443" target="_blank" rel="noopener">https://github.com/pytorch/pytorch/issues/10443</a></li>
<li><a href="https://github.com/pypa/pip/issues/4330" target="_blank" rel="noopener">https://github.com/pypa/pip/issues/4330</a></li>
</ul>
<h1 id="安装的启示"><a href="#安装的启示" class="headerlink" title="安装的启示"></a>安装的启示</h1><h3 id="环境问题"><a href="#环境问题" class="headerlink" title="环境问题"></a>环境问题</h3><ul>
<li>最基本的就是先去网上查这个错误的解决方法</li>
<li>网上的解决不了的，先猜猜大概率是哪方面的问题。<ul>
<li>比如大概率是各种版本互相之间不适配的问题，那就调试版本，一般都会告诉你哪个有问题，比如上面的scikit-learn问题。</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>install</category>
      </categories>
      <tags>
        <tag>allennlp</tag>
        <tag>包安装</tag>
      </tags>
  </entry>
  <entry>
    <title>pandas的数据类型操作</title>
    <url>/post/Pandas_data_type_manipulation/</url>
    <content><![CDATA[<blockquote>
<p>在<a href="https://juejin.im/post/5acc36e66fb9a028d043c2a5" target="_blank" rel="noopener">原文链接</a>中摘抄出部分信息作为记录形成本文。</p>
</blockquote>
<a id="more"></a>
<h3 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h3><div class="table-container">
<table>
<thead>
<tr>
<th>Pandas dtype</th>
<th>Python 类型</th>
<th>NumPy 类型</th>
<th>用途</th>
</tr>
</thead>
<tbody>
<tr>
<td>object</td>
<td>str</td>
<td>string_, unicode_</td>
<td>文本</td>
</tr>
<tr>
<td>int64</td>
<td>int</td>
<td>int_, int8, int16, int32, int64, uint8, uint16, uint32, uint64</td>
<td>整数</td>
</tr>
<tr>
<td>float64</td>
<td>float</td>
<td>float_, float16, float32, float64</td>
<td>浮点数</td>
</tr>
<tr>
<td>bool</td>
<td>bool</td>
<td>bool_</td>
<td>布尔值</td>
</tr>
<tr>
<td>datetime64</td>
<td>NA</td>
<td>NA</td>
<td>日期时间</td>
</tr>
<tr>
<td>timedelta[ns]</td>
<td>NA</td>
<td>NA</td>
<td>时间差</td>
</tr>
<tr>
<td>category</td>
<td>NA</td>
<td>NA</td>
<td>有限长度的文本值列表</td>
</tr>
</tbody>
</table>
</div>
<h3 id="数据类型操作"><a href="#数据类型操作" class="headerlink" title="数据类型操作"></a>数据类型操作</h3><ul>
<li><p>使用<code>df.dtypes</code>可以显示数据所有列的类型</p>
</li>
<li><p><code>df.info（）</code> 函数可以显示更有用的信息</p>
</li>
</ul>
<h2 id="使用-astype-函数"><a href="#使用-astype-函数" class="headerlink" title="使用 astype() 函数"></a>使用 <code>astype()</code> 函数</h2><h3 id="使用条件"><a href="#使用条件" class="headerlink" title="使用条件"></a>使用条件</h3><ul>
<li>数据是干净的，可以简单地解释为一个数字</li>
<li>你想要将一个数值转换为一个字符串对象</li>
</ul>
<p>如果数据具有非数字字符或它们间不同质（homogeneous），那么 <code>astype()</code> 并不是类型转换的好选择。你需要进行额外的变换才能完成正确的类型转换。</p>
<h3 id="使用方式"><a href="#使用方式" class="headerlink" title="使用方式"></a>使用方式</h3><p>为了真正修改原始 dataframe 中数据类型，记得把 <code>astype()</code> 函数的返回值重新赋值给 dataframe，因为 <code>astype()</code> 仅返回数据的副本而不原地修改。</p>
<h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ul>
<li><a href="https://juejin.im/post/5acc36e66fb9a028d043c2a5" target="_blank" rel="noopener">https://juejin.im/post/5acc36e66fb9a028d043c2a5</a></li>
</ul>
]]></content>
      <categories>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>pandas</tag>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title>《An Automatic Knowledge Graph Construction System for K-12 Education》阅读笔记</title>
    <url>/post/An%20Automatic%20Knowledge%20Graph%20Construction%20System%20for%20K-12%20Education/</url>
    <content><![CDATA[<blockquote>
<p><a href="http://delivery.acm.org/10.1145/3240000/3231698/a40-chen.pdf?ip=59.64.129.211&amp;id=3231698&amp;acc=NO%20RULES&amp;key=BF85BBA5741FDC6E%2E66A15327C2E204FC%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1547779677_7b4dce90b008fa6300a47916eeb139d9" target="_blank" rel="noopener">论文原址</a>。本篇文章主要提出了一个自动化构建数学领域知识图谱的系统，主要应用的事NER和数据挖掘技术，其中NER主要是抽取数学概念，概念间的关系是作者自己构建的（例如先修关系）。对于数据集，作者主要从the Chinese curriculum standards of mathematics上提取的概念实体，从自己的SLP平台上，通过对学生表现来提取关系（把这部分作为数据挖掘）。<strong>本篇文章实际上可以作为构建特定领域的知识图谱的一个参考。</strong></p>
</blockquote>
<a id="more"></a>
<h3 id="challenges"><a href="#challenges" class="headerlink" title="challenges"></a>challenges</h3><ul>
<li>the desired educational concept entities are more abstract than real world entities like PERSON, ORGANIZATION, LOCATION</li>
<li>the desired relations are more cognitive and implicit, so cannot be derived from the literal meanings of text like generic knowledge graphs</li>
</ul>
<h3 id="contributions"><a href="#contributions" class="headerlink" title="contributions"></a>contributions</h3><ul>
<li>a novel but practical system</li>
<li>entity recognition (NER) &amp; association rule mining algorithms</li>
<li>demonstrate an exemplary case with constructing a knowledge graph for the subject of mathematics</li>
</ul>
<h2 id="SYSTEM-OVERVIEW"><a href="#SYSTEM-OVERVIEW" class="headerlink" title="SYSTEM OVERVIEW"></a>SYSTEM OVERVIEW</h2><p><img src="https://i.loli.net/2019/01/19/5c428c37364b4.jpg" alt></p>
<ul>
<li>Educational Concept Extraction Module:</li>
<li>Implicit Relation Identification Module</li>
</ul>
<h2 id="CONCEPT-EXTRACTION"><a href="#CONCEPT-EXTRACTION" class="headerlink" title="CONCEPT EXTRACTION"></a>CONCEPT EXTRACTION</h2><ul>
<li><p>线性链式CRF模型</p>
<p><img src="https://i.loli.net/2019/01/18/5c4141ca00be1.jpg" alt></p>
</li>
<li><p>标签预测</p>
<p><img src="https://i.loli.net/2019/01/18/5c4141ed6ac27.jpg" alt></p>
</li>
</ul>
<h2 id="RELATION-IDENTIFICATION"><a href="#RELATION-IDENTIFICATION" class="headerlink" title="RELATION IDENTIFICATION"></a>RELATION IDENTIFICATION</h2><h3 id="两种方法"><a href="#两种方法" class="headerlink" title="两种方法"></a>两种方法</h3><ul>
<li>support</li>
<li>confidence</li>
</ul>
<blockquote>
<p>From the perspective of prerequisite relation, if concept si is a prerequisite of concept sj, learners who do not master sivery likely do not master sj, and learners who master sjmost likely master si. </p>
<p><img src="https://i.loli.net/2019/01/18/5c41425aafc99.jpg" alt></p>
</blockquote>
<h2 id="EXEMPLARY-CASE-AND-SYSTEM-EVALUATION"><a href="#EXEMPLARY-CASE-AND-SYSTEM-EVALUATION" class="headerlink" title="EXEMPLARY CASE AND SYSTEM EVALUATION"></a>EXEMPLARY CASE AND SYSTEM EVALUATION</h2><h3 id="Concept-Extraction"><a href="#Concept-Extraction" class="headerlink" title="Concept Extraction"></a>Concept Extraction</h3><h4 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h4><p>the Chinese curriculum standards of mathematics published by the ministry of education as the main data source</p>
<h4 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h4><ul>
<li>adopt precision, recall and F1- score </li>
<li>The ground truth is manually labeled by two domain experts.</li>
</ul>
<h3 id="Relation-Identification"><a href="#Relation-Identification" class="headerlink" title="Relation Identification"></a>Relation Identification</h3><h4 id="Dataset-1"><a href="#Dataset-1" class="headerlink" title="Dataset"></a>Dataset</h4><p> students’ performance data collected by our SLP platform.</p>
<h4 id="Evaluation-1"><a href="#Evaluation-1" class="headerlink" title="Evaluation"></a>Evaluation</h4><p>The ground truth of the prerequisite relations between selected 9 concepts are annotated manually by two domain experts.</p>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>NER</tag>
        <tag>知识图谱</tag>
      </tags>
  </entry>
  <entry>
    <title>ubantu系统安装pytorch GPU版本</title>
    <url>/post/ubantu%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85pytorch-GPU%E7%89%88%E6%9C%AC/</url>
    <content><![CDATA[<h1 id="0-准备工作"><a href="#0-准备工作" class="headerlink" title="0 准备工作"></a><strong>0 准备工作</strong></h1><p>用conda安装Pytorch过程中会连接失败，这是因为Anaconda.org的服务器在国外，需要切换到国内镜像源：</p>
<pre class=" language-bash"><code class="language-bash">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/
</code></pre>
<p>设置搜索时显示通道地址</p>
<pre class=" language-bash"><code class="language-bash">conda config --set show_channel_urls <span class="token function">yes</span>
</code></pre>
<p>创建虚拟环境并激活之，在该环境下安装下面的包：</p>
<pre class=" language-bash"><code class="language-bash">conda create -n pytorch_gpu python<span class="token operator">=</span>3.6
<span class="token function">source</span> activate pytorch_gpu
</code></pre>
<h1 id="1-安装显卡驱动"><a href="#1-安装显卡驱动" class="headerlink" title="1 安装显卡驱动"></a>1 安装显卡驱动</h1><blockquote>
<p>先用nvidia-smi进行查询，如果可以查到显卡驱动就不需要安装了。</p>
</blockquote>
<h3 id="1-1-查看显卡硬件型号"><a href="#1-1-查看显卡硬件型号" class="headerlink" title="1.1. 查看显卡硬件型号"></a><strong>1.1. 查看显卡硬件型号</strong></h3><p>在终端输入：<code>ubuntu-drivers devices</code>，可以看到如下界面：</p>
<p><img src="http://image.nysdy.com/20191205157552667542415.png" alt="20191205157552667542415.png"></p>
<p>从上图可以看出，我的显卡是：<code>[GeForce GTX 1080 Ti]</code>，所以推荐安装的版本号是<code>nvidia-driver-435 - distro non-free recommended</code>。</p>
<h3 id="1-2-开始安装"><a href="#1-2-开始安装" class="headerlink" title="1.2. 开始安装"></a><strong>1.2. 开始安装</strong></h3><ul>
<li>如果同意安装推荐版本，那我们只需要终端输入：<code>sudo ubuntu-drivers autoinstall</code> 就可以自动安装了。</li>
<li>当然我们也可以使用 apt 命令安装自己想要安装的版本，比如我想安装 <code>340</code> 这个版本号的版本，终端输入：<code>sudo apt install nvidia-340</code> 就自动安装了。</li>
<li>安装过程中按照提示操作，除非你知道每个提示的真实含义，否则所有的提示都选择默认就可以了，安装完成后重启系统，NVIDIA 显卡就可以正常工作了。安装完成后你可以参照 <code>https://linuxconfig.org/benchmark-your-graphics-card-on-linux</code> 上的介绍测试你的显卡。</li>
<li>最后<code>reboot</code>重启就可以了</li>
</ul>
<h2 id="1-3-查看NVIDIA驱动版本"><a href="#1-3-查看NVIDIA驱动版本" class="headerlink" title="1.3. 查看NVIDIA驱动版本"></a>1.3. 查看NVIDIA驱动版本</h2><p>输入<code>nvidia-smi</code>：显示如下：</p>
<p><img src="http://image.nysdy.com/20200821155338.png" alt></p>
<h1 id="2-安装CUDA"><a href="#2-安装CUDA" class="headerlink" title="2 安装CUDA"></a>2 安装CUDA</h1><h2 id="0-CUDA对应的NVIDIA驱动版本对照表"><a href="#0-CUDA对应的NVIDIA驱动版本对照表" class="headerlink" title="0 CUDA对应的NVIDIA驱动版本对照表"></a>0 CUDA对应的NVIDIA驱动版本对照表</h2><p>一般而言，不同版本的CUDA要求不同的NVIDIA驱动版本,同时显卡驱动版本要不低于CUDA的安装版本，具体的对照关系如下：<a href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html" target="_blank" rel="noopener">官网地址</a></p>
<p><img src="http://image.nysdy.com/20200821155720.png" alt></p>
<h2 id="1-安装CUDA"><a href="#1-安装CUDA" class="headerlink" title="1 安装CUDA"></a><strong>1 安装CUDA</strong></h2><p>按照上述对应表，找到要按照的CUDA版本，比如按照上图来说应该安装9.2版本</p>
<pre class=" language-bash"><code class="language-bash">conda <span class="token function">install</span> cudatoolkit<span class="token operator">=</span>9.2 -n pytorch_gpu -c https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/linux-64/
</code></pre>
<h2 id="2-安装cuDNN"><a href="#2-安装cuDNN" class="headerlink" title="2 安装cuDNN"></a><strong>2 安装cuDNN</strong></h2><p>所安装的cuDNN版本注意和CUDA对应，可以在<a href="https://developer.nvidia.com/rdp/cudnn-archive" target="_blank" rel="noopener">CUDA官网</a>找到版本对应关系：</p>
<pre class=" language-bash"><code class="language-bash">conda <span class="token function">install</span> cudnn<span class="token operator">=</span>7.6.5 -n pytorch_gpu -c https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/linux-64/
</code></pre>
<h2 id="3-找到对应pytorch版本"><a href="#3-找到对应pytorch版本" class="headerlink" title="3 找到对应pytorch版本"></a>3 <strong>找到对应pytorch版本</strong></h2><p>先到官网找到在你的操作系统、包、CUDA版本、语言版本下对应的安装脚本，<a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener">官网地址</a>，直接根据你的实际情况选择Pytorch安装包版本，然后复制页面自动生成的脚本进行安装。</p>
<p><img src="http://image.nysdy.com/20200821164952.png" alt></p>
<p>运行：</p>
<pre class=" language-shell"><code class="language-shell">conda install pytorch torchvision cudatoolkit=9.2 -c pytorch
</code></pre>
<p>如果显示如下错误:</p>
<pre class=" language-shell"><code class="language-shell">CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/pytorch/linux-64/torchvision-0.7.0-py36_cu92.tar.bz2>
Elapsed: -

An HTTP error occurred when trying to retrieve this URL.
HTTP errors are often intermittent, and a simple retry will get you on your way.

CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/pytorch/linux-64/pytorch-1.6.0-py3.6_cuda9.2.148_cudnn7.6.3_0.tar.bz2>
Elapsed: -

An HTTP error occurred when trying to retrieve this URL.
HTTP errors are often intermittent, and a simple retry will get you on your way.
</code></pre>
<p>可以换成下面的方式</p>
<pre class=" language-shell"><code class="language-shell">conda install pytorch torchvision cudatoolkit=9.2 -c https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/
</code></pre>
<h2 id="3-测试"><a href="#3-测试" class="headerlink" title="# 3 测试"></a># <strong>3 测试</strong></h2><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> torch
flag <span class="token operator">=</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>flag<span class="token punctuation">)</span>

ngpu<span class="token operator">=</span> <span class="token number">1</span>
<span class="token comment" spellcheck="true"># Decide which device we want to run on</span>
device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda:0"</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">and</span> ngpu <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>device<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>get_device_name<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
<p>结果：</p>
<pre class=" language-python"><code class="language-python"><span class="token boolean">True</span>
cuda<span class="token punctuation">:</span><span class="token number">0</span>
GeForce GTX <span class="token number">1080</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.9530</span><span class="token punctuation">,</span> <span class="token number">0.4746</span><span class="token punctuation">,</span> <span class="token number">0.9819</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.7192</span><span class="token punctuation">,</span> <span class="token number">0.9427</span><span class="token punctuation">,</span> <span class="token number">0.6768</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.8594</span><span class="token punctuation">,</span> <span class="token number">0.9490</span><span class="token punctuation">,</span> <span class="token number">0.6551</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">)</span>
</code></pre>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ol>
<li><a href="https://blog.csdn.net/baidu_26646129/article/details/88380598" target="_blank" rel="noopener">Anaconda环境安装GPU版本Pytorchanaconda pytorch gpu</a></li>
<li><a href="https://blog.csdn.net/zhw864680355/article/details/90411288" target="_blank" rel="noopener">CUDA对应的NVIDIA驱动版本对照表_zhw864680355的博客-CSDN博客_cuda对应的驱动版本</a></li>
<li><a href="https://blog.csdn.net/weixin_35576881/article/details/89709116?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-5.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-5.channel_param" target="_blank" rel="noopener">pytorch：测试GPU是否可用_明月几时有，把酒问青天-CSDN博客_torch gpu</a></li>
</ol>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>python爬取中文网页时中文字符变英文的解决方法</title>
    <url>/post/solution_of_python_for_Chinese_characters_to_become_English_when_crawling_Chinese_web_pages/</url>
    <content><![CDATA[<blockquote>
<p>使用python的scrapy爬取网页时，源代码中的中文字符在爬取下来后变成了英文字符。</p>
</blockquote>
<a id="more"></a>
<h3 id="问题举例"><a href="#问题举例" class="headerlink" title="问题举例"></a>问题举例</h3><p>例如，原网页为：</p>
<p><img src="https://i.loli.net/2018/10/25/5bd16544a16a3.jpg" alt></p>
<p>爬取结果为：</p>
<p><img src="https://i.loli.net/2018/10/25/5bd165a84336a.jpg" alt></p>
<h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p><strong>修改请求头</strong>：在<code>settings.py</code>文件中找到下属代码：</p>
<pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Override the default request headers:</span>
<span class="token comment" spellcheck="true">#DEFAULT_REQUEST_HEADERS = {</span>
<span class="token comment" spellcheck="true">#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',</span>
<span class="token comment" spellcheck="true">#   'Accept-Language': 'en',</span>
<span class="token comment" spellcheck="true">#}</span>
</code></pre>
<p>改为：</p>
<pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Override the default request headers:</span>
DEFAULT_REQUEST_HEADERS <span class="token operator">=</span> <span class="token punctuation">{</span>
   <span class="token string">'Accept'</span><span class="token punctuation">:</span> <span class="token string">'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'</span><span class="token punctuation">,</span>
   <span class="token string">'Accept-Language'</span><span class="token punctuation">:</span> <span class="token string">'zh-CN'</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span>
</code></pre>
<p>修改结果展示：</p>
<p><img src="https://i.loli.net/2018/10/25/5bd168d7293ce.jpg" alt></p>
<h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ul>
<li><a href="https://blog.csdn.net/wuqili_1025/article/details/79690103" target="_blank" rel="noopener">https://blog.csdn.net/wuqili_1025/article/details/79690103</a></li>
</ul>
]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title>《Bidirectional LSTM-CRF Models for Sequence Tagging》阅读笔记</title>
    <url>/post/read_Bidirectional_LSTM-CRF_Models_for_Sequence_Tagging/</url>
    <content><![CDATA[<blockquote>
<p>这篇论文可以作为一个RNN和LSTM学习的一个例子来看，有利于新手对LSTM的理解。对于NER的处理主要是作为一个序列标注问题。但是作为经典文章还是可以读一读了解一下的。</p>
</blockquote>
<a id="more"></a>
<p>在本篇论文中，作者提出了4种模型：LSTM、BI-LSTM、LSTM-CRF和BI-LSTM-CRF。</p>
<h2 id="contribution-贡献"><a href="#contribution-贡献" class="headerlink" title="contribution(贡献)"></a>contribution(贡献)</h2><ol>
<li>作者在NLP标注数据集上系统的对比了以上四个模型；</li>
<li>作者是首先提出把BI-LSTM-CRF模型用于NLP序列标注，并且达到了state-of-the-art的水平；</li>
<li>作者展示了BI-LSTM-CRF是robust，并且极少依赖于词向量。</li>
</ol>
<h2 id="model-模型"><a href="#model-模型" class="headerlink" title="model(模型)"></a>model(模型)</h2><h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p>首先，作者先介绍了RNN的结构和工作原理，如图：<img src="https://i.loli.net/2018/10/04/5bb5a3e22e140.jpg" alt></p>
<p>其中输入为句子：EU rejects German call to boycott British lamb。输出为标签：B-ORG O B-MISC O O O B-MISC O O，其中B-，I-表示实体开始和中间位置。标签种类为：other (O)和四种实体标签：Person (PER), Location (LOC), Organization (ORG), and Miscellaneous (MISC).</p>
<p>输入层表示在时间步 t 的特征。它们可以是 one-hot-encoding 的词特征，稠密或者稀疏的向量特征。输入层与特征有相同大小的维度。输出层表示在时间步 t 的标签上的概率分布，维度与标注数量相同。相比前馈神经网络，RNN 引入前一个隐藏状态和当前隐藏状态的结合，因此可以储存历史信息。</p>
<p>涉及公式为：<img src="https://i.loli.net/2018/10/04/5bb5a556ad0ab.jpg" alt></p>
<p><img src="https://i.loli.net/2018/10/04/5bb5a57379fd7.jpg" alt></p>
<p>其中，U，W，V都是权重，函数f，g分别为sigmoid和softmax函数。  </p>
<p>接下来，作者展示了LSTM的结构和原理，如图：<img src="https://i.loli.net/2018/10/04/5bb5a61bdb483.jpg" alt></p>
<p>公式：<img src="https://i.loli.net/2018/10/04/5bb5a670e3b88.jpg" alt></p>
<p>其中，σ是逻辑sigmoid函数，i, f, o 和 c分别是输入门，忘记门，输出门和细胞向量，所有的大小都和向量h一样。w权重的含义如其下表所示。</p>
<p>LSTM序列标注模型如图所示：<img src="https://i.loli.net/2018/10/04/5bb5a84cd95d5.jpg" alt></p>
<p>其中，中间的画斜线的格子即为图2中所示部分。</p>
<h3 id="Bidirectional-LSTM-双向LSTM"><a href="#Bidirectional-LSTM-双向LSTM" class="headerlink" title="Bidirectional LSTM(双向LSTM)"></a>Bidirectional LSTM(双向LSTM)</h3><p>作者展示了双向LSTM的结构，如图所示：<img src="https://i.loli.net/2018/10/04/5bb5a8f44f16e.jpg" alt></p>
<p>双向LSTM网络可以有效利用过去特征和未来特征。在作者的实现中，对于整个句子的前向和后向操作，作者只需要在每个句子开始时将隐藏状态重置为0。作者采用批处理，使得可以同时处理多个句子。</p>
<h3 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h3><p>使用邻居标记信息预测当前标记有两种不同的方法：</p>
<ol>
<li>预测每个时间步长的标签分布，然后使用波束式解码来找到最优的标签序列，代表方法：MEMMs</li>
<li>注重句子层次而不是个体位置，代表方法：CRF，输入和输出是直接相连的；如图：<img src="https://i.loli.net/2018/10/04/5bb5ac191d2dd.jpg" alt></li>
</ol>
<p>研究表明，CRFs一般能够产生更高的标签精度。</p>
<h3 id="LSTM-CRF"><a href="#LSTM-CRF" class="headerlink" title="LSTM-CRF"></a>LSTM-CRF</h3><p>作者展示了LSTM-CRF的结构，如图：<img src="https://i.loli.net/2018/10/04/5bb5ad05ebec0.jpg" alt></p>
<p>这网络可以有效地通过 LSTM 利用过去的输入特征和通过 CRF 利用句子级的标注信息。图中CRF层由连接连续输出层的线表示。CRF层有一个状态转移矩阵作为参数。</p>
<p>公式为：<img src="https://i.loli.net/2018/10/04/5bb5b6ea7f87e.jpg" alt></p>
<p>函数f为网络的输出分数，[x]为输入， [fθ]i,t 为带有参数θ（句子x，第i 个标签，第t个单词）的网络输出；</p>
<p>[A]i,j为转移分数，从连续的时间步i状态到j状态的转移分数。注意，该转换矩阵是位置无关的。</p>
<h3 id="BI-LSTM-CRF"><a href="#BI-LSTM-CRF" class="headerlink" title="BI-LSTM-CRF"></a>BI-LSTM-CRF</h3><p>作者展示了BI-LSTM-CRF的结构，如图所示：<img src="https://i.loli.net/2018/10/04/5bb5b87ca950f.jpg" alt></p>
<p>作者在实验中展示了额外的未来特征可以提高标签的准确率。</p>
<h2 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h2><p>本文使用的所有模型都共享一个通用SGD前向和后向训练过程。作者展示了BI-LSTM-CRF的算法，如图<img src="https://i.loli.net/2018/10/04/5bb5ba7ce18c7.jpg" alt></p>
<p>作者设置了批次大小为100。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="data"><a href="#data" class="headerlink" title="data"></a>data</h3><p>作者在以下三个数据集上测试自己的模型：Penn TreeBank (PTB) POS tagging, CoNLL 2000 chunking, and CoNLL 2003 named entity tagging.数据集信息展示如下：<img src="https://i.loli.net/2018/10/04/5bb5bc263ec9f.jpg" alt></p>
<h3 id="Features"><a href="#Features" class="headerlink" title="Features"></a>Features</h3><p>作者从三个数据集中提取出其公共特征。特征可以分为拼写特征和上下文特征。最终，作者对于POS（词性标注）、chunking（组块）和NER（命名实体识别）分别提取401K，76K和341K个特征。</p>
<h3 id="spelling-features（拼写特征）"><a href="#spelling-features（拼写特征）" class="headerlink" title="spelling features（拼写特征）"></a>spelling features（拼写特征）</h3><p>除了小写字母特征之外，我们提取给定单词的以下特征。</p>
<p><img src="https://i.loli.net/2018/10/04/5bb5bdde879c9.jpg" alt></p>
<h3 id="context-featurs（上下文特征）"><a href="#context-featurs（上下文特征）" class="headerlink" title="context featurs（上下文特征）"></a>context featurs（上下文特征）</h3><p>对于单词特征，作者使用unigram和bi-grams特征。对于在CoNLL2000数据集中的POS特征和在CoNLL2003数据集中的 POS &amp; CHUNK特征，作者使用了unigram，bi-gram和tri-gram特征。</p>
<h3 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h3><p>词向量在改进序列标注任务的表现方面起着至关重要的作用，我们使用 130K 词汇并且每个词汇的词向量维度是 50 维。我们将 one-hot-encoding词表示替换每个词对应的词向量。</p>
<h3 id="Features-connection-tricks"><a href="#Features-connection-tricks" class="headerlink" title="Features connection tricks"></a>Features connection tricks</h3><p>我们可以将拼写和上下文特征与单词特征一样对待。这样网络的输入包括单词，单词的拼写和上下文特征。然而，==我们发现将拼写和上下文特征与输出直接连接可以加速训练过程，同时也能保持标注的准确率，==如下图所示：<img src="https://i.loli.net/2018/10/04/5bb5bfc63f961.jpg" alt></p>
<p>我们注意到，这种特征的使用与使用的最大熵特征类似。区别在于采用特征三列技术可能会发生特征冲突。由于序列标注数据集中的输出标签小于语言模型（通常为数十万），所以我们可以在特征和输出之间建立完整的连接，以避免潜在的特征冲突。</p>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>在相同的数据集上分别训练LSTM，BI-LSTM，CRF，LSTM-CRF和BI-LSTM-CRF模型，并且采用两种方式初始化word embedding：随机和Senna方式。模型的训练速率为0.1，隐藏层数量为300.不同模型在不同word embedding下的结果如表2所示，同时列出了之前最好模型Cov-CRF。</p>
<p><img src="https://i.loli.net/2018/10/04/5bb5c1da95e97.jpg" alt></p>
<ul>
<li><p>与Cov-CRF比较</p>
<p>实验中设置了3个基准模型，分别为LSTM、BI-LSTM和CRF，结果中LSTM在三个数据集中效果最差，BI-LSTM跟CRF在POS和chunking中效果接近，但是在NER中后者要优于前者。有趣的是表现最好的模型BI-LSTM-CRF相对于Cov-CRF来说对Senna embedding的依赖程度更小。</p>
</li>
<li><p>(robustness)模型鲁棒性  </p>
<p>为验证模型的鲁棒性，对不同模型只采用word feature特征进行训练，训练结果如表3，括号中数字表示相比于全部特征，模型的结果下降数值。<img src="https://i.loli.net/2018/10/05/5bb6ee3958635.jpg" alt></p>
</li>
<li><p>与其他系统的比较</p>
<p>这里就不贴图了，总之就是阐述作者自己模型好。</p>
</li>
</ul>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>总之作者的模型是基于之前模型的一些改进，主要运用了IBI-LSTM和CRF的结合。</p>
<h3 id="论文下载地址"><a href="#论文下载地址" class="headerlink" title="论文下载地址"></a><a href="https://arxiv.org/pdf/1508.01991.pdf" target="_blank" rel="noopener">论文下载地址</a></h3>]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>NER</tag>
        <tag>LSTM</tag>
        <tag>BI-LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title>《Differentiable Learning of Logical Rules for Knowledge Base Reasoning》阅读笔记</title>
    <url>/post/Differentiable%20Learning%20of%20Logical%20Rules%20for%20Knowledge%20Base%20Reasoning/</url>
    <content><![CDATA[<blockquote>
<p>论文<a href="http://papers.nips.cc/paper/6826-differentiable-learning-of-logical-rules-for-knowledge-base-reasoning.pdf" target="_blank" rel="noopener">下载地址</a>，本文研究用于于知识图谱推理的学习概率一阶逻辑规则的问题，提出了Neural Logic Programming（Neural-LP）框架，它结合了端到端可微分模型中一阶逻辑规则的参数和结构学习。为了在可微分的框架中同时学习参数和结构，作者设计了一个具有注意机制和记忆的神经控制器系统，以学习顺序组成TensorLog使用的原始可微操作。作者采用的注意机制是作为逻辑规则的置信度并且有寓意含义的。</p>
</blockquote>
<a id="more"></a>
<p>下图展示了一个使用逻辑规则进行知识图谱推理的例子</p>
<p><img src="https://i.loli.net/2019/03/05/5c7dc90edacd1.jpg" alt></p>
<p>使用概率逻辑的优点是通过为逻辑规则配备概率，可以更好地模拟统计复杂和噪声数据。</p>
<p>statistical relational learning（统计关系学习）：学习关系规则的集合</p>
<p>==inductive logic programming（归纳逻辑规划）：==当学习涉及提出新的逻辑规则时。（这应该和我正在做的方向是相关的，都是带有归纳性质的，有新的东西产生）。</p>
<h1 id="Framework"><a href="#Framework" class="headerlink" title="Framework"></a>Framework</h1><h2 id="Knowledge-base-reasoning"><a href="#Knowledge-base-reasoning" class="headerlink" title="Knowledge base reasoning"></a>Knowledge base reasoning</h2><p>为了推理知识库，对于每个查询我们都有兴趣学习以下形式的加权链式逻辑规则，类似于<strong>==随机逻辑程序==</strong>：</p>
<p><img src="https://i.loli.net/2019/03/05/5c7dcd6e1a5ac.jpg" alt></p>
<p>其中$\alpha$是和规则有关的置信度，R是知识库中的关系，query(Y,X) 表示一个三元组，query 表示一个关系。</p>
<h2 id="TensorLog-for-KB-reasoning"><a href="#TensorLog-for-KB-reasoning" class="headerlink" title="TensorLog for KB reasoning"></a>TensorLog for KB reasoning</h2><p>将实体转换成one-hot变量；并用一个矩阵$M_R$表示关系，该矩阵只在（i，j）处为1，i、j为第i、j个实体。</p>
<p>结合两个操作，逻辑规则推理$R(Y,X) \gets P(Y,X) \bigwedge Q(Z,X)$可以被表示为：$M_P \cdot M_P \cdot v_x \doteq s$，向量s中为1的位置就是Y的答案。</p>
<p>对于一条查询，所有的逻辑规则的右边部分被表示为以下形式：</p>
<p><img src="https://i.loli.net/2019/03/05/5c7dda071bcef.jpg" alt></p>
<p>其中，l表示所有的可能规则的个数，$\alpha_l$是规则l的置信度，$\beta_l$是某特定关系里的有序关系列表，所以在inference时，给定实体$v_x​$，实体y的score等于向量s中的对应y的位置的值。对于推理，给定实体x，实体y的score等于向量s中的对应y的位置的值。</p>
<p><img src="https://i.loli.net/2019/03/10/5c85104e2a53b.jpg" alt></p>
<p>所以总结本文关心的优化问题如下：</p>
<p><img src="https://i.loli.net/2019/03/05/5c7e1568d9ec7.jpg" alt></p>
<h2 id="Learning-the-logical-rules"><a href="#Learning-the-logical-rules" class="headerlink" title="Learning the logical rules"></a>Learning the logical rules</h2><p>在上式的优化问题中，算法需要学习的部分分为两个：一个是规则的结构，即一个规则是由哪些条件组合而成的；另一个是规则的置信度。由于每一条规则的置信度都是依赖于具体的规则形式，而规则结构的组成也是一个离散化的过程，因此上式整体是不可微的。因此作者对前面的式子做了以下更改：<img src="https://i.loli.net/2019/03/05/5c7e160404436.jpg" alt></p>
<p>对比与式（2）：主要交换了连乘和累加的计算顺序，对预一个关系的相关的规则，为每个关系在每个步骤都学习了一个权重，即上式的 $a_t^k$。</p>
<p>由于上式固定了每个规则的长度都为 T，这显然是不合适的。为了能够学习到变长的规则，Neural LP中设计了记忆向量 $u_t$,表示每个步骤输出的答案—每个实体作为答案的概率分布，还设计了两个注意力向量：一个为记忆注意力向量 $b_t$ ——表示在步骤 t 时对于之前每个步骤的注意力；一个为算子注意力向量 $a_t$ ——表示在步骤 t 时对于每个关系算子的注意力。每个步骤的输出由下面三个式子生成：<img src="https://i.loli.net/2019/03/05/5c7e18ac09662.jpg" alt></p>
<p>其中$b_t$和$a_t$由以下公式通过RNN获得：</p>
<p><img src="https://i.loli.net/2019/03/05/5c7e1a015f0ac.jpg" alt></p>
<p>推理机的整体框架是：</p>
<p><img src="https://i.loli.net/2019/03/05/5c7e1aec1aea9.jpg" alt></p>
<p>其中memory存的就是每步的推理结果（实体），最后的输出（例如$u_{T+1}$，目标就是最大化 $logv_y^Tu$，加log是因为非线性能让效果变好。</p>
<p>整个算法如下：<img src="https://i.loli.net/2019/03/05/5c7e1c0d5cf32.jpg" alt></p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="（1）-两个标准数据集上的统计关系学习相关的实验"><a href="#（1）-两个标准数据集上的统计关系学习相关的实验" class="headerlink" title="（1） 两个标准数据集上的统计关系学习相关的实验"></a>（1） 两个标准数据集上的统计关系学习相关的实验</h2><ul>
<li>Unified Medical Language System (UMLS)：The entities are biomedical concepts (e.g. disease, antibiotic) and relations are like treats and diagnoses.</li>
<li>Kinship：contains kinship relationships among members of the Alyawarra tribe from Central Australia [</li>
</ul>
<p><img src="https://i.loli.net/2019/03/05/5c7e2270289d0.jpg" alt></p>
<h2 id="（2）-在-16-16-的网格上的路径寻找的实验"><a href="#（2）-在-16-16-的网格上的路径寻找的实验" class="headerlink" title="（2） 在$16*16$的网格上的路径寻找的实验"></a>（2） 在$16*16$的网格上的路径寻找的实验</h2><p><img src="https://i.loli.net/2019/03/05/5c7e235b430cb.jpg" alt></p>
<h2 id="（3）-知识库补全实验"><a href="#（3）-知识库补全实验" class="headerlink" title="（3） 知识库补全实验"></a>（3） 知识库补全实验</h2><p>实验所用数据集信息：</p>
<p>FB15KSelected：这是通过从FB15K中去除近似重复和反向关系而构造的</p>
<p><img src="https://i.loli.net/2019/03/05/5c7e2378e1c64.jpg" alt></p>
<p>实验结果：<img src="https://i.loli.net/2019/03/05/5c7e23c43db48.jpg" alt></p>
<p>为了证明Neural LP的归纳推理的能力，本文还特别设计了一个实验，在训练数据集中去掉所有涉及测试集中包含的实体的三元组，然后训练并预测，得到结果如下：<img src="https://i.loli.net/2019/03/05/5c7e23ebbca69.jpg" alt></p>
<h2 id="（4）-知识库问答的实验"><a href="#（4）-知识库问答的实验" class="headerlink" title="（4） 知识库问答的实验"></a>（4） 知识库问答的实验</h2><p><img src="https://i.loli.net/2019/03/05/5c7e24aa6a427.jpg" alt></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h1><p>本文提出了一个可微的规则学习模型，并强调了知识库中的规则应该是实体无关的，对于我目前在做的方向，本体论也是与实体无关的，这种规则学习有一定的借鉴性，但是好像所区别。这个规则推理也可以看成某些关系之间的包含关系3.1中举的HasOfficeInCity(New York,Uber) and CityInCountry(USA,New York)的例子，可以看作是2对于1有包含关系。并且可以看到本篇论文中，作者设计了丰富的实验。</p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul>
<li><a href="https://toutiao.io/posts/wrxf4z/preview" target="_blank" rel="noopener">https://toutiao.io/posts/wrxf4z/preview</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/46024825" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/46024825</a></li>
</ul>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
        <tag>知识图谱推理</tag>
        <tag>规则学习</tag>
      </tags>
  </entry>
  <entry>
    <title>《Bootstrapping Entity Alignment with Knowledge Graph Embedding》阅读笔记</title>
    <url>/post/Bootstrapping%20Entity%20Alignment%20with%20Knowledge%20Graph%20Embedding/</url>
    <content><![CDATA[<blockquote>
<p><a href="https://www.ijcai.org/proceedings/2018/0611.pdf" target="_blank" rel="noopener">论文下载地址</a>，采用了bootstrapping方法来解决缺乏训练数据的过程，提出了截断均匀负采样来提高负样例对于目标函数的贡献，采用基于限制的目标函数来按需调整正负样例的得分。</p>
</blockquote>
<a id="more"></a>
<p>基于嵌入的实体对齐将不同的知识图谱（KG）表示为低维嵌入，并通过测量实体嵌入之间的相似性来查找实体对齐。其中，大量方法所面临的一个挑战是：缺乏足够的先前对齐作为标记的训练数据。</p>
<h2 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h2><ul>
<li>作者把实体对齐建模为一个分类问题，其基于KG嵌入来寻求最大化所有标记和未标记的实体对齐可能性</li>
<li>对于面向对齐的KG嵌入，作者提出了一种基于限制的目标函数；为了对不太可能区分的负三元组进行抽样，作者提出了一种截断均匀的负抽样方法。</li>
<li>作者提出了一个自举过程（bootstrapping）来克服缺乏足够训练数据，通过标记可能的对齐并迭代地将其添加到训练数据中来更新面向对齐的嵌入。</li>
<li>作者在三个跨语言和两个大型数据集上评估了所提出的方法，表明所提出的方法明显优于三种最先进的实体对齐方法。</li>
</ul>
<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>最大似然准则指导选择实现最高对齐可能性的最佳θ</p>
<p><img src="http://image.nysdy.com/20190409155477420219240.png" alt="20190409155477420219240.png"></p>
<p>其中，L_x代表实体x的真实标签，1_[]是一个指示函数，表示给定命题的真值（0或1）。但是对于没有标签的实体，想要通过上述来得到theta就很困难。</p>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><h3 id="面向对齐的KG嵌入"><a href="#面向对齐的KG嵌入" class="headerlink" title="面向对齐的KG嵌入"></a>面向对齐的KG嵌入</h3><p>作者提出了一个目标函数：<img src="http://image.nysdy.com/2019040915547745103625.png" alt="2019040915547745103625.png"></p>
<p>该目标函数有两个期望的属性：</p>
<ul>
<li>预期正三元组得分较低，而负三元组得分较高。例如f(r)&lt;= r_1 并且 f(r’)&gt;=r_2，设置时r_2&gt;r_1,且r_1是一个小的正值。</li>
<li>仍然可以得到f(r’)-f(r)&gt;=r_2 - r_1，这表明所提出的目标函数仍然保留了基于边际排序损失的特征。</li>
</ul>
<h4 id="截断均匀负采样"><a href="#截断均匀负采样" class="headerlink" title="截断均匀负采样"></a>截断均匀负采样</h4><p>如果样例太容易区分，那么对整个的嵌入学习的贡献会很小。</p>
<p>所以，作者采用在嵌入空间中s最近的邻居作为候选集，剔除那些和实体x相似度过低的数据。</p>
<h3 id="引导对齐（Bootstrapping-Alignment）"><a href="#引导对齐（Bootstrapping-Alignment）" class="headerlink" title="引导对齐（Bootstrapping Alignment）"></a>引导对齐（Bootstrapping Alignment）</h3><p>作者迭代地将可能的对齐标记作为训练数据，并使用它来进一步改进实体嵌入和对齐。</p>
<h4 id="可能的对齐标签和编辑"><a href="#可能的对齐标签和编辑" class="headerlink" title="可能的对齐标签和编辑"></a>可能的对齐标签和编辑</h4><p>作者为了实现最大化对齐可能性并遵守一对一对齐约束，提出以下优化问题来标记第t次迭代：</p>
<p><img src="http://image.nysdy.com/20190409155477548037155.png" alt="20190409155477548037155.png"></p>
<p>Y’_x = {y|y ∈ Y’ and  π(y|x; Θ^(t)) &gt; γ3}代表标签x的候选集；ψ^(t)(·)是一个指示函数，只有当x在第t次迭代时标签为y时为1，其它情况为0。两个限制条件保证了一对一的标签。这时得到了一个新的标签对齐：<img src="http://image.nysdy.com/20190409155477586716499.png" alt="20190409155477586716499.png"></p>
<p>为了提高标签质量并满足一对一的对齐约束，在自举过程中，一旦被标记的实体可以在随后的标记中重新标记或变为未标记的实体。</p>
<p>当发生两个标签冲突时，我们通过计算下面的似然差异来确定保留哪个：<img src="http://image.nysdy.com/20190409155477607781047.png" alt="20190409155477607781047.png"></p>
<p>当该值大于0说明前者具有更大的对齐概率。</p>
<h4 id="从整体角度学习"><a href="#从整体角度学习" class="headerlink" title="从整体角度学习"></a>从整体角度学习</h4><p>为了获得标记和未标记实体的整理观察，作者定义了概率分布φx来描述所有x可能的概率分布。</p>
<p><img src="http://image.nysdy.com/20190409155477633963211.png" alt="20190409155477633963211.png"></p>
<p>由此，作者得到了最小化下面的似然函数来得到Θ：<img src="http://image.nysdy.com/20190409155477641976513.png" alt="20190409155477641976513.png"></p>
<p>因为，嵌入不仅应该捕获对齐可能性，还应该模拟KG的语义，所以作者最后定义联合目标函数：</p>
<p><img src="http://image.nysdy.com/20190409155477650541518.png" alt="20190409155477650541518.png"></p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><ul>
<li>DBP15K [Sun et al., 2017]包含三个跨语言数据集，这些数据集是从DBpedia的多语言版本构建的。DBPZH-EN(Chinese to English), DBPJA-EN(Japanese to English) and DBPFR-EN(French to English)每个数据集包含15，000个参考实体对齐。</li>
<li>DWY100K包含从DBpedia，Wikidata和YAGO3中提取的两个大型数据集，由DBP-WD和DBP-YG表示。 每个数据集都有10万个参考实体对齐<img src="http://image.nysdy.com/20190409155477678270217.png" alt="20190409155477678270217.png"></li>
</ul>
<h3 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h3><p>作者选取了三种最先进的基于嵌入的方法来实现实体对齐。</p>
<ul>
<li>MTransE [Chen et al., 2017]，选取了第四种变体（表现最佳）。</li>
<li>IPTransE[Zhu et al., 2017]是一个迭代方法</li>
<li>JAPE [Sun et al., 2017]结合了实体对齐的关系和属性嵌入</li>
<li>AlignE面向对齐的KG嵌入模型的实现，具有截断的均匀负采样和参数交换，它优化了公式（3），但是没有自举</li>
</ul>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>表2中我们观察到AlignE明显优于MTransE，IPTransE和JAPE，因为它采用面向对齐的嵌入。而BootEA显着改善了AlignE的结果，表明了自举的良好性能是由于其能够准确地将可能的对齐标记为训练数据。</p>
<p><img src="http://image.nysdy.com/20190409155477712242929.png" alt="20190409155477712242929.png"></p>
<h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><h4 id="截断均匀负抽样的有效性"><a href="#截断均匀负抽样的有效性" class="headerlink" title="截断均匀负抽样的有效性"></a>截断均匀负抽样的有效性</h4><p><img src="http://image.nysdy.com/20190409155477794964299.png" alt="20190409155477794964299.png">从图中可以看出，与MTransE，IPTransE和JAPE相比，具有均匀负采样的AlignE仍然获得了优异的结果，并且随着采样离x更加接近，效果呈上升趋势。</p>
<h4 id="可能对齐的准确性"><a href="#可能对齐的准确性" class="headerlink" title="可能对齐的准确性"></a>可能对齐的准确性</h4><p><img src="http://image.nysdy.com/20190409155477815850691.png" alt="20190409155477815850691.png">可以看到以作者的标记方法S3表现最佳。这些结果证实作者的方法可以保证使用未标记数据的安全性。</p>
<h4 id="对先前对准比例的敏感性"><a href="#对先前对准比例的敏感性" class="headerlink" title="对先前对准比例的敏感性"></a>对先前对准比例的敏感性</h4><p><img src="http://image.nysdy.com/20190409155477832196109.png" alt="20190409155477832196109.png"></p>
<p>正如预期的那样，随着比例的增加，所有五个数据集的结果都变得更好，因为更多的先前对齐可以提供更多信息来对齐两个KG。</p>
<h4 id="F1-score-w-r-t-关系三元数的分布"><a href="#F1-score-w-r-t-关系三元数的分布" class="headerlink" title="F1-score w.r.t. 关系三元数的分布"></a>F1-score w.r.t. 关系三元数的分布</h4><p><img src="http://image.nysdy.com/20190409155477850975486.png" alt="20190409155477850975486.png"></p>
<p>BootEA在所有时间间隔都优于MTransE，IPTransE和JAPE，这再次证实了BootEA的有效性。而且BootEA可以在稀疏数据上取得有希望的结果。</p>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
        <tag>知识图谱嵌入</tag>
        <tag>实体对齐</tag>
      </tags>
  </entry>
  <entry>
    <title>《Efficient Estimation of Word Representations in Vector Space》阅读笔记</title>
    <url>/post/Efficient%20Estimation%20of%20Word%20Representations%20in%20Vector%20Space/</url>
    <content><![CDATA[<blockquote>
<p><a href="https://arxiv.org/pdf/1301.3781.pdf" target="_blank" rel="noopener">论文下载地址</a>，该篇论文的大篇幅都在讨论实验结果的分析，模型的部分比较简单，没有详细分析，本来是想读一下CBOW和skip-gram的原始论文，发现并没有想象中的那么大的用处。</p>
</blockquote>
<a id="more"></a>
<h2 id="Goals-of-paper"><a href="#Goals-of-paper" class="headerlink" title="Goals of paper"></a>Goals of paper</h2><ul>
<li>开发了两种新模型，并保留了单词之间的线性规律</li>
<li>设计了一个新的综合测试集，用于测量句法和语义规律</li>
<li>讨论了训练时间和准确性如何取决于单词向量的维度和训练数据的数量</li>
</ul>
<h2 id="Model-Architectures"><a href="#Model-Architectures" class="headerlink" title="Model Architectures"></a>Model Architectures</h2><p>训练复杂度：</p>
<p><img src="http://image.nysdy.com/20190506155712909488421.png" alt="20190506155712909488421.png"></p>
<p>其中，E是训练次数，T是训练集单词数量，Q是模型结构。</p>
<h3 id="Feedforward-Neural-Net-Language-Model-NNLM"><a href="#Feedforward-Neural-Net-Language-Model-NNLM" class="headerlink" title="Feedforward Neural Net Language Model (NNLM)"></a>Feedforward Neural Net Language Model (NNLM)</h3><p>它由输入，映射，隐藏和输出层组成。通过简化方法，Q= N x D x H</p>
<h3 id="Recurrent-Neural-Net-Language-Model-RNNLM"><a href="#Recurrent-Neural-Net-Language-Model-RNNLM" class="headerlink" title="Recurrent Neural Net Language Model (RNNLM)"></a>Recurrent Neural Net Language Model (RNNLM)</h3><p>克服了模型需要固定的上下文长度的问题，并且只有输入，隐藏和输出层。</p>
<p>Q= H x H + H x V，其中H = D（单词表示），H x V 可以通过分级softmax被简化为H x log_2(V)。所以主要的复杂度来自于H x H。</p>
<h3 id="Parallel-Training-of-Neural-Networks"><a href="#Parallel-Training-of-Neural-Networks" class="headerlink" title="Parallel Training of Neural Networks"></a>Parallel Training of Neural Networks</h3><p>模型使用的DistBelief框架允许我们并行运行同一模型的多个副本，每个副本通过集中的服务器同步其梯度更新，该服务器保留所有参数</p>
<h2 id="New-Log-linear-Models"><a href="#New-Log-linear-Models" class="headerlink" title="New Log-linear Models"></a>New Log-linear Models</h2><p>大多数复杂性是由于模型中的非线性隐藏层引起的。模型结构如下：<img src="http://image.nysdy.com/20190506155713050638684.png" alt="20190506155713050638684.png"></p>
<h3 id="Continuous-Bag-of-Words-Model-CBOW"><a href="#Continuous-Bag-of-Words-Model-CBOW" class="headerlink" title="Continuous Bag-of-Words Model(CBOW)"></a>Continuous Bag-of-Words Model(CBOW)</h3><p>第一个提出的体系结构类似于前馈NNLM，其中去除了非线性隐藏层，并且所有单词（不仅仅是投影矩阵）共享投影层。 因此，所有单词都被投射到相同的位置（它们的向量被平均）。 将这个架构称为词袋模型，因为历史中的单词顺序不会影响投影。</p>
<p>模型的复杂度：Q = N × D + D × log_2(V )</p>
<h3 id="Continuous-Skip-gram-Model"><a href="#Continuous-Skip-gram-Model" class="headerlink" title="Continuous Skip-gram Model"></a>Continuous Skip-gram Model</h3><p>基于同一句子中的另一个单词最大化单词的分类。 更准确地说，使用每个当前单词作为具有连续投影层的对数线性分类器的输入，并预测当前单词之前和之后的特定范围内的单词。</p>
<p>模型的复杂度：Q = C × (D + D × log2(V ))，其中C是单词的最大距离。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="任务描述"><a href="#任务描述" class="headerlink" title="任务描述"></a>任务描述</h3><p>为了度量词向量的质量，我们定义了一个复杂的测试集，它包括了五种类型的语义问题。九个类型的句法问题。包括每个类别的两个样本集在上表展示；总之，共拥有8869个语义问题和10675个句法问题</p>
<p>作者通过：最大化精确度 ，模型体系结构的比较，模型的大规模并行训练来证明提出模型的运速度和精确的优势。</p>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>word2vec</tag>
      </tags>
  </entry>
  <entry>
    <title>《Interaction Embeddings for Prediction and Explanation》阅读笔记</title>
    <url>/post/Interaction%20Embeddings%20for%20Prediction%20and%20Explanation/</url>
    <content><![CDATA[<blockquote>
<p><a href="https://www.zora.uzh.ch/id/eprint/162876/1/interaction-embeddings-prediction_merlin_version.pdf" target="_blank" rel="noopener">论文下载地址</a>，此论文主要提出了实体和关系的交互作用对于知识图谱嵌入的影响，和提出了新的嵌入评估方案 - 搜索预测解释。</p>
</blockquote>
<a id="more"></a>
<h2 id="论文贡献"><a href="#论文贡献" class="headerlink" title="论文贡献"></a>论文贡献</h2><ul>
<li>提出了CrossE，一种通过学习一个交互矩阵来给实体和关系的交互建模的新型知识图谱嵌入。</li>
<li>我们使用三个基准数据集评估CrossE与链接预测任务上的各种其他KGE的比较，并显示CrossE在具有适度参数大小的复杂且更具挑战性的数据集上实现最先进的结果。</li>
<li>我们提出了一种新的嵌入评估方案 - 搜索预测解释，并表明CrossE能够生成比其他方法更可靠的解释。 这表明交互嵌入更能在不同的三元组环境中捕捉实体和关系之间的相似性。</li>
</ul>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>给定知识图谱和一个要预测的三元组的头实体和关系，在预测尾实体的过程中，头实体和关系之间是有交叉交互的crossover interaction, 即关系决定了在预测的过程中哪些头实体的信息是有用的，而对预测有用的头实体的信息又决定了采用什么逻辑去推理出尾实体，文中通过一个模拟的知识图谱进行了说明如下图所示：</p>
<p><img src="http://image.nysdy.com/20190321155314902217434.png" alt="20190321155314902217434.png"></p>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>论文中在这部分对KGE（Knowledge graph embedding）进行了分类总结：</p>
<ul>
<li>KGEs with general embeddings</li>
<li>KGEs with multiple embeddings.</li>
<li>KGEs that utilize extra information.</li>
</ul>
<p>这部分总结中对大量的方法进行描述，可以作为背景知识进行阅读。</p>
<h2 id="CrossE模型"><a href="#CrossE模型" class="headerlink" title="CrossE模型"></a>CrossE模型</h2><p>基于对头实体和关系之间交叉交互的观察，本文提出了一个新的知识图谱表示学习模型CrossE. CrossE除了学习实体和关系的向量表示，同时还学习了一个交互矩阵C，C与关系相关，并且用于生成实体和关系经过交互之后的向量表示，所以在CrossE中实体和关系不仅仅有通用向量表示，同时还有很多交互向量表示。CrossE核心想法如下图：<img src="http://image.nysdy.com/20190321155314970714298.png" alt="20190321155314970714298.png"></p>
<p>目标函数粉四步生成：</p>
<ol>
<li>Interaction Embedding for Entities：根据头实体向量和交互矩阵（以关系确定的）来确定头实体的交互表示。</li>
<li>Interaction Embedding for Relations：根据头实体的交互表示和关系作用生成关系的交互表示</li>
<li>Combination Operator：将头实体的交互表示和关系的交互表示相结合，并进行非线性处理（tanh）</li>
<li>Similarity Operator：计算结合后表示和尾实体表示之间的相似度。</li>
</ol>
<p>最后分数函数：</p>
<p><img src="http://image.nysdy.com/20190321155315073712112.png" alt="20190321155315073712112.png"></p>
<p>损失函数：（这里就是一个交叉熵函数，但是写的有问题f(x)项应该在括号外）<img src="http://image.nysdy.com/20190321155315081421205.png" alt="20190321155315081421205.png"></p>
<h2 id="对于预测的解释"><a href="#对于预测的解释" class="headerlink" title="对于预测的解释"></a>对于预测的解释</h2><p>这部分作者描述了如何生成预测三元组的解释，并介绍了基于嵌入的路径搜索算法，主要步骤如下：</p>
<ol>
<li>Search for similar relations：修剪掉不合理路径</li>
<li>Search for paths between h and t：作者定义了6种路径（班汉一个或两个关系）</li>
<li>Search similar entities：捕获实体之间的相似性方面越有能力，就越有可能存在（hs，r，ts）</li>
<li>: Search for similar structures as supports：我们只将知识图中至少有一个支持的路径视为解释。</li>
</ol>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p><img src="http://image.nysdy.com/20190321155315142112584.png" alt="20190321155315142112584.png"></p>
<h3 id="链接预测"><a href="#链接预测" class="headerlink" title="链接预测"></a>链接预测</h3><p><img src="/Users/dy/Library/Application Support/typora-user-images/image-20190321145750608.png" alt="image-20190321145750608"></p>
<p><img src="http://image.nysdy.com/20190321155315147862891.png" alt="20190321155315147862891.png"></p>
<p>从实验结果中我们可以看出，CrossE实现了较好的链接预测结果。我们去除CrossE中的头实体和关系的交叉交互，构造了模型 CrossES，CrossE 和 CrossES 的比较说明了交叉交互的有效性。</p>
<h3 id="生成解释"><a href="#生成解释" class="headerlink" title="生成解释"></a>生成解释</h3><p>我们提出了一种基于相似结构通过知识图谱的表示学习结果生成预测结果解释的方法，并提出了两种衡量解释结果的指标，AvgSupport和Recall。Recall是指模型能给出解释的预测结果的占比，其介于0和1之间且值越大越好；AvgSupport是模型能给出解释的预测结果的平均support个数，AvgSupport是一个大于0的数且越大越好。可解释的评估结果如下：</p>
<p><img src="http://image.nysdy.com/2019032115531515625385.png" alt="2019032115531515625385.png"></p>
<p>链接预测和可解释的实验从两个不同的方面评估了知识图谱表示学习的效果，同时也说明了链接预测的准确性和可解释性没有必然联系，链接预测效果好的模型并不一定能够更好地提供解释，反之亦然。</p>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul>
<li><a href="http://blog.openkg.cn/%E8%AE%BA%E6%96%87%E6%B5%85%E5%B0%9D-interaction-embeddings-for-prediction-and-explanation/" target="_blank" rel="noopener">http://blog.openkg.cn/%E8%AE%BA%E6%96%87%E6%B5%85%E5%B0%9D-interaction-embeddings-for-prediction-and-explanation/</a></li>
</ul>
<h2 id><a href="#" class="headerlink" title=" "></a> </h2>]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
        <tag>知识图谱推理</tag>
        <tag>知识图谱嵌入</tag>
      </tags>
  </entry>
  <entry>
    <title>《Differentiating Concepts and Instances for Knowledge Graph Embedding》阅读笔记</title>
    <url>/post/read_Differentiating_Concepts_and_Instances_for_Knowledge_Graph_Embedding/</url>
    <content><![CDATA[<blockquote>
<p><a href="http://aclweb.org/anthology/D18-1222" target="_blank" rel="noopener">论文获取地址</a>。这篇文章最大的亮点就是把concept映射为一个球面，然后把instance映射为一个向量，通过这种空间关系来进行embedding。如果instance和concept满足InstanceOf的关系，则instance应该在球内；如果两个concept满足SubClassOf的关系，则一个球会在另一个球面内。</p>
</blockquote>
<h3 id="concept"><a href="#concept" class="headerlink" title="concept"></a>concept</h3><p>A concept is a fundamental category of existence (Rosch, 1973) and can be reified by all of its actual or potential instances.Concepts, which <strong>represent a group of different instances sharing common properties</strong>, are essential information in knowledge representation. </p>
<p><img src="https://i.loli.net/2018/12/16/5c1634f6c26c3.jpg" alt></p>
<h2 id="drawback-of-the-previous-method"><a href="#drawback-of-the-previous-method" class="headerlink" title="drawback of the previous method"></a>drawback of the previous method</h2><p> ignore to distinguish between concepts and instances will lead to two drawbacks:</p>
<ul>
<li><p><strong>Insufficient concept representation</strong>：</p>
<p>cannot explicitly represent the difference between concepts and instances</p>
</li>
<li><p><strong>Lack transitivity of both isA relations</strong>:</p>
<p><em>instanceOf</em> and <em>subClassOf</em> (generally known as isA)isA relations exhibit transitivity</p>
</li>
</ul>
<h2 id="contributions"><a href="#contributions" class="headerlink" title="contributions"></a>contributions</h2><ul>
<li><strong>the first to propose</strong> and formalize the problem of knowledge graph embedding which <strong>differentiates</strong> between concepts and instances</li>
<li>a novel knowledge embedding method named <strong>TransC</strong></li>
<li><strong>state-of-the-art</strong> on link prediction and triple classification</li>
</ul>
<h2 id="Translation-based-Models"><a href="#Translation-based-Models" class="headerlink" title="Translation-based Models"></a>Translation-based Models</h2><h3 id="TransE"><a href="#TransE" class="headerlink" title="TransE"></a>TransE</h3><ul>
<li>triple (h, r, t) should satisfy <strong>h + r ≈ t</strong></li>
<li>loss function:$f_r(h,t) = ||h + r - t||^2_2$</li>
<li>suitable for 1-to-1 relations</li>
</ul>
<h3 id="TransH"><a href="#TransH" class="headerlink" title="TransH"></a>TransH</h3><ul>
<li>It regards a relation vector r as a translation on a hyperplane with $w_r$ as the normal vector. </li>
<li>loss function:$f_r(h,t) = ||h_{\bot} + r - t_{\bot}||^2_2$，其中$h_{\bot}=h-w^{\top}_r h w_r$，$t_{\bot}=t-w^{\top}_r t w_r$</li>
<li>suitable for 1-to-N, N-to-1, and N-to-N relations</li>
</ul>
<h3 id="TransR-CTransR"><a href="#TransR-CTransR" class="headerlink" title="TransR/CTransR"></a>TransR/CTransR</h3><ul>
<li>addresses the issue in TransE and TransH that some entities are <strong>similar in the entity space</strong> but comparably <strong>different in other specific aspects</strong>.</li>
<li>loss function:$f_r(h,t) = ||M_rh +r -M_rt||^2_2$，$M_r$ for each relation r</li>
</ul>
<h3 id="TransD"><a href="#TransD" class="headerlink" title="TransD"></a>TransD</h3><ul>
<li>considers the different types of entities and relations at the same time</li>
<li>loss function:$f_r(h,t) = ||h_{\bot} + r - t_{\bot}||^2_2$，$h_{\bot} = M_{rh}h$和$t_{\bot} = M_{rt}t$，$M_{r,e}$ for each relation-entity pair (r, e)</li>
</ul>
<h2 id="Bilinear-Models"><a href="#Bilinear-Models" class="headerlink" title="Bilinear Models"></a>Bilinear Models</h2><h3 id="RESCAL"><a href="#RESCAL" class="headerlink" title="RESCAL"></a>RESCAL</h3><ul>
<li>the <strong>first</strong> bilinear model</li>
<li>It associates each entity with a vector to capture its <strong>latent semantics</strong>. Each relation is represented as a <strong>matrix</strong> which models pairwise interactions between latent factors.</li>
</ul>
<h2 id="External-Information-Learning-Models"><a href="#External-Information-Learning-Models" class="headerlink" title="External Information Learning Models"></a>External Information Learning Models</h2><ul>
<li>textual information</li>
<li>entity descriptions</li>
</ul>
<h2 id="Problem-Formulation"><a href="#Problem-Formulation" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h2><p>这部分中作者详细介绍了知识图谱的组成部分：概念和实例集、关系集（包括instanceOf、subClassOf和instance relation），三元组集（按照关系集同样分为三个部分）。为了能表达is A关系的传递性，作者将instanceOf和subClassof两种关系进行了精心的设计，也是该论文的重点。</p>
<blockquote>
<p>For each concept c ∈ C, we learn a sphere s(p, m) with $p \in R^k$ and m denoting the sphere center and radius.</p>
</blockquote>
<h2 id="TranC"><a href="#TranC" class="headerlink" title="TranC"></a>TranC</h2><p>Specifically, TransC encodes each concept in knowledge graph as a sphere and each instance as a vector in the same semantic space. </p>
<h3 id="InstanceOf-Triple-Representation"><a href="#InstanceOf-Triple-Representation" class="headerlink" title="InstanceOf Triple Representation"></a>InstanceOf Triple Representation</h3><p>loss function：$f_e(i,c) = ||i-p||_2 - m$，当该函数大于0时进行优化，使其小于零。</p>
<h3 id="SubClassOf-Triple-Representation"><a href="#SubClassOf-Triple-Representation" class="headerlink" title="SubClassOf Triple Representation"></a>SubClassOf Triple Representation</h3><p><img src="https://i.loli.net/2019/03/01/5c789a0f00510.jpg" alt></p>
<p>如图所示，其中子图（a）为目标状态。两个概念的的圆心距离：$d = ||p_i - p_j||_2$。需要做到的就是$d-(m_j -m_i)  \leq 0$并且$ (m_j &gt; m_i)$。</p>
<h3 id="Relational-Triple-Representation"><a href="#Relational-Triple-Representation" class="headerlink" title="Relational Triple Representation"></a>Relational Triple Representation</h3><p>这部分按照TranE的思路进行处理，$||h+r-t||^2_3$</p>
<h2 id="train-model"><a href="#train-model" class="headerlink" title="train model"></a>train model</h2><h3 id="margin-based-loss详解"><a href="#margin-based-loss详解" class="headerlink" title="margin based loss详解"></a><a href="https://zhuanlan.zhihu.com/p/27748177" target="_blank" rel="noopener">margin based loss</a>详解</h3><h3 id="unit-and-bern"><a href="#unit-and-bern" class="headerlink" title="unit and bern"></a><a href="https://pdfs.semanticscholar.org/2a3f/862199883ceff5e3c74126f0c80770653e05.pdf" target="_blank" rel="noopener">unit and bern</a></h3><blockquote>
<p> Regarding the strategy of constructing negative labels, we use “unif” to denote the traditional way of replacing head or tail with equal probability, and use “bern.” to denote reducing false negative labels by replacing head or tail with different probabilities.</p>
</blockquote>
<h2 id="the-following-research-directions"><a href="#the-following-research-directions" class="headerlink" title="the following research directions"></a>the following research directions</h2><ul>
<li><p>find a more expressive model instead of spheres to represent concepts</p>
</li>
<li><p>A concept may have different meanings in different triples. </p>
<p>use several typical vectors of instances as a concept’s centers to represent different meanings of a concept. </p>
</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
      </tags>
  </entry>
  <entry>
    <title>《Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation》阅读笔记</title>
    <url>/post/Learning_Phrase_Representations_using_RNN_Encoder%E2%80%93Decoder_for_Statistical_Machine_Translation/</url>
    <content><![CDATA[<blockquote>
<p><a href="https://www.aclweb.org/anthology/D14-1179" target="_blank" rel="noopener">原文链接</a>。该论文是Sequence to Sequence学习的最早原型，论文中提出一种崭新的RNN(GRU) Encoder-Decoder算法，虽然文章属于比较旧的文章，但作为seq2seq的基础原型，还是需要阅读了解一下的。文章写的比较详细，各部分细节都有讲解。</p>
</blockquote>
<a id="more"></a>
<h2 id="文章的主要结构"><a href="#文章的主要结构" class="headerlink" title="文章的主要结构"></a>文章的主要结构</h2><p><img src="https://i.loli.net/2019/01/05/5c3019fd55653.jpg" alt></p>
<h2 id="contribution"><a href="#contribution" class="headerlink" title="contribution"></a>contribution</h2><ul>
<li><p>a novel RNN Encoder–Decoder</p>
<p>能够处理变长序列</p>
</li>
<li><p>a novel hidden unit</p>
<ul>
<li>reset gate</li>
<li>update gate</li>
</ul>
</li>
</ul>
<h2 id="RNN-Encoder–Decoder"><a href="#RNN-Encoder–Decoder" class="headerlink" title="RNN Encoder–Decoder"></a>RNN Encoder–Decoder</h2><p>模型结构图如下：</p>
<p><img src="https://i.loli.net/2019/01/05/5c301b9717fd8.jpg" alt></p>
<p>文中作者对齐进行总体概述为：</p>
<blockquote>
<p>From a probabilistic perspective, this new model is a general method to learn the conditional distribution over a variable-length sequence conditioned on yet another variable-length sequence</p>
</blockquote>
<p>从概率的角度来看，这个新模型是学习在另一个可变长度序列条件下的可变长度序列上的条件分布的一般方法</p>
<h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>这部分是一个RNN单元。每个时间步，我们向Encoder中输入一个字/词（一般为向量形式），直到我们输入这个句子的最后一个字/词$X_T$，然后输入整个句子的语义向量c。由于RNN的特带你就是把前面每一步的输入信息都考虑进来，所以理论上这个c就包含了整个句子的所有信息。我们可以把当成这个句子的一个语义表示。</p>
<h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>Decoder是另一个RNN，其被训练出来以通过预测隐藏状态$h_t$的下一个符号$y_t$来生成输出序列。计算公式如下</p>
<script type="math/tex; mode=display">h_t = f(h_{t-1},y_{t-1},c)</script><p>下一个序列的计算公式如下：</p>
<script type="math/tex; mode=display">P(y_t|y_{t-1},y_{t-2},\dots,y_1,c)=g(h_t,y_{t-1},c)</script><h2 id="Hidden-Unit"><a href="#Hidden-Unit" class="headerlink" title="Hidden Unit"></a>Hidden Unit</h2><p>该部分是对各部分具体的公式讲解，实际是GRU的具体公式算法，不再此详细叙述了。</p>
<h3 id="reset-gate"><a href="#reset-gate" class="headerlink" title="reset gate"></a>reset gate</h3><blockquote>
<p>In this formulation, when the reset gate is close to 0, the hidden state is forced to ignore the pre- vious hidden state and reset with the current input only. This effectively allows the hidden state to drop any information that is found to be irrelevant later in the future, thus, allowing a more compact representation.</p>
</blockquote>
<p>这段原文主要讲解了复位门的作用：有效地允许隐藏状态丢弃在将来稍后发现不相关的任何信息，从而允许更紧凑的表示。</p>
<p><strong>当捕获短期依赖时，复位门活跃</strong></p>
<h3 id="update-gate"><a href="#update-gate" class="headerlink" title="update gate"></a>update gate</h3><blockquote>
<p>the update gate controls how much information from the previous hidden state will carry over to the current hidden state.</p>
</blockquote>
<p>更新门控制来自先前隐藏状态的多少信息将转移到当前隐藏状态。</p>
<p><strong>当捕获长期依赖时，更新门活跃</strong></p>
<h2 id="Statistical-Machine-Translation-SMT"><a href="#Statistical-Machine-Translation-SMT" class="headerlink" title="Statistical Machine Translation(SMT)"></a>Statistical Machine Translation(SMT)</h2><p><img src="https://i.loli.net/2019/01/05/5c30217d42b8e.jpg" alt></p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>这部分作者主要做了量化分析和性质分析，主要就是说他的模型怎么厉害。。。（没有具体的数值指标，翻译的还不是中英翻译，想看的话可以去看一下，就不贴实验结果了）。</p>
<h2 id="future"><a href="#future" class="headerlink" title="future"></a>future</h2><p>这里作者提出了可以用decoder生成的目标短语来替换原句中短语的思路，如果没记错的话，这个想法好像对后面的机器翻译有很大的指导作用。</p>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>seq2seq</tag>
      </tags>
  </entry>
  <entry>
    <title>《Entity Alignment between Knowledge Graphs Using Attribute Embeddings》阅读笔记</title>
    <url>/post/Entity%20Alignment%20between%20Knowledge%20Graphs%20Using%20Attribute%20Embeddings/</url>
    <content><![CDATA[<blockquote>
<p><a href="https://people.eng.unimelb.edu.au/jianzhongq/papers/AAAI2019_EntityAlignment.pdf" target="_blank" rel="noopener">论文下载地址</a>，知识图之间的实体对齐的任务旨在在代表相同现实世界实体的两个知识图中找到实体。本文最主要就是提出了属性字符嵌入(attribute character embeddings)的方法。</p>
</blockquote>
<a id="more"></a>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>我们的模型利用知识图中存在的大量属性三元组(attribute triples)并生成属性字符嵌入。 属性字符嵌入(attribute character embeddings)通过基于实体的属性计算实体之间的相似性，将实体嵌入从两个知识图移位到同一空间中。<br>我们使用传递规则来进一步丰富实体的属性数量以增强属性字符嵌入。</p>
<h2 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h2><ul>
<li>提出了两个KG之间实体对齐的框架，它由谓词对齐模块，嵌入学习模块和实体对齐模块组成。</li>
<li>提出了一种新颖的嵌入模型，它将实体嵌入与属性嵌入集成在一起，以便为两个KG学习统一的嵌入空间。</li>
<li>我们在三个真正的KG对上评估建议的模型。<br>结果表明，我们的模型在实体对齐任务上始终优于最先进的模型，hits@1超过50％。</li>
</ul>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><h3 id="模型总览"><a href="#模型总览" class="headerlink" title="模型总览"></a>模型总览</h3><p>predicate alignment, embedding learning, and entity alignment</p>
<p><img src="http://image.nysdy.com/20190329155385087492759.png" alt="20190329155385087492759.png"></p>
<h3 id="Predicate-Alignment"><a href="#Predicate-Alignment" class="headerlink" title="Predicate Alignment"></a>Predicate Alignment</h3><p>谓词对齐模块通过使用统一的命名方案重命名两个KG的谓词来合并两个KG，以便为关系嵌入提供统一的向量空间。dbp:bornIn vs. yago:wasBornIn 统一命名为 :bornIn。</p>
<p>为了找到部分匹配的谓词，作者计算谓词URI的最后部分的编辑距离（例如，bornIn与wasBornIn）并将0.95设置为相似性阈值。</p>
<h2 id="Embedding-Learning"><a href="#Embedding-Learning" class="headerlink" title="Embedding Learning"></a>Embedding Learning</h2><h3 id="Structure-Embedding"><a href="#Structure-Embedding" class="headerlink" title="Structure Embedding"></a>Structure Embedding</h3><p>作者采用TransE来学习对于实体的结构嵌入。与TransE不同的是，模型希望更关注已对齐的三元组，也就是包含对齐谓词的三元组。模型通过添加权重来实现这一目的。Structure embedding的目标函数如下：</p>
<p><img src="http://image.nysdy.com/2019040115540798067483.png" alt="2019040115540798067483.png"></p>
<p>count(r)是关系r出现的数量。</p>
<h3 id="Attribute-Character-Embedding"><a href="#Attribute-Character-Embedding" class="headerlink" title="Attribute Character Embedding"></a>Attribute Character Embedding</h3><p>对于属性字符嵌入，也参考TransE的思想，将谓词r解释为从头部实体h到属性a的转换。但是，相同的属性a可以在两个KG中以不同的形式出现，例如50.9989对50.9988888889作为实体的纬度; “Barack Obama”与“Barack Hussein Obama”作为人名等。因此，本文提出使用组合函数对属性值进行编码，并将属性三元组中每个元素的关系定义为h +r≈fa（a）。 这里，fa（a）是组合函数，a是属性值a = {c1，c2，c3，…，ct}的字符序列。 组合函数将属性值编码为单个向量，并将类似的属性值映射到类似的向量表示。 作者定义了三个组成函数如下。</p>
<h4 id="Sum-compositional-function-SUM"><a href="#Sum-compositional-function-SUM" class="headerlink" title="Sum compositional function (SUM)"></a>Sum compositional function (SUM)</h4><p>存在问题：包含相同字符不同顺序的属性值会有相同的向量表示</p>
<p><img src="http://image.nysdy.com/20190401155408045744772.png" alt="20190401155408045744772.png"></p>
<h4 id="LSTM-based-compositional-function-LSTM"><a href="#LSTM-based-compositional-function-LSTM" class="headerlink" title="LSTM-based compositional function (LSTM)."></a>LSTM-based compositional function (LSTM).</h4><p><img src="http://image.nysdy.com/20190401155408065416786.png" alt="20190401155408065416786.png"></p>
<h4 id="N-gram-based-compositional-function-N-gram"><a href="#N-gram-based-compositional-function-N-gram" class="headerlink" title="N-gram-based compositional function (N-gram)"></a>N-gram-based compositional function (N-gram)</h4><p><img src="http://image.nysdy.com/20190401155408070480904.png" alt="20190401155408070480904.png"></p>
<p>最后attribute character embedding目标函数：<img src="http://image.nysdy.com/20190401155408075852436.png" alt="20190401155408075852436.png"></p>
<h3 id="Joint-Learning-of-Structure-Embedding-and-Attribute-Character-Embedding"><a href="#Joint-Learning-of-Structure-Embedding-and-Attribute-Character-Embedding" class="headerlink" title="Joint Learning of Structure Embedding and Attribute Character Embedding"></a>Joint Learning of Structure Embedding and Attribute Character Embedding</h3><p>作者使用属性字符嵌入通过最小化以下目标函数将结构嵌入移动到相同的向量空间：</p>
<p><img src="http://image.nysdy.com/20190401155408233583946.png" alt="20190401155408233583946.png"></p>
<p>本文整体损失函数：</p>
<p><img src="http://image.nysdy.com/20190401155408096945378.png" alt="20190401155408096945378.png"></p>
<h3 id="Entity-Alignment"><a href="#Entity-Alignment" class="headerlink" title="Entity Alignment"></a>Entity Alignment</h3><p>在经过上述训练过程之后，来自不同KG的相似的实体将会有相似的向量表示，因此可通过</p>
<p><img src="http://image.nysdy.com/2019040115540811204332.png" alt="2019040115540811204332.png"></p>
<p>获得潜在实体对齐对<h_1, h_map>。此外，模型设定相似度阈值来过滤潜在实体对齐对，得到最终的对齐结果。</h_1,></p>
<h3 id="Triple-Enrichment-via-Transitivity-Rule"><a href="#Triple-Enrichment-via-Transitivity-Rule" class="headerlink" title="Triple Enrichment via Transitivity Rule"></a>Triple Enrichment via Transitivity Rule</h3><p>作者利用一阶逻辑传递关系来丰富三元组。即：存在<h_1,t_1,t>和<t, r_2,t_2>则可以推理出h_1+ (r_1.r_2) ≈ t_2</t,></h_1,t_1,t></p>
<h2 id="Database"><a href="#Database" class="headerlink" title="Database"></a>Database</h2><p>本文从 DBpedia (DBP)、LinkedGeoData (LGD)、Geonames (GEO) 和 YAGO 四个 KG 中抽取构建了三个数据集，分别是DBP-LGD、DBP-GEO和DBP-YAGO。具体的数据统计如下：</p>
<p><img src="http://image.nysdy.com/20190401155408149973345.png" alt="20190401155408149973345.png"></p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Entity-Alignment-Results"><a href="#Entity-Alignment-Results" class="headerlink" title="Entity Alignment Results"></a>Entity Alignment Results</h3><p>本文对比了三个相关的模型，分别是 TransE、MTransE 和 JAPE。试验结果表明，本文提出的模型在实体对齐任务上取得了全面的较大的提升，在三种组合函数中，N-gram函数的优势较为明显。此外，基于传递规则的三元组丰富模型对结果也有一定的提升。具体结果如下<img src="http://image.nysdy.com/20190401155408163227980.png" alt="20190401155408163227980.png"></p>
<h3 id="Rule-based-Entity-Alignment-Results"><a href="#Rule-based-Entity-Alignment-Results" class="headerlink" title="Rule-based Entity Alignment Results"></a>Rule-based Entity Alignment Results</h3><p>为了进一步衡量 attribute character embedding 捕获实体间相似信息的能力，本文设计了基于规则的实体对齐模型。本实验对比了三种不同的模型：以label的字符串相似度作为基础模型；针对数据集特点，在基础模型的基础之上增加了坐标属性，以此作为第二个模型；第三个模型是把本文提出的模型作为附加模型，与基础模型相结合。具体结果如下：</p>
<p><img src="http://image.nysdy.com/20190401155408175594725.png" alt="20190401155408175594725.png"></p>
<h3 id="KG-Completion-Results"><a href="#KG-Completion-Results" class="headerlink" title="KG Completion Results"></a>KG Completion Results</h3><p>本文还在KG补全任务上验证了模型的有效性。模型主要测试了链接预测和三元组分类两个标准任务，在这两个任务中，模型也取得了不错的效果。具体结果如下：</p>
<p><img src="http://image.nysdy.com/20190401155408179146202.png" alt="20190401155408179146202.png"></p>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
        <tag>知识图谱嵌入</tag>
      </tags>
  </entry>
  <entry>
    <title>《OK Google, What Is Your Ontology? Or/ Exploring Freebase Classification to Understand Google’s Knowledge Graph？》阅读笔记</title>
    <url>/post/OK%20Google,%20What%20Is%20Your%20Ontology?%20Or/%20Exploring%20Freebase%20Classification%20to%20Understand%20Google%E2%80%99s%20Knowledge%20Graph%EF%BC%9F/</url>
    <content><![CDATA[<blockquote>
<p>本论文详细阐述Freebase中的数据格式，并进行了重构。通过考虑整体架构的三个部分：Freebase类型系统及其缺乏继承和依赖于不兼容性，允许表示值的不确定性的实现，以及合并和拆分对象的实现。来对本体进行阐述。<a href="https://arxiv.org/pdf/1805.03885.pdf" target="_blank" rel="noopener">论文下载地址</a></p>
</blockquote>
<a id="more"></a>
<p>这篇论文重构了Freebase数据转储来理解谷歌语义搜索特征背后的本体。论文将会探索Freebase本体如何由许多力量塑造的，这些力量也通过深入研究本体论和一个小的相关性研究来形成分类系统。这些发现将会提供知识图谱专有黑盒的一瞥。</p>
<blockquote>
<p>The structures found in the Freebase/Knowledge Graph ontology will be analyzed in light of the findings on classification systems in a key text by Bowker and Star (2000) [5].</p>
</blockquote>
<h1 id="术语定义"><a href="#术语定义" class="headerlink" title="术语定义"></a>术语定义</h1><p><img src="https://i.loli.net/2019/03/03/5c7b61c1bd122.jpg" alt></p>
<h3 id="Object"><a href="#Object" class="headerlink" title="Object"></a>Object</h3><p>Freebase对象是一个全局唯一的标识符，它是Freebase中世界上某种东西的表示。</p>
<h3 id="Type"><a href="#Type" class="headerlink" title="Type"></a>Type</h3><p>Freebase类型用来表达类的概念。</p>
<h3 id="Property"><a href="#Property" class="headerlink" title="Property"></a>Property</h3><p>Freebase属性是描述对象如何链接到其他值或对象的关系。</p>
<h3 id="Property-Detail"><a href="#Property-Detail" class="headerlink" title="Property Detail"></a>Property Detail</h3><p>属性详细信息指的是可以通过属性链接的对象或值的约束。</p>
<h3 id="RDF-triple"><a href="#RDF-triple" class="headerlink" title="RDF triple"></a>RDF triple</h3><p>资源描述格式（RDF）是用于“三元组”（或N = 3元组）格式的数据表示的规范[17]。</p>
<h3 id="Ontology"><a href="#Ontology" class="headerlink" title="Ontology"></a>Ontology</h3><p>对于本文，Freebase本体是类型，属性和属性详细信息的正式结构和描述，用于指定对象如何相互关联。</p>
<h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><p>在本文中，架构指的是可以在本体中找到的一般模式和关系。</p>
<p><strong>==本体是否允许类（或Freebase用语中的类型）之间的继承？ 是否有与属性相关的默认值？ 如何处理“零”或空值？ 这些类型的问题不一定关注本体（飞机，火车或汽车）中具体表达的内容，而是关于本体表达方式的更多问题应该通过检查架构来解决。==</strong></p>
<h1 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h1><p>作者把数据进行切分：按照RDF中三元组的谓语进行分类，例如：<img src="https://i.loli.net/2019/03/03/5c7bc58f2052d.jpg" alt></p>
<h1 id="Freebase-Ontology-and-Classification"><a href="#Freebase-Ontology-and-Classification" class="headerlink" title="Freebase Ontology and Classification"></a>Freebase Ontology and Classification</h1><blockquote>
<p> As Bowker and Star note, “Information infrastructure is a tricky thing to analyze…the easier they are to use, the harder they are to see.” [5]. What does the system make sense of? What is left out? What is privileged and by extension what is ignored by Google?</p>
</blockquote>
<p>虽然Freebase本体可能不会立即看起来像一个分类系统，但类型（类）和属性的结构是一个基于对各种事物进行分类的系统。</p>
<p>作为对世界事物表征进行排序和分类的系统，将根据Bowker和Star的分类结果讨论Freebase本体。他们将对亚里士多德和原型分类（Aristotelian and prototype classification）进行了区分。</p>
<p>亚里士多德的分类“按照一组二元特征进行操作，被分类的物体呈现或不呈现”，而原型分类则认为“在我们心目中对于椅子是什么的广泛描述; 我们用隐喻和类比来扩展这张图片“</p>
<h2 id="5-1-Freebase’s-Type-System"><a href="#5-1-Freebase’s-Type-System" class="headerlink" title="5.1. Freebase’s Type System"></a>5.1. Freebase’s Type System</h2><p>不兼容性的概念出现在Freebase系统中，用于表示对象如何具有某些类型，而这些类型必须将其排除在其他类型之外。</p>
<p>没有继承（not implement inheritance）：上述不兼容性在确保数据不表达可能在Google KP中提供的令人尴尬，有害或不正确的陈述方面发挥了足够强大的作用。</p>
<p>缺乏继承也可能是一种允许实体具有更大灵活性的特征。这里作者举了一个狗为电影演员的例子。</p>
<h2 id="5-2-Has-Value-or-Has-No-Value"><a href="#5-2-Has-Value-or-Has-No-Value" class="headerlink" title="5.2. Has Value or Has No Value?"></a>5.2. Has Value or Has No Value?</h2><p>三元组如何表达估计值，不确定值或空值？实际处理时用“Has Value” (HV) and “Has No Value” (HNV)来分别表达不确定值和空值。</p>
<p>以这种方式表达未知数和空值的有趣实现可能表明Freebase / KG最初并不是为了支持这种不确定性而建立的。Google的数据编码某些不确定性的概念并未向最终用户公开，尽管它肯定以这种独特的方式实现。</p>
<h2 id="5-3-Dealing-with-Doppelgangers-and-Chimeras"><a href="#5-3-Dealing-with-Doppelgangers-and-Chimeras" class="headerlink" title="5.3. Dealing with Doppelgangers and Chimeras"></a>5.3. Dealing with Doppelgangers and Chimeras</h2><p>涉及Freebase如何处理“合并”重复对象（doppelgangers）和“拆分”混合对象（嵌合体）。 </p>
<blockquote>
<p>the property “/dataworld/gardening hint/replaced by” is used to implement merges be- tween various objects (e.g. by saying “/m/xyz123 - Replaced By - /m/abc123”).</p>
</blockquote>
<h1 id="A-Small-Correlational-Study"><a href="#A-Small-Correlational-Study" class="headerlink" title="A Small Correlational Study"></a>A Small Correlational Study</h1><p>主要探索这个问题：域的本体的复杂性（人物，电影等领域的类型，属性等）与表达与本体相关的事实（“知识库”）的三元组数量之间是否存在关联？</p>
<p>对于本研究，通过考虑与域相关的属性详细信息量（多少描述，约束等）来实现“复杂性”和“成熟度”。</p>
<p>对于89个域中的每一个，获得了关于每个域的本体的以下统计：</p>
<ul>
<li>==<strong>域中的类型和属性数</strong>==</li>
<li>==<strong>每种类型和属性的描述数</strong>==</li>
<li>==<strong>每种类型和属性的属性详细信息数==</strong></li>
</ul>
<p><strong>通过获取域中每种类型和属性的平均描述数和属性详细信息来计算简单的复杂性分数。 所有域的RDF三元组计数与此复杂性得分之间的Pearson相关系数与0.2824呈正相关，简单线性回归的斜率为78,424.08（见图6）。 当排除异常音乐切片时，相关性和斜率分别变为0.6680和33,899.53。 虽然需要进一步的工作来探索这个研究问题，但这个小的相关性研究为进一步的实验提供了一些有希望的初步结果</strong></p>
<h1 id="discussion"><a href="#discussion" class="headerlink" title="discussion"></a>discussion</h1><p>考虑整体架构的三个部分：Freebase类型系统及其缺乏继承和依赖于不兼容性，允许表示值的不确定性的实现，以及合并和拆分对象的实现。此外，还进行了一项小型相关研究，以检验基于Bowker和Star推动的预感的假设。在很大程度上，分类系统中的许多特征也可以在Freebase的本体和体系结构中找到。</p>
<p>本文具体而言，探讨了支持整个交付流程的基础结构（本体和体系结构），而不是Freebase / KG中表示的特定事实。</p>
<h1 id="conclusion"><a href="#conclusion" class="headerlink" title="conclusion"></a>conclusion</h1><p> 应通过探索Freebase本体和体系结构的其他方面以及对Freebase进行更全面的实验分析来进行进一步的研究。 </p>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
        <tag>Ontology</tag>
        <tag>freebase</tag>
      </tags>
  </entry>
  <entry>
    <title>《RECURRENT NEURAL NETWORK REGULARIZATION》阅读笔记</title>
    <url>/post/RECURRENT%20NEURAL%20NETWORK%20REGULARIZATION/</url>
    <content><![CDATA[<blockquote>
<p><a href="http://arxiv.org/abs/1409.2329" target="_blank" rel="noopener">论文链接</a>。这篇论文提出了LSTM的dropout策略来防止过拟合，即只在非循环链接处采取dropout。在BasicLSTMCell的接口就是依据这篇论文实现的。</p>
</blockquote>
<h2 id="文章整体架构和重点"><a href="#文章整体架构和重点" class="headerlink" title="文章整体架构和重点"></a>文章整体架构和重点</h2><p><img src="https://i.loli.net/2019/01/07/5c32ad34c99e0.jpg" alt></p>
<h2 id="contribution"><a href="#contribution" class="headerlink" title="contribution"></a>contribution</h2><ul>
<li>present a simple regularization technique</li>
</ul>
<h2 id="background"><a href="#background" class="headerlink" title="background"></a>background</h2><ul>
<li><p>dropout Srivastava(2013)，对于前向反馈网络最有力的正则化方法并不能很好的应用在RNNs上。这导致RNNs规模都很小，因为太大会过拟合。</p>
</li>
<li><p>Bayer et al. (2013)指出了卷积dropout不能在RNNs上很好工作的原因是循环会放大噪音。</p>
</li>
</ul>
<h2 id="model"><a href="#model" class="headerlink" title="model"></a>model</h2><h3 id="仿射变换（affine-transform）"><a href="#仿射变换（affine-transform）" class="headerlink" title="仿射变换（affine transform）"></a>仿射变换（affine transform）</h3><p>关于仿射变换：线性变换加上平移，盗个知乎上的图（<a href="https://www.zhihu.com/question/20666664" target="_blank" rel="noopener">原文链接</a>）</p>
<p><img src="https://i.loli.net/2019/01/07/5c32a7c05c698.jpg" alt></p>
<h3 id="模型主体"><a href="#模型主体" class="headerlink" title="模型主体"></a>模型主体</h3><p>采用的是Graves et al. (2013)<img src="https://i.loli.net/2019/01/07/5c32aaae11c93.jpg" alt></p>
<h3 id="dropout策略"><a href="#dropout策略" class="headerlink" title="dropout策略"></a>dropout策略</h3><blockquote>
<p>The main contribution of this paper is a recipe for applying dropout to LSTMs in a way that success- fully reduces overfitting. The main idea is to apply the dropout operator only to the non-recurrent connections.</p>
</blockquote>
<p><img src="https://i.loli.net/2019/01/07/5c32aaf4084fc.jpg" alt></p>
<p>观察公式，实际上就是通过在层间传递中应用dropout。如下图中虚线所示。</p>
<p><img src="https://i.loli.net/2019/01/07/5c32a99676abf.jpg" alt></p>
<p>从上图中也可以看到，该dropout的次数只和网络深度有关（数值为网络深度+1）。</p>
<h3 id="experiments"><a href="#experiments" class="headerlink" title="experiments"></a>experiments</h3><p>实验部分作者做了4部分实验来证明自己采用的方法有效，分别为language modeling, speech recognition, machine translation, image caption generation。这部分没什么需要解释了，感兴趣可以自己看一下实验。</p>
]]></content>
      <categories>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title>安装Ubuntu18.04</title>
    <url>/post/install%20Ubuntu18.04/</url>
    <content><![CDATA[<h1 id="t630安装Ubuntu18-04"><a href="#t630安装Ubuntu18-04" class="headerlink" title="t630安装Ubuntu18.04"></a>t630安装Ubuntu18.04</h1><ol>
<li>刻录u盘必须用过主引导分区格式。可以用<a href="https://www.balena.io" target="_blank" rel="noopener">制作启动盘工具</a></li>
<li>系统尽量选择sever版本，不需要图形界面。</li>
<li>采用UEFI方式安装，dell一般只支持这一种方式安装。重启服务器按F2，在boot中设置为UEFI格式。</li>
</ol>
<p><img src="http://image.nysdy.com/20200913223801.png" alt></p>
<p><img src="http://image.nysdy.com/20200914115538.png" alt></p>
<p><img src="http://image.nysdy.com/20200914115604.png" alt></p>
<p><img src="http://image.nysdy.com/20200914115626.png" alt></p>
<ol>
<li><p>完成后回退到到初始界面，按F11，设置U盘启动，安装系统。<img src="http://image.nysdy.com/20200914115720.png" alt></p>
<p><img src="http://image.nysdy.com/20200914115740.png" alt></p>
<p><img src="http://image.nysdy.com/20200914115803.png" alt></p>
<p>这时候等待安装即可。</p>
<p><img src="http://image.nysdy.com/20200914115845.png" alt></p>
<p>到这个界面，拔下U盘，点击回车。<img src="http://image.nysdy.com/20200914115958.png" alt></p>
</li>
</ol>
<p>其中：安装全部默认配置就行，IP地址不需要静态的，直接用动态的就好。</p>
<ol>
<li>安装过程可以参考[ <a href="https://blog.csdn.net/zhengchaooo/article/details/80145744" target="_blank" rel="noopener">Ubuntu 18.04 Server 版安装过程图文详解</a> ]（不看也行，全部默认操作即可。不用配置ip，网络选择默认DHCP。安装磁盘选择200G的系统盘即可）</li>
</ol>
<h1 id="ssh"><a href="#ssh" class="headerlink" title="ssh"></a>ssh</h1><p>安装过程中有让选择是否安装ssh，这步直接跳过就行，反正也没有外网，无法通过GitHub等地方下载ssh。</p>
<h2 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h2><h3 id="0-判断咱们的机器是否安装ssh服务，可以使用如下命令："><a href="#0-判断咱们的机器是否安装ssh服务，可以使用如下命令：" class="headerlink" title="0 判断咱们的机器是否安装ssh服务，可以使用如下命令："></a>0 判断咱们的机器是否安装ssh服务，可以使用如下命令：</h3><pre><code>ssh localhost
</code></pre><p>如果显示：</p>
<pre><code>ssh: connect to host localhost port 22: Connection refused
</code></pre><p>这个就表示没有还没有安装SSH</p>
<h3 id="1-安装openssh-client"><a href="#1-安装openssh-client" class="headerlink" title="1 安装openssh-client"></a>1 安装openssh-client</h3><pre><code>sudo apt-get install openssh-client
</code></pre><h3 id="2-安装openssh-server"><a href="#2-安装openssh-server" class="headerlink" title="2 安装openssh-server"></a>2 安装openssh-server</h3><pre><code>sudo apt-get install openssh-service
</code></pre><h3 id="3-启动ssh服务"><a href="#3-启动ssh服务" class="headerlink" title="3 启动ssh服务"></a>3 启动ssh服务</h3><pre><code>sudo service ssh start
</code></pre><p>然后可以查看ssh服务是否开启成功。</p>
<pre><code>sudo ps -e | grep ssh
</code></pre><p>然后要远程登陆，可以查看用ipconfig查看地址，如果出现command not found，则说明没有安装这个包。</p>
<pre><code>sudo apt install net-tools
</code></pre><p>apt-get install与apt install的区别前者是老版的用法，后者是新版的用法。apt与apt-get有一些类似的命令选项，但是apt并不能向下兼容apt-get。</p>
<h2 id="花絮踩坑记"><a href="#花絮踩坑记" class="headerlink" title="花絮踩坑记"></a>花絮踩坑记</h2><h3 id="1-第一个坑：ubuntu-18-04-Unable-to-locate-package-openssh-service"><a href="#1-第一个坑：ubuntu-18-04-Unable-to-locate-package-openssh-service" class="headerlink" title="1 第一个坑：ubuntu 18.04: Unable to locate package openssh-service"></a>1 第一个坑：ubuntu 18.04: Unable to locate package openssh-service</h3><pre><code>sudo apt-get update
sudo apt-get upgrade
sudo apt-get update
</code></pre><p>如果上述方法不行，则更换镜像源，见下一节。</p>
<p>参考链接：</p>
<ul>
<li><p><a href="https://www.cnblogs.com/duanxz/p/5532932.html" target="_blank" rel="noopener">https://www.cnblogs.com/duanxz/p/5532932.html</a></p>
</li>
<li><p><a href="https://www.centos.bz/2019/01/%E8%A7%A3%E5%86%B3ssh%E6%97%A0%E6%B3%95%E8%BF%9E%E6%8E%A5%E8%BF%9C%E7%A8%8Bubuntu%EF%BC%8Cuuntu%E5%AE%89%E8%A3%85ssh-server/" target="_blank" rel="noopener">https://www.centos.bz/2019/01/%E8%A7%A3%E5%86%B3ssh%E6%97%A0%E6%B3%95%E8%BF%9E%E6%8E%A5%E8%BF%9C%E7%A8%8Bubuntu%EF%BC%8Cuuntu%E5%AE%89%E8%A3%85ssh-server/</a></p>
</li>
<li><a href="https://blog.csdn.net/coolljp21/article/details/104090258" target="_blank" rel="noopener">https://blog.csdn.net/coolljp21/article/details/104090258</a></li>
</ul>
<h2 id="替换清华源"><a href="#替换清华源" class="headerlink" title="替换清华源"></a>替换清华源</h2><p>因为只有内网，可以用当前服务器去连接其他服务器进行sources.list文件替换，省得还要自己敲。</p>
<h2 id="防火墙知识"><a href="#防火墙知识" class="headerlink" title="防火墙知识"></a>防火墙知识</h2><pre><code>- 安装：sudo apt-get install ufw
- 查看状态：sudo ufw status
- 开启/关闭：sudo ufw enable | disable
- 默认允许/禁止：sudo ufw default allow | deny
- 允许/禁止：sudo ufw allow|deny 服务 | port，如：sudo ufw deny 22
- 移除规则：sudo ufw delete deny 22
- 允许范围端口：sudo ufw allow 5901:5999/tcp
</code></pre><h2 id="ubuntu-—-apt-get-install的时候提示unable-to-locate-package"><a href="#ubuntu-—-apt-get-install的时候提示unable-to-locate-package" class="headerlink" title="ubuntu — apt-get install的时候提示unable to locate package"></a>ubuntu — apt-get install的时候提示unable to locate package</h2><blockquote>
<p>由于ubuntu镜像一般默认自带的都是us的官方源<a href="http://us.archive.ubuntu.com，和http://security.ubuntu.com。这些镜像源的地址在中国大多数难以连接，因此需要换国内的源，国内的源有网易源、阿里源、科大源等等，替换阿里源。" target="_blank" rel="noopener">http://us.archive.ubuntu.com，和http://security.ubuntu.com。这些镜像源的地址在中国大多数难以连接，因此需要换国内的源，国内的源有网易源、阿里源、科大源等等，替换阿里源。</a></p>
</blockquote>
<pre><code>sudo su     #输入密码
cd /etc/apt  #切换到apt源文件
mv  sources.list sources.list_bak  #备份源文件
vim sources.list  #新建一个，然后将下面的内容copy进去
</code></pre><ul>
<li><p>替换全部为一下内容</p>
<pre><code># 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释
deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse
# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse
deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse
# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse
deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse
# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse
deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse
# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse

# 预发布软件源，不建议启用
# deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse
# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse
</code></pre></li>
<li><p>保存，然后执行</p>
</li>
</ul>
<pre><code>sudo apt-get update
</code></pre><p><strong>参考链接：</strong></p>
<ol>
<li><p><a href="https://www.codenong.com/cs106225795/" target="_blank" rel="noopener">https://www.codenong.com/cs106225795/</a></p>
</li>
<li><p><a href="https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/" target="_blank" rel="noopener">https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/</a></p>
</li>
</ol>
<h2 id="解决Host-key-verification-failed"><a href="#解决Host-key-verification-failed" class="headerlink" title="解决Host key verification failed"></a>解决Host key verification failed</h2><p>发生问题：</p>
<pre class=" language-shell"><code class="language-shell">$ ssh root@108.61.163.242
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!
Someone could be eavesdropping on you right now (man-in-the-middle attack)!
It is also possible that a host key has just been changed.
The fingerprint for the ECDSA key sent by the remote host is
SHA256:HDjXJvu0VYXWF+SKMZjSGn4FQmg/+w6eV9ljJvIXpx0.
Please contact your system administrator.
Add correct host key in /Users/wangdong/.ssh/known_hosts to get rid of this message.
Offending ECDSA key in /Users/wangdong/.ssh/known_hosts:46
ECDSA host key for 108.61.163.242 has changed and you have requested strict checking.
Host key verification failed.
</code></pre>
<p>一般这个问题，是你重置过你的服务器后。你再次想访问会出现这个问题。</p>
<p>解决问题也很简单：</p>
<pre class=" language-shell"><code class="language-shell">ssh-keygen -R 你要访问的IP地址
</code></pre>
<p><strong>参考链接：</strong></p>
<ul>
<li><a href="https://blog.csdn.net/wd2014610/article/details/85639741" target="_blank" rel="noopener">https://blog.csdn.net/wd2014610/article/details/85639741</a></li>
</ul>
<h1 id="安装显卡驱动"><a href="#安装显卡驱动" class="headerlink" title="安装显卡驱动"></a><a href="https://zhuanlan.zhihu.com/p/59618999" target="_blank" rel="noopener">安装显卡驱动</a></h1><h2 id="1-使用-Ubuntu-软件仓库中的稳定版本安装"><a href="#1-使用-Ubuntu-软件仓库中的稳定版本安装" class="headerlink" title="1. 使用 Ubuntu 软件仓库中的稳定版本安装"></a><strong>1. 使用 Ubuntu 软件仓库中的稳定版本安装</strong></h2><h3 id="1-1-查看显卡硬件型号"><a href="#1-1-查看显卡硬件型号" class="headerlink" title="1.1. 查看显卡硬件型号"></a><strong>1.1. 查看显卡硬件型号</strong></h3><p>在终端输入：<code>ubuntu-drivers devices</code>，可以看到如下界面：</p>
<p><img src="http://image.nysdy.com/20191205157552667542415.png" alt="20191205157552667542415.png"></p>
<p>从上图可以看出，我的显卡是：<code>[GeForce GTX 1080 Ti]</code>，所以推荐安装的版本号是<code>nvidia-driver-435 - distro non-free recommended</code>。</p>
<h3 id="1-2-开始安装"><a href="#1-2-开始安装" class="headerlink" title="1.2. 开始安装"></a><strong>1.2. 开始安装</strong></h3><ul>
<li>如果同意安装推荐版本，那我们只需要终端输入：<code>sudo ubuntu-drivers autoinstall</code> 就可以自动安装了。</li>
<li>当然我们也可以使用 apt 命令安装自己想要安装的版本，比如我想安装 <code>340</code> 这个版本号的版本，终端输入：<code>sudo apt install nvidia-340</code> 就自动安装了。</li>
<li><p>安装过程中按照提示操作，除非你知道每个提示的真实含义，否则所有的提示都选择默认就可以了，安装完成后重启系统，NVIDIA 显卡就可以正常工作了。安装完成后你可以参照 <code>https://linuxconfig.org/benchmark-your-graphics-card-on-linux</code> 上的介绍测试你的显卡。</p>
</li>
<li><p>最后<code>reboot</code>重启就可以了</p>
</li>
</ul>
<h2 id="其他方法"><a href="#其他方法" class="headerlink" title="其他方法"></a>其他方法</h2><p>见<a href="https://zhuanlan.zhihu.com/p/59618999" target="_blank" rel="noopener">链接</a></p>
<h2 id="命令使用"><a href="#命令使用" class="headerlink" title="命令使用"></a>命令使用</h2><h3 id="查看NVIDIA显卡的驱动版本"><a href="#查看NVIDIA显卡的驱动版本" class="headerlink" title="查看NVIDIA显卡的驱动版本"></a>查看NVIDIA显卡的驱动版本</h3><pre class=" language-shell"><code class="language-shell">cat /proc/driver/nvidia/version
</code></pre>
<h3 id="查看显卡名称以及驱动版本"><a href="#查看显卡名称以及驱动版本" class="headerlink" title="查看显卡名称以及驱动版本"></a>查看显卡名称以及驱动版本</h3><pre class=" language-shell"><code class="language-shell">nvidia-smi
</code></pre>
<h1 id="安装anaconda3"><a href="#安装anaconda3" class="headerlink" title="安装anaconda3"></a>安装anaconda3</h1><h3 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a>Linux</h3><ol>
<li>先去anaconda官网下好linux版本的anaconda，然后上传至服务器。</li>
<li>为了使所有用户都使用 Anaconda 自带的 Python，不能把 Anaconda 安装到默认的当前用户的 Home 目录。推荐安装到 <code>/opt</code> 目录。</li>
</ol>
<pre><code>bash anaconda-Linux-x86_64.sh -p /opt/anaconda3
</code></pre><p>接下来需要修改全局的环境变量，添加<code>export PATH=/opt/anaconda3/bin:$PATH</code>到<code>/etc/profile</code></p>
<h2 id><a href="#" class="headerlink" title=" "></a> </h2>]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>ubantu</tag>
        <tag>ssh</tag>
        <tag>显卡</tag>
      </tags>
  </entry>
  <entry>
    <title>《RelNN A Deep Neural Model for Relational Learning》阅读笔记</title>
    <url>/post/RelNN%20A%20Deep%20Neural%20Model%20for%20Relational%20Learning/</url>
    <content><![CDATA[<blockquote>
<p><a href="https://arxiv.org/pdf/1712.02831.pdf" target="_blank" rel="noopener">论文下载地址</a>，这篇文章相当于结合了统计学习和深度神经网络。里面有些公式没有理解，应该是有许多先前论文需要阅读。但是本篇论文扩展了思路如何结合统计学和深度学习，并且基于其余数据来预测一个类中对象的一个属性，想法也比较好。</p>
</blockquote>
<a id="more"></a>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>作者主要集中于基于其余数据来预测一个类中对象的一个属性。</p>
<h2 id="Challenge"><a href="#Challenge" class="headerlink" title="Challenge"></a>Challenge</h2><p>当类中每个对象的属性依赖于不同数量的其他对象的属性和关系时，此问题具有挑战性。 在StarAI社区中，此问题称为聚合（aggregation）。</p>
<h2 id="Relational-Logistic-Regression-and-Markov-Logic-Networks"><a href="#Relational-Logistic-Regression-and-Markov-Logic-Networks" class="headerlink" title="Relational Logistic Regression and Markov Logic Networks"></a>Relational Logistic Regression and Markov Logic Networks</h2><p>StarAI模型旨在模拟对象之间关系的概率。 </p>
<h3 id="Relational-logistic-regression-RLR-Kazemi-et-al-2014"><a href="#Relational-logistic-regression-RLR-Kazemi-et-al-2014" class="headerlink" title="Relational logistic regression (RLR) (Kazemi et al. 2014)"></a>Relational logistic regression (RLR) (Kazemi et al. 2014)</h3><p>定义的概率公式如下：</p>
<p><img src="http://image.nysdy.com/20190327155364804947436.png" alt="20190327155364804947436.png"></p>
<p>上面定义的RLR模型仅适用于布尔值或多值父项。作者采用的是连续的原子（continuous atoms）（Fatemi, Kazemi, and Poole (2016)）</p>
<h3 id="Relational-Neural-Networks"><a href="#Relational-Neural-Networks" class="headerlink" title="Relational Neural Networks"></a>Relational Neural Networks</h3><p>作者通过设计神经网络中线性层（LL），激活层（AL）和误差层（EL）的关系对应物，对具有分层架构的RLR / MLN模型进行编码。</p>
<p>关系神经网络（RelNN）是包含作为图形彼此连接的若干RLL和RAL的结构。</p>
<p><img src="http://image.nysdy.com/20190327155364863172988.png" alt="20190327155364863172988.png"></p>
<h2 id="Motivations-for-hidden-layers"><a href="#Motivations-for-hidden-layers" class="headerlink" title="Motivations for hidden layers"></a>Motivations for hidden layers</h2><ul>
<li>使喜欢看动作电影的人数增加时，男性的概率变为[0, 1]重的任何数值，不至于直接变为0或者1。</li>
<li>因此，隐藏层通过使模型能够学习通用规则并相应地对对象进行分类，然后以不同方式处理不同类别的对象，从而提高了建模能力。</li>
<li>使用RLR表示不同类型的现有显式聚合器，然而有些情况需要使用2个RLLs和2个RALs</li>
</ul>
<h2 id="Learning-latent-properties-directly"><a href="#Learning-latent-properties-directly" class="headerlink" title="Learning latent properties directly"></a>Learning latent properties directly</h2><p>对象可能包含无法使用常规规则指定的潜在属性，但可以在训练期间直接从数据中学习。</p>
<p><img src="http://image.nysdy.com/20190327155364936417890.png" alt="20190327155364936417890.png"></p>
<p>考虑图2中的模型，让Latent（m）成为电影的数字潜在属性，其值将在训练期间学习。</p>
<h2 id="From-ConvNet-Primitives-to-RelNNs"><a href="#From-ConvNet-Primitives-to-RelNNs" class="headerlink" title="From ConvNet Primitives to RelNNs"></a>From ConvNet Primitives to RelNNs</h2><p>我们解释为什么RelNN也可以被视为ConvNets的一个实例。</p>
<p>ConvNets的输入矩阵中的单元（例如，图像像素）具有空间相关性和空间冗余：彼此更接近的单元比更远的单元更依赖。 例如，如果M表示图像的输入通道，则M [i，j]和M [i + 1，j + 1]之间的依赖性可能远大于M [i，j]和M [i，j+20]之间的依赖性。</p>
<p>对于关系数据，输入矩阵中的依赖关系（关系）是不同的：同一行或列中的单元（即同一对象的关系）具有比不同行和列中的单元更高的依赖性（即不同对象的关系）。 因此，为了使ConvNets适应关系数据，我们需要矢量形状的过滤器，这些过滤器对行和列交换是不变的，并且更好地捕获关系依赖性和可交换性假设。</p>
<p><img src="http://image.nysdy.com/2019032715536511162060.png" alt="2019032715536511162060.png"></p>
<h2 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h2><h3 id="Movielens-1M-dataset-Harper-and-Konstan-2015"><a href="#Movielens-1M-dataset-Harper-and-Konstan-2015" class="headerlink" title="Movielens 1M dataset (Harper and Konstan 2015)"></a>Movielens 1M dataset (Harper and Konstan 2015)</h3><p>第一个数据集是Movielens 1M dataset (Harper and Konstan 2015)，忽略了实际的评级，只考虑电影是否被评级，只考虑动作和戏剧类型。</p>
<h3 id="PAKDD15"><a href="#PAKDD15" class="headerlink" title="PAKDD15"></a>PAKDD15</h3><p><a href="https://knowledgepit.fedcsis.org/contest/view.php?id=107" target="_blank" rel="noopener">获取地址</a></p>
<h3 id="all-Chinese-and-Mexican-restaurants-in-Yelp-dataset-challenge"><a href="#all-Chinese-and-Mexican-restaurants-in-Yelp-dataset-challenge" class="headerlink" title="all Chinese and Mexican restaurants in Yelp dataset challenge"></a>all Chinese and Mexican restaurants in Yelp dataset challenge</h3><p><a href="https://www.yelp.com/dataset_challenge" target="_blank" rel="noopener">获取地址</a></p>
<h2 id="Empirical-Results"><a href="#Empirical-Results" class="headerlink" title="Empirical Results"></a>Empirical Results</h2><p>作者提出了三个问题来进行实验：</p>
<h3 id="Q1：RelNN的性能与其他众所周知的关系学习算法相比如何？"><a href="#Q1：RelNN的性能与其他众所周知的关系学习算法相比如何？" class="headerlink" title="Q1：RelNN的性能与其他众所周知的关系学习算法相比如何？"></a>Q1：RelNN的性能与其他众所周知的关系学习算法相比如何？</h3><p><img src="http://image.nysdy.com/20190327155365126319780.png" alt="20190327155365126319780.png"></p>
<h3 id="Q2：基于数字和规则的潜在属性如何影响RelNN的性能"><a href="#Q2：基于数字和规则的潜在属性如何影响RelNN的性能" class="headerlink" title="Q2：基于数字和规则的潜在属性如何影响RelNN的性能?"></a>Q2：基于数字和规则的潜在属性如何影响RelNN的性能?</h3><p>更改了RelNN中隐藏图层和数字潜在属性的数量，以查看它们如何影响性能。</p>
<p><img src="http://image.nysdy.com/20190327155365168099042.png" alt="20190327155365168099042.png"></p>
<p>请注意，添加图层只会添加一定数量的参数，但添加k个数字潜在属性会增加k * |Δm|参数。</p>
<h3 id="Q3：RelNN如何推断出看不见的案例并解决指向的规模大小问题-（Poole-et-al-2014）"><a href="#Q3：RelNN如何推断出看不见的案例并解决指向的规模大小问题-（Poole-et-al-2014）" class="headerlink" title="Q3：RelNN如何推断出看不见的案例并解决指向的规模大小问题 （Poole et al.2014）?"></a>Q3：RelNN如何推断出看不见的案例并解决指向的规模大小问题 （Poole et al.2014）?</h3><p>作者实施了两个实验：</p>
<ul>
<li>我们在大量数据中训练一个RelNN，并在一小数据上进行测试：该实验可以看作每个模型受冷启动问题的严重程度</li>
<li>然后我们在一小数据上训练一个RelNN并在一大数据上进行测试：可以看作这些模型对更大群体的推断</li>
</ul>
<p><img src="http://image.nysdy.com/20190327155365190971381.png" alt="20190327155365190971381.png"></p>
<h2 id="Future"><a href="#Future" class="headerlink" title="Future"></a>Future</h2><p>作者将从数据中自动学习这些结构的问题留作未来的工作。</p>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>关系抽取</tag>
        <tag>统计学习</tag>
      </tags>
  </entry>
  <entry>
    <title>更新博客主题</title>
    <url>/post/Update-blog-theme/</url>
    <content><![CDATA[<p>正好赶上最近有空，修改升级一下自己的博客，之前用的是<code>next</code>主题，偶然间发现了<code>matery</code>主题，感觉更加漂亮一些，就决定更换一下，顺便记录自己的一些更换操作。</p>
<h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><h3 id="切换主题"><a href="#切换主题" class="headerlink" title="切换主题"></a>切换主题</h3><ol>
<li><p>下载相应主题放入<code>\theme</code>路径下即可</p>
</li>
<li><p>修改 Hexo 根目录下的 <code>_config.yml</code> 的 <code>theme</code> 的值：<code>theme: hexo-theme-matery</code></p>
</li>
</ol>
<h4 id="config-yml-文件的其它修改建议"><a href="#config-yml-文件的其它修改建议" class="headerlink" title="_config.yml 文件的其它修改建议:"></a><code>_config.yml</code> 文件的其它修改建议:</h4><ul>
<li>请修改 <code>_config.yml</code> 的 <code>url</code> 的值为你的网站主 <code>URL</code>（如：<code>http://xxx.github.io</code>）。</li>
<li>建议修改两个 <code>per_page</code> 的分页条数值为 <code>6</code> 的倍数，如：<code>12</code>、<code>18</code> 等，这样文章列表在各个屏幕下都能较好的显示。</li>
<li>如果你是中文用户，则建议修改 <code>language</code> 的值为 <code>zh-CN</code>。</li>
</ul>
<h3 id="新建分类-categories-页"><a href="#新建分类-categories-页" class="headerlink" title="新建分类 categories 页"></a>新建分类 categories 页</h3><p><code>categories</code> 页是用来展示所有分类的页面，如果在你的博客 <code>source</code> 目录下还没有 <code>categories/index.md</code> 文件，那么你就需要新建一个，命令如下：</p>
<pre class=" language-bash"><code class="language-bash">hexo new page <span class="token string">"categories"</span>
</code></pre>
<p>编辑你刚刚新建的页面文件 <code>/source/categories/index.md</code>，至少需要以下内容：</p>
<pre class=" language-yaml"><code class="language-yaml"><span class="token punctuation">---</span>
<span class="token key atrule">title</span><span class="token punctuation">:</span> 分类
<span class="token key atrule">date</span><span class="token punctuation">:</span> <span class="token datetime number">2018-09-24 12:03:14</span>
<span class="token key atrule">type</span><span class="token punctuation">:</span> <span class="token string">"categories"</span>
<span class="token key atrule">layout</span><span class="token punctuation">:</span> <span class="token string">"categories"</span>
<span class="token punctuation">---</span>
</code></pre>
<p>相比<code>next</code>而言，页面文件中多加了<code>layout</code>一项，该项中是否加入双引号均可。</p>
<h3 id="新建标签-tags-页"><a href="#新建标签-tags-页" class="headerlink" title="新建标签 tags 页"></a>新建标签 tags 页</h3><p>面，如果在你的博客 <code>source</code> 目录下还没有 <code>tags/index.md</code> 文件，那么你就需要新建一个，命令如下：</p>
<pre class=" language-bash"><code class="language-bash">hexo new page <span class="token string">"tags"</span>
</code></pre>
<p>编辑你刚刚新建的页面文件 <code>/source/tags/index.md</code>，至少需要以下内容：</p>
<pre class=" language-yaml"><code class="language-yaml"><span class="token punctuation">---</span>
<span class="token key atrule">title</span><span class="token punctuation">:</span> 标签
<span class="token key atrule">type</span><span class="token punctuation">:</span> tags
<span class="token key atrule">date</span><span class="token punctuation">:</span> <span class="token datetime number">2018-09-24 12:06:54</span>
<span class="token key atrule">layout</span><span class="token punctuation">:</span> tags
<span class="token punctuation">---</span>
</code></pre>
<h3 id="新建关于我-about-页"><a href="#新建关于我-about-页" class="headerlink" title="新建关于我 about 页"></a>新建关于我 about 页</h3><p><code>about</code> 页是用来展示<strong>关于我和我的博客</strong>信息的页面，如果在你的博客 <code>source</code> 目录下还没有 <code>about/index.md</code> 文件，那么你就需要新建一个，命令如下：</p>
<pre class=" language-bash"><code class="language-bash">hexo new page <span class="token string">"about"</span>
</code></pre>
<p>编辑你刚刚新建的页面文件 <code>/source/about/index.md</code>，至少需要以下内容：</p>
<pre class=" language-yaml"><code class="language-yaml"><span class="token punctuation">---</span>
<span class="token key atrule">title</span><span class="token punctuation">:</span> about
<span class="token key atrule">date</span><span class="token punctuation">:</span> <span class="token datetime number">2018-09-30 17:25:30</span>
<span class="token key atrule">type</span><span class="token punctuation">:</span> <span class="token string">"about"</span>
<span class="token key atrule">layout</span><span class="token punctuation">:</span> <span class="token string">"about"</span>
<span class="token punctuation">---</span>
</code></pre>
<h3 id="新建友情连接-friends-页（可选的）"><a href="#新建友情连接-friends-页（可选的）" class="headerlink" title="新建友情连接 friends 页（可选的）"></a>新建友情连接 friends 页（可选的）</h3><p><code>friends</code> 页是用来展示<strong>友情连接</strong>信息的页面，如果在你的博客 <code>source</code> 目录下还没有 <code>friends/index.md</code> 文件，那么你就需要新建一个，命令如下：</p>
<pre class=" language-bash"><code class="language-bash">hexo new page <span class="token string">"friends"</span>
</code></pre>
<p>编辑你刚刚新建的页面文件 <code>/source/friends/index.md</code>，至少需要以下内容：</p>
<pre class=" language-yaml"><code class="language-yaml"><span class="token punctuation">---</span>
<span class="token key atrule">title</span><span class="token punctuation">:</span> friends
<span class="token key atrule">date</span><span class="token punctuation">:</span> <span class="token datetime number">2020-03-25 00:35:38</span>
<span class="token key atrule">type</span><span class="token punctuation">:</span> <span class="token string">"friends"</span>
<span class="token key atrule">layout</span><span class="token punctuation">:</span> <span class="token string">"friends"</span>
<span class="token punctuation">---</span>
</code></pre>
<p>同时，在你的博客 <code>source</code> 目录下新建 <code>_data</code> 目录，在 <code>_data</code> 目录中新建 <code>friends.json</code> 文件，文件内容如下所示：</p>
<pre class=" language-json"><code class="language-json"><span class="token punctuation">[</span><span class="token punctuation">{</span>
    <span class="token property">"avatar"</span><span class="token operator">:</span> <span class="token string">"http://image.luokangyuan.com/1_qq_27922023.jpg"</span><span class="token punctuation">,</span>
    <span class="token property">"name"</span><span class="token operator">:</span> <span class="token string">"码酱"</span><span class="token punctuation">,</span>
    <span class="token property">"introduction"</span><span class="token operator">:</span> <span class="token string">"我不是大佬，只是在追寻大佬的脚步"</span><span class="token punctuation">,</span>
    <span class="token property">"url"</span><span class="token operator">:</span> <span class="token string">"http://luokangyuan.com/"</span><span class="token punctuation">,</span>
    <span class="token property">"title"</span><span class="token operator">:</span> <span class="token string">"前去学习"</span>
<span class="token punctuation">}</span><span class="token punctuation">,</span> <span class="token punctuation">{</span>
    <span class="token property">"avatar"</span><span class="token operator">:</span> <span class="token string">"http://image.luokangyuan.com/4027734.jpeg"</span><span class="token punctuation">,</span>
    <span class="token property">"name"</span><span class="token operator">:</span> <span class="token string">"闪烁之狐"</span><span class="token punctuation">,</span>
    <span class="token property">"introduction"</span><span class="token operator">:</span> <span class="token string">"编程界大佬，技术牛，人还特别好，不懂的都可以请教大佬"</span><span class="token punctuation">,</span>
    <span class="token property">"url"</span><span class="token operator">:</span> <span class="token string">"https://blinkfox.github.io/"</span><span class="token punctuation">,</span>
    <span class="token property">"title"</span><span class="token operator">:</span> <span class="token string">"前去学习"</span>
<span class="token punctuation">}</span><span class="token punctuation">,</span> <span class="token punctuation">{</span>
    <span class="token property">"avatar"</span><span class="token operator">:</span> <span class="token string">"http://image.luokangyuan.com/avatar.jpg"</span><span class="token punctuation">,</span>
    <span class="token property">"name"</span><span class="token operator">:</span> <span class="token string">"ja_rome"</span><span class="token punctuation">,</span>
    <span class="token property">"introduction"</span><span class="token operator">:</span> <span class="token string">"平凡的脚步也可以走出伟大的行程"</span><span class="token punctuation">,</span>
    <span class="token property">"url"</span><span class="token operator">:</span> <span class="token string">"ttps://me.csdn.net/jlh912008548"</span><span class="token punctuation">,</span>
    <span class="token property">"title"</span><span class="token operator">:</span> <span class="token string">"前去学习"</span>
<span class="token punctuation">}</span><span class="token punctuation">]</span>
</code></pre>
<h3 id="代码高亮"><a href="#代码高亮" class="headerlink" title="代码高亮"></a>代码高亮</h3><p>由于 Hexo 自带的代码高亮主题显示不好看，所以主题中使用到了 <a href="https://github.com/ele828/hexo-prism-plugin" target="_blank" rel="noopener">hexo-prism-plugin</a> 的 Hexo 插件来做代码高亮，安装命令如下：</p>
<pre class=" language-bash"><code class="language-bash"><span class="token function">npm</span> i -S hexo-prism-plugin
</code></pre>
<p>然后，修改 Hexo 根目录下 <code>_config.yml</code> 文件中 <code>highlight.enable</code> 的值为 <code>false</code>，并新增 <code>prism</code> 插件相关的配置，主要配置如下：</p>
<pre class=" language-yaml"><code class="language-yaml"><span class="token key atrule">highlight</span><span class="token punctuation">:</span>
  <span class="token key atrule">enable</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>

<span class="token key atrule">prism_plugin</span><span class="token punctuation">:</span>
  <span class="token key atrule">mode</span><span class="token punctuation">:</span> <span class="token string">'preprocess'</span>    <span class="token comment" spellcheck="true"># realtime/preprocess</span>
  <span class="token key atrule">theme</span><span class="token punctuation">:</span> <span class="token string">'tomorrow'</span>
  <span class="token key atrule">line_number</span><span class="token punctuation">:</span> <span class="token boolean important">false    </span><span class="token comment" spellcheck="true"># default false</span>
  <span class="token key atrule">custom_css</span><span class="token punctuation">:</span>
</code></pre>
<h3 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h3><p>本主题中还使用到了 <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener">hexo-generator-search</a> 的 Hexo 插件来做内容搜索，安装命令如下：</p>
<pre class=" language-bash"><code class="language-bash"><span class="token function">npm</span> <span class="token function">install</span> hexo-generator-search --save
</code></pre>
<p>在 Hexo 根目录下的 <code>_config.yml</code> 文件中，新增以下的配置项：</p>
<pre class=" language-yaml"><code class="language-yaml"><span class="token key atrule">search</span><span class="token punctuation">:</span>
  <span class="token key atrule">path</span><span class="token punctuation">:</span> search.xml
  <span class="token key atrule">field</span><span class="token punctuation">:</span> post
  <span class="token key atrule">format</span><span class="token punctuation">:</span> html
  <span class="token key atrule">limit</span><span class="token punctuation">:</span> <span class="token number">10000</span>
</code></pre>
<h3 id="中文链接转拼音（可选的）-未使用"><a href="#中文链接转拼音（可选的）-未使用" class="headerlink" title="中文链接转拼音（可选的）(未使用)"></a>中文链接转拼音（可选的）(未使用)</h3><p>如果你的文章名称是中文的，那么 Hexo 默认生成的永久链接也会有中文，这样不利于 <code>SEO</code>，且 <code>gitment</code> 评论对中文链接也不支持。我们可以用 <a href="https://github.com/viko16/hexo-permalink-pinyin" target="_blank" rel="noopener">hexo-permalink-pinyin</a> Hexo 插件使在生成文章时生成中文拼音的永久链接。</p>
<p>安装命令如下：</p>
<pre class=" language-bash"><code class="language-bash"><span class="token function">npm</span> i hexo-permalink-pinyin --save
</code></pre>
<p>在 Hexo 根目录下的 <code>_config.yml</code> 文件中，新增以下的配置项：</p>
<pre class=" language-yaml"><code class="language-yaml"><span class="token key atrule">permalink_pinyin</span><span class="token punctuation">:</span>
  <span class="token key atrule">enable</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
  <span class="token key atrule">separator</span><span class="token punctuation">:</span> <span class="token string">'-'</span> <span class="token comment" spellcheck="true"># default: '-'</span>
</code></pre>
<blockquote>
<p><strong>注</strong>：除了此插件外，<a href="https://github.com/rozbo/hexo-abbrlink" target="_blank" rel="noopener">hexo-abbrlink</a> 插件也可以生成非中文的链接。</p>
</blockquote>
<h3 id="文章字数统计插件（可选的）"><a href="#文章字数统计插件（可选的）" class="headerlink" title="文章字数统计插件（可选的）"></a>文章字数统计插件（可选的）</h3><p>如果你想要在文章中显示文章字数、阅读时长信息，可以安装 <a href="https://github.com/willin/hexo-wordcount" target="_blank" rel="noopener">hexo-wordcount</a>插件。</p>
<p>安装命令如下：</p>
<pre class=" language-bash"><code class="language-bash"><span class="token function">npm</span> i --save hexo-wordcount
</code></pre>
<p>然后只需在本主题下的 <code>_config.yml</code> 文件中，激活以下配置项即可：</p>
<pre class=" language-yaml"><code class="language-yaml"><span class="token key atrule">wordCount</span><span class="token punctuation">:</span>
  <span class="token key atrule">enable</span><span class="token punctuation">:</span> <span class="token boolean important">false </span><span class="token comment" spellcheck="true"># 将这个值设置为 true 即可.</span>
  <span class="token key atrule">postWordCount</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
  <span class="token key atrule">min2read</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
  <span class="token key atrule">totalCount</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
</code></pre>
<h3 id="添加-RSS-订阅支持（可选的）"><a href="#添加-RSS-订阅支持（可选的）" class="headerlink" title="添加 RSS 订阅支持（可选的）"></a>添加 RSS 订阅支持（可选的）</h3><p>本主题中还使用到了 <a href="https://github.com/hexojs/hexo-generator-feed" target="_blank" rel="noopener">hexo-generator-feed</a> 的 Hexo 插件来做 <code>RSS</code>，安装命令如下：</p>
<pre class=" language-bash"><code class="language-bash"><span class="token function">npm</span> <span class="token function">install</span> hexo-generator-feed --save
</code></pre>
<p>在 Hexo 根目录下的 <code>_config.yml</code> 文件中，新增以下的配置项：</p>
<pre class=" language-yaml"><code class="language-yaml"><span class="token key atrule">feed</span><span class="token punctuation">:</span>
  <span class="token key atrule">type</span><span class="token punctuation">:</span> atom
  <span class="token key atrule">path</span><span class="token punctuation">:</span> atom.xml
  <span class="token key atrule">limit</span><span class="token punctuation">:</span> <span class="token number">20</span>
  <span class="token key atrule">hub</span><span class="token punctuation">:</span>
  <span class="token key atrule">content</span><span class="token punctuation">:</span>
  <span class="token key atrule">content_limit</span><span class="token punctuation">:</span> <span class="token number">140</span>
  <span class="token key atrule">content_limit_delim</span><span class="token punctuation">:</span> <span class="token string">' '</span>
  <span class="token key atrule">order_by</span><span class="token punctuation">:</span> <span class="token punctuation">-</span>date
</code></pre>
<p>执行 <code>hexo clean &amp;&amp; hexo g</code> 重新生成博客文件，然后在 <code>public</code> 文件夹中即可看到 <code>atom.xml</code> 文件，说明你已经安装成功了。</p>
<h3 id="修改页脚"><a href="#修改页脚" class="headerlink" title="修改页脚"></a>修改页脚</h3><p>页脚信息可能需要做定制化修改，而且它不便于做成配置信息，所以可能需要你自己去再修改和加工。修改的地方在主题文件的 <code>/layout/_partial/footer.ejs</code> 文件中，包括站点、使用的主题、访问量等。</p>
<h3 id="修改社交链接"><a href="#修改社交链接" class="headerlink" title="修改社交链接"></a>修改社交链接</h3><p>在主题的 <code>_config.yml</code> 文件中，默认支持 <code>QQ</code>、<code>GitHub</code> 和邮箱的配置，你可以在主题文件的 <code>/layout/_partial/social-link.ejs</code> 文件中，新增、修改你需要的社交链接地址，增加链接可参考如下代码：</p>
<pre class=" language-html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">href</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>https://github.com/blinkfox<span class="token punctuation">"</span></span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>tooltipped<span class="token punctuation">"</span></span> <span class="token attr-name">target</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>_blank<span class="token punctuation">"</span></span> <span class="token attr-name">data-tooltip</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>访问我的GitHub<span class="token punctuation">"</span></span> <span class="token attr-name">data-position</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>top<span class="token punctuation">"</span></span> <span class="token attr-name">data-delay</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>50<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>i</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>fa fa-github<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>i</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">></span></span>
</code></pre>
<p>其中，社交图标（如：<code>fa-github</code>）你可以在 <a href="https://fontawesome.com/icons" target="_blank" rel="noopener">Font Awesome</a> 中搜索找到。以下是常用社交图标的标识，供你参考：</p>
<ul>
<li>Facebook: <code>fa-facebook</code></li>
<li>Twitter: <code>fa-twitter</code></li>
<li>Google-plus: <code>fa-google-plus</code></li>
<li>Linkedin: <code>fa-linkedin</code></li>
<li>Tumblr: <code>fa-tumblr</code></li>
<li>Medium: <code>fa-medium</code></li>
<li>Slack: <code>fa-slack</code></li>
<li>新浪微博: <code>fa-weibo</code></li>
<li>微信: <code>fa-wechat</code></li>
<li>QQ: <code>fa-qq</code></li>
</ul>
<blockquote>
<p><strong>注意</strong>: 本主题中使用的 <code>Font Awesome</code> 版本为 <code>4.7.0</code>。</p>
</blockquote>
<h3 id="修改打赏的二维码图片"><a href="#修改打赏的二维码图片" class="headerlink" title="修改打赏的二维码图片"></a>修改打赏的二维码图片</h3><p>在主题文件的 <code>source/medias/reward</code> 文件中，你可以替换成你的的微信和支付宝的打赏二维码图片。</p>
<h3 id="配置音乐播放器（可选的）-未使用"><a href="#配置音乐播放器（可选的）-未使用" class="headerlink" title="配置音乐播放器（可选的）(未使用)"></a>配置音乐播放器（可选的）(未使用)</h3><p>要支持音乐播放，就必须开启音乐的播放配置和音乐数据的文件。</p>
<p>首先，在你的博客 <code>source</code> 目录下的 <code>_data</code> 目录（没有的话就新建一个）中新建 <code>musics.json</code> 文件，文件内容如下所示：</p>
<pre class=" language-json"><code class="language-json"><span class="token punctuation">[</span><span class="token punctuation">{</span>
    <span class="token property">"name"</span><span class="token operator">:</span> <span class="token string">"五月雨变奏电音"</span><span class="token punctuation">,</span>
    <span class="token property">"artist"</span><span class="token operator">:</span> <span class="token string">"AnimeVibe"</span><span class="token punctuation">,</span>
    <span class="token property">"url"</span><span class="token operator">:</span> <span class="token string">"http://xxx.com/music1.mp3"</span><span class="token punctuation">,</span>
    <span class="token property">"cover"</span><span class="token operator">:</span> <span class="token string">"http://xxx.com/music-cover1.png"</span>
<span class="token punctuation">}</span><span class="token punctuation">,</span> <span class="token punctuation">{</span>
    <span class="token property">"name"</span><span class="token operator">:</span> <span class="token string">"Take me hand"</span><span class="token punctuation">,</span>
    <span class="token property">"artist"</span><span class="token operator">:</span> <span class="token string">"DAISHI DANCE,Cecile Corbel"</span><span class="token punctuation">,</span>
    <span class="token property">"url"</span><span class="token operator">:</span> <span class="token string">"/medias/music/music2.mp3"</span><span class="token punctuation">,</span>
    <span class="token property">"cover"</span><span class="token operator">:</span> <span class="token string">"/medias/music/cover2.png"</span>
<span class="token punctuation">}</span><span class="token punctuation">,</span> <span class="token punctuation">{</span>
    <span class="token property">"name"</span><span class="token operator">:</span> <span class="token string">"Shape of You"</span><span class="token punctuation">,</span>
    <span class="token property">"artist"</span><span class="token operator">:</span> <span class="token string">"J.Fla"</span><span class="token punctuation">,</span>
    <span class="token property">"url"</span><span class="token operator">:</span> <span class="token string">"http://xxx.com/music3.mp3"</span><span class="token punctuation">,</span>
    <span class="token property">"cover"</span><span class="token operator">:</span> <span class="token string">"http://xxx.com/music-cover3.png"</span>
<span class="token punctuation">}</span><span class="token punctuation">]</span>
</code></pre>
<blockquote>
<p><strong>注</strong>：以上 JSON 中的属性：<code>name</code>、<code>artist</code>、<code>url</code>、<code>cover</code> 分别表示音乐的名称、作者、音乐文件地址、音乐封面。</p>
</blockquote>
<p>然后，在主题的 <code>_config.yml</code> 配置文件中激活配置即可：</p>
<pre class=" language-yaml"><code class="language-yaml"><span class="token comment" spellcheck="true"># 是否在首页显示音乐.</span>
<span class="token key atrule">music</span><span class="token punctuation">:</span>
  <span class="token key atrule">enable</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
  <span class="token key atrule">showTitle</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>
  <span class="token key atrule">title</span><span class="token punctuation">:</span> 听听音乐
  <span class="token key atrule">fixed</span><span class="token punctuation">:</span> <span class="token boolean important">false </span><span class="token comment" spellcheck="true"># 是否开启吸底模式</span>
  <span class="token key atrule">autoplay</span><span class="token punctuation">:</span> <span class="token boolean important">false </span><span class="token comment" spellcheck="true"># 是否自动播放</span>
  <span class="token key atrule">theme</span><span class="token punctuation">:</span> '<span class="token comment" spellcheck="true">#42b983'</span>
  <span class="token key atrule">loop</span><span class="token punctuation">:</span> <span class="token string">'all'</span> <span class="token comment" spellcheck="true"># 音频循环播放, 可选值: 'all', 'one', 'none'</span>
  <span class="token key atrule">order</span><span class="token punctuation">:</span> <span class="token string">'list'</span> <span class="token comment" spellcheck="true"># 音频循环顺序, 可选值: 'list', 'random'</span>
  <span class="token key atrule">preload</span><span class="token punctuation">:</span> <span class="token string">'auto'</span> <span class="token comment" spellcheck="true"># 预加载，可选值: 'none', 'metadata', 'auto'</span>
  <span class="token key atrule">volume</span><span class="token punctuation">:</span> <span class="token number">0.7 </span><span class="token comment" spellcheck="true"># 默认音量，请注意播放器会记忆用户设置，用户手动设置音量后默认音量即失效</span>
  <span class="token key atrule">listFolded</span><span class="token punctuation">:</span> <span class="token boolean important">false </span><span class="token comment" spellcheck="true"># 列表默认折叠</span>
  <span class="token key atrule">listMaxHeight</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># 列表最大高度</span>
</code></pre>
<h3 id="Front-matter-选项详解"><a href="#Front-matter-选项详解" class="headerlink" title="Front-matter 选项详解"></a>Front-matter 选项详解</h3><p><code>Front-matter</code> 选项中的所有内容均为<strong>非必填</strong>的。但我仍然建议至少填写 <code>title</code> 和 <code>date</code> 的值。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">配置选项</th>
<th style="text-align:left">默认值</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">title</td>
<td style="text-align:left"><code>Markdown</code> 的文件标题</td>
<td style="text-align:left">文章标题，强烈建议填写此选项</td>
</tr>
<tr>
<td style="text-align:left">date</td>
<td style="text-align:left">文件创建时的日期时间</td>
<td style="text-align:left">发布时间，强烈建议填写此选项，且最好保证全局唯一</td>
</tr>
<tr>
<td style="text-align:left">author</td>
<td style="text-align:left">根 <code>_config.yml</code> 中的 <code>author</code></td>
<td style="text-align:left">文章作者</td>
</tr>
<tr>
<td style="text-align:left">img</td>
<td style="text-align:left"><code>featureImages</code> 中的某个值</td>
<td style="text-align:left">文章特征图，推荐使用图床(腾讯云、七牛云、又拍云等)来做图片的路径.如: <code>http://xxx.com/xxx.jpg</code></td>
</tr>
<tr>
<td style="text-align:left">top</td>
<td style="text-align:left"><code>true</code></td>
<td style="text-align:left">推荐文章（文章是否置顶），如果 <code>top</code> 值为 <code>true</code>，则会作为首页推荐文章</td>
</tr>
<tr>
<td style="text-align:left">cover</td>
<td style="text-align:left"><code>false</code></td>
<td style="text-align:left"><code>v1.0.2</code>版本新增，表示该文章是否需要加入到首页轮播封面中</td>
</tr>
<tr>
<td style="text-align:left">coverImg</td>
<td style="text-align:left">无</td>
<td style="text-align:left"><code>v1.0.2</code>版本新增，表示该文章在首页轮播封面需要显示的图片路径，如果没有，则默认使用文章的特色图片</td>
</tr>
<tr>
<td style="text-align:left">password</td>
<td style="text-align:left">无</td>
<td style="text-align:left">文章阅读密码，如果要对文章设置阅读验证密码的话，就可以设置 <code>password</code> 的值，该值必须是用 <code>SHA256</code> 加密后的密码，防止被他人识破。前提是在主题的 <code>config.yml</code> 中激活了 <code>verifyPassword</code> 选项</td>
</tr>
<tr>
<td style="text-align:left">toc</td>
<td style="text-align:left"><code>true</code></td>
<td style="text-align:left">是否开启 TOC，可以针对某篇文章单独关闭 TOC 的功能。前提是在主题的 <code>config.yml</code> 中激活了 <code>toc</code> 选项</td>
</tr>
<tr>
<td style="text-align:left">mathjax</td>
<td style="text-align:left"><code>false</code></td>
<td style="text-align:left">是否开启数学公式支持 ，本文章是否开启 <code>mathjax</code>，且需要在主题的 <code>_config.yml</code> 文件中也需要开启才行</td>
</tr>
<tr>
<td style="text-align:left">summary</td>
<td style="text-align:left">无</td>
<td style="text-align:left">文章摘要，自定义的文章摘要内容，如果这个属性有值，文章卡片摘要就显示这段文字，否则程序会自动截取文章的部分内容作为摘要</td>
</tr>
<tr>
<td style="text-align:left">categories</td>
<td style="text-align:left">无</td>
<td style="text-align:left">文章分类，本主题的分类表示宏观上大的分类，只建议一篇文章一个分类</td>
</tr>
<tr>
<td style="text-align:left">tags</td>
<td style="text-align:left">无</td>
<td style="text-align:left">文章标签，一篇文章可以多个标签</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p><strong>注意</strong>:</p>
<ol>
<li>如果 <code>img</code> 属性不填写的话，文章特色图会根据文章标题的 <code>hashcode</code> 的值取余，然后选取主题中对应的特色图片，从而达到让所有文章都的特色图<strong>各有特色</strong>。</li>
<li><code>date</code> 的值尽量保证每篇文章是唯一的，因为本主题中 <code>Gitalk</code> 和 <code>Gitment</code> 识别 <code>id</code> 是通过 <code>date</code> 的值来作为唯一标识的。</li>
<li>如果要对文章设置阅读验证密码的功能，不仅要在 Front-matter 中设置采用了 SHA256 加密的 password 的值，还需要在主题的 <code>_config.yml</code> 中激活了配置。有些在线的 SHA256 加密的地址，可供你使用：<a href="http://tool.oschina.net/encrypt?type=2" target="_blank" rel="noopener">开源中国在线工具</a>、<a href="http://encode.chahuo.com/" target="_blank" rel="noopener">chahuo</a>、<a href="http://tool.chinaz.com/tools/hash.aspx" target="_blank" rel="noopener">站长工具</a>。</li>
</ol>
</blockquote>
<p>以下为文章的 <code>Front-matter</code> 示例。</p>
<h3 id="最简示例"><a href="#最简示例" class="headerlink" title="最简示例"></a>最简示例</h3><pre class=" language-yaml"><code class="language-yaml"><span class="token punctuation">---</span>
<span class="token key atrule">title</span><span class="token punctuation">:</span> typora<span class="token punctuation">-</span>vue<span class="token punctuation">-</span>theme主题介绍
<span class="token key atrule">date</span><span class="token punctuation">:</span> <span class="token datetime number">2018-09-07 09:25:00</span>
<span class="token punctuation">---</span>
</code></pre>
<h3 id="最全示例"><a href="#最全示例" class="headerlink" title="最全示例"></a>最全示例</h3><pre class=" language-yaml"><code class="language-yaml"><span class="token punctuation">---</span>
<span class="token key atrule">title</span><span class="token punctuation">:</span> typora<span class="token punctuation">-</span>vue<span class="token punctuation">-</span>theme主题介绍
<span class="token key atrule">date</span><span class="token punctuation">:</span> <span class="token datetime number">2018-09-07 09:25:00</span>
<span class="token key atrule">author</span><span class="token punctuation">:</span> 赵奇
<span class="token key atrule">img</span><span class="token punctuation">:</span> /source/images/xxx.jpg
<span class="token key atrule">top</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
<span class="token key atrule">cover</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
<span class="token key atrule">coverImg</span><span class="token punctuation">:</span> /images/1.jpg
<span class="token key atrule">password</span><span class="token punctuation">:</span> 8d969eef6ecad3c29a3a629280e686cf0c3f5d5a86aff3ca12020c923adc6c92
<span class="token key atrule">toc</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>
<span class="token key atrule">mathjax</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>
<span class="token key atrule">summary</span><span class="token punctuation">:</span> 这是你自定义的文章摘要内容，如果这个属性有值，文章卡片摘要就显示这段文字，否则程序会自动截取文章的部分内容作为摘要
<span class="token key atrule">categories</span><span class="token punctuation">:</span> Markdown
<span class="token key atrule">tags</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> Typora
  <span class="token punctuation">-</span> Markdown
<span class="token punctuation">---</span>
</code></pre>
<h3 id="修改主题颜色"><a href="#修改主题颜色" class="headerlink" title="修改主题颜色"></a>修改主题颜色</h3><p>在主题文件的 <code>/source/css/matery.css</code> 文件中，搜索 <code>.bg-color</code> 来修改背景颜色：</p>
<pre class=" language-css"><code class="language-css"><span class="token comment" spellcheck="true">/* 整体背景颜色，包括导航、移动端的导航、页尾、标签页等的背景颜色. */</span>
<span class="token selector"><span class="token class">.bg-color</span> </span><span class="token punctuation">{</span>
    <span class="token property">background-image</span><span class="token punctuation">:</span> <span class="token function">linear-gradient</span><span class="token punctuation">(</span>to right, <span class="token hexcode">#4cbf30</span> <span class="token number">0%</span>, <span class="token hexcode">#0f9d58</span> <span class="token number">100%</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>

<span class="token atrule"><span class="token rule">@-webkit-keyframes</span> rainbow</span> <span class="token punctuation">{</span>
   <span class="token comment" spellcheck="true">/* 动态切换背景颜色. */</span>
<span class="token punctuation">}</span>

<span class="token atrule"><span class="token rule">@keyframes</span> rainbow</span> <span class="token punctuation">{</span>
    <span class="token comment" spellcheck="true">/* 动态切换背景颜色. */</span>
<span class="token punctuation">}</span>
</code></pre>
<h3 id="修改-banner-图和文章特色图"><a href="#修改-banner-图和文章特色图" class="headerlink" title="修改 banner 图和文章特色图"></a>修改 banner 图和文章特色图</h3><p>你可以直接在 <code>/source/medias/banner</code> 文件夹中更换你喜欢的 <code>banner</code> 图片，主题代码中是每天动态切换一张，只需 <code>7</code> 张即可。如果你会 <code>JavaScript</code> 代码，可以修改成你自己喜欢切换逻辑，如：随机切换等，<code>banner</code> 切换的代码位置在 <code>/layout/_partial/bg-cover-content.ejs</code> 文件的 `` 代码中：</p>
<pre class=" language-javascript"><code class="language-javascript"><span class="token function">$</span><span class="token punctuation">(</span><span class="token string">'.bg-cover'</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">css</span><span class="token punctuation">(</span><span class="token string">'background-image'</span><span class="token punctuation">,</span> <span class="token string">'url(/medias/banner/'</span> <span class="token operator">+</span> <span class="token keyword">new</span> <span class="token class-name">Date</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">getDay</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">'.jpg)'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
<p>在 <code>/source/medias/featureimages</code> 文件夹中默认有 24 张特色图片，你可以再增加或者减少，并需要在 <code>_config.yml</code> 做同步修改。</p>
<h2 id="动态标签栏"><a href="#动态标签栏" class="headerlink" title="动态标签栏"></a>动态标签栏</h2><p>在<code>theme/matery/layout/layout.ejs</code>下添加如下代码：</p>
<pre class=" language-html"><code class="language-html">    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>script</span> <span class="token attr-name">type</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>text/javascript<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token script language-javascript"> <span class="token keyword">var</span> OriginTitile <span class="token operator">=</span> document<span class="token punctuation">.</span>title<span class="token punctuation">,</span> st<span class="token punctuation">;</span> document<span class="token punctuation">.</span><span class="token function">addEventListener</span><span class="token punctuation">(</span><span class="token string">"visibilitychange"</span><span class="token punctuation">,</span> <span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span> document<span class="token punctuation">.</span>hidden <span class="token operator">?</span> <span class="token punctuation">(</span>document<span class="token punctuation">.</span>title <span class="token operator">=</span> <span class="token string">"Σ(っ °Д °;)っ喔哟，崩溃啦！"</span><span class="token punctuation">,</span> <span class="token function">clearTimeout</span><span class="token punctuation">(</span>st<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">:</span> <span class="token punctuation">(</span>document<span class="token punctuation">.</span>title <span class="token operator">=</span> <span class="token string">"φ(゜▽゜*)♪咦，又好了！"</span><span class="token punctuation">,</span> st <span class="token operator">=</span> <span class="token function">setTimeout</span><span class="token punctuation">(</span><span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span> document<span class="token punctuation">.</span>title <span class="token operator">=</span> OriginTitile <span class="token punctuation">}</span><span class="token punctuation">,</span> <span class="token number">3e3</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">}</span><span class="token punctuation">)</span>
    </span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>script</span><span class="token punctuation">></span></span>
</code></pre>
<h2 id="添加数学公式显示"><a href="#添加数学公式显示" class="headerlink" title="添加数学公式显示"></a>添加数学公式显示</h2><h3 id="安装与配置"><a href="#安装与配置" class="headerlink" title="安装与配置"></a>安装与配置</h3><pre><code>$ npm install hexo-math --save
</code></pre><p>在站点配置文件 <em>_config.yml</em> 中添加：</p>
<pre><code>math:
  engine: &#39;mathjax&#39; # or &#39;katex&#39;
  mathjax:
    # src: custom_mathjax_source
    config:
      # MathJax config
</code></pre><p>在 next 主题配置文件中 <em>themes/next-theme/_config.yml</em> 中将 mathJax 设为 true:
　　</p>
<pre><code># MathJax Support
mathjax:
  enable: true
  per_page: true
  cdn: //cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML
</code></pre><p>也可以在文章的开始集成插件支持，但不建议这么做：
　　</p>
<pre><code>&lt;script type=&quot;text/javascript&quot;
   src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;
&lt;/script&gt;
</code></pre><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><p>公式插入格式：
　　</p>
<pre><code>$数学公式$ 行内 不独占一行
$$数学公式$$ 行间 独占一行
</code></pre><p>例如：
　　</p>
<pre><code>$f(x)=ax+b$
</code></pre><p>如果是行间则使用：
　　</p>
<pre><code>$$f(x)=ax+b$$
</code></pre><p>更换 Hexo 的 markdown 渲染引擎，hexo-renderer-kramed 引擎是在默认的渲染引擎 hexo-renderer-marked 的基础上修改了一些 bug ，两者比较接近，也比较轻量级。</p>
<pre class=" language-shell"><code class="language-shell">$ npm uninstall hexo-renderer-marked --save
$ npm install hexo-renderer-kramed --save
</code></pre>
<p>执行上面的命令即可，先卸载原来的渲染引擎，再安装新的。<br>然后，跟换引擎后行间公式可以正确渲染了，但是这样还没有完全解决问题，行内公式的渲染还是有问题，因为 hexo-renderer-kramed 引擎也有语义冲突的问题。接下来到博客根目录下，找到node_modules\kramed\lib\rules\inline.js，把第11行的 escape 变量的值做相应的修改：</p>
<pre class=" language-markdown"><code class="language-markdown">  //escape: /^\\([\\`<span class="token italic"><span class="token punctuation">*</span>{}\[\]()#$+\-.!_>])/,
  escape: /^\\([`<span class="token punctuation">*</span></span>\[\]()#$+\-.!_>])/,
</code></pre>
<p>这一步是在原基础上取消了对\,{,}的转义(escape)。<br>同时把第20行的em变量也要做相应的修改。</p>
<pre class=" language-markdown"><code class="language-markdown">  //em: /^\b<span class="token italic"><span class="token punctuation">_</span>((?:<span class="token punctuation">_</span></span><span class="token italic"><span class="token punctuation">_</span>|[\s\S])+?)<span class="token punctuation">_</span></span>\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,
  em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,
</code></pre>
<p>重新启动hexo（先clean再generate）,问题完美解决。哦，如果不幸还没解决的话，看看是不是还需要在使用的主题中配置mathjax开关。</p>
<h2 id="解决MathJax和prism插件冲突"><a href="#解决MathJax和prism插件冲突" class="headerlink" title="解决MathJax和prism插件冲突"></a>解决MathJax和prism插件冲突</h2><p>文章开头分析的结果，只要我们把语法响亮的<code>pre</code>的<code>class</code>属性由原的<code>class=&quot;language-lang-*&quot;</code>变成了<code>class=&quot;language-*&quot;</code>即可对语法进行识别了，于是我对<code>kramed</code>的文件进行了分析，根据以往在<code>next</code>主题下的经验，修改两处了<code>langPrefix:</code>的参数。</p>
<p>一处在<code>/node_modules/kramed/lib/kramed.js</code>中的<strong>第134行</strong>：</p>
<pre class=" language-diff"><code class="language-diff">  // Renderer options
<span class="token deleted">-  langPrefix: 'lang-',</span>
<span class="token inserted">+  langPrefix: '',</span>
  smartypants: false,
  headerPrefix: '',
  headerAutoId: true,
  xhtml: false,
</code></pre>
<p>还有一处在<code>`/node_modules/kramed/lib/renderer.js</code>中的<strong>第10行</strong>：</p>
<pre class=" language-diff"><code class="language-diff">var defaultOptions = {
<span class="token deleted">-  langPrefix: 'lang-',</span>
<span class="token inserted">+  langPrefix: '',</span>
  smartypants: false,
  headerPrefix: '',
  headerAutoId: true,
  xhtml: false,
};
</code></pre>
<p>然后在重新进行渲染，原来的<code>class=&quot;language-lang-*&quot;</code>变成了<code>class=&quot;language-*&quot;</code>，语法响亮也就正常了。</p>
<hr>
<h2 id="增加建站时间"><a href="#增加建站时间" class="headerlink" title="增加建站时间"></a>增加建站时间</h2><p>修改<code>/themes/matery/layout/_partial/footer.ejs</code>文件，在最后加上</p>
<pre class=" language-js"><code class="language-js"><span class="token operator">&lt;</span>script language<span class="token operator">=</span>javascript<span class="token operator">></span>
    <span class="token keyword">function</span> <span class="token function">siteTime</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
        window<span class="token punctuation">.</span><span class="token function">setTimeout</span><span class="token punctuation">(</span><span class="token string">"siteTime()"</span><span class="token punctuation">,</span> <span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">var</span> seconds <span class="token operator">=</span> <span class="token number">1000</span><span class="token punctuation">;</span>
        <span class="token keyword">var</span> minutes <span class="token operator">=</span> seconds <span class="token operator">*</span> <span class="token number">60</span><span class="token punctuation">;</span>
        <span class="token keyword">var</span> hours <span class="token operator">=</span> minutes <span class="token operator">*</span> <span class="token number">60</span><span class="token punctuation">;</span>
        <span class="token keyword">var</span> days <span class="token operator">=</span> hours <span class="token operator">*</span> <span class="token number">24</span><span class="token punctuation">;</span>
        <span class="token keyword">var</span> years <span class="token operator">=</span> days <span class="token operator">*</span> <span class="token number">365</span><span class="token punctuation">;</span>
        <span class="token keyword">var</span> today <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Date</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">var</span> todayYear <span class="token operator">=</span> today<span class="token punctuation">.</span><span class="token function">getFullYear</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">var</span> todayMonth <span class="token operator">=</span> today<span class="token punctuation">.</span><span class="token function">getMonth</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">;</span>
        <span class="token keyword">var</span> todayDate <span class="token operator">=</span> today<span class="token punctuation">.</span><span class="token function">getDate</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">var</span> todayHour <span class="token operator">=</span> today<span class="token punctuation">.</span><span class="token function">getHours</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">var</span> todayMinute <span class="token operator">=</span> today<span class="token punctuation">.</span><span class="token function">getMinutes</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">var</span> todaySecond <span class="token operator">=</span> today<span class="token punctuation">.</span><span class="token function">getSeconds</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment" spellcheck="true">/* Date.UTC() -- 返回date对象距世界标准时间(UTC)1970年1月1日午夜之间的毫秒数(时间戳)
        year - 作为date对象的年份，为4位年份值
        month - 0-11之间的整数，做为date对象的月份
        day - 1-31之间的整数，做为date对象的天数
        hours - 0(午夜24点)-23之间的整数，做为date对象的小时数
        minutes - 0-59之间的整数，做为date对象的分钟数
        seconds - 0-59之间的整数，做为date对象的秒数
        microseconds - 0-999之间的整数，做为date对象的毫秒数 */</span>
        <span class="token keyword">var</span> t1 <span class="token operator">=</span> Date<span class="token punctuation">.</span><span class="token function">UTC</span><span class="token punctuation">(</span><span class="token number">2017</span><span class="token punctuation">,</span> <span class="token number">09</span><span class="token punctuation">,</span> <span class="token number">11</span><span class="token punctuation">,</span> <span class="token number">00</span><span class="token punctuation">,</span> <span class="token number">00</span><span class="token punctuation">,</span> <span class="token number">00</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//北京时间2018-2-13 00:00:00</span>
        <span class="token keyword">var</span> t2 <span class="token operator">=</span> Date<span class="token punctuation">.</span><span class="token function">UTC</span><span class="token punctuation">(</span>todayYear<span class="token punctuation">,</span> todayMonth<span class="token punctuation">,</span> todayDate<span class="token punctuation">,</span> todayHour<span class="token punctuation">,</span> todayMinute<span class="token punctuation">,</span> todaySecond<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">var</span> diff <span class="token operator">=</span> t2 <span class="token operator">-</span> t1<span class="token punctuation">;</span>
        <span class="token keyword">var</span> diffYears <span class="token operator">=</span> Math<span class="token punctuation">.</span><span class="token function">floor</span><span class="token punctuation">(</span>diff <span class="token operator">/</span> years<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">var</span> diffDays <span class="token operator">=</span> Math<span class="token punctuation">.</span><span class="token function">floor</span><span class="token punctuation">(</span><span class="token punctuation">(</span>diff <span class="token operator">/</span> days<span class="token punctuation">)</span> <span class="token operator">-</span> diffYears <span class="token operator">*</span> <span class="token number">365</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">var</span> diffHours <span class="token operator">=</span> Math<span class="token punctuation">.</span><span class="token function">floor</span><span class="token punctuation">(</span><span class="token punctuation">(</span>diff <span class="token operator">-</span> <span class="token punctuation">(</span>diffYears <span class="token operator">*</span> <span class="token number">365</span> <span class="token operator">+</span> diffDays<span class="token punctuation">)</span> <span class="token operator">*</span> days<span class="token punctuation">)</span> <span class="token operator">/</span> hours<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">var</span> diffMinutes <span class="token operator">=</span> Math<span class="token punctuation">.</span><span class="token function">floor</span><span class="token punctuation">(</span><span class="token punctuation">(</span>diff <span class="token operator">-</span> <span class="token punctuation">(</span>diffYears <span class="token operator">*</span> <span class="token number">365</span> <span class="token operator">+</span> diffDays<span class="token punctuation">)</span> <span class="token operator">*</span> days <span class="token operator">-</span> diffHours <span class="token operator">*</span> hours<span class="token punctuation">)</span> <span class="token operator">/</span> minutes<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">var</span> diffSeconds <span class="token operator">=</span> Math<span class="token punctuation">.</span><span class="token function">floor</span><span class="token punctuation">(</span><span class="token punctuation">(</span>diff <span class="token operator">-</span> <span class="token punctuation">(</span>diffYears <span class="token operator">*</span> <span class="token number">365</span> <span class="token operator">+</span> diffDays<span class="token punctuation">)</span> <span class="token operator">*</span> days <span class="token operator">-</span> diffHours <span class="token operator">*</span> hours <span class="token operator">-</span> diffMinutes <span class="token operator">*</span> minutes<span class="token punctuation">)</span> <span class="token operator">/</span> seconds<span class="token punctuation">)</span><span class="token punctuation">;</span>
        document<span class="token punctuation">.</span><span class="token function">getElementById</span><span class="token punctuation">(</span><span class="token string">"sitetime"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>innerHTML <span class="token operator">=</span> <span class="token string">"本站已运行 "</span> <span class="token operator">+</span>diffYears<span class="token operator">+</span><span class="token string">" 年 "</span><span class="token operator">+</span>diffDays <span class="token operator">+</span> <span class="token string">" 天 "</span> <span class="token operator">+</span> diffHours <span class="token operator">+</span> <span class="token string">" 小时 "</span> <span class="token operator">+</span> diffMinutes <span class="token operator">+</span> <span class="token string">" 分钟 "</span> <span class="token operator">+</span> diffSeconds <span class="token operator">+</span> <span class="token string">" 秒"</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span><span class="token comment" spellcheck="true">/*因为建站时间还没有一年，就将之注释掉了。需要的可以取消*/</span>
    <span class="token function">siteTime</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token operator">&lt;</span><span class="token operator">/</span>script<span class="token operator">></span>
</code></pre>
<p>然后在合适的地方（比如copyright声明后面）加上下面的代码就行了：</p>
<pre class=" language-html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>span</span> <span class="token attr-name">id</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>sitetime<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>span</span><span class="token punctuation">></span></span>
</code></pre>
<h2 id="在-hexo-new-之后立即打开新建的-Markdown-文稿"><a href="#在-hexo-new-之后立即打开新建的-Markdown-文稿" class="headerlink" title="在 hexo new 之后立即打开新建的 Markdown 文稿"></a>在 hexo new 之后立即打开新建的 Markdown 文稿</h2><ol>
<li>Tommy 指出，可以在 Hexo 目录下的 <code>scripts</code> 目录（若没有，则新建一个）中创建一个 JavaScript 脚本（该脚本名称随意指定），监听 <code>hexo new</code> 这个动作。并在检测到 <code>hexo new</code> 之后，执行编辑器打开的命令。</li>
</ol>
<pre class=" language-js"><code class="language-js"><span class="token keyword">var</span> exec <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">'child_process'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>exec<span class="token punctuation">;</span>

<span class="token comment" spellcheck="true">// Hexo 3</span>
hexo<span class="token punctuation">.</span><span class="token function">on</span><span class="token punctuation">(</span><span class="token string">'new'</span><span class="token punctuation">,</span> <span class="token keyword">function</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">{</span>
    <span class="token function">exec</span><span class="token punctuation">(</span><span class="token string">'open -a "/Applications/Typora.app" '</span> <span class="token operator">+</span> data<span class="token punctuation">.</span>path<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
<ol>
<li><p>安装<code>shelljs</code>模块，实现自动部署加载js脚本，键入以下命令：</p>
<pre><code>npm install --save shelljs
</code></pre><p>不知道该操作是否必要，但是这两步都做完后是可以完成自动打开功能了。</p>
</li>
</ol>
<h3 id="修改页面上方横条颜色"><a href="#修改页面上方横条颜色" class="headerlink" title="修改页面上方横条颜色"></a>修改页面上方横条颜色</h3><p>在<code>/themes/matery/source/css/matery.css</code>中对以下部分作出修改：</p>
<pre class=" language-css"><code class="language-css"><span class="token selector"><span class="token class">.bg-color</span> </span><span class="token punctuation">{</span>
    <span class="token property">background-image</span><span class="token punctuation">:</span> <span class="token function">linear-gradient</span><span class="token punctuation">(</span>to right, <span class="token hexcode">#a524be</span> <span class="token number">0%</span>, <span class="token hexcode">#0f9d58</span> <span class="token number">100%</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre>
<h3 id="增加首页封面轮播"><a href="#增加首页封面轮播" class="headerlink" title="增加首页封面轮播"></a>增加首页封面轮播</h3><pre class=" language-yml"><code class="language-yml"># Index cover carousel configuration.
# 首页封面轮播图的相关配置.
cover:
  showPrevNext: true # 是否显示左右切换按钮. Whether to display the left and right toggle buttons.
  showIndicators: true # 是否显示指示器. # Whether to display the indicators
  autoLoop: true # 是否自动轮播. Whether it is automatically rotated.
  duration: 120 # 切换延迟时间. Switching delay time.
  intervalTime: 5000 # 自动切换下一张的间隔时间. Automatically switch the interval of the next one.
</code></pre>
<h3 id="使用-LocalSearch-搜索功能"><a href="#使用-LocalSearch-搜索功能" class="headerlink" title="使用 LocalSearch 搜索功能"></a>使用 LocalSearch 搜索功能</h3><h4 id="安装相关插件"><a href="#安装相关插件" class="headerlink" title="安装相关插件"></a>安装相关插件</h4><p>安装搜索插件： <code>hexo-generator-searchdb</code></p>
<p>在博客根目录下执行以下命令：</p>
<pre><code>$ npm install hexo-generator-searchdb --save
</code></pre><h4 id="配置博客"><a href="#配置博客" class="headerlink" title="配置博客"></a>配置博客</h4><p>安装完成，编辑博客配置文件：<code>_config.yml</code></p>
<pre><code>search:
  path: search.xml
  field: post
  format: html
  limit: 10000
</code></pre><h4 id="配置主题"><a href="#配置主题" class="headerlink" title="配置主题"></a>配置主题</h4><p>主题自带搜索设置，编辑主题配置文件：<code>_config.yml</code></p>
<p>找到文件中 Local search 的相关配置，设为 <code>true</code></p>
<pre><code># Local search
local_search:
  enable: true
</code></pre><p>hexo 重新部署</p>
<h2 id="Hexo博文置顶（自定义排序）未设置"><a href="#Hexo博文置顶（自定义排序）未设置" class="headerlink" title="Hexo博文置顶（自定义排序）未设置"></a>Hexo博文置顶（自定义排序）未设置</h2><p>使用的是<code>top</code>属性，<code>top</code>值越高，排序越在前，不设置<code>top</code>值得博文按照时间顺序排序。<br>修改Hexo文件夹下的node_modules/hexo-generator-index/lib/generator.js</p>
<p>打开在最后添加如下<code>javascript</code>代码</p>
<pre class=" language-js"><code class="language-js">posts<span class="token punctuation">.</span>data <span class="token operator">=</span> posts<span class="token punctuation">.</span>data<span class="token punctuation">.</span><span class="token function">sort</span><span class="token punctuation">(</span><span class="token keyword">function</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span> <span class="token punctuation">{</span>
<span class="token keyword">if</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>top <span class="token operator">&amp;&amp;</span> b<span class="token punctuation">.</span>top<span class="token punctuation">)</span> <span class="token punctuation">{</span> <span class="token comment" spellcheck="true">// 两篇文章top都有定义</span>
    <span class="token keyword">if</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>top <span class="token operator">==</span> b<span class="token punctuation">.</span>top<span class="token punctuation">)</span> <span class="token keyword">return</span> b<span class="token punctuation">.</span>date <span class="token operator">-</span> a<span class="token punctuation">.</span>date<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 若top值一样则按照文章日期降序排</span>
    <span class="token keyword">else</span> <span class="token keyword">return</span> b<span class="token punctuation">.</span>top <span class="token operator">-</span> a<span class="token punctuation">.</span>top<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 否则按照top值降序排</span>
<span class="token punctuation">}</span>
<span class="token keyword">else</span> <span class="token keyword">if</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>top <span class="token operator">&amp;&amp;</span> <span class="token operator">!</span>b<span class="token punctuation">.</span>top<span class="token punctuation">)</span> <span class="token punctuation">{</span> <span class="token comment" spellcheck="true">// 以下是只有一篇文章top有定义，那么将有top的排在前面（这里用异或操作居然不行233）</span>
    <span class="token keyword">return</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
<span class="token keyword">else</span> <span class="token keyword">if</span><span class="token punctuation">(</span><span class="token operator">!</span>a<span class="token punctuation">.</span>top <span class="token operator">&amp;&amp;</span> b<span class="token punctuation">.</span>top<span class="token punctuation">)</span> <span class="token punctuation">{</span>
    <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
<span class="token keyword">else</span> <span class="token keyword">return</span> b<span class="token punctuation">.</span>date <span class="token operator">-</span> a<span class="token punctuation">.</span>date<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 都没定义按照文章日期降序排</span>
</code></pre>
<h2 id="添加评论插件"><a href="#添加评论插件" class="headerlink" title="添加评论插件"></a>添加评论插件</h2><h2 id="Valine"><a href="#Valine" class="headerlink" title="Valine"></a>Valine</h2><p>Valine官网：<a href="https://valine.js.org/" target="_blank" rel="noopener">Valine</a><br>安装步骤：</p>
<hr>
<p><strong>获取 APP ID 和 APP KEY</strong></p>
<ul>
<li><a href="https://leancloud.cn/dashboard/login.html#/signup" target="_blank" rel="noopener">点击这里</a>登录或注册Leancloud，或者<a href="https://avoscloud.com/" target="_blank" rel="noopener">点击这里</a></li>
<li><a href="https://leancloud.cn/dashboard/applist.html#/newapp" target="_blank" rel="noopener">点击这里</a>创建应用，应用名随意填写。</li>
<li>选择刚刚创建的应用&gt;设置&gt;选择应用 Key，然后你就能看到你的APP ID和APP KEY了，参考下图：<img src="http://image.nysdy.com/20200914104550.png" alt></li>
<li>为了您的数据安全，请填写应用&gt;设置&gt;安全设置中的Web 安全域名</li>
</ul>
<p><strong>修改主题下的_config.yml文件，添加valine配置</strong></p>
<pre class=" language-txt"><code class="language-txt"># valine配置
valine_appid: '填写leancloud的appid'
valine_appkey: '填写leancloud的appkey'
</code></pre>
<p><strong>修改themes/matery/layout/_partial/article.ejs，添加一段代码</strong></p>
<pre><code>&lt;% if (theme.valine_appid &amp;&amp; theme.valine_appkey){ %&gt;
    &lt;%- partial(&#39;post/valine&#39;, {
        key: post.slug,
        title: post.title,
        url: config.url+url_for(post.path)
      }) %&gt;
    &lt;% } %&gt;
</code></pre><p>注意：这段代码不可在尾部添加，应该添加至</p>
<pre><code>&lt;% if (!index &amp;&amp; post.comments){ %&gt;
//添加到这里
&lt;% } %&gt;
</code></pre><p>否则会出现主页冒出评论框的状况。</p>
<p><strong>新增themes/matery/layout/_partial/post/valine.ejs</strong></p>
<pre><code>&lt;div id=&quot;comment&quot;&gt;&lt;/div&gt;
&lt;script src=&quot;//cdn1.lncld.net/static/js/3.0.4/av-min.js&quot;&gt;&lt;/script&gt;
&lt;script src=&#39;//unpkg.com/valine/dist/Valine.min.js&#39;&gt;&lt;/script&gt;
&lt;script&gt;
new Valine({
    el: &#39;#comment&#39; ,
    notify:false, 
    verify:false, 
    appId: &#39;&lt;%=theme.valine_appid%&gt;&#39;,
    appKey: &#39;&lt;%=theme.valine_appkey%&gt;&#39;,
    placeholder: &#39;ヾﾉ≧∀≦)o欢迎评论!&#39;,
    path:window.location.pathname, 
    avatar:&#39;mm&#39; 
});
&lt;/script&gt;
</code></pre><h3 id="Valine-评论系统中的邮件提醒设置"><a href="#Valine-评论系统中的邮件提醒设置" class="headerlink" title="Valine 评论系统中的邮件提醒设置"></a>Valine 评论系统中的邮件提醒设置</h3><ul>
<li>进入Leancloud&gt;选择你的评论所存放的应用&gt;设置&gt;邮件模板，按下图设置好用于重置密码的邮件主题&gt;然后保存:</li>
</ul>
<p><img src="http://image.nysdy.com/20200914105503.png" alt></p>
<ul>
<li>修改邮件主题：你在的评论收到了新的回复</li>
<li>修改内容：将下面的代码复制到“内容”中，并将其中的你的网址首页链接改为你的网址首页链接。</li>
</ul>
<pre><code>&lt;p&gt;Hi, {{username}}&lt;/p&gt;
&lt;p&gt;
你在 {{appname}} 的评论收到了新的回复，请点击查看：
&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;你的网址首页链接&quot; style=&quot;display: inline-block; padding: 10px 20px; border-radius: 4px; background-color: #3090e4; color: #fff; text-decoration: none;&quot;&gt;马上查看&lt;/a&gt;&lt;/p&gt;
</code></pre><ul>
<li>点击“保存”按钮。</li>
<li>修改yilia主题配置文件，位置：你的网站根目录\themes\yilia\layout_partial\post\valine.ejs，将notify和verify的属性修改为true。</li>
</ul>
<pre><code>&lt;div id=&quot;comment&quot;&gt;&lt;/div&gt;
&lt;script src=&quot;//cdn1.lncld.net/static/js/3.0.4/av-min.js&quot;&gt;&lt;/script&gt;
&lt;script src=&#39;//unpkg.com/valine/dist/Valine.min.js&#39;&gt;&lt;/script&gt;
&lt;script&gt;
new Valine({
    el: &#39;#comment&#39; ,
    notify:true, 
    verify:true, 
    appId: &#39;&lt;%=theme.valine_appid%&gt;&#39;,
    appKey: &#39;&lt;%=theme.valine_appkey%&gt;&#39;,
    placeholder: &#39;吾以为inkbottle必有高论，岂期出此鄙言！吾有一言，诸军静听&#39;,
    path:window.location.pathname, 
    avatar:&#39;mm&#39; 
});
&lt;/script&gt;
</code></pre><ul>
<li>配置完毕后，重新部署，需等待一段时间。</li>
</ul>
<pre><code>hexo g -d
</code></pre><ul>
<li>进行测试，查看自己评论的回复能否收到邮件提醒。</li>
<li>配置完成！</li>
</ul>
<blockquote>
<p>注意事项：</p>
<ul>
<li>发送次数过多，可能会暂时被Leancloud 屏蔽邮件发送功能</li>
<li>由于邮件提醒功能使用的Leancloud的密码重置邮件提醒，只能传递昵称、邮箱两个属性，所以邮件提醒链&gt; 接无法直达指定文章页。请悉知。</li>
<li>开启邮件提醒会默认开启验证码选项。</li>
<li>该功能目前还在测试阶段，谨慎使用。</li>
<li>目前邮件提醒正处于测试阶段，仅在子级对存在邮件地址的父级发表评论时发送邮件</li>
</ul>
</blockquote>
<h1 id="待解决"><a href="#待解决" class="headerlink" title="待解决"></a>待解决</h1><ul>
<li></li>
<li>代码块形式</li>
</ul>
<h1 id="参考链接："><a href="#参考链接：" class="headerlink" title="参考链接："></a>参考链接：</h1><ul>
<li><a href="https://blinkfox.github.io/" target="_blank" rel="noopener">闪烁之狐</a> <a href="https://blinkfox.github.io/2018/09/28/qian-duan/hexo-bo-ke-zhu-ti-zhi-hexo-theme-matery-de-jie-shao/" target="_blank" rel="noopener">Hexo博客主题之hexo-theme-matery的介绍</a></li>
<li><a href="https://godweiyang.com/2018/04/13/hexo-blog/#toc-heading-19" target="_blank" rel="noopener">韦阳的博客</a></li>
<li><a href="https://shengbinyu.top/mathjaxprism.html" target="_blank" rel="noopener">https://shengbinyu.top/mathjaxprism.html</a></li>
<li><a href="https://yashuning.github.io/2018/06/29/hexo-Next-主题添加搜索功能/" target="_blank" rel="noopener">https://yashuning.github.io/2018/06/29/hexo-Next-%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%90%9C%E7%B4%A2%E5%8A%9F%E8%83%BD/</a></li>
<li><a href="https://ink-bottle.github.io/2019/03/03/hexo-%E5%8D%9A%E5%AE%A2%E5%A2%9E%E5%8A%A0Valine-gitalk%E8%AF%84%E8%AE%BA%E6%8F%92%E4%BB%B6/" target="_blank" rel="noopener">https://ink-bottle.github.io/2019/03/03/hexo-%E5%8D%9A%E5%AE%A2%E5%A2%9E%E5%8A%A0Valine-gitalk%E8%AF%84%E8%AE%BA%E6%8F%92%E4%BB%B6/</a></li>
</ul>
]]></content>
      <categories>
        <category>博客</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>正则表达式</title>
    <url>/post/Regular%20expression/</url>
    <content><![CDATA[<blockquote>
<p>本文参考了一些链接，记录了一些常用正则表达式的详细使用方法。</p>
</blockquote>
<a id="more"></a>
<p>在自然语言处理中，很多时候我们都需要从文本或字符串中抽取出想要的信息，并进一步做语义理解或其它处理。在本文中，作者由基础到高级介绍了很多正则表达式，这些表达式或规则在很多编程语言中都是通用的。</p>
<p><a href="https://regex101.com/r/cO8lqs/11" target="_blank" rel="noopener">书写正则表达式网站</a></p>
<p>正则表达式（regex 或 regexp）对于从文本中抽取信息极其有用，它一般会搜索匹配特定模式的语句，而这种模式及具体的 ASCII 序列或 Unicode 字符。从解析/替代字符串、预处理数据到网页爬取，正则表达式的应用范围非常广。</p>
<p>其中一个比较有意思的地方是，只要我们学会了正则表达式的语句，我们几乎可以将其应用于多有的编程语言，包括 JavaScript、Python、Ruby 和 Java 等。只不过对于各编程语言所支持的最高级特征与语法有细微的区别。</p>
<p>下面我们可以具体讨论一些案例与解释。</p>
<h1 id="基本语句"><a href="#基本语句" class="headerlink" title="基本语句"></a><strong>基本语句</strong></h1><h2 id="锚点：-和"><a href="#锚点：-和" class="headerlink" title="锚点：^ 和 $"></a><strong>锚点：^ 和 $</strong></h2><pre><code>^The        匹配任何以“The”开头的字符串 
end$        匹配以“end”为结尾的字符串
^The end$   抽取匹配从“The”开始到“end”结束的字符串
roar        匹配任何带有文本“roar”的字符串
</code></pre><h2 id="数量符：-、-、？和"><a href="#数量符：-、-、？和" class="headerlink" title="数量符：*、+、？和 {}**"></a>数量符：*、+、？和 {}**</h2><pre><code>abc*        匹配在“ab”后面跟着零个或多个“c”的字符串 
abc+        匹配在“ab”后面跟着一个或多个“c”的字符串
abc?        匹配在“ab”后面跟着零个或一个“c”的字符串
abc{2}      匹配在“ab”后面跟着两个“c”的字符串
abc{2,}     匹配在“ab”后面跟着两个或更多“c”的字符串
abc{2,5}    匹配在“ab”后面跟着2到5个“c”的字符串
a(bc)*      匹配在“a”后面跟着零个或更多“bc”序列的字符串
a(bc){2,5}  匹配在“a”后面跟着2到5个“bc”序列的字符串
</code></pre><h2 id="或运算符：-、"><a href="#或运算符：-、" class="headerlink" title="或运算符：| 、 []"></a><strong>或运算符：| 、 []</strong></h2><pre><code>a(b|c)     匹配在“a”后面跟着“b”或“c”的字符串 
a[bc]      匹配在“a”后面跟着“b”或“c”的字符串
</code></pre><h2 id="字符类：-d、-w、-s-和"><a href="#字符类：-d、-w、-s-和" class="headerlink" title="字符类：\d、\w、\s 和 .**"></a>字符类：\d、\w、\s 和 .**</h2><pre><code>\d         匹配数字型的单个字符 
\w         匹配单个词字（字母加下划线） 
\s         匹配单个空格字符（包括制表符和换行符） 
.          匹配任意字符
</code></pre><p>使用「.」运算符需要非常小心，因为常见类或排除型字符类都要更快与精确。\d、\w 和\s 同样有它们各自的排除型字符类，即\D、\W 和\S。例如\D 将执行与\d 完全相反的匹配方法：</p>
<pre><code>\D         匹配单个非数字型的字符
</code></pre><p>为了正确地匹配，我们必须使用转义符反斜杠「\」定义我们需要匹配的符号「^.[$()|*+?{\」，因为我们可能认为这些符号在原文本中有特殊的含义。</p>
<pre><code>\$\d       匹配在单个数字前有符号“$”的字符串 -&gt; Try it! (https://regex101.com/r/cO8lqs/9)
</code></pre><p>注意我们同样能匹配 non-printable 字符，例如 Tab 符「\t」、换行符「\n」和回车符「\r」</p>
<h2 id="Flags"><a href="#Flags" class="headerlink" title="Flags"></a><strong>Flags</strong></h2><p>我们已经了解如何构建正则表达式，但仍然遗漏了一个非常基础的概念：flags。</p>
<p>正则表达式通常以/abc/这种形式出现，其中搜索模式由两个反斜杠「/」分离。而在模式的结尾，我们通常可以指定以下 flag 配置或它们的组合：</p>
<ul>
<li>g（global）在第一次完成匹配后并不会返回结果，它会继续搜索剩下的文本。</li>
<li>m（multi line）允许使用^和$匹配一行的开始和结尾，而不是整个序列。</li>
<li>i（insensitive）令整个表达式不区分大小写（例如/aBc/i 将匹配 AbC）。</li>
</ul>
<h1 id="中级语句"><a href="#中级语句" class="headerlink" title="中级语句"></a><strong>中级语句</strong></h1><h2 id="分组和捕获："><a href="#分组和捕获：" class="headerlink" title="分组和捕获：()"></a><strong>分组和捕获：()</strong></h2><pre><code>a(bc)           圆括弧会创建一个捕获性分组，它会捕获匹配项“bc” 
a(?:bc)*        使用 “?:” 会使捕获分组失效，只需要匹配前面的“a” 
a(?&lt;foo&gt;bc)     使用 “?&lt;foo&gt;” 会为分组配置一个名称
</code></pre><p>捕获性圆括号 () 和非捕获性圆括弧 (?:) 对于从字符串或数据中抽取信息非常重要，我们可以使用 Python 等不同的编程语言实现这一功能。从多个分组中捕获的多个匹配项将以经典的数组形式展示：我们可以使用匹配结果的索引访问它们的值。</p>
<p>如果需要为分组添加名称（使用 (?<foo>…)），我们就能如字典那样使用匹配结果检索分组的值，其中字典的键为分组的名称。</foo></p>
<h2 id="方括弧表达式："><a href="#方括弧表达式：" class="headerlink" title="方括弧表达式：[]"></a><strong>方括弧表达式：[]</strong></h2><pre><code>[abc]            匹配带有一个“a”、“ab”或“ac”的字符串 -&gt; 与 a|b|c 一样 
[a-c]            匹配带有一个“a”、“ab”或“ac”的字符串 -&gt; 与 a|b|c 一样
[a-fA-F0-9]      匹配一个代表16进制数字的字符串，不区分大小写 
[0-9]%           匹配在%符号前面带有0到9这几个字符的字符串
[^a-zA-Z]        匹配不带a到z或A到Z的字符串，其中^为否定表达式
</code></pre><p>记住在方括弧内，所有特殊字符（包括反斜杠\）都会失去它们应有的意义。</p>
<h2 id="Greedy-和-Lazy-匹配"><a href="#Greedy-和-Lazy-匹配" class="headerlink" title="==Greedy 和 Lazy 匹配=="></a><strong>==Greedy 和 Lazy 匹配==</strong></h2><p>数量符（* + {}）是一种贪心运算符，所以它们会遍历给定的文本，并尽可能匹配。例如，&lt;.+&gt; 可以匹配文本「This is a <div> simple div</div> test」中的「<div>simple div</div>」。为了仅捕获 div 标签，我们需要使用「？」令贪心搜索变得 Lazy 一点：</p>
<pre><code>&lt;.+?&gt;            一次或多次匹配 “&lt;” 和 “&gt;” 里面的任何字符，可按需扩展
</code></pre><p>注意更好的解决方案应该需要避免使用「.」，这有利于实现更严格的正则表达式：</p>
<pre><code>&lt;[^&lt;&gt;]+&gt;         一次或多次匹配 “&lt;” 和 “&gt;” 里面的任何字符，除去 “&lt;” 或 “&gt;” 字符
</code></pre><p><strong>高级语句</strong></p>
<p><strong>边界符：\b 和 \B</strong></p>
<pre><code>\babc\b          执行整词匹配搜索
</code></pre><p>\b 如插入符号那样表示一个锚点（它与$和^相同）来匹配位置，其中一边是一个单词符号（如\w），另一边不是单词符号（例如它可能是字符串的起始点或空格符号）。</p>
<p>它同样能表达相反的非单词边界「\B」，它会匹配「\b」不会匹配的位置，如果我们希望找到被单词字符环绕的搜索模式，就可以使用它。</p>
<pre><code>\Babc\B          只要是被单词字符环绕的模式就会匹配
</code></pre><h2 id="前向匹配和后向匹配：-和-lt"><a href="#前向匹配和后向匹配：-和-lt" class="headerlink" title="前向匹配和后向匹配：(?=) 和 (?&lt;=)"></a><strong>前向匹配和后向匹配：(?=) 和 (?&lt;=)</strong></h2><pre><code>d(?=r)       只有在后面跟着“r”的时候才匹配“d”，但是“r”并不会成为整个正则表达式匹配的一部分 
(?&lt;=r)d      只有在前面跟着“r”时才匹配“d”，但是“r”并不会成为整个正则表达式匹配的一部分
</code></pre><p>我们同样能使用否定运算子：</p>
<pre><code>d(?!r)       只有在后面不跟着“r”的时候才匹配“d”，但是“r”并不会成为整个正则表达式匹配的一部分 
(?&lt;!r)d      只有在前面不跟着“r”时才匹配“d”，但是“r”并不会成为整个正则表达式匹配的一部分
</code></pre><p><strong>结语</strong></p>
<p>正如上文所示，正则表达式的应用领域非常广，很可能各位读者在开发的过程中已经遇到了它，下面是正则表达式常用的领域：</p>
<ul>
<li>数据验证，例如检查时间字符串是否符合格式；</li>
<li>数据抓取，以特定顺序抓取包含特定文本或内容的网页；</li>
<li>数据包装，将数据从某种原格式转换为另外一种格式；</li>
<li>字符串解析，例如捕获所拥有 URL 的 GET 参数，或捕获一组圆括弧内的文本；</li>
<li>字符串替代，将字符串中的某个字符替换为其它字符。</li>
</ul>
<h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><p><a href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650749657&amp;idx=4&amp;sn=da4852cb0c4919316d801fe19a64901d&amp;chksm=871afea7b06d77b1bda42ac134c5dddad5af24647f62a5a8e7bdcef4499f40fd4c97045a6f3d&amp;mpshare=1&amp;scene=23&amp;srcid=1009dbNFxJJahsQK6NGw4wS3%23rd" target="_blank" rel="noopener">https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650749657&amp;idx=4&amp;sn=da4852cb0c4919316d801fe19a64901d&amp;chksm=871afea7b06d77b1bda42ac134c5dddad5af24647f62a5a8e7bdcef4499f40fd4c97045a6f3d&amp;mpshare=1&amp;scene=23&amp;srcid=1009dbNFxJJahsQK6NGw4wS3%23rd</a></p>
<h1 id="python正则表达式的使用"><a href="#python正则表达式的使用" class="headerlink" title="python正则表达式的使用"></a>python<a href="https://www.cnblogs.com/huxi/archive/2010/07/04/1771073.html" target="_blank" rel="noopener">正则表达式的使用</a></h1><h1 id="正则表达式match和search的区别"><a href="#正则表达式match和search的区别" class="headerlink" title="正则表达式match和search的区别"></a><a href="https://segmentfault.com/a/1190000006736033" target="_blank" rel="noopener">正则表达式match和search的区别</a></h1>]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title>用python3读写csv文档</title>
    <url>/post/read%20the%20CSV%20document%20using%20python3/</url>
    <content><![CDATA[<blockquote>
<p>对于大多数的CSV格式的数据读写问题，都可以使用 <code>csv</code> 库。</p>
</blockquote>
<a id="more"></a>
<p> 例如：假设你在一个名叫stocks.csv文件中有一些股票市场数据，就像这样：</p>
<pre class=" language-csv"><code class="language-csv">Symbol,Price,Date,Time,Change,Volume
"AA",39.48,"6/11/2007","9:36am",-0.18,181800
"AIG",71.38,"6/11/2007","9:36am",-0.15,195500
"AXP",62.58,"6/11/2007","9:36am",-0.46,935000
"BA",98.31,"6/11/2007","9:36am",+0.12,104800
"C",53.08,"6/11/2007","9:36am",-0.25,360900
"CAT",78.29,"6/11/2007","9:36am",-0.23,225400
</code></pre>
<h1 id="csv文档的读取"><a href="#csv文档的读取" class="headerlink" title="csv文档的读取"></a>csv文档的读取</h1><h2 id="1-常规读取"><a href="#1-常规读取" class="headerlink" title="1. 常规读取"></a>1. 常规读取</h2><p>下面向你展示如何将这些数据读取为一个元组的序列：</p>
<pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> csv
<span class="token keyword">with</span> open<span class="token punctuation">(</span><span class="token string">'stocks.csv'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
    f_csv <span class="token operator">=</span> csv<span class="token punctuation">.</span>reader<span class="token punctuation">(</span>f<span class="token punctuation">)</span>
    headers <span class="token operator">=</span> next<span class="token punctuation">(</span>f_csv<span class="token punctuation">)</span>
    <span class="token keyword">for</span> row <span class="token keyword">in</span> f_csv<span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># Process row</span>
        <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
</code></pre>
<p>在上面的代码中， <code>row</code> 会是一个列表。因此，为了访问某个字段，你需要使用下标，如 <code>row[0]</code>访问Symbol， <code>row[4]</code> 访问Change。==这样可以通过外建字典来存储读出的csv数据。==</p>
<h2 id="2-命名元组"><a href="#2-命名元组" class="headerlink" title="2. 命名元组"></a>2. 命名元组</h2><p>由于这种下标访问通常会引起混淆，你可以考虑使用==命名元组==。例如：</p>
<pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> collections <span class="token keyword">import</span> namedtuple
<span class="token keyword">with</span> open<span class="token punctuation">(</span><span class="token string">'stock.csv'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
    f_csv <span class="token operator">=</span> csv<span class="token punctuation">.</span>reader<span class="token punctuation">(</span>f<span class="token punctuation">)</span>
    headings <span class="token operator">=</span> next<span class="token punctuation">(</span>f_csv<span class="token punctuation">)</span>
    Row <span class="token operator">=</span> namedtuple<span class="token punctuation">(</span><span class="token string">'Row'</span><span class="token punctuation">,</span> headings<span class="token punctuation">)</span>
    <span class="token keyword">for</span> r <span class="token keyword">in</span> f_csv<span class="token punctuation">:</span>
        row <span class="token operator">=</span> Row<span class="token punctuation">(</span><span class="token operator">*</span>r<span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># Process row</span>
        <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
</code></pre>
<p>它允许你使用列名如 <code>row.Symbol</code> 和 <code>row.Change</code> 代替下标访问。 需要注意的是这个只有在列名是合法的Python标识符的时候才生效。如果不是的话， 你可能需要修改下原始的列名(如将非标识符字符替换成下划线之类的)。</p>
<h2 id="3-字典"><a href="#3-字典" class="headerlink" title="3. 字典"></a>3. 字典</h2><p>另外一个选择就是将数据读取到一个字典序列中去。可以这样做：</p>
<pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> csv
<span class="token keyword">with</span> open<span class="token punctuation">(</span><span class="token string">'stocks.csv'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
    f_csv <span class="token operator">=</span> csv<span class="token punctuation">.</span>DictReader<span class="token punctuation">(</span>f<span class="token punctuation">)</span>
    <span class="token keyword">for</span> row <span class="token keyword">in</span> f_csv<span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># process row</span>
        <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
</code></pre>
<p>在这个版本中，你可以使用列名去访问每一行的数据了。比如，<code>row[&#39;Symbol&#39;]</code> 或者 <code>row[&#39;Change&#39;]</code>。</p>
<p><code>fieldnames</code> 是dict_reader的一个属性，表示CSV文档的数据名称。可以通过<code>f_csv.fieldnames</code>来访问数据名称那一行。</p>
<h1 id="CSV文件写入"><a href="#CSV文件写入" class="headerlink" title="CSV文件写入"></a>CSV文件写入</h1><p>为了写入CSV数据，你仍然可以使用csv模块，不过这时候先创建一个 <code>writer</code> 对象。例如:</p>
<pre class=" language-python"><code class="language-python">headers <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'Symbol'</span><span class="token punctuation">,</span><span class="token string">'Price'</span><span class="token punctuation">,</span><span class="token string">'Date'</span><span class="token punctuation">,</span><span class="token string">'Time'</span><span class="token punctuation">,</span><span class="token string">'Change'</span><span class="token punctuation">,</span><span class="token string">'Volume'</span><span class="token punctuation">]</span>
rows <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'AA'</span><span class="token punctuation">,</span> <span class="token number">39.48</span><span class="token punctuation">,</span> <span class="token string">'6/11/2007'</span><span class="token punctuation">,</span> <span class="token string">'9:36am'</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.18</span><span class="token punctuation">,</span> <span class="token number">181800</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
         <span class="token punctuation">(</span><span class="token string">'AIG'</span><span class="token punctuation">,</span> <span class="token number">71.38</span><span class="token punctuation">,</span> <span class="token string">'6/11/2007'</span><span class="token punctuation">,</span> <span class="token string">'9:36am'</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.15</span><span class="token punctuation">,</span> <span class="token number">195500</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
         <span class="token punctuation">(</span><span class="token string">'AXP'</span><span class="token punctuation">,</span> <span class="token number">62.58</span><span class="token punctuation">,</span> <span class="token string">'6/11/2007'</span><span class="token punctuation">,</span> <span class="token string">'9:36am'</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.46</span><span class="token punctuation">,</span> <span class="token number">935000</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
       <span class="token punctuation">]</span>

<span class="token keyword">with</span> open<span class="token punctuation">(</span><span class="token string">'stocks.csv'</span><span class="token punctuation">,</span><span class="token string">'w'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
    f_csv <span class="token operator">=</span> csv<span class="token punctuation">.</span>writer<span class="token punctuation">(</span>f<span class="token punctuation">)</span>
    f_csv<span class="token punctuation">.</span>writerow<span class="token punctuation">(</span>headers<span class="token punctuation">)</span>
    f_csv<span class="token punctuation">.</span>writerows<span class="token punctuation">(</span>rows<span class="token punctuation">)</span>
</code></pre>
<p>如果你有一个字典序列的数据，可以像这样做：</p>
<pre class=" language-python"><code class="language-python">headers <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'Symbol'</span><span class="token punctuation">,</span> <span class="token string">'Price'</span><span class="token punctuation">,</span> <span class="token string">'Date'</span><span class="token punctuation">,</span> <span class="token string">'Time'</span><span class="token punctuation">,</span> <span class="token string">'Change'</span><span class="token punctuation">,</span> <span class="token string">'Volume'</span><span class="token punctuation">]</span>
rows <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">{</span><span class="token string">'Symbol'</span><span class="token punctuation">:</span><span class="token string">'AA'</span><span class="token punctuation">,</span> <span class="token string">'Price'</span><span class="token punctuation">:</span><span class="token number">39.48</span><span class="token punctuation">,</span> <span class="token string">'Date'</span><span class="token punctuation">:</span><span class="token string">'6/11/2007'</span><span class="token punctuation">,</span>
        <span class="token string">'Time'</span><span class="token punctuation">:</span><span class="token string">'9:36am'</span><span class="token punctuation">,</span> <span class="token string">'Change'</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">0.18</span><span class="token punctuation">,</span> <span class="token string">'Volume'</span><span class="token punctuation">:</span><span class="token number">181800</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
        <span class="token punctuation">{</span><span class="token string">'Symbol'</span><span class="token punctuation">:</span><span class="token string">'AIG'</span><span class="token punctuation">,</span> <span class="token string">'Price'</span><span class="token punctuation">:</span> <span class="token number">71.38</span><span class="token punctuation">,</span> <span class="token string">'Date'</span><span class="token punctuation">:</span><span class="token string">'6/11/2007'</span><span class="token punctuation">,</span>
        <span class="token string">'Time'</span><span class="token punctuation">:</span><span class="token string">'9:36am'</span><span class="token punctuation">,</span> <span class="token string">'Change'</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">0.15</span><span class="token punctuation">,</span> <span class="token string">'Volume'</span><span class="token punctuation">:</span> <span class="token number">195500</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
        <span class="token punctuation">{</span><span class="token string">'Symbol'</span><span class="token punctuation">:</span><span class="token string">'AXP'</span><span class="token punctuation">,</span> <span class="token string">'Price'</span><span class="token punctuation">:</span> <span class="token number">62.58</span><span class="token punctuation">,</span> <span class="token string">'Date'</span><span class="token punctuation">:</span><span class="token string">'6/11/2007'</span><span class="token punctuation">,</span>
        <span class="token string">'Time'</span><span class="token punctuation">:</span><span class="token string">'9:36am'</span><span class="token punctuation">,</span> <span class="token string">'Change'</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">0.46</span><span class="token punctuation">,</span> <span class="token string">'Volume'</span><span class="token punctuation">:</span> <span class="token number">935000</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
        <span class="token punctuation">]</span>

<span class="token keyword">with</span> open<span class="token punctuation">(</span><span class="token string">'stocks.csv'</span><span class="token punctuation">,</span><span class="token string">'w'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
    f_csv <span class="token operator">=</span> csv<span class="token punctuation">.</span>DictWriter<span class="token punctuation">(</span>f<span class="token punctuation">,</span> headers<span class="token punctuation">)</span>
    f_csv<span class="token punctuation">.</span>writeheader<span class="token punctuation">(</span><span class="token punctuation">)</span>
    f_csv<span class="token punctuation">.</span>writerows<span class="token punctuation">(</span>rows<span class="token punctuation">)</span>
</code></pre>
<p>其中<code>f_csv.writeheader()</code>也可以替换成<code>f_csv.writerow(dict(zip(headers, headers)))</code></p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul>
<li><a href="https://python3-cookbook.readthedocs.io/zh_CN/latest/c06/p01_read_write_csv_data.html" target="_blank" rel="noopener">https://python3-cookbook.readthedocs.io/zh_CN/latest/c06/p01_read_write_csv_data.html</a></li>
<li><a href="https://blog.csdn.net/guoziqing506/article/details/52014506" target="_blank" rel="noopener">https://blog.csdn.net/guoziqing506/article/details/52014506</a></li>
</ul>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>文件读取</tag>
        <tag>csv</tag>
      </tags>
  </entry>
  <entry>
    <title>初次见面，你好NYSDY！</title>
    <url>/post/essay/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>在macOS Catalina中配置VS Code C++开发环境</title>
    <url>/post/Configure%20VS%20Code%20C++%20development%20environment%20in%20macOS%20Catalina/</url>
    <content><![CDATA[<h1 id="一、安装VS-Code及扩展"><a href="#一、安装VS-Code及扩展" class="headerlink" title="一、安装VS Code及扩展"></a>一、安装VS Code及扩展</h1><ol>
<li>在官网下载安装mac版本VS Code</li>
<li>安装C/C++、C/C++ Clang Command Adapter及<strong>CodeLLDB</strong>扩展</li>
<li>按⇧⌘P，输入shell，选择如图命令</li>
</ol>
<p><img src="http://image.nysdy.com/20200926121753.png" alt></p>
<h1 id="二、搭建测试项目"><a href="#二、搭建测试项目" class="headerlink" title="二、搭建测试项目"></a>二、搭建测试项目</h1><p>在Terminal输入以下命令</p>
<pre class=" language-shell"><code class="language-shell">mkdir projects
cd projects
mkdir hello
cd hello
code .
</code></pre>
<p>上述输入<code>code .</code>后，会直接在vscode中打开hello文件夹。</p>
<p>上述步骤也可以在vscode中创建一个新的<code>hello</code>文件夹代替。</p>
<h1 id="三、设置编译器路径"><a href="#三、设置编译器路径" class="headerlink" title="三、设置编译器路径"></a>三、设置编译器路径</h1><p>⇧⌘P，输入C/C++，选择Edit Configurations (UI)。Edit Configurations (JSON)应该也可以，没有测试过<img src="http://image.nysdy.com/20200926122158.png" alt></p>
<p>保持默认设置即可，中间设置的解释可参考开头官网链接<img src="http://image.nysdy.com/20200926122327.png" alt></p>
<h1 id="四、创建build-task"><a href="#四、创建build-task" class="headerlink" title="四、创建build task"></a>四、创建build task</h1><ol>
<li>按<code>⇧⌘P</code>，输入Task</li>
<li>选择tasks: Configure Default Build Task</li>
<li>选择Create tasks.json file from template</li>
<li>选择Others</li>
<li>VS Code会创建一个tasks.json，在编辑器中打开</li>
<li>按照如下设置task.json</li>
</ol>
<pre class=" language-json"><code class="language-json"><span class="token punctuation">{</span>
    // See https<span class="token operator">:</span>//go.microsoft.com/fwlink/?LinkId=<span class="token number">733558</span>
    // for the documentation about the tasks.json format

    <span class="token property">"version"</span><span class="token operator">:</span> <span class="token string">"2.0.0"</span><span class="token punctuation">,</span>
    <span class="token property">"tasks"</span><span class="token operator">:</span> <span class="token punctuation">[</span>
        <span class="token punctuation">{</span>
            <span class="token property">"label"</span><span class="token operator">:</span> <span class="token string">"Build with Clang"</span><span class="token punctuation">,</span>//这个任务的名字在launch.json最后一项配置
            <span class="token property">"type"</span><span class="token operator">:</span> <span class="token string">"shell"</span><span class="token punctuation">,</span>
            <span class="token property">"command"</span><span class="token operator">:</span> <span class="token string">"clang++"</span><span class="token punctuation">,</span>
            <span class="token property">"args"</span><span class="token operator">:</span> <span class="token punctuation">[</span>
                <span class="token string">"-std=c++17"</span><span class="token punctuation">,</span>
                <span class="token string">"-stdlib=libc++"</span><span class="token punctuation">,</span>
                //<span class="token string">"test.cpp"</span><span class="token punctuation">,</span>这里是官方写法，不具有普遍性，注意两个配置文件的统一性即可
                <span class="token string">"${fileBasenameNoExtension}.cpp"</span><span class="token punctuation">,</span>
                <span class="token string">"-o"</span><span class="token punctuation">,</span>
                //<span class="token string">"test.out"</span><span class="token punctuation">,</span>
                <span class="token string">"${fileBasenameNoExtension}"</span><span class="token punctuation">,</span>
                <span class="token string">"--debug"</span>
            <span class="token punctuation">]</span><span class="token punctuation">,</span>
            <span class="token property">"group"</span><span class="token operator">:</span> <span class="token punctuation">{</span>
                <span class="token property">"kind"</span><span class="token operator">:</span> <span class="token string">"build"</span><span class="token punctuation">,</span>
                <span class="token property">"isDefault"</span><span class="token operator">:</span> <span class="token boolean">true</span>
            <span class="token punctuation">}</span>
        <span class="token punctuation">}</span>
    <span class="token punctuation">]</span>
<span class="token punctuation">}</span>
</code></pre>
<h1 id="五、设置debug-setting"><a href="#五、设置debug-setting" class="headerlink" title="五、设置debug setting"></a>五、设置debug setting</h1><ol>
<li>按<code>⇧⌘P</code>，输入launch，选择Debug：Open launch.json</li>
<li>选择LLDB</li>
<li>生成如下launch.json文件，在编辑器打开</li>
<li>替换成如下内容</li>
</ol>
<pre class=" language-json"><code class="language-json"><span class="token punctuation">{</span>
    // 使用 IntelliSense 了解相关属性。 
    // 悬停以查看现有属性的描述。
    // 欲了解更多信息，请访问<span class="token operator">:</span> https<span class="token operator">:</span>//go.microsoft.com/fwlink/?linkid=<span class="token number">830387</span>

    <span class="token property">"version"</span><span class="token operator">:</span> <span class="token string">"0.2.0"</span><span class="token punctuation">,</span>
    <span class="token property">"configurations"</span><span class="token operator">:</span> <span class="token punctuation">[</span>
        <span class="token punctuation">{</span>
            <span class="token property">"type"</span><span class="token operator">:</span> <span class="token string">"lldb"</span><span class="token punctuation">,</span>
            <span class="token property">"request"</span><span class="token operator">:</span> <span class="token string">"launch"</span><span class="token punctuation">,</span>
            <span class="token property">"name"</span><span class="token operator">:</span> <span class="token string">"Debug"</span><span class="token punctuation">,</span>
            //<span class="token property">"program"</span><span class="token operator">:</span> <span class="token string">"${workspaceFolder}/test.out"</span><span class="token punctuation">,</span>
            //上一行是官方写法，但是不同的cpp调试都要改配置，非常麻烦
            <span class="token property">"program"</span><span class="token operator">:</span> <span class="token string">"${workspaceFolder}/${fileBasenameNoExtension}"</span><span class="token punctuation">,</span>
            <span class="token property">"args"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
            <span class="token property">"cwd"</span><span class="token operator">:</span> <span class="token string">"${workspaceFolder}"</span><span class="token punctuation">,</span>
            <span class="token property">"preLaunchTask"</span><span class="token operator">:</span> <span class="token string">"Build with Clang"</span>
        <span class="token punctuation">}</span>
    <span class="token punctuation">]</span>
<span class="token punctuation">}</span>
</code></pre>
<h1 id="六、添加源文件"><a href="#六、添加源文件" class="headerlink" title="六、添加源文件"></a>六、添加源文件</h1><p>在<code>hello</code>文件夹下创建<code>helloworld.cpp</code>，输入如下代码</p>
<pre class=" language-cpp"><code class="language-cpp"><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;iostream></span></span>
<span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;vector></span></span>
<span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;string></span></span>

<span class="token keyword">using</span> <span class="token keyword">namespace</span> std<span class="token punctuation">;</span>

<span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">{</span>

    vector<span class="token operator">&lt;</span>string<span class="token operator">></span> msg <span class="token punctuation">{</span><span class="token string">"Hello"</span><span class="token punctuation">,</span> <span class="token string">"C++"</span><span class="token punctuation">,</span> <span class="token string">"World"</span><span class="token punctuation">,</span> <span class="token string">"from"</span><span class="token punctuation">,</span> <span class="token string">"VS Code!"</span><span class="token punctuation">}</span><span class="token punctuation">;</span>

    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">const</span> string<span class="token operator">&amp;</span> word <span class="token operator">:</span> msg<span class="token punctuation">)</span>
    <span class="token punctuation">{</span>
        cout <span class="token operator">&lt;&lt;</span> word <span class="token operator">&lt;&lt;</span> <span class="token string">" "</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    cout <span class="token operator">&lt;&lt;</span> endl<span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre>
<h1 id="七、Build"><a href="#七、Build" class="headerlink" title="七、Build"></a>七、Build</h1><p>按<code>⇧⌘B</code>来编译</p>
<p>会出现如下提示<img src="http://image.nysdy.com/20200926123016.png" alt></p>
<h1 id="八、运行-amp-Debug"><a href="#八、运行-amp-Debug" class="headerlink" title="八、运行&amp;Debug"></a>八、运行&amp;Debug</h1><p>点击Debug  即可直接运行出结果</p>
<p><img src="http://image.nysdy.com/20200926123236.png" alt></p>
<p>设置断点，点击Debug</p>
<p><img src="http://image.nysdy.com/20200926130522.png" alt></p>
<p>如果改变<code>hello.cpp</code>的文件位置，则task.json和launch.json需要相应作出改变，如下所示：</p>
<p>lauch.json:</p>
<pre class=" language-json"><code class="language-json"><span class="token punctuation">{</span>
    // 使用 IntelliSense 了解相关属性。 
    // 悬停以查看现有属性的描述。
    // 欲了解更多信息，请访问<span class="token operator">:</span> https<span class="token operator">:</span>//go.microsoft.com/fwlink/?linkid=<span class="token number">830387</span>

    <span class="token property">"version"</span><span class="token operator">:</span> <span class="token string">"0.2.0"</span><span class="token punctuation">,</span>
    <span class="token property">"configurations"</span><span class="token operator">:</span> <span class="token punctuation">[</span>
        <span class="token punctuation">{</span>
            <span class="token property">"type"</span><span class="token operator">:</span> <span class="token string">"lldb"</span><span class="token punctuation">,</span>
            <span class="token property">"request"</span><span class="token operator">:</span> <span class="token string">"launch"</span><span class="token punctuation">,</span>
            <span class="token property">"name"</span><span class="token operator">:</span> <span class="token string">"Debug"</span><span class="token punctuation">,</span>
            //<span class="token property">"program"</span><span class="token operator">:</span> <span class="token string">"${workspaceFolder}/test.out"</span><span class="token punctuation">,</span>
            //上一行是官方写法，但是不同的cpp调试都要改配置，非常麻烦
            <span class="token property">"program"</span><span class="token operator">:</span> <span class="token string">"${workspaceFolder}/src/${fileBasenameNoExtension}"</span><span class="token punctuation">,</span>
            <span class="token property">"args"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
            <span class="token property">"cwd"</span><span class="token operator">:</span> <span class="token string">"${workspaceFolder}"</span><span class="token punctuation">,</span>
            <span class="token property">"preLaunchTask"</span><span class="token operator">:</span> <span class="token string">"Build with Clang"</span>
        <span class="token punctuation">}</span>
    <span class="token punctuation">]</span>
<span class="token punctuation">}</span>
</code></pre>
<p>task.json:</p>
<pre class=" language-json"><code class="language-json"><span class="token punctuation">{</span>
    // See https<span class="token operator">:</span>//go.microsoft.com/fwlink/?LinkId=<span class="token number">733558</span>
    // for the documentation about the tasks.json format

    <span class="token property">"version"</span><span class="token operator">:</span> <span class="token string">"2.0.0"</span><span class="token punctuation">,</span>
    <span class="token property">"tasks"</span><span class="token operator">:</span> <span class="token punctuation">[</span>
        <span class="token punctuation">{</span>
            <span class="token property">"label"</span><span class="token operator">:</span> <span class="token string">"Build with Clang"</span><span class="token punctuation">,</span>//这个任务的名字在launch.json最后一项配置
            <span class="token property">"type"</span><span class="token operator">:</span> <span class="token string">"shell"</span><span class="token punctuation">,</span>
            <span class="token property">"command"</span><span class="token operator">:</span> <span class="token string">"clang++"</span><span class="token punctuation">,</span>
            <span class="token property">"args"</span><span class="token operator">:</span> <span class="token punctuation">[</span>
                <span class="token string">"-std=c++17"</span><span class="token punctuation">,</span>
                <span class="token string">"-stdlib=libc++"</span><span class="token punctuation">,</span>
                //<span class="token string">"test.cpp"</span><span class="token punctuation">,</span>这里是官方写法，不具有普遍性，注意两个配置文件的统一性即可
                <span class="token string">"./src/${fileBasenameNoExtension}.cpp"</span><span class="token punctuation">,</span>
                <span class="token string">"-o"</span><span class="token punctuation">,</span>
                //<span class="token string">"test.out"</span><span class="token punctuation">,</span>
                <span class="token string">"./src/${fileBasenameNoExtension}"</span><span class="token punctuation">,</span>
                <span class="token string">"--debug"</span>
            <span class="token punctuation">]</span><span class="token punctuation">,</span>
            <span class="token property">"group"</span><span class="token operator">:</span> <span class="token punctuation">{</span>
                <span class="token property">"kind"</span><span class="token operator">:</span> <span class="token string">"build"</span><span class="token punctuation">,</span>
                <span class="token property">"isDefault"</span><span class="token operator">:</span> <span class="token boolean">true</span>
            <span class="token punctuation">}</span>
        <span class="token punctuation">}</span>
    <span class="token punctuation">]</span>
</code></pre>
<p>运行结果和文件结构如下所示：<img src="http://image.nysdy.com/20200926130948.png" alt></p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul>
<li><a href="https://zhuanlan.zhihu.com/p/100896963" target="_blank" rel="noopener">在macOS Catalina中配置VS Code C++开发环境</a></li>
<li><a href="https://code.visualstudio.com/docs/cpp/config-clang-mac" target="_blank" rel="noopener">官网链接</a></li>
</ul>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>vscode</tag>
        <tag>mac</tag>
      </tags>
  </entry>
  <entry>
    <title>范数与距离的联系</title>
    <url>/post/the_relation_between_norm_and_distance/</url>
    <content><![CDATA[<h1 id="1-范数"><a href="#1-范数" class="headerlink" title="1 范数"></a>1 范数</h1><p>向量的范数可以简单形象的理解为向量的长度，或者向量到零点的距离，或者相应的两个点之间的距离。</p>
<p>常用的向量的范数：</p>
<p><strong>L0范数</strong>:为x向量各个非零元素的个数。<br><strong>L2范数</strong>:为x向量各个元素平方和的1/2次方。L2范数又称Euclidean范数或者Frobenius范数<br><strong>Lp范数</strong>:为x向量各个元素绝对值p次方和的1/p次方</p>
<p><strong>L∞范数</strong>:为x向量各个元素绝对值最大那个元素的绝对值，如下：$|\mathrm{x}| \infty=\max \left(\left|\mathrm{x}_{1}\right|,\left|\mathrm{x}_{2}\right|, \ldots,\left|\mathrm{x}_{\mathrm{n}}\right|\right)$</p>
<h2 id="联系和区别"><a href="#联系和区别" class="headerlink" title="联系和区别"></a>联系和区别</h2><p>也就是如果我们使用L0范数，即希望w的大部分元素都是0. （w是稀疏的）所以可以用于ML中做稀疏编码，特征选择。通过最小化L0范数，来寻找最少最优的稀疏特征项。但不幸的是，L0范数的最优化问题是一个<strong>NP hard</strong>问题，而且理论上有证明，<strong>L1范数是L0范数的最优凸近似</strong>，因此通常使用L1范数来代替。</p>
<p>L1范数的解通常是<strong>稀疏性</strong>的，倾向于选择数目较少的一些非常大的值或者数目较多的insignificant的小值。</p>
<p>L2范数越小，可以使得w的每个元素都很小，接近于0，但与L1范数不同的是他不会让它等于0而是接近于0.<br>由于<strong>L1范数并没有平滑的函数</strong>表示，起初L1最优化问题解决起来非常困难，但随着计算机技术的到来，利用很多凸优化算法使得L1最优化成为可能。</p>
<h1 id="2-距离"><a href="#2-距离" class="headerlink" title="2 距离"></a>2 距离</h1><p><strong>欧式距离（对应L2范数）</strong>：最常见的两点之间或多点之间的距离表示法，又称之为欧几里得度量，它定义于欧几里得空间中。n维空间中两个点x1(x11,x12,…,x1n)与 x2(x21,x22,…,x2n)间的欧氏距离：$d_{12}=\sqrt{\sum_{k=1}^{n}\left(x_{1 k}-x_{2 k}\right)^{2}}$， 也可以用表示成向量运算的形式：$d_{12}=\sqrt{(a-b)(a-b)^{T}}$</p>
<p><strong>曼哈顿距离(对应L1-范数)</strong>：也就是在欧几里得空间的固定直角坐标系上两点所形成的线段对轴产生的投影的距离总和。例如在平面上，坐标（x1, y1）的点P1与坐标（x2, y2）的点P2的曼哈顿距离为：$\left|x_{1}-x_{2}\right|+\left|y_{1}-y_{2}\right|$, 要注意的是，曼哈顿距离依赖座标系统的转度，而非系统在座标轴上的平移或映射。</p>
<p><strong>切比雪夫距离(对应L∞范数)</strong>:若二个向量或二个点x1和x2，其坐标分别为(x11, x12, x13, … , x1n)和(x21, x22, x23, … , x2n)，则二者的切比雪夫距离为：d = max(|x1i - x2i|)，i从1到n。</p>
<p><strong>闵可夫斯基距离(Minkowski Distance)</strong>，闵氏距离不是一种距离，而是一组距离的定义。对应Lp范数，p为参数。</p>
<p>闵氏距离的定义：两个n维变量（或者两个n维空间点）x1(x11,x12,…,x1n)与 x2(x21,x22,…,x2n)间的闵可夫斯基距离定义为： $d_{12}=\sqrt[p]{\sum_{k=1}^{n}\left|x_{1 k}-x_{2 k}\right|^{p}}$</p>
<ul>
<li>其中p是一个变参数。</li>
<li>当p=1时，就是曼哈顿距离，</li>
<li>当p=2时，就是欧氏距离，</li>
<li>当p→∞时，就是切比雪夫距离，    </li>
</ul>
<p>根据变参数的不同，闵氏距离可以表示一类的距离。</p>
<p>下图是二维空间p取不同值时，与原点的$L_p$距离为1的图形。</p>
<p><img src="http://image.nysdy.com/20201018161325.png" alt></p>
<p><a href="https://zhuanlan.zhihu.com/p/46626607" target="_blank" rel="noopener">Mahalanobis距离</a>：也称作马氏距离。在近邻分类法中，常采用欧式距离和马氏距离。</p>
<p>一些参考资料供进一步理解：</p>
<ul>
<li><a href="https://blog.csdn.net/v_july_v/article/details/8203674" target="_blank" rel="noopener">从K近邻算法、距离度量谈到KD树、SIFT+BBF算法</a></li>
</ul>
<h1 id="3-在机器学习中的应用"><a href="#3-在机器学习中的应用" class="headerlink" title="3 在机器学习中的应用"></a>3 在机器学习中的应用</h1><p>L1范数和L2范数，用于机器学习的L1正则化、L2正则化。对于线性回归模型，使用L1正则化的模型建叫做Lasso回归，使用L2正则化的模型叫做Ridge回归（岭回归）。</p>
<p>其作用是：</p>
<p>L1正则化是指权值向量w中各个元素的绝对值之和，可以产生稀疏权值矩阵（稀疏矩阵指的是很多元素为0，只有少数元素是非零值的矩阵，即得到的线性回归模型的大部分系数都是0. ），即产生一个稀疏模型，可以用于特征选择；</p>
<p>L2正则化是指权值向量w中各个元素的平方和然后再求平方根，可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合。</p>
<p>至于为什么L1正则化能增加稀疏性，L2正则化能防止过拟合，原理可查看参考资料：</p>
<ul>
<li><a href="https://blog.csdn.net/jinping_shi/article/details/52433975" target="_blank" rel="noopener">机器学习中正则化项L1和L2的直观理解</a></li>
</ul>
<h1 id="参考链接："><a href="#参考链接：" class="headerlink" title="参考链接："></a><strong>参考链接：</strong></h1><ol>
<li><a href="https://blog.csdn.net/kingzone_2008/article/details/15073987" target="_blank" rel="noopener">范数与距离的关系以及在机器学习中的应用</a></li>
<li><a href="https://blog.csdn.net/SanyHo/article/details/105803103" target="_blank" rel="noopener">https://blog.csdn.net/SanyHo/article/details/105803103</a></li>
<li><a href="https://blog.csdn.net/SanyHo/article/details/105803103" target="_blank" rel="noopener">曼哈顿距离（L1范数）&amp; 欧式距离（L2范数）区别</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/46626607" target="_blank" rel="noopener">马氏距离(Mahalanobis Distance)</a></li>
</ol>
<h1 id><a href="#" class="headerlink" title=" "></a> </h1>]]></content>
      <categories>
        <category>数学基础</category>
      </categories>
      <tags>
        <tag>范数</tag>
        <tag>度量距离</tag>
      </tags>
  </entry>
</search>
