<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Understanding LSTM Networks]]></title>
    <url>%2Fpost%2FUnderstanding%20LSTM%20Networks%2F</url>
    <content type="text"><![CDATA[原文链接。这篇文章很好很细的一步一步的分解讲解了LSTM，之前看过一篇翻译的博客，现在自己翻译一遍，感觉对LSTM的认识加深了许多，虽然还是对LSTM中存有一些问题，比如为什么用tanh，sigmoid，为什不采用其他的？，但是看过之后至少对LSTM没有那么畏惧，不觉得过于复杂了。 LSTMRecurrent Neural Networks 循环允许信息从一个网络传入下一个。 一个循环网络可以被认为是相同网络的多个复制，每一个网络都将信息传递给后继者。这种类似链的性质表明，递归神经网络与序列和列表密切相关。 The Problem of Long-Term DependenciesRNN的一个吸引力是他们可能能够将先前信息连接到当前任务。 有时，我们只需要查看最近的信息来执行当前任务。例如： If we are trying to predict the last word in “the clouds are in the sky,” we don’t need any further context – it’s pretty obvious the next word is going to be sky. 在这种情况下，如果相关信息与待预测地方之间的差距很小，RNN可以学习使用过去的信息。 但是，对于一些情况，我们需要更多的上下文信息。 Consider trying to predict the last word in the text “I grew up in France… I speak fluent French.” 这时，相关信息与需要变得非常大的点之间的差距完全有可能。不幸的是，随着差距的扩大，RNN无法学会连接信息。 The problem was explored in depth by Hochreiter (1991) [German] and Bengio, et al. (1994), who found some pretty fundamental reasons why it might be difficult. LSTM Networks LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn! 标准RNNs的重复模块只有一个简单的结构，比如tanh层。 LSTMs也有像这种链式结构，但是它的重复模块具有不同的结构。 有四个，而不是一个神经网络层，以一种非常特殊的方式进行交互。 基本符号如下： 在上图中，每一行都携带一个完整的向量，从一个节点的输出到其他节点的输入。 粉色圆圈表示逐点运算，如矢量加法，而黄色框表示神经网络层。 行合并表示连接，而行分叉表示其内容被复制，副本将转移到不同的位置。 The Core Idea Behind LSTMsLSTM的关键是单元状态，水平线贯穿图的顶部。 单元状态有点像传送带。 它直接沿着整个链运行，只有一些微小的线性相互作用。 信息很容易沿着它不变地流动。 LSTM确实能够移除或添加信息到细胞状态，由称为门的结构精心调节。 门是一种可选择通过信息的方式。 它们由Sigmoid神经网络层和逐点乘法运算组成。 sigmoid层输出0到1之间的数字，描述每个组件应该通过多少。 值为零意味着“不让任何东西通过”，而值为1则意味着“让一切都通过！” LSTM具有三个这样的门，用于保护和控制单元状态。 Step-by-Step LSTM Walk Through我们的第一步就是确定我们将从单元状态中丢弃的信息。这个决定是由一个称为“遗忘门层”的sigmoid层决定的。它查看$h_{t-1}$和$x_t$，并为单元状态$C_{t-1}$中的每一个数字输出一个介于0和1之间的数字。1代表“完全保留这个”，而0代表“完全舍弃这个”。 让我们回到我们的语言模型示例，试图根据以前的所有单词预测下一个单词。 在这样的问题中，单元状态可能包括当前受试者的性别，因此可以使用正确的代词。 当我们看到一个新主题时，我们想要忘记旧主题的性别。 下一步是确定我们将在单元状态中存储哪些新信息。 这有两个部分。 首先，称为“输入门层”的sigmoid层决定我们将更新哪些值。 接下来，tanh层创建可以添加到状态的新候选值$\tilde{C}_t$的向量。 在下一步中，我们将结合这两个来创建状态更新。 在我们的语言模型的例子中，我们想要将新主题的性别添加到单元格状态，以替换我们忘记的旧主题。 现在是时候将旧的单元状态$C_{T-1}$更新为新的单元状态$C_t$。 前面的步骤已经决定要做什么，我们只需要实际做到这一点。 我们将旧状态乘以$f_t$，忘记我们之前决定忘记的事情。 然后我们添加$i_t * \tilde{C}_t$。 这是新的候选值，根据我们决定更新每个状态的值来缩放。 在语言模型的情况下，我们实际上放弃了关于旧主题的性别的信息并添加新信息，正如我们在前面的步骤中所做的那样。 最后，我们需要决定我们要输出的内容。 此输出将基于我们的单元状态，但将是过滤版本。 首先，我们运行一个sigmoid层，它决定我们要输出的单元状态的哪些部分。 然后，我们将单元格状态设置为tanh（将值推到介于-1和1之间）并将其乘以sigmoid门的输出，以便我们只输出我们决定的部分。 对于语言模型示例，由于它只是看到一个主题，它可能想要输出与动词相关的信息，以防接下来会发生什么。 例如，它可能输出主语是单数还是复数，以便我们知道动词应该与什么形式共轭，如果接下来的话。 Variants on Long Short Term Memory到目前为止我所描述的是一个非常正常的LSTM。 但并非所有LSTM都与上述相同。 事实上，似乎几乎所有涉及LSTM的论文都使用略有不同的版本。 差异很小，但值得一提的是其中一些。 One popular LSTM variant, introduced by Gers &amp; Schmidhuber (2000), is adding “peephole connections.” This means that we let the gate layers look at the cell state. 上面的图表为所有门增加了窥视孔（peephole），但是许多论文会给一些窥视孔而不是其他的。 另一种变化是使用耦合的遗忘和输入门。 我们不是单独决定忘记什么以及应该添加新信息，而是一起做出这些决定。我们仅仅会当我们在当前位置将要输入时忘记。我们仅仅输入新的值到那些我们已经忘记旧的信息的那些状态 。 另一个改动较大的变体是 Gated Recurrent Unit (GRU)，这是由 Cho, et al. (2014) 提出。它将遗忘和输入门组合成一个“更新门”。它还合并了单元状态和隐藏状态，并进行了一些其他更改。 由此产生的模型比标准LSTM模型简单，并且越来越受欢迎。 这些只是最着名的LSTM变种中的一小部分。 还有很多其他的东西，如 Yao, et al. (2015) 提出的 Depth Gated RNN。 还有一些完全不同的解决长期依赖关系的方法，如 Koutnik, et al. (2014) 提出的 Clockwork RNN。 哪种变体最好？ 差异是否重要？ Greff, et al. (2015) 对流行变体进行了很好的比较，发现它们几乎完全相同。Jozefowicz, et al. (2015) 测试了超过一万个RNN架构，找到了一些在某些任务上比LSTM更好的架构。 Conclusion早些时候，我提到了人们用RNN取得的显着成果。基本上所有这些都是使用LSTM实现的。对于大多数任务来说，它们确实工作得更好！ 作为一组方程写下来，LSTM看起来非常令人生畏。希望，在这篇文章中逐步走过它们使他们更加平易近人。 LSTM是我们用RNN实现的重要一步。很自然地想知道：还有另一个重要的一步吗？研究人员的共同观点是：“是的！下一步是它的注意！“我们的想法是让RNN的每一步都从一些更大的信息集中选择信息。例如，如果您使用RNN创建描述图像的标题，则可能会选择图像的一部分来查看其输出的每个单词。 事实上， Xu, et al.(2015) 做到这一点 - 如果你想探索注意力，这可能是一个有趣的起点！使用注意力已经取得了许多非常令人兴奋的结果，似乎还有更多的事情即将来临…… 注意力不是RNN研究中唯一令人兴奋的问题。例如，Kalchbrenner, et al. (2015) 的Grid LSTMs似乎非常有希望。在生成模型中使用RNN工作 - 例如Gregor, et al. (2015), Chung, et al. (2015), 或者 Bayer &amp; Osendorfer (2015) 似乎也很有趣。过去几年对于反复出现的神经网络来说是一个激动人心的时刻，即将到来的那些承诺只会更加如此！]]></content>
      <categories>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation》阅读笔记]]></title>
    <url>%2Fpost%2FLearning_Phrase_Representations_using_RNN_Encoder%E2%80%93Decoder_for_Statistical_Machine_Translation%2F</url>
    <content type="text"><![CDATA[原文链接。该论文是Sequence to Sequence学习的最早原型，论文中提出一种崭新的RNN(GRU) Encoder-Decoder算法，虽然文章属于比较旧的文章，但作为seq2seq的基础原型，还是需要阅读了解一下的。文章写的比较详细，各部分细节都有讲解。 文章的主要结构 contribution a novel RNN Encoder–Decoder 能够处理变长序列 a novel hidden unit reset gate update gate RNN Encoder–Decoder模型结构图如下： 文中作者对齐进行总体概述为： From a probabilistic perspective, this new model is a general method to learn the conditional distribution over a variable-length sequence conditioned on yet another variable-length sequence 从概率的角度来看，这个新模型是学习在另一个可变长度序列条件下的可变长度序列上的条件分布的一般方法 Encoder这部分是一个RNN单元。每个时间步，我们向Encoder中输入一个字/词（一般为向量形式），直到我们输入这个句子的最后一个字/词$X_T$，然后输入整个句子的语义向量c。由于RNN的特带你就是把前面每一步的输入信息都考虑进来，所以理论上这个c就包含了整个句子的所有信息。我们可以把当成这个句子的一个语义表示。 DecoderDecoder是另一个RNN，其被训练出来以通过预测隐藏状态$h_t$的下一个符号$y_t$来生成输出序列。计算公式如下 $$h_t = f(h_{t-1},y_{t-1},c)$$ 下一个序列的计算公式如下： $$P(y_t|y_{t-1},y_{t-2},\dots,y_1,c)=g(h_t,y_{t-1},c)$$ Hidden Unit该部分是对各部分具体的公式讲解，实际是GRU的具体公式算法，不再此详细叙述了。 reset gate In this formulation, when the reset gate is close to 0, the hidden state is forced to ignore the pre- vious hidden state and reset with the current input only. This effectively allows the hidden state to drop any information that is found to be irrelevant later in the future, thus, allowing a more compact representation. 这段原文主要讲解了复位门的作用：有效地允许隐藏状态丢弃在将来稍后发现不相关的任何信息，从而允许更紧凑的表示。 当捕获短期依赖时，复位门活跃 update gate the update gate controls how much information from the previous hidden state will carry over to the current hidden state. 更新门控制来自先前隐藏状态的多少信息将转移到当前隐藏状态。 当捕获长期依赖时，更新门活跃 Statistical Machine Translation(SMT) Experiments这部分作者主要做了量化分析和性质分析，主要就是说他的模型怎么厉害。。。（没有具体的数值指标，翻译的还不是中英翻译，想看的话可以去看一下，就不贴实验结果了）。 future这里作者提出了可以用decoder生成的目标短语来替换原句中短语的思路，如果没记错的话，这个想法好像对后面的机器翻译有很大的指导作用。]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>seq2seq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Differentiating Concepts and Instances for Knowledge Graph Embedding》阅读笔记]]></title>
    <url>%2Fpost%2Fread_Differentiating_Concepts_and_Instances_for_Knowledge_Graph_Embedding%2F</url>
    <content type="text"><![CDATA[这篇文章最大的亮点就是把concept映射为一个球面，然后把instance映射为一个向量，通过这种空间关系来进行embedding。如果instance和concept满足InstanceOf的关系，则instance应该在球内；如果两个concept满足SubClassOf的关系，则一个球会在另一个球面内。 conceptA concept is a fundamental category of existence (Rosch, 1973) and can be reified by all of its actual or potential instances.Concepts, which represent a group of different instances sharing common properties, are essential information in knowledge representation. drawback of the previous method ignore to distinguish between concepts and instances will lead to two drawbacks: Insufficient concept representation： cannot explicitly represent the difference between concepts and instances Lack transitivity of both isA relations: instanceOf and subClassOf (generally known as isA)isA relations exhibit transitivity contributions the first to propose and formalize the problem of knowledge graph embedding which differentiates between concepts and instances a novel knowledge embedding method named TransC state-of-the-art on link prediction and triple classification Translation-based ModelsTransE triple (h, r, t) should satisfy h + r ≈ t loss function:$f_r(h,t) = ||h + r - t||^2_2$ suitable for 1-to-1 relations TransH It regards a relation vector r as a translation on a hyperplane with $w_r$ as the normal vector. loss function:$f_r(h,t) = ||h_{\bot} + r - t_{\bot}||^2_2$，其中$h_{\bot}=h-w^{\top}_r h w_r$，$t_{\bot}=t-w^{\top}_r t w_r$ suitable for 1-to-N, N-to-1, and N-to-N relations TransR/CTransR addresses the issue in TransE and TransH that some entities are similar in the entity space but comparably different in other specific aspects. loss function:$f_r(h,t) = ||M_rh +r -M_rt||^2_2$，$M_r$ for each relation r TransD considers the different types of entities and relations at the same time loss function:$f_r(h,t) = ||h_{\bot} + r - t_{\bot}||^2_2$，$h_{\bot} = M_{rh}h$和$t_{\bot} = M_{rt}t$，$M_{r,e}$ for each relation-entity pair (r, e) Bilinear ModelsRESCAL the first bilinear model It associates each entity with a vector to capture its latent semantics. Each relation is represented as a matrix which models pairwise interactions between latent factors. External Information Learning Models textual information entity descriptions TranCSpecifically, TransC encodes each concept in knowledge graph as a sphere and each instance as a vector in the same semantic space. InstanceOf Triple RepresentationSubClassOf Triple Representation SubClassOf Triple Representationtrain modelmargin based loss详解unit and bernthe following research directions find a more expressive model instead of spheres to represent concepts A concept may have different meanings in different triples. use several typical vectors of instances as a concept’s centers to represent different meanings of a concept.]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Discourse-Level Named Entity Recognition and Relation Extraction Dataset for Chinese Literature Text 阅读笔记]]></title>
    <url>%2Fpost%2FREAD_A_Discourse-Level_Named_Entity_Recognition_and_Relation_Extraction_Dataset_for_Chinese_Literature_Text%2F</url>
    <content type="text"><![CDATA[该论文最主要的贡献就是这个数据，数据集地址。论文中提到的标标签过程也是一个创新点，运用了启发式和机器辅助标标签，这样可以提高准确度并减少标注人员工作。 contribution provide a new dataset for joint learning of NER and RE for Chinese literature text the proposed dataset is based on the discourse level which provides additional context information introduce some widely used models to conduct experiments tagging processtwo methods:one is a heuristic tagging method and another is a machine auxiliary tagging method. Step 1: First Tagging Processfind a problem of data inconsistency. Step 2: Heuristic Tagging Based on Generic disambiguating Rules For example, remove all adjective words and only tag “entity header” . re-annotate all articles and correct all inconsistency entities and relations based on the heuristic rules. Step 3: Machine Auxiliary Tagging The core idea is to train a model to learn annotation guidelines on the subset of the corpus and produce predicted tags on the rest data. CRF tagging set Annotation FormatEntityEach entity is identified by T tag, which takes several attributes. Id: a unique number identifying the entity within the document. It starts at 0, and is incremented every time a new entity is identified within the same document. Type: one of the entity tags. Begin Index: the begin index of an entity. It starts at 0, and is incremented every character. End Index: the end index of an entity. It starts at 0, and is incremented every character. Value: words being referred to an identifiable object. RelationEach relation is identified by R tag, which can take several attributes: Id: a unique number identifying the relation within the document. It starts at 0, and is incremented every time a new relation is identified within the same document. Arg1 and Arg2: two entities associated with a relation. Type: one of the relation tags.]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>NER</tag>
        <tag>RE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas的数据类型操作]]></title>
    <url>%2Fpost%2FPandas_data_type_manipulation%2F</url>
    <content type="text"><![CDATA[在原文链接中摘抄出部分信息作为记录形成本文。 数据类型 Pandas dtype Python 类型 NumPy 类型 用途 object str string_, unicode_ 文本 int64 int int_, int8, int16, int32, int64, uint8, uint16, uint32, uint64 整数 float64 float float_, float16, float32, float64 浮点数 bool bool bool_ 布尔值 datetime64 NA NA 日期时间 timedelta[ns] NA NA 时间差 category NA NA 有限长度的文本值列表 数据类型操作 使用df.dtypes可以显示数据所有列的类型 df.info（） 函数可以显示更有用的信息 使用 astype() 函数使用条件 数据是干净的，可以简单地解释为一个数字 你想要将一个数值转换为一个字符串对象 如果数据具有非数字字符或它们间不同质（homogeneous），那么 astype() 并不是类型转换的好选择。你需要进行额外的变换才能完成正确的类型转换。 使用方式为了真正修改原始 dataframe 中数据类型，记得把 astype() 函数的返回值重新赋值给 dataframe，因为 astype() 仅返回数据的副本而不原地修改。 参考链接 https://juejin.im/post/5acc36e66fb9a028d043c2a5]]></content>
      <categories>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>pandas</tag>
        <tag>python</tag>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬取中文网页时中文字符变英文的解决方法]]></title>
    <url>%2Fpost%2Fsolution_of_python_for_Chinese_characters_to_become_English_when_crawling_Chinese_web_pages%2F</url>
    <content type="text"><![CDATA[使用python的scrapy爬取网页时，源代码中的中文字符在爬取下来后变成了英文字符。 问题举例例如，原网页为： 爬取结果为： 解决方法修改请求头：在settings.py文件中找到下属代码： 12345# Override the default request headers:#DEFAULT_REQUEST_HEADERS = &#123;# 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',# 'Accept-Language': 'en',#&#125; 改为： 12345# Override the default request headers:DEFAULT_REQUEST_HEADERS = &#123; 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Language': 'zh-CN',&#125; 修改结果展示： 参考链接 https://blog.csdn.net/wuqili_1025/article/details/79690103]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python动态网页爬取之安装docker和splash]]></title>
    <url>%2Fpost%2FPython_dynamic_web_crawler_installed_docker_and_splash%2F</url>
    <content type="text"><![CDATA[利用python进行动态网页爬取时，在安装docker和splash时踩过的坑，记录了一下自己的安装过程。用的系统是mac os。 安装scrapy-splash 利用pip安装scrapy-splash库：$ pip install scrapy-splash 安装Docker==下面👇这样安不下去了== 如果是Mac的话需要使用brew安装，如下：brew install docker 报错： 1Error: Failure while executing; `git config --local --replace-all homebrew.private true` exited with 1. 解决方法： 1xcode-select --install 然后在执行： 1brew install docker 再继续： service docker start 报错： -bash: service: command not found上网上查一堆乱七八糟的解决方式，该路径啥的，真的不想改路径，怕把其他的改崩了。最后放弃这种方式，如果有兴趣也可以尝试解决。 ==尝试如下安装DOCKER方法== 去官网下载这种方法下载docker客户端需要从服务器下载，自己电脑下载12k/s，简直慢死了。 拉取镜像(pull the image)：docker pull scrapinghub/splash 用docker运行scrapinghub/splash： docker run -p 8050:8050 scrapinghub/splash 在浏览器中输入localhost:8050 ==安装成功== 参考链接 http://www.morecoder.com/article/1001249.html https://www.jianshu.com/p/e54a407c8a0a]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>splash</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用python3读写csv文档]]></title>
    <url>%2Fpost%2Fread%20the%20CSV%20document%20using%20python3%2F</url>
    <content type="text"><![CDATA[对于大多数的CSV格式的数据读写问题，都可以使用 csv 库。 例如：假设你在一个名叫stocks.csv文件中有一些股票市场数据，就像这样： 1234567Symbol,Price,Date,Time,Change,Volume&quot;AA&quot;,39.48,&quot;6/11/2007&quot;,&quot;9:36am&quot;,-0.18,181800&quot;AIG&quot;,71.38,&quot;6/11/2007&quot;,&quot;9:36am&quot;,-0.15,195500&quot;AXP&quot;,62.58,&quot;6/11/2007&quot;,&quot;9:36am&quot;,-0.46,935000&quot;BA&quot;,98.31,&quot;6/11/2007&quot;,&quot;9:36am&quot;,+0.12,104800&quot;C&quot;,53.08,&quot;6/11/2007&quot;,&quot;9:36am&quot;,-0.25,360900&quot;CAT&quot;,78.29,&quot;6/11/2007&quot;,&quot;9:36am&quot;,-0.23,225400 csv文档的读取1. 常规读取下面向你展示如何将这些数据读取为一个元组的序列： 1234567import csvwith open('stocks.csv') as f: f_csv = csv.reader(f) headers = next(f_csv) for row in f_csv: # Process row ... 在上面的代码中， row 会是一个列表。因此，为了访问某个字段，你需要使用下标，如 row[0]访问Symbol， row[4] 访问Change。==这样可以通过外建字典来存储读出的csv数据。== 2. 命名元组由于这种下标访问通常会引起混淆，你可以考虑使用==命名元组==。例如： 123456789from collections import namedtuplewith open('stock.csv') as f: f_csv = csv.reader(f) headings = next(f_csv) Row = namedtuple('Row', headings) for r in f_csv: row = Row(*r) # Process row ... 它允许你使用列名如 row.Symbol 和 row.Change 代替下标访问。 需要注意的是这个只有在列名是合法的Python标识符的时候才生效。如果不是的话， 你可能需要修改下原始的列名(如将非标识符字符替换成下划线之类的)。 3. 字典另外一个选择就是将数据读取到一个字典序列中去。可以这样做： 123456import csvwith open('stocks.csv') as f: f_csv = csv.DictReader(f) for row in f_csv: # process row ... 在这个版本中，你可以使用列名去访问每一行的数据了。比如，row[&#39;Symbol&#39;] 或者 row[&#39;Change&#39;]。 fieldnames 是dict_reader的一个属性，表示CSV文档的数据名称。可以通过f_csv.fieldnames来访问数据名称那一行。 CSV文件写入为了写入CSV数据，你仍然可以使用csv模块，不过这时候先创建一个 writer 对象。例如: 12345678910headers = ['Symbol','Price','Date','Time','Change','Volume']rows = [('AA', 39.48, '6/11/2007', '9:36am', -0.18, 181800), ('AIG', 71.38, '6/11/2007', '9:36am', -0.15, 195500), ('AXP', 62.58, '6/11/2007', '9:36am', -0.46, 935000), ]with open('stocks.csv','w') as f: f_csv = csv.writer(f) f_csv.writerow(headers) f_csv.writerows(rows) 如果你有一个字典序列的数据，可以像这样做： 12345678910111213headers = ['Symbol', 'Price', 'Date', 'Time', 'Change', 'Volume']rows = [&#123;'Symbol':'AA', 'Price':39.48, 'Date':'6/11/2007', 'Time':'9:36am', 'Change':-0.18, 'Volume':181800&#125;, &#123;'Symbol':'AIG', 'Price': 71.38, 'Date':'6/11/2007', 'Time':'9:36am', 'Change':-0.15, 'Volume': 195500&#125;, &#123;'Symbol':'AXP', 'Price': 62.58, 'Date':'6/11/2007', 'Time':'9:36am', 'Change':-0.46, 'Volume': 935000&#125;, ]with open('stocks.csv','w') as f: f_csv = csv.DictWriter(f, headers) f_csv.writeheader() f_csv.writerows(rows) 其中f_csv.writeheader()也可以替换成f_csv.writerow(dict(zip(headers, headers))) 参考链接 https://python3-cookbook.readthedocs.io/zh_CN/latest/c06/p01_read_write_csv_data.html https://blog.csdn.net/guoziqing506/article/details/52014506]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>文件读取</tag>
        <tag>csv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neo4j初始化节点显示设置]]></title>
    <url>%2Fpost%2FNeo4j_initializes_the_node_display_Settings%2F</url>
    <content type="text"><![CDATA[问题描述：neo4j中有默认的初始化节点显示设置为300个节点，如果想要显示的节点多于300个，则会只显示300个，并给予以下提示语句： Not all return nodes are being displayed due to Initial Node Display setting. Only 300 of 300 nodes are being displayed. 解决方法：在如图所示initial Node Display处可以修改，在此处修改为300000.]]></content>
      <categories>
        <category>Neo4j</category>
      </categories>
      <tags>
        <tag>Neo4j</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Bidirectional LSTM-CRF Models for Sequence Tagging》阅读笔记]]></title>
    <url>%2Fpost%2Fread_Bidirectional_LSTM-CRF_Models_for_Sequence_Tagging%2F</url>
    <content type="text"><![CDATA[这篇论文可以作为一个RNN和LSTM学习的一个例子来看，有利于新手对LSTM的理解。对于NER的处理主要是作为一个序列标注问题。但是作为经典文章还是可以读一读了解一下的。 在本篇论文中，作者提出了4种模型：LSTM、BI-LSTM、LSTM-CRF和BI-LSTM-CRF。 contribution(贡献) 作者在NLP标注数据集上系统的对比了以上四个模型； 作者是首先提出把BI-LSTM-CRF模型用于NLP序列标注，并且达到了state-of-the-art的水平； 作者展示了BI-LSTM-CRF是robust，并且极少依赖于词向量。 model(模型)LSTM首先，作者先介绍了RNN的结构和工作原理，如图： 其中输入为句子：EU rejects German call to boycott British lamb。输出为标签：B-ORG O B-MISC O O O B-MISC O O，其中B-，I-表示实体开始和中间位置。标签种类为：other (O)和四种实体标签：Person (PER), Location (LOC), Organization (ORG), and Miscellaneous (MISC). 输入层表示在时间步 t 的特征。它们可以是 one-hot-encoding 的词特征，稠密或者稀疏的向量特征。输入层与特征有相同大小的维度。输出层表示在时间步 t 的标签上的概率分布，维度与标注数量相同。相比前馈神经网络，RNN 引入前一个隐藏状态和当前隐藏状态的结合，因此可以储存历史信息。 涉及公式为： 其中，U，W，V都是权重，函数f，g分别为sigmoid和softmax函数。 接下来，作者展示了LSTM的结构和原理，如图： 公式： 其中，σ是逻辑sigmoid函数，i, f, o 和 c分别是输入门，忘记门，输出门和细胞向量，所有的大小都和向量h一样。w权重的含义如其下表所示。 LSTM序列标注模型如图所示： 其中，中间的画斜线的格子即为图2中所示部分。 Bidirectional LSTM(双向LSTM)作者展示了双向LSTM的结构，如图所示： 双向LSTM网络可以有效利用过去特征和未来特征。在作者的实现中，对于整个句子的前向和后向操作，作者只需要在每个句子开始时将隐藏状态重置为0。作者采用批处理，使得可以同时处理多个句子。 CRF使用邻居标记信息预测当前标记有两种不同的方法： 预测每个时间步长的标签分布，然后使用波束式解码来找到最优的标签序列，代表方法：MEMMs 注重句子层次而不是个体位置，代表方法：CRF，输入和输出是直接相连的；如图： 研究表明，CRFs一般能够产生更高的标签精度。 LSTM-CRF作者展示了LSTM-CRF的结构，如图： 这网络可以有效地通过 LSTM 利用过去的输入特征和通过 CRF 利用句子级的标注信息。图中CRF层由连接连续输出层的线表示。CRF层有一个状态转移矩阵作为参数。 公式为： 函数f为网络的输出分数，[x]为输入， [fθ]i,t 为带有参数θ（句子x，第i 个标签，第t个单词）的网络输出； [A]i,j为转移分数，从连续的时间步i状态到j状态的转移分数。注意，该转换矩阵是位置无关的。 BI-LSTM-CRF作者展示了BI-LSTM-CRF的结构，如图所示： 作者在实验中展示了额外的未来特征可以提高标签的准确率。 训练过程本文使用的所有模型都共享一个通用SGD前向和后向训练过程。作者展示了BI-LSTM-CRF的算法，如图 作者设置了批次大小为100。 实验data作者在以下三个数据集上测试自己的模型：Penn TreeBank (PTB) POS tagging, CoNLL 2000 chunking, and CoNLL 2003 named entity tagging.数据集信息展示如下： Features作者从三个数据集中提取出其公共特征。特征可以分为拼写特征和上下文特征。最终，作者对于POS（词性标注）、chunking（组块）和NER（命名实体识别）分别提取401K，76K和341K个特征。 spelling features（拼写特征）除了小写字母特征之外，我们提取给定单词的以下特征。 context featurs（上下文特征）对于单词特征，作者使用unigram和bi-grams特征。对于在CoNLL2000数据集中的POS特征和在CoNLL2003数据集中的 POS &amp; CHUNK特征，作者使用了unigram，bi-gram和tri-gram特征。 词向量词向量在改进序列标注任务的表现方面起着至关重要的作用，我们使用 130K 词汇并且每个词汇的词向量维度是 50 维。我们将 one-hot-encoding词表示替换每个词对应的词向量。 Features connection tricks我们可以将拼写和上下文特征与单词特征一样对待。这样网络的输入包括单词，单词的拼写和上下文特征。然而，==我们发现将拼写和上下文特征与输出直接连接可以加速训练过程，同时也能保持标注的准确率，==如下图所示： 我们注意到，这种特征的使用与使用的最大熵特征类似。区别在于采用特征三列技术可能会发生特征冲突。由于序列标注数据集中的输出标签小于语言模型（通常为数十万），所以我们可以在特征和输出之间建立完整的连接，以避免潜在的特征冲突。 结果在相同的数据集上分别训练LSTM，BI-LSTM，CRF，LSTM-CRF和BI-LSTM-CRF模型，并且采用两种方式初始化word embedding：随机和Senna方式。模型的训练速率为0.1，隐藏层数量为300.不同模型在不同word embedding下的结果如表2所示，同时列出了之前最好模型Cov-CRF。 与Cov-CRF比较 实验中设置了3个基准模型，分别为LSTM、BI-LSTM和CRF，结果中LSTM在三个数据集中效果最差，BI-LSTM跟CRF在POS和chunking中效果接近，但是在NER中后者要优于前者。有趣的是表现最好的模型BI-LSTM-CRF相对于Cov-CRF来说对Senna embedding的依赖程度更小。 (robustness)模型鲁棒性 为验证模型的鲁棒性，对不同模型只采用word feature特征进行训练，训练结果如表3，括号中数字表示相比于全部特征，模型的结果下降数值。 与其他系统的比较 这里就不贴图了，总之就是阐述作者自己模型好。 结论总之作者的模型是基于之前模型的一些改进，主要运用了IBI-LSTM和CRF的结合。 论文下载地址]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>NER</tag>
        <tag>LSTM</tag>
        <tag>BI-LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CN-DBpedia A Never-Ending Chinese Knowledge Extraction SystemCN-DBpedia System]]></title>
    <url>%2Fpost%2Fessay%2F</url>
    <content type="text"><![CDATA[前言 本篇论文为2016年的一篇论文，主要介绍了作者构建中文知识图谱所遇到的一些问题和解决方法。 challenge 如何降低人力成本？ 如何保持知识库的新鲜度？ 贡献 在构建中文知识库中降低了人力成本： 重复利用已经存在的本体论 提出了一个不用人工监督的端到端的深度学习模型 提出了一个智能主动更新策略 系统结构 提高知识库质量： Normalization： normalize the attributes and values Enrichment：reuse the ontology Correction：two steps error detection: rule-based detection based on user feedbacks error correction crowd-sourcing 降低人力成本这部分作者采用了两种方法： 重复利用已经存在在知识库的本体论和类型化的中文实体 构建一个端到端提取器 Cross-Lingual Entity Typing（跨语言的实体类型） 第一步是通过用英文DBpedia类型来类型化中文实体。为了达到这个目的，作者提出了如下系统：系统建立了监督层次分类模型，系统输入为没有标记类型的中文实体，输出为在DB中所有有效的英文类型。作者将中文实体与共享相同中文标签名称的英语实体配对，这样中文实体以及配对英语实体的类型自然是标记样本。 用上述方法得到的训练集可能出现下面一些问题： 英文DBpedia实体类型在许多情况下可能不完全； 英文DBpedia实体类型在许多情况下可能是错误的； 中英文链接可能出错； 中文实体的特征通常不完整。 为了解决以上问题，作者提出了两种方法： 完善英文DBpedia实体类型； 设计一个过滤步骤来剔除错误样本。 infobox completion Infobox completion is a task to extract object for a given pair of entity and predicate from encyclopedia articles. 作者建模了一个seq2seq模型，输入为包含tokens的自然语言句子，输出为每个token的标签。对于标签为0或1。 对于建立一个有效的提取器有以下两个关键： 如何构建训练集：作者采用远程监督方法（利用Wikipedia） 如何选取期望的提取模型：LSTM-RNN，如图所示 知识库更新作者采用动态更新：识别新实体或可能包含新事实的旧实体 作者根据以下两方面来辨别这些实体： 近期热点新闻中提及的实体 在搜索引擎的流行搜索关键字或其他流行网页中提到的实体 对于如何从新闻标题和搜素指令中提取实体名字，作者采用简单的词分割方法，从百科全书中判断其是否为实体，并提出IDF值低的分割子串。 统计数据 论文下载链接]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
        <tag>机器学习</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ontology reasoning with deep neural networks]]></title>
    <url>%2Fpost%2Fessay%2F</url>
    <content type="text"><![CDATA[Ontology reasoning with deep neural networks（基于深度神经网络的本体推理）前言 本论文属于知识图谱的下游任务，也就是知识图谱的应用的一个例子。这篇论文的方法根据作者描述RRN是第一个基于深度学习的全面本体推理方法。 目标获得一个可以在不同的场景进行有效推理的模 问题描述基于机器学习的推理文章通常假设了一个特定的应用案例：自然语言或视觉输入的推理。作者采用一个不同的方法：将正式的推理问题作为起点。对于特定的问题选择：选择一种在表现力与另一方面复杂性之间取得适当平衡的方法通常是明智的。OWL RL本体推理是指一种常见的场景，在这种场景中，用于推理的推理规则（在此上下文中称为本体）与我们寻求推理的事实信息一起指定。 本体推理是一种非常灵活的工具，它允许对大量不同的场景进行建模，因此满足了我们对适用于各种应用的系统的需求。==首先引出了什么是本质推理，然后进一步阐述为什么要用机器学习== 今天用于推理的大多数KRR形式都植根于符号逻辑,这些方法在实践中会遇到许多问题：例如处理不完整，冲突或不确定数据的困难机器学习模型通常具有高度可扩展性，更能抵抗数据中的干扰，并且即使所提供的形式是错误的也能够提供预测。 作者的目标是通过采用尖端的深度学习技术，目标是在近似于形式方法的高度期望（理论）属性和另一方面利用机器学习的稳健性之间管理平衡行为。 对于用于推理的知识图谱：作者采用的是由个体、类和二元关系组成的信息构成，其中个体对应于顶点，关系对应于被标记的有向边缘，类对应于二进制顶点标签。关系是主体和客体之间的关系或者个人和类之间的关系。这与关系学习不同：在关系学习的背景下，知识图通常通过将类视为个人以及将成员视为普通关系来简化。然而，就作者的目的而言，明确区分类和关系是很重要的，因为在用于推理的知识图谱中类和关系可能不同。 模型总览整个模型是以RRN为基础进行构建的，每个RRN都针对特定的本体进行训练。当训练模型应用于一组特定的事实时，它分为如下两个步骤： 它为所有的步骤生成矢量表示，也就是嵌入在所考虑数据中出现的个体。 它仅基于这些生成向量计算查询预测 在图中， a中它考虑一个事实三元组，并根据数据集重复多次。 b中它每读取一个事实就获取三元组中的个体潜入，并将他们的反馈送入更新层，该层产生已提供的嵌入的更新版本，然后将其存储在前一个版本的位置。 c中从随机生成的向量开始，逐步更新嵌入，以便对关于它们所代表的个体的事实和推论进行编码。 评估作者在四个不同的数据集上训练和评估了RRN，其中两个是人工生成的玩具数据集，两个是从现实世界的数据库中提取的。这样做的原因： 玩具问题具有很大的优势，即很明显某些推论是多么困难，从而为我们提供了对模型能力的相当好的印象。 在现实环境中评估方法当然是性能不可或缺的衡量标准 作者为了评估真实世界数据的RRN模型，还从从两个著名的知识库DBpedia和Claros中提取了数据集。 结果 RRN能够有效地编码提供的关于类和关系的事实 对于关系的推理，可以看到DBpedia的准确度略低于98.9％，而其他数据集中的可导出关系在所有情况中至少99.6％被正确预测。 可以预测该模型在预测可推断类别方面比在关系方面表现更好，因为大多数这些都是仅依赖于单个三元组的推论。 为了评估作者提出的KRR方法常常遇到的问题，作者进行了如下实验： 对于缺少信息的问题，作者随机删除了一个无法通过每个样本的符号推理推断出的事实，并检查模型是否能够正确地重建它。结果：对于DBpedia来说，33.8％的失踪三元组就是这种情况，而对于Claros来说，38.4％被正确预测 对于冲突的问题，作者通过在每个测试样本中随机选择一个事实来测试模型解决冲突的能力，并添加相同的否定版本作为另一个事实。对于DBpedia，RRN正确解决了88.4％的引入冲突，而对于Claros，它甚至达到了96.2％。然而，最重要的是，对于任何一个损坏的数据集，之前报告的总精度都没有下降超过0.9。 所有RRN的查询预测都完全基于它为各个数据集中的个体生成的嵌入，这就是为什么仔细研究这样一组嵌入向量是有益的。 思考本论文属于知识图谱的下游任务，也就是知识图谱的应用的一个例子。这篇论文的方法根据作者描述RRN是第一个基于深度学习的全面本体推理方法。但是具体的操作方法论文中写的比较清晰，感觉自己是理解了。重点就是对于个体的嵌入表示，如果类比的话就是词向量，作者通过不断的处理更新这个词向量，最后通过所获的词向量进行推理。并且从这篇文章中可以看到作者使用的知识图谱和我之前在弄的关系三元组有所区别。 论文下载链接]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
        <tag>深度学习</tag>
        <tag>Ontology</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初次见面，你好NYSDY！]]></title>
    <url>%2Fpost%2Fessay%2F</url>
    <content type="text"></content>
  </entry>
</search>
