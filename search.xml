<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[2016 TransG : A Generative Model for Knowledge Graph Embedding阅读笔记]]></title>
    <url>%2Fpost%2FTransG_%3A_A_Generative_Model_for_Knowledge_Graph_Embedding%2F</url>
    <content type="text"><![CDATA[论文下载地址 解决问题multiple relation semantics（多重关系语义）：一个关系可能具有与对应的三元组关联的实体对揭示的多种含义。 这里以TransE的可视化为例，表明：特定关系有不同的聚类，并且不同的聚类表示不同的潜在语义，证实了该问题的存在性。 该现象产生原因 人为简化 知识库策展人不能涉及太多相似关系，因此将多个相似关系抽象为一个特定关系是一种常见的技巧 知识性质 语言和知识表示形式常常涉及不明确的信息。 知识的模糊性意味着语义上的混合 TransG利用贝叶斯非参数无限混合模型通过为关系生成多个翻译组件来处理多个关系语义]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>2016</tag>
        <tag>ACL</tag>
        <tag>KGE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2016 Knowledge Graph Completion with Adaptive Sparse Transfer Matrix阅读笔记]]></title>
    <url>%2Fpost%2F2016_Knowledge_Graph_Completion_with_Adaptive_Sparse_Transfer_Matrix%2F</url>
    <content type="text"><![CDATA[为解决heterogeneity和imbalance问题，作者针对同一关系链接实体对的数量和同一关系不同头尾实体数量，分别设计了不同稀疏程度的转移矩阵。存在缺点：并没有同时解决这两个问题。 论文下载地址 Problem Statement heterogeneity（异质性）：一些关系链接了很多实体对，另一些没有 imbalance（不平衡）： 在一种关系中，头实体和尾实体的数量不同 Contribution 作者提出了一种新颖的方法，该方法考虑了先前模型中未使用的异质性和不平衡性，以嵌入知识图来完成它们； 作者的方法高效且参数较少，因此很容易扩展到大规模知识图； 作者为转移矩阵提供了两种稀疏模式，并分析了它们的优缺点； 在三元组分类和链接预测任务中，作者的方法达到了最先进的性能 Sparse Matrix定义稀疏矩阵是指大多数条目（entries）为零的矩阵。 零元素占矩阵元素总数的比例称为稀疏度（sparse degree）。 类别 结构化的 非结构化的 两者重要区别 结构化模式有利于矩阵向量乘积运算。 非结构化往往可以带来更好的实验结果：由于更加灵活地远范围线性组合。 Sparse Matrix vs Low-Rank Matrix特点 low-rank矩阵强制一些变量要满足特定的约束，因此，矩阵M无法自由地进行赋值。 sparse矩阵是作者令其中的部分元素值为0，并且在训练过程中不改变它的值，其他的非零值进行训练。 对比 稀疏矩阵比低秩矩阵更灵活，可以有更大的自由度：使用低秩矩阵，那么矩阵的自由度会受到严格的秩限制。然而，sparse matrix的稀疏性只是控制矩阵元素中零元素的个数。 稀疏矩阵比低秩矩阵更有效率：只有非零条目参与计算，极大地减少了计算量 ModelTranSpare(share) 解决heterogeneity问题 特点 转移矩阵的稀疏度由关系链接的实体对的数量确定 并且关系的两侧共享相同的转移矩阵 对于复杂关系的转移矩阵更加稀疏 不知道为什么复杂转移矩阵会更加稀疏？ 转移矩阵的稀疏程度 \theta_{r}=1-\left(1-\theta_{\min }\right) N_{r} / N_{r^{*}}其中，$N_r$代表链接关系$r$的实体对的数量，$r^$代表链接最多实体对的关系，$\theta_{\min }\left(0 \leq \theta_{\min } \leq 1\right)$是一个超参数，代表矩阵$M_{r^{}}$的最小系数程度。 不理解这里为什么把稀疏程度定义成这么麻烦，为什不直接定义成$\theta_{\min} N_{r} / N_{r^{*}}$ 映射向量 \mathbf{h}_{p}=\mathbf{M}_{r}\left(\theta_{r}\right) \mathbf{h}, \quad \mathbf{t}_{p}=\mathbf{M}_{r}\left(\theta_{r}\right) \mathbf{t}TranSpare(separate) 解决imbalance问题 特点 每个关系具有两个单独的稀疏传递矩阵，一个用于头实体，另一个用于尾实体 稀疏度取决于通过关系链接的头（尾）实体的数量 转移矩阵的稀疏程度 \theta_{r}^{l}=1-\left(1-\theta_{\min }\right) N_{r}^{l} / N_{r^{*}}^{l^{*}} \quad(l=h, t)和share类似，只是头尾实体不相同，增加l来代表头尾实体数量。 映射向量 \theta_{r}^{l}=1-\left(1-\theta_{\min }\right) N_{r}^{l} / N_{r^{*}}^{l^{*}} \quad(l=h, t)分数函数两者的分数函数相同均为： f_{r}(\mathbf{h}, \mathbf{t})=\left\|\mathbf{h}_{p}+\mathbf{r}-\mathbf{t}_{p}\right\|_{\ell_{1 / 2}}^{2}总loss采用margin-based ranking loss $L=\sum_{(h, r, t) \in \Delta\left(h^{\prime}, r, t\right) \in \Delta^{\prime}}\left[\gamma+f_{r}(\mathbf{h}, \mathbf{t})-f_{r}\left(\mathbf{h}^{\prime}, \mathbf{t}^{\prime}\right)\right]_{+}$训练过程为了加速训练时收敛以及防止过拟合，作者使用TransE算法的结果进行初始化实体和关系的embedding向量，对于转化矩阵，作者使用单位矩阵进行初始化。但是这不是非必须的 对于转化矩阵(假设为单位矩阵)，非零向量的个数$n z=\lfloor\theta \times n \times n\rfloor$，由于作者使用单位向量初始化，所以除了对角线上的非零元素之外，其他非零元素的个数为$n z^{\prime}=n z-n$，如果$n z \leq n$，那么作者设置$n z^{\prime}=0$。 在构建结构化的转化矩阵$\mathbf{M}(\theta)$的时候，作者要让$n z^{\prime}$非零元素对称分布在对角线的两边，如果$n z^{\prime}$不能满足要求，那么作者选择另外一个整数。 在构建非结构化的转化矩阵$\mathbf{M}(\theta)$的时候，作者只随机散布$\mathbf{M}(\theta)$中的$n z^{\prime}$非零元素（但不在对角线上）。 在训练前，作者首先设置超参数$\theta_{\min }$，然后计算每个转化矩阵的稀疏程度，然后，作者使用结构化或非结构化模式构建稀疏转化矩阵。 实验与常规不同的就是加了一个实验]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>2016</tag>
        <tag>KGE</tag>
        <tag>AAAI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2015 TransA An Adaptive Approach for Knowledge Graph Embeddin阅读笔记]]></title>
    <url>%2Fpost%2F2015_TransA_An_Adaptive_Approach_for_Knowledge_Graph_Embeddin%2F</url>
    <content type="text"><![CDATA[这篇文章具体的公式操作比较难以理解，但是对于我来说，它的每一维度加权和最后判断时也应该考虑不同维度差异和我的思路比较像。 论文下载地址 Problem Statement之前的机遇翻译的方法过于简化损失度量，使得模型没有足够的能力来模拟知识库中的各种复杂实体/关系。 Introduction 由于损失度量的不灵活性，当前基于翻译的方法应用具有不同合理性的球面等势超表面，其中三元组更靠近中心，三元组更合理。如图1（a）所示，球面等势超表面不够灵活，不足以表征拓扑结构。 其次，由于过度简化的损失度量，当前基于翻译的方法用同性的欧氏距离无法体现出各个维度的重要性，而是将各个维度的特征都等同看待。导致图2中出现的问题： 由于对每一维度处理相同，不正确的实体将被匹配。但是通过对不同维度进行不同加权将会避免这种由于距离相同被匹配的错误。 Model 作者提出TransA，一种利用自适应和灵活度量的嵌入方法： TransA采用椭圆形表面而不是球形表面：通过这种方式，复杂关系引起的复杂嵌入拓扑结构可以得到更好的表现。 如“自适应度量方法”中所分析的那样，TransA可以被视为加权变换特征维度。 因此，来自不相关尺寸的噪声被抑制。 自适应度量分数函数TransA采用自适应Mahalanobis绝对损失距离： f_{r}(h, t)=(|\mathbf{h}+\mathbf{r}-\mathbf{t}|)^{\top} \mathbf{W}_{\mathbf{r}}(|\mathbf{h}+\mathbf{r}-\mathbf{t}|)其中 |\mathbf{h}+\mathbf{r}-\mathbf{t}| \doteq\left(\left|h_{1}+r_{1}-t_{1}\right|,\left|h_{2}+r_{2}-t_{2}\right|, \ldots, | h_{n}+\right.\left.\left.r_{n}-t_{n}\right\rfloor\right)，$W_r$是对应于特定关系的对称非负权重矩阵，也是自适应权重矩阵。与传统的得分函数不同，作者取绝对值，因为想要测量（h + r）和t之间的绝对损失。原因有以下两点： 作者将的分数函数扩展为诱导规范： N_{r}(\mathbf{e})=\sqrt{f_{r}(h, t)}这样可以用以下方式来简化运算： \begin{array}{l}{\text { inequality } N_{r}\left(\mathbf{e}_{1}+\mathbf{e}_{2}\right)=\sqrt{\left|\mathbf{e}_{1}+\mathbf{e}_{2}\right|^{\top} \mathbf{W}_{\mathbf{r}}\left|\mathbf{e}_{\mathbf{1}}+\mathbf{e}_{\mathbf{2}}\right|} \leq} {\sqrt{\left|\mathbf{e}_{\mathbf{1}}\right|^{\top} \mathbf{W}_{\mathbf{r}}\left|\mathbf{e}_{\mathbf{1}}\right|}+\sqrt{\left|\mathbf{e}_{\mathbf{2}}\right|^{\top} \mathbf{W}_{\mathbf{r}}\left|\mathbf{e}_{\mathbf{2}}\right|}=N_{r}\left(\mathbf{e}_{\mathbf{1}}\right)+N_{r}\left(\mathbf{e}_{\mathbf{2}}\right)}\end{array} 在几何中，负值或正值表示向下或向上的方向。而在作者的方法中，作者不考虑这个因素。 让作者看一下如图2所示的实例。 对于实体Goniff，其损耗向量的x轴分量是负的，因此扩大该分量将使整体损失更小，而这种情况应该使整体损失更大。 因此，绝对算子对作者的方法至关重要。 对于没有绝对算子的数值例子，当嵌入维数为2时，权重矩阵为[0 1; 1 0]和损失矢量（h + r - t）=（e1，e2），总损失为2e1e2。 如果e1≥0且e2≤0，则绝对更大的e2将减少总损耗，这是不希望的 从等势面的角度对于其他基于翻译的方法，等势超曲面是欧几里德距离定义的球体： \|(\mathbf{t}-\mathbf{h})-\mathbf{r}\|_{2}^{2}=\mathcal{C}其中，$\mathcal{C}$表示阈值或等势值。 而对于TransA来说，等势面是椭圆 |(\mathbf{t}-\mathbf{h})-\mathbf{r}|^{\top} \mathbf{W}_{\mathbf{r}}|(\mathbf{t}-\mathbf{h})-\mathbf{r}|=\mathcal{C}​ 由于知识库是大规模且非常复杂的实际情况，嵌入的拓扑结构不能像球体那样均匀分布，如图1所示。 因此，用椭圆形替换球面等势超曲面将增强嵌入 从特征权重角度TransA可以看做是带有权重的特征变换，假设权重矩阵$W_r$是对称矩阵，那么可以通过LDL分解将权重分解为 \begin{array}{c}{\mathbf{W}_{\mathbf{r}}=\mathbf{L}_{\mathbf{r}}^{\top} \mathbf{D}_{\mathbf{r}} \mathbf{L}_{\mathbf{r}}} \\ {f_{r}=\left(\mathbf{L}_{\mathbf{r}}|\mathbf{h}+\mathbf{r}-\mathbf{t}|\right)^{\top} \mathbf{D}_{\mathbf{r}}\left(\mathbf{L}_{\mathbf{r}}|\mathbf{h}+\mathbf{r}-\mathbf{t}|\right)}\end{array}相当于对loss向量通过$L_r$进行特征变换，其中，$\mathbf{D}_{r}=\operatorname{diag}\left(w_{1}, w_{2}, \ldots\right)$是一个对角矩阵，对角元素的值就是不同嵌入维度的权值。 对比之前方法与关于旋转和缩放嵌入空间的TransR，TransA具有两个优势。 首先，作者对特征尺寸进行加权以避免噪音。 其次，作者放松了PSD条件以获得灵活的表示。 与使用预先计算的系数重新构造对特征尺寸进行加权的TransM，TransA具有两个优势。 首先，作者从数据中学习权重，这使得分数函数更具适应性。 其次，作者应用特征转换，使嵌入更有效 算法训练loss函数为： \begin{array}{c}{\mathcal{L}=\sum_{(h, l, t) \in \Delta\left(h^{\prime}, l^{\prime}, t^{\prime}\right) \in \Delta^{\prime}}\left[\gamma+f_{r}(\mathrm{h}, \mathrm{t})-f_{r^{\prime}}\left(\mathrm{h}^{\prime}, \mathrm{t}^{\prime}\right)\right]_{+}+\lambda\left(\sum_{r \in R}\|\mathbf{W} r\|_{F}^{2}\right)+C\left(\sum_{e \in E}\|\mathbf{e}\|_{2}^{2}+\sum_{r \in R}\left\|\mathbf{r}_{2}^{2}\right\|\right)} \\ {\text { s.t. }\left[\mathbf{W}_{r}\right]_{i j} \geq 0}\end{array}保证非负性，作者将所有否定条目权重的值赋为0 \mathbf{W}_{r}=-\sum_{(h, r, t) \in \Delta}\left(|\mathbf{h}+\mathbf{r}-\mathbf{t} \| \mathbf{h}+\mathbf{r}-\mathbf{t}|^{\top}\right)+\sum_{\left(h^{\prime}, r^{\prime}, t^{\prime}\right) \in \Delta^{\prime}}\left(\left|\mathbf{h}^{\prime}+\mathbf{r}^{\prime}-\mathbf{t}^{\prime}\right|\left|\mathbf{h}^{\prime}+\mathbf{r}^{\prime}-\mathbf{t}^{\prime}\right|^{\top}\right)模型复杂度至于作者模型的复杂性，权重矩阵完全由现有的嵌入向量计算，这意味着TransA几乎具有与TransE相同的自由参数数。 至于作者模型的效率，权重矩阵有一个封闭的解决方案，这在很大程度上加快了培训过程 实验作者统计了一个ATPE值（Averaged Triple number Per Entity.）该数量衡量数据集的多样性和复杂性 作者对于TransA在WN18上表现不好进行了分析解释。 TransA在WN18数据集上的平均排名表现不佳。 深入研究详细情况，我们发现有27个测试三元组（测试集的0.54％），其排名超过30,000，这几个案例将导致约162个平均等级损失。 所有这些三元组的尾部或头部实体从未与训练集中的相应关系共存。 训练数据不足导致权重矩阵过度扭曲，权重矩阵过度扭曲导致平均排名不良 作者还做了一个实验 作者解释： 精度随重量差异而变化，这意味着特征加权有利于精确度。 这证明了TransA的理论分析和有效性 但是我并不认为这个可以说明什么，那是不是调高其他关系的权重会带来该关系效果的提升？]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>KG</category>
      </categories>
      <tags>
        <tag>KGE</tag>
        <tag>TransA</tag>
        <tag>2015</tag>
        <tag>arxiv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Transition-based Knowledge Graph Embedding with Relational Mapping Properties阅读笔记]]></title>
    <url>%2Fpost%2FTransition-based_Knowledge_Graph_Embedding_with_Relational_Mapping_Properties%2F</url>
    <content type="text"><![CDATA[论文下载地址 Related Work把知识图谱映射到低维向量的原因：符号和离散的存储结构使我们很难利用这些知识来增强其他智能获取的应用程序（例如问答系统），因为许多与AI相关的算法更倾向于进行关于连续数据计算。 文中用ONE-TO-ONE (husband-to-wife), MANY-TO-ONE (children-to-father), ONE-TO- MANY (mother-to-children), MANY-TO-MANY (parents-to-children) 来进行非单一关系的例子觉得很好。 以前算法的目标函数和参数复杂度： Model理解不了是如何区分1对多关系的？？？？ 作者将最优函数将通过对应于该关系的预先计算的权重给出每个训练三元组的不同方面。 实际上，大约只有26.2％的ONE-TO-ONE三元组适合由TransE建模。 另一方面，其余三元组（73.8％）受到影响，如图1左侧所示。 动机根据作者的观察，三元组的映射属性在很大程度上取决于它的关系。 目标函数权重是关系特定的，作者为三元组（h，r，t）提出的新评分函数是： f_{r}(h, t)=w_{\mathbf{r}}\|\mathbf{h}+\mathbf{r}-\mathbf{t}\|_{L_{1} / L_{2}} w_{r}=\frac{1}{\log \left(h_{r} p t_{r}+t_{r} p h_{r}\right)} 测量关系的映射属性程度的一种简单方法是计算每个不同头部实体的尾部实体的平均数量，反之亦然。 这里还是不理解这个权重的意义在哪里？🧐 损失函数 \begin{array}{l}{\mathcal{L}=\min \sum_{(h, r, t) \in \Delta\left(h^{\prime}, r, t^{\prime}\right) \in \Delta_{(h, r, t)}^{\prime}}\left[\gamma+f_{r}(h, t)-f_{r}\left(h^{\prime}, t^{\prime}\right)\right]_{+}} \\ {\text {s.t. } \quad \forall e \in E,\|e\|_{2}=1}\end{array}对于$|e|_{2}=1$的解释：约束位于单位球上的每个实体的原因是为了保证它们可以以相同的比例更新，而不是太大或太小而不能满足最佳目标。 Experiments见论文，没什么好阐述的。 All the codes for the related models can be downloaded from https://github.com/glorotxa/SME]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>KG</category>
      </categories>
      <tags>
        <tag>KGE</tag>
        <tag>2014</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Embedding Edge-attributed Relational Hierarchies阅读笔记]]></title>
    <url>%2Fpost%2FEmbedding_Edge-attributed_Relational_Hierarchies%2F</url>
    <content type="text"><![CDATA[论文下载地址]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Knowledge Graph Embedding via Dynamic Mapping Matrix阅读笔记]]></title>
    <url>%2Fpost%2FKnowledge_Graph_Embedding_via_Dynamic_Mapping_Matrix%2F</url>
    <content type="text"><![CDATA[论文下载地址 Problem Statement 对于特定的关系$r$，所有的实体都共享相同的映射矩阵$M_r$。然而，由关系链接的实体总是包含各种类型和属性。 投影是实体和关系之间的交互过程，映射矩阵只能由关系决定是不合理的。 矩阵向量乘法使其具有大量计算，并且当关系数大时，它还具有比TransE和TransH更多的参数。 由于复杂性，TransR / CTransR难以应用于大规模知识图 Contribution 作者构建了一个新颖的模型TransD，通过同时考虑实体和关系的多样性，为每一个实体-关系构建动态映射矩阵。它为实体表示映射到关系向量空间提供灵活的样式。 与TransR / CTransR相比，TransD具有更少的参数并且没有矩阵向量乘法 在实验中，作者的方法优于之前的模型。 Model模型在TransD中，每个命名的符号对象（实体和关系）由两个向量表示。 第一个捕获实体（关系）的含义，另一个用于构造映射矩阵。 对于一个三元组$(h,r,t)$,向量一共有$h, h_p, r, r_p, t, t_p$,其中带$p$的为映射向量，则有 \begin{aligned} \mathbf{M}_{r h} &=\mathbf{r}_{p} \mathbf{h}_{p}^{\top}+\mathbf{I}^{m \times n} \\ \mathbf{M}_{r t} &=\mathbf{r}_{p} \mathbf{t}_{p}^{\top}+\mathbf{I}^{m \times n} \end{aligned}故 \mathbf{h}_{\perp}=\mathbf{M}_{r h} \mathbf{h}, \quad \mathbf{t}_{\perp}=\mathbf{M}_{r t} \mathbf{t}可以综合为： \begin{aligned} \mathbf{h}_{\perp} &=\mathbf{M}_{r h} \mathbf{h}=\mathbf{h}+\mathbf{h}_{p}^{\top} \mathbf{h} \mathbf{r}_{p} \\ \mathbf{t}_{\perp} &=\mathbf{M}_{r t} \mathbf{t}=\mathbf{t}+\mathbf{t}_{p}^{\top} \mathbf{t} \mathbf{r}_{p} \end{aligned}这样就没有矩阵和向量间的乘法运算，变成向量间运算，提升计算速度。 Experiments and Results Analysis常规实验：triplets classification and link prediction不再赘述。 作者在实验过程中关注了一些具有更低accuracy的关系。 分析： 1. 对于$similar_to$关系主要因为训练数据不充足，只占了1.5%。 2. 对于最右侧的图说明了bern方法的效果要好于unif Properties of Projection Vectors作者还做了case study，通过不同类型实体和关系的投影向量的相似性表明了作者方法的合理性]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>KGE</tag>
        <tag>TransD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[From Knowledge Graph Embedding to Ontology Embedding An Analysis of the Compatibility between Vector Space Representations and Rules阅读笔记]]></title>
    <url>%2Fpost%2FFrom_Knowledge_Graph_Embedding_to_Ontology_Embedding_An_Analysis_of_the_Compatibility_between_Vector_Space_Representations_and_Rules%2F</url>
    <content type="text"><![CDATA[论文下载地址]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Knowledge graph embedding with concepts阅读笔记]]></title>
    <url>%2Fpost%2FKnowledge_graph_embedding_with_concepts%2F</url>
    <content type="text"><![CDATA[这篇论文，运用skip-gram方法，将实体对应相关概念引入实体向量表示，以增强表示效果。实体和概念在同一空间中，但是概念是空间中的一个超平面（类似于transH）。文中举例很多例子来辅助说明，使得文章可读性大幅提升。文中实验最后俩个比较有意思。本文值得思考借鉴的东西不少，值得再好好回顾。 论文下载地址 import csv with open('stocks.csv') as f: f_csv = csv.DictReader(f) for row in f_csv: # process row problem statement 已经存在的KGE模型主要集中于实体-关系-实体三元组或者文本语料交互。 三元组是缺少信息的，并且域内文本不总是可以获得的——导致嵌入结果偏离实际 常识概念知识发挥很重要的作用。 background For example, for two triplets (Apple, Developer, IPhone) and (Apple, Developer, Samsung Mobile), it is quite difficult to distinguish which is the true triplet that contains fact triplets only, because ‘‘IPhone’’ and ‘‘Samsung Mobile’’ both belong to mobile phones. However, in the concept graph, ‘‘IPhone’’ has a concept ‘‘apple device’’, but ‘‘Samsung Mobile’’ does not. Thus, it is easy to infer the correct triplet by mapping ‘‘IPhone’’ to the ‘‘apple device’’concept 很好的一个举例关于如何运用concept来辅助关系识别 Specifically, when a corpus about technology is provided, embedding methods with technical textual descriptions could easily infer the fact (Apple, Developer, IPhone), because the keywords ‘‘hardware products’’ and ‘‘iPhone smartphone’’ occur frequently in the textual description of ‘‘Apple’’. However, it is difficult to infer the fact (Apple, Taste, Sweet), which is irrelevant to textual descriptions of ‘‘Apple’’ about the specific topic of ‘‘technology company 这里作者举例说明：与具有文本信息的嵌入方法相比，具有概念信息的嵌入方法在其任务中更加通用，并且它不依赖于语料库的主题。 作者把KGE分成了三类，如下： Embedding with symbolic triplets：trans系列都放到了这部分中 Embedding with textual information Embedding with category information Methodologyconcept graph embedding作者采用skip-gram来学习可以捕获其语义相关性的概念和实体的表示。 其中，每个实体对应多个概念，每个概念又包含多个实体（这些实体作为实体的上下文）。 则，skip-gram函数可以写为： \begin{array}{l}{P\left(e_{c} | e_{t}\right)=\frac{\exp \left(e_{c} \cdot e_{t}\right)}{\sum_{e \in E} \exp \left(e \cdot e_{t}\right)}} \\ {P\left(e_{c} | c_{i}\right)=\frac{\exp \left(e_{c} \cdot c_{i}\right)}{\sum_{e \in E} \exp \left(e \cdot c_{i}\right)}}\end{array}故损失函数为： L=\frac{1}{|D|} \sum_{\left(e_{c}, e_{t}\right) \in D}\left[\log P\left(e_{c} | e_{t}\right)+\sum_{c_{i} \in C\left(e_{t}\right)} \log P\left(e_{c} | c_{i}\right)\right]学习率设为： α = starting_alpha×(1−count_actual/(real)(iter × total_size+1)) 这里作者说为了避免过拟合，对优化目标采用“负抽样”方法。&quot;负抽样&quot;方法还可以避免过拟合？ knowledge graph embedding将特定三元组嵌入到概念子空间中，首先构建一个超平面，其中法向量$c$为概念子空间： c=C\left(e_{h}, e_{t}\right)=\frac{e_{h}-e_{t}}{\left\|e_{h}-e_{t}\right\|_{2}^{2}}根据TransE三元组的嵌入损失为： l=h+r-t所以，可以计算出法向量方向上的损失分量是： \left(c^{T} l c\right)然后，投影到超平面上的另一个正交分量是： \left(l-c^{T} l c\right) 定义总损失函数： f_{r}(h, t)=-\lambda\left\|l-c^{T} l c\right\|_{2}^{2}+\|l\|_{2}^{2}Model interpretation 可以通过概念来辅助三元组识别，文中以(Christopher Plummer, /people/person/nationality, Canada)举例 可以解决在当两个候选实体在KGE，中计算loss相等时辨别这两个哪个是真实的。文中以“which the director made the film ‘‘WALL-E’’”为例来进行说明 都是通过查询实体对应概念来进行辅助 Objectives and trainingmargin-based loss function： L=\sum_{(h, r, t) \in S} \sum_{\left(h^{\prime}, r, t^{\prime}\right) \in S_{(h, r, t)}^{\prime}}\left[\gamma+f_{r}(h, t)-f_{r^{\prime}}\left(h^{\prime}, t^{\prime}\right)\right]_{+}train 先预训练概念图模型嵌入，获得在概念空间中的实体向量 利用1中获得的实体向量进行更新。 datasets WN18 and FB15K Microsoft Concept Graph 其中，relations表示频率 真的有统计频率的这种 ExperimentsKnowledge graph completionEntity classificationConcept relevance analysis 这个实验比较有意思：每个单元格中的数字表示在TransE中排名大于m且在我们的模型中小于n的三元组的数量。 Precise semantic expression analysis我们在链接预测（换句话说，这些是TransE的难以证明的例子）中收集那些得分略高于真实三元组作为负三元组 然后在KEC中对比两者的分数差值。 右边条表示KEC在TransE失败时作出正确决定，左边条表示KEC和TransE都失败。]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>KG</category>
      </categories>
      <tags>
        <tag>KGE</tag>
        <tag>ontology</tag>
        <tag>concept</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Universal Representation Learning of Knowledge Bases by Jointly Embedding Instances and Ontological Concepts阅读笔记]]></title>
    <url>%2Fpost%2FUniversal_Representation_Learning_of_Knowledge_Bases_by_Jointly_Embedding_Instances_and_Ontological_Concepts%2F</url>
    <content type="text"><![CDATA[ 论文下载地址 Problem StatementExisting KG embedding models merely focus on representing of an ontology view for abstract and commonsense concepts or an instance view for special entities that are instantiated from ontological concepts. Challenge mappings difficult :the semantic mappings from entities to concepts and from relations to meta-relations are complicated and difficult to be precisely captured by any current embedding models inadequate cross-view links: the known cross-view links inadequately cover a vast number of entities, which leads to insufficient information to align both views of the KB, and curtails discovering new cross-view links the scales and topological structures are different in ontological views and instance views Introduction 从两种视图来学习表示有以下两点好处： instance embeddings provide detailed and rich information for their corresponding ontological concepts. a concept embedding provides a high-level summary of its instances, which is extremely helpful when an instance is rarely observed. contribution a novel KG embedding model named JOIE, which jointly encodes both the ontology and instance views of a KB cross-view association model : a novel KG embedding model named JOIE, which jointly encodes both the ontology and instance views of a KB cross-view grouping technique : assumes that the two views can be forced into the same embedding space cross-view transformation technique : enables non-linear transformations from the instance embedding space to the ontology embedding space intra-view embedding model : characterizes the relational facts of ontology and instance views in two separate embedding spaces three state-of-the-art translational or similarity-based relational embedding techniques hierarchy-aware embedding: based on intra-view non- linear transformations to preserve ontologies hierarchical substructures. implement two experiments: the triple completion task : confirm the effectiveness of JOIE for populating knowledge in both ontology and instance-view KGs, which has significantly outperformed various baseline models. the entity typing task : show that JOIE is competent in discovering cross-view links to align the ontology-view and the instance-view KGs. Modeling Cross-view Association Model Cross-view Grouping (CG)该模型可以视为grouping-based regularization， 假设本体视图KG和实例视图KG可以被嵌入到同一空间中，并强制使实例向量靠近与它相关联的概念向量，如图3(a)所示 定义损失函数如下： J_{\text { Cross }}^{\mathrm{CG}}=\frac{1}{|\mathcal{S}|} \sum_{(e, c) \in \mathcal{S}}\left[\|\mathbf{c}-\mathbf{e}\|_{2}-\gamma^{\mathrm{CG}}\right]_{+}Cross-view Transformation (CT)试图在实体嵌入空间和概念空间之间转换信息，如图3(b)所示 定义映射函数，将实例映射到本体视图空间，该映射后向量应该靠近它的相关联概念： \mathbf{c} \leftarrow f_{\mathrm{CT}}(\mathbf{e}), \forall(e, c) \in \mathcal{S}其中， f_{\mathrm{CT}}(\mathbf{e})=\sigma\left(\mathbf{W}_{\mathrm{ct}} \cdot \mathbf{e}+\mathbf{b}_{\mathrm{ct}}\right)整个CT的损失函数为： J_{\text { Cross }}^{\mathrm{CT}}=\frac{1}{|\mathcal{S}|} \sum_{(e, c) \in \mathcal{S} \atop \wedge\left(e, c^{\prime}\right) \in \mathcal{S}}\left[\gamma^{\mathrm{CT}}+\left\|\mathbf{c}-f_{\mathrm{CT}}(\mathbf{e})\right\|_{2}-\left\|\mathbf{c}^{\prime}-f_{\mathrm{CT}}(\mathbf{e})\right\|_{2}\right]_{+}Intra-view Model该模型的目的：在两个嵌入空间中分别保留KB的每个视图中的原始结构信息。 Default Intra-view Model作者采用三种方式： \begin{aligned} f_{\text { TransE }}(\mathbf{h}, \mathbf{r}, \mathbf{t}) &=-\|\mathbf{h}+\mathbf{r}-\mathbf{t}\|_{2} \\ f_{\text { Mult }}(\mathbf{h}, \mathbf{r}, \mathbf{t}) &=(\mathbf{h} \circ \mathbf{t}) \cdot \mathbf{r} \\ f_{\text { HolE }}(\mathbf{h}, \mathbf{r}, \mathbf{t}) &=(\mathbf{h} \star \mathbf{t}) \cdot \mathbf{r} \end{aligned}损失函数： J_{\text { Intra }}^{G}=\frac{1}{|\mathcal{G}|} \sum_{(h, r, t) \in \mathcal{G}}\left[\gamma^{\mathcal{G}}+f\left(\mathbf{h}^{\prime}, \mathbf{r}, \mathbf{t}^{\prime}\right)-f(\mathbf{h}, \mathbf{r}, \mathbf{t})\right]_{+}intra损失函数： J_{\text { Intra }}=J_{\text { Intra }}^{\mathcal{G}_{I}}+\alpha_{1} \cdot J_{\text { Intra }}^{\mathcal{G}_{O}}Hierarchy-Aware Intra-view Model for the Ontology进一步区分了构成本体层次结构的元关系和视图内模型中的规则语义关系(如“related_to”)。 给定概念对（cl，ch），我们将这种层次结构建模为粗略概念和相关更精细概念之间的非线性变换: g_{\mathrm{HA}}\left(\mathbf{c}_{h}\right)=\sigma\left(\mathbf{W}_{\mathrm{HA}} \cdot \mathbf{c}_{l}+\mathbf{b}_{\mathrm{HA}}\right)损失函数为： J_{\text { Intra }}^{\mathrm{HA}}=\frac{1}{|\mathcal{T}|} \sum_{\left(c_{l}, c_{h}\right) \in \mathcal{T}}\left[\gamma^{\mathrm{HA}}+\left\|\mathbf{c}_{h}-g\left(\mathbf{c}_{l}\right)\right\|_{2}-\left\|\mathbf{c}_{\mathrm{h}}^{\prime}-g\left(\mathbf{c}_{1}\right)\right\|_{2}\right]_{+}故，该部分损失函数为： J_{\text { Intra }}=J_{\text { Intra }}^{G_{I}}+\alpha_{1} \cdot J_{\text { Intra }}^{\mathcal{G} o \backslash \mathcal{T}}+\alpha_{2} \cdot J_{\text { Intra }}^{\mathrm{HA}} $J_{\text { Intra }}^{\mathcal{G} o} \backslash \mathcal{T}$: 默认的视图内模型的丢失，该模型仅在具有规则语义关系的三元组上训练 $J_{\text { Intra }}^{\mathrm{HA}}$明确训练三元组与形成本体层次结构的元关系 感觉这部分就是传递关系，类似推理性质的。 没明白两种ontology关系的区分点 Joint Training on Two-View KBs联合损失函数： J=J_{\text { Intra }}+\omega \cdot J_{\text { Cross }}作者并不直接更新$J$，而是交替更新$J_{\text { Intra }}^{\mathcal{G}_{I}}, J_{\text { Intra }}^{\mathcal{G} O} \text { and } J_{\text { Cross }}$. EXPERIMENTS具体细节直接见论文 dataset数据集是作者自己构建的，信息如上图所示。 Case StudyOntology Population作者想预测在元关系词表中并不存在的元关系，例如：预测(“Office Holder”, ?r, “Country”) 这里，作者采取的方式是将概念通过之前提到的实体空间到概念空间的映射来进行反映射。然后按照$f_{\mathrm{CT}}^{\mathrm{inv}}\left(\mathbf{c}_{\text { country }}\right)-f_{\mathrm{CT}}^{\mathrm{inv}}\left(\mathbf{c}_{\text { office }}\right)$来在实体嵌入空间进行搜索与之相近的实体间关系。 Long-tail entity typing In KGs, the frequency of entities and relations often follow a long-tail distribution (Zipf’s law) 作者抽取了低频次实体进行了训练，发现JOIE模型的效果虽然有下降，但尚在可以接受的程度内。 FUTURE WORK Particularly, instead of optimizing structure loss with triples (first-order neighborhood) locally, we plan to adopt more complex embedding models which leverage information from higher order neighborhood, logic paths or even global knowledge graph structures. We also plan to explore the alignment on relations and meta-relations like entity-concept. exploring different triple encoding techniques Note that we are also aware of the fact that there are more comprehensive properties of relations and meta-relations in the two views such as logical rules of relations and entity types. Incorporating such properties into the learning process is left as future work. 思考这篇论文和之前跟张老师定的我的论文的思路基本一致，额，有点感觉有点受打击。这篇文章也是该作者博士毕业论文中的一部分，所以应该是这个作者早就有这个思路了，所以也没什么好纠结的。这篇文章也是走的trans的路线，和刘的论文又不一样的思路，但是都是概念本体这类的。其中有一点不一样，就是is_a关系可能两篇论文用的不一样。这篇论文中提到了数据集开源，可是github的链接中并没有数据集。虽然轮文中说他结合了概念和实例的视图，但是其实像刘的论文就已经提出结合了概念和实例的角度了。]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>KG</category>
      </categories>
      <tags>
        <tag>KGE</tag>
        <tag>ontology</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DocRED A Large-Scale Document-Level Relation Extraction Dataset阅读笔记]]></title>
    <url>%2Fpost%2FDocRED_A_Large-Scale_Document-Level_Relation_Extraction_Dataset%2F</url>
    <content type="text"><![CDATA[这是一个介绍数据集的论文，主要是文档级别的关系抽取数据集。 论文下载地址 Problem Statementexisting datasets for document-level RE either only have a small number of manually-annotated relations and entities, or exhibit noisy annotations from distant supervision, or serve specific domains or approaches. Contribution (DocRED) constructed from Wikipedia and Wikidata DocRED contains 132, 375 entities and 56, 354 relational facts annotated on 5, 053 Wikipedia documents As at least 40.7% of the relational facts in DocRED can only be extracted from multiple sentences also provide large-scale distantly supervised data to support weakly supervised RE research indicate the existing methods deal with the taks document level RE is more difficult sentence-level RE. data]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>RE</tag>
        <tag>dataset</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Learning Entity and Relation Embeddings for Knowledge Graph Completion阅读笔记]]></title>
    <url>%2Fpost%2FLearning_Entity_and_Relation_Embeddings_for_Knowledge_Graph_Completion%2F</url>
    <content type="text"><![CDATA[TransR embeds entities and relations in distinct entity space and relation space, and learns embeddings via translation between projected entities.CTransR models internal complicated correlations within each relation type. 论文下载地址 Problem StatementIn fact, an entity may have multiple aspects and various relaitons may focus on different aspects of entities, which makes a common space insurficient for modeling. Contribution propose a TransR model which models entities and relations in distinct spaces CTransR models internal complicated correlations within each relation type. experiment on benchmark datasets of WordNet and Freebase and gain consistent improvements compared to state-of-the-art models Future work Existing models including TransR consider each relational fact separately. relation transitive explore a unified embedding model of both text side and knowledge graph modeling internal correlations within each relation type TransR for each triple$(h, r, t)$, entities embeddings are set as $\mathbf{h}, \mathbf{t} \in \mathbb{R}^{k}$ and relation embedding is set as $\mathbf{r} \in \mathbb{R}^{d}$, $k \neq d$ for each relation $r$, set a projection matrix $\mathbf{M}_{r} \in\mathbb{R}^{k \times d}$ projects entities from entity space to relation space projected vectors of entities as \mathbf{h}_{r}=\mathbf{h} \mathbf{M}_{r}, \quad \mathbf{t}_{r}=\mathbf{t} \mathbf{M}_{r} score function: f_{r}(h, t)=\left\|\mathbf{h}_{r}+\mathbf{r}-\mathbf{t}_{r}\right\|_{2}^{2} Cluster-based TransR (CTransR)why propose CTransRTransE, TransH and TransR, learn a unique vector for each relation, which may be under-representative to fit all entity pairs under this relation, because these relations are usually rather diverse. basic idea incorporate the idea of piecewise linear regression Ritzema and others 1994 segment input instances into several groups process for a specific relation r, all entity pairs (h, t) in the training data are clustered into multiple groups, and entity pairs in each group are expected to exhibit similar r relation. All entity pairs (h, t) are represented with their vector offsets (h − t) for clustering, where h and t are obtained with TransE. learn a separate relation vector $r_c$for each cluster and matrix $M_r$ for each relation, respectively projected vectors of entities as $\mathbf{h}_{r, c}=\mathbf{h} \mathbf{M}_{r} \text { and } \mathbf{t}_{r, c}=\mathbf{t} \mathbf{M}_{r}$ sorce fuction f_{r}(h, t)=\left\|\mathbf{h}_{r, c}+\mathbf{r}_{c}-\mathbf{t}_{r, c}\right\|_{2}^{2}+\alpha\left\|\mathbf{r}_{c}-\mathbf{r}\right\|_{2}^{2}the later item aims to ensure cluster-specific relation vector rcnot too far away from the original relation vector r dataset采用和前人所用一样的数据集 Dataset #Rel #Ent #Train #Valid # Test WN18 18 40,943 141,442 5,000 5,000 FB15K 1,345 14,951 483,142 50,000 59,071 WN11 11 38,696 112,581 2,609 10,544 FB13 13 75,043 316,232 5,908 23,733 FB40K 1,336 39528 370,648 67,946 96,678 Experiment作者采取常规实验 Link Prediction这里作者对关系中聚类进行了展示： 我觉得这种方式是值得尝试的。 Triple classificationMoreover, the “bern” sampling technique improves the performance of TransE, TransH and TransR on all three data sets. bern采样方法需要掌握。 Relation Extraction from Text]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>KGE</tag>
        <tag>KGR</tag>
        <tag>TransR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neural Relation Extraction with Selective Attention over Instances阅读笔记]]></title>
    <url>%2Fpost%2FNeural_Relation_Extraction_with_Selective_Attention_over_Instances%2F</url>
    <content type="text"><![CDATA[这篇文章之前看过😂。 论下载地址 Problem Statement​ Distant supervision inevitably accompanies with the wrong labelling problem, and thse noisy data will substantially hurt the performance of relation extraction. Contribution As compared to existing neural relation extraction model, our model can make full use of all informative sentences of each entity pair. To address the wrong labelling problem in distant supervision, we propose selective attention to de-emphasize those noisy instances. In the experiments, we show that selective attention is beneficial to two kinds of CNN models in the task of relation extraction. Methodology模型整体架构如下所示：]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>RE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Learning as the Unsupervised Alignment of Conceptual Systems阅读笔记]]></title>
    <url>%2Fpost%2FLearning_as_the_Unsupervised_Alignment_of_Conceptual_Systems%2F</url>
    <content type="text"><![CDATA[这篇文章没怎么看懂，主要思想应该是代表同时概念的不同形式（文本，图像，语音等）应该具有相似的分布，以此来进行无监督的概念对齐。这种思路挺不错的，不过还没有深入的想法，算是拓展视野吧！ KEYThe key insight is that each concept has a unique signature within one conceptual system (e.g., images) that is recapitulated in other systems (e.g., text or audio) Problem Statement For supervised approaches, as the number of concepts grows, so does the number of required training examples V. W. Quine argued, even supervised instruction contains a substantial amount of ambiguity (Quine, 1960).Quine suggested that meaning may derive from something’s place within a conceptual system. ModelIn order to solve Quinne’s problem, we align a system of word labels and a system of visual semantics that both refer to the same underlying reality and therefore have related structure that can be discovered by unsupervised means (Figure 1）]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ERNIE Enhanced Language Representation with Informative Entities阅读笔记]]></title>
    <url>%2Fpost%2FERNIE_Enhanced_Language_Representation_with_Informative_Entities%2F</url>
    <content type="text"><![CDATA[该篇论文借鉴BERT，试图将实体信息（TransE）融入token(singal word)中，通过类似实体对齐的方法将实体与token对齐（并采取mask方式进行预训练），通过infromation fusion 将token与实体融合映射入相关联的两个向量空间。 论文下载地址 Problem Statementthe existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. ChallengeFor incorporating external knowledge into language representation models Structured Knowledge Encoding regarding to the given text, how to effectively extract and encode its related informative facts in KGs for language representation models Heterogeneous Information Fusion how to design a special pre-training objective to fuse the lexical, syntactic, and knowledge information is another challenge. Methodology Model ArchitectureERNIE the underlying textual encoder (T-Encoder)负责从文本中捕获基本的词法和语法信息 \left\{\boldsymbol{w}_{1}, \ldots, \boldsymbol{w}_{n}\right\}=\mathrm{T}-\operatorname{Encoder}\left(\left\{w_{1}, \ldots, w_{n}\right\}\right)T-Encoder(·) is a multi-layer bidirectional Transformer encoder the upper knowledgeable encoder (K-Encoder) entity embeddings are pre-trained by TransE负责将知识图谱集成到底层的文本信息中 Knowledgeable Encoder the knowledgeable encoder K-Encoder consists of stacked aggregators designed for encoding both tokens and entities as well as fusing their heterogeneous features. In the i-th aggregator the input: token embeddings: $\left\{\boldsymbol{w}_{1}^{(i-1)}, \ldots, \boldsymbol{w}_{n}^{(i-1)}\right\}$ entity embeddings :$\left\{\boldsymbol{e}_{1}^{(i-1)}, \ldots, \boldsymbol{e}_{m}^{(i-1)}\right\}$ fed into two multi-head self-attentions(MH-ATTs) $\left\{\tilde{\boldsymbol{w}}_{1}^{(i)}, \ldots, \tilde{\boldsymbol{w}}_{n}^{(i)}\right\}=\mathrm{MH}-\operatorname{ATT}\left(\left\{\boldsymbol{w}_{1}^{(i-1)}, \ldots, \boldsymbol{w}_{n}^{(i-1)}\right\}\right)$ $\left\{\tilde{\boldsymbol{e}}_{1}^{(i)}, \ldots, \tilde{\boldsymbol{e}}_{m}^{(i)}\right\}=\mathrm{MH}-\operatorname{ATT}\left(\left\{\boldsymbol{e}_{1}^{(i-1)}, \ldots, \boldsymbol{e}_{m}^{(i-1)}\right\}\right)$ an information fusion layer \begin{aligned} \boldsymbol{h}_{j} &=\sigma\left(\tilde{\boldsymbol{W}}_{t}^{(i)} \tilde{\boldsymbol{w}}_{j}^{(i)}+\tilde{\boldsymbol{W}}_{e}^{(i)} \tilde{\boldsymbol{e}}_{k}^{(i)}+\tilde{\boldsymbol{b}}^{(i)}\right) \\ \boldsymbol{w}_{j}^{(i)} &=\sigma\left(\boldsymbol{W}_{t}^{(i)} \boldsymbol{h}_{j}+\boldsymbol{b}_{t}^{(i)}\right) \\ \boldsymbol{e}_{k}^{(i)} &=\sigma\left(\boldsymbol{W}_{e}^{(i)} \boldsymbol{h}_{j}+\boldsymbol{b}_{e}^{(i)}\right) \end{aligned} $h_j$ is the inner hidden state For the tokens without corresponding entities \begin{aligned} \boldsymbol{h}_{j} &=\sigma\left(\tilde{\boldsymbol{W}}_{t}^{(i)} \tilde{\boldsymbol{w}}_{j}^{(i)}+\tilde{\boldsymbol{b}}^{(i)}\right) \\ \boldsymbol{w}_{j}^{(i)} &=\sigma\left(\boldsymbol{W}_{t}^{(i)} \boldsymbol{h}_{j}+\boldsymbol{b}_{t}^{(i)}\right) \end{aligned}Pre-training for Injecting KnowledgeIn order to inject knowledge into language rep- resentation by informative entities. Randomly masks some token-entity alignments and then requires the system to predict all corresponding entities based on aligned tokens. denoising entity auto-encoder (dEA) define the aligned entity distribution for the token $w_i$ as follows: p\left(e_{j} | w_{i}\right)=\frac{\exp \left(\text { linear }\left(\boldsymbol{w}_{i}^{o}\right) \cdot \boldsymbol{e}_{j}\right)}{\sum_{k=1}^{m} \exp \left(\text { linear }\left(\boldsymbol{w}_{i}^{o}\right) \cdot \boldsymbol{e}_{k}\right)} linear(·) is a linear layer For dEA, perform the following operations: in 5% of the time, replace the entity with another random aims to train model to correct the errors that the token is aligned with a wrong entity; In 15% of the time, mask token-entity alignments aims to train model to correct the errors that entity alignment system doesn’t extract all existing alignments; in the rest of the time, keep token-entity alignments unchanged aims to encourage our model to integrate the entity information into token representations for better language understanding. Fine-tuning for Specific Tasks We can take the final output embedding for the first token, which corresponds to the special [CLS] token, as the representation of the input sequence for specific tasks. For some knowledge-driven tasks, we design special fine-tuning procedure: relation classification design different tokens [HD] and [TL] for head entities and tail entities respectively a similar role like position embeddings in the conventional relation classification models (Zeng et al., 2015) entity typing the mention mark token [ENT] 这里的CLS不知道有什么作用，所有的任务都有，是不同的任务重CLS的embedding有所不同吗？个人目前觉得是这样的。 作者这里采用的mark token的方法代替position embedding，不知道两个对比那种效果会更好一些。直观觉得都是标记位置信息。 ExperimentsPre-training Dataset we use English Wikipedia as our pre-training corpus and align text to Wiki-data 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities before pre-training ERINE, entity embeddings by TransE sample part of Wikidata which contains 5, 040, 986 entities and 24, 267, 796 fact triples Training Details We also fine-tune ERNIE on the distant supervised dataset, i.e., FIGER (Ling et al., 2015) we use TAGME (Ferragina and Scaiella, 2010) to extract the entity mentions in the sentences and link them to their corresponding entities in KGs Entity Typingdatasettwo well-established datasets FIGER (Ling et al., 2015) and Open Entity (Choi et al., 2018). The training set of FIGER is labeled with distant supervision, and its test set is annotated by human. Open Entity is a completely manually-annotated dataset. Comparble model NFGEC NFGEC is a hybrid model proposed by Shimaoka et al. (2016) UFET (Choi et al., 2018) The results on FIGER:However, BERT has lower accuracy than the best NFGEC model. As strict accuracy is the ratio of instances whose predictions are identical to human annotations, it illustrates some wrong labels from distant supervision are learned by BERT due to its powerful fitting ability. Relation Classificationdatasettwo well-established datasets FewRel (Han et al., 2018b) and TACRED (Zhang et al., 2017). FewRel As FewRel does not have any null instance where there isn’t any relation between entities, we adopt macro averaged metrics to present the model performances. Since FewRel is built by checking whether the sentences contain facts in Wiki-data, we drop the related facts in KGs before pretraining for fair comparison TACRED In TACRED, there are nearly 80% null instances so that we follow the previous work (Zhang et al., 2017) to adopt micro averaged metrics to represent the model performances instead of the macro Comparble model CNN:(Zeng et al., 2015). PA-LSTM C-GCN :Zhang et al. (2018) adopt the graph convolution operations to model dependency trees for relation classification. GLUEThe General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is a collection of diverse natural language understanding tasks Ablation Studyexplore the effects of the informative entities and the knowledgeable pretraining task (dEA) for ERNIE using FewRel dataset 实验部分做的很丰富，既有两个任务的对比实验，也有对自身模块的对比实验，并且还对比了bert来检测自己模型是否对GLUE任务效果有降低。 future research1) inject knowledge into feature-based pre-training models such as ELMo (Peters et al., 2018); (2) introduce diverse structured knowledge into language representation models such as ConceptNet (Speer and Havasi, 2012) which is different from world knowledge database Wikidata; (3) annotate more real-world corpora heuristically for larger pre-training data 参考链接 https://blog.csdn.net/summerhmh/article/details/91042273]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>KG</category>
      </categories>
      <tags>
        <tag>KGR</tag>
        <tag>BERT</tag>
        <tag>KG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Incorporating Literals into Knowledge Graph Embeddings阅读笔记]]></title>
    <url>%2Fpost%2FIncorporating_Literals_into_Knowledge_Graph_Embeddings%2F</url>
    <content type="text"><![CDATA[读完了前两章，简单的看了一下作者提出的模型，感觉并没有太大价值，就是给实体输入多加入了一个literal的信息（加入方法可以采用线性、非线性或者神经网络）。 读论文前需要先熟悉DistMult、ComlLEx和ConvE模型，此论文方法是添加在这些方法上的。]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>KG</category>
      </categories>
      <tags>
        <tag>KGE</tag>
        <tag>link prediction</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Learning Knowledge Embeddings by Combining Limit-based Scoring Loss阅读笔记]]></title>
    <url>%2Fpost%2FLearning_Knowledge_Embeddings_by_Combining_Limit-based_Scoring_Loss%2F</url>
    <content type="text"><![CDATA[此篇文章最为重要的就是作者设计的 margin-based ranking loss 的改进，对两个超参数$\lambda$和$\gamma$的实验，对于实验结果有很多值得分析与思考的地方。 论文下载地址 Problem StatementThe margin-based ranking loss function cannot ensure the fact that the scoring of correct triplets must be low enough to fulfill the translation. research objectivereduce the scoring of correct triplets to fulfill the translation by mending the margin-based ranking loss function Contributions proposing a limit-based ranking loss item combined with margin-based ranking loss extending TransE and TransH to TransE-RS and TransH-RS ModelMargin-based Tanking Lossformula: L_{R}=\sum_{(h, r, t) \in \Delta} \sum_{\left(h^{\prime}, r, t^{\prime}\right) \in \Delta^{\prime}}\left[\gamma_{1}+f_{r}(h, t)-f_{r}\left(h^{\prime}, t^{\prime}\right)\right) ]_{+} The margin-based ranking loss function aims to make the score $f_{r}\left(h^{\prime}, t^{\prime}\right)$ of corrupted triplet higher by at least $\gamma_{1}$ than of positive triplet. cannot be proved $f_{r}(h, t)&lt;\varepsilon$ Limit-based Scoring Lossformula: L_{S}=\sum_{(h, r, t) \in \Delta}\left[f_{r}(h, t)-\gamma_{2}\right]_{+}Finally lossformula: L_{R S}=L_{R}+\lambda L_{S}, \quad(\lambda>0)detail is : \begin{array}{c}{L_{R S}=\sum_{(h, r, t) \in \Delta} \sum_{\left(h^{\prime}, r, t^{\prime}\right) \in \Delta^{\prime}}\left\{\left[\gamma_{1}+f_{r}(h, t)-f_{r}\left(h^{\prime}, t^{\prime}\right)\right]_{+}\right.} \\ {+\lambda\left[f_{r}(h, t)-\gamma_{2}\right]_{+} \}}\end{array}Experimentsdataset Link prediction 思考 作者只是对表格的数据进行了陈述，有一些问题并没有进行分析解释 并没有分析比如说为什么改进loss后的transE为什么会比TransH（R、D）效果要好？ 为什么在n-to-1中的表现效果没有达到最好（其他的都达到了最好）？ 通过这种改进可以发现，transH相比于TransE并没有显著提升，原因是什么？ Triple Classification TransE-RS and TransH-RS have same parameter and operation complexities as TransE and TransH, which is lower than TransR and TransD. Our models randomly initial the entities, not use the learned embeddings by TransE as TransR and TransD. It means that our models have much better ability to overcome the problem of overfitting Distributions of Triplets’ Scoresaimanalyze the difference between $L_R$ Loss and our $L_RS$ Loss Parameters 思考： 对于我自己正在做的实验：是不是我自己用的间隔太小了 result 思考 这部分的实验值得借鉴，它可以相对于直观的可以展示出为什么效果会好。 比如对于上述为什么改进后的transE的效果会更好 看到最后的分数分布transE-RS的分布效果和Trans-H的十分接近， 而transE的模型较为简单，可能最终loss最小化会使得模型充分表达，而其他模型引入了更多的假设可能会带来更多的噪声 也可能当loss很小时，其他的假设条件发挥作用的很小（至少从实验结果来看是的，但是还有待于进一步设计实验验证） Discussion of ParametersDiscussion on γ1 and γ2. We find that γ2 = 3γ1 or γ2 = 4γ1 is better for link prediction, but for triplet classification there are not obvious characteristics on γ1 and γ2. a lower γ2 is expected to ensure the golden condition $\mathbf{h}+\mathbf{r} \approx \mathbf{t}$ for positive triplets, but an entity needs to satisfy many golden coditions at the same time. 思考 既然如作者说，那么理论上transH的效果应该很好才对，但是结果并不是这样的，这又产生矛盾。 Discussion on λ 思考 看到λ并没有对模型影响并没很大 λ在1左右是效果会比较好 λ和margin会不会产生关联？]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>KG</category>
      </categories>
      <tags>
        <tag>KG</tag>
        <tag>transH</tag>
        <tag>margin loss</tag>
        <tag>transE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Knowledge Graph Embedding by Translating on Hyperplanes阅读笔记]]></title>
    <url>%2Fpost%2FKnowledge%20Graph%20Embedding%20by%20Translating%20on%20Hyperplanes%2F</url>
    <content type="text"><![CDATA[作为trans系列经典文献，必读。文章主要精华在于这种超平面想法的由来解决了同一实体的多关系问题。 Authors proposed TransH which models a relation as a hyperplane together with a translation operation on it. It solves the problem of multi-relation and makes a good trade-off between model capacity and efficiency. 推测transH的想法来源 既然实际是表达同一关系不同实体最后通过TransE后会趋于一致，那么我直接通过一个中介来进行映射将同一表示映射成不同向量表示，那么这些向量表示就可以代表不同的实体，就达到了不同实体拥有不同表示的目的。因为关系是不变的所以想到了将关系作为映射平面，让实体向量向其中映射。 research objective solves the problem of multi-relation makes a good trade-off between model capacity and efficiency Problem Statement TransE can’t deal with reflexive, one-to-many, many-to-many and many -to-one relations some complex model sacrifice efficiency in the process(although can deal with transE’s problem) Contribution proposing a method named translation on hyperplanes(TransH) interpreting a relation as a translating operation on a hyperplane proposing a simple trick to reduce the chance of false negative labeling Embedding by Translating on HyperplanesRelations’ Mapping Properties in EmbeddingtransE the representation of an entity is the same when involved in any relations, ignoring distributed representations of entities when invovled in different relaions Translating on Hyperplanes (TransH)同一个实体在不同关系中的意义不同，同时不同实体，在同一关系中的意义，也可以相同。 将每个关系定义在一个独特的平面呢，在该平面内有符合该关系的transE的表示（h,r,t)，多加入的代表该平面的法向量完成了将不同实体向平面内和h，t转化的任务，使得同一关系的不同实体拥有不同的表示，但是在关系平面内的投影相同；同一实体可以在不同的关系平面内拥有不同的含义（平面内的投影） 如图所示，对于正确的三元组来说$(h, r, t) \in \Delta$，所需满足的关系如图所示。那么对于一个实体$h’’$如果满足$\left(h^{\prime \prime}, r, t\right) \in \Delta $，在transE中是需要$h’’=h$，而在transH中则将约束放宽到$h,h’’$在$W_r$上的投影相同就可以了，也可以实现将$h,h’’$区分开并且具有不同的表示。 目标函数scoring function： d(h+r, t)=f_{r}(h, t)=\left\|h_{\perp}+d_{r}-t_{\perp}\right\|_{2}^{2}As the hyperplane $W_r$, the $w_r$ is the normal vector of it, and $\left|w_{r}\right|_{2}^{2}=1$, so the projection $h$ in $w_r$ is: h_{w_{r}}=w_r^{T} h w_r其中，$w_r^{T} h=|w_r||h| \cos \theta$可以表示$h$在$w_r$上的投影的长度和$w_r$长度的乘积，因为$\left|w_{r}\right|_{2}^{2}=1$,所以可以代表投影的长度，再乘上单位向量即可表示投影向量。所以： \mathbf{h}_{\perp}=\mathbf{h}-\mathbf{w}_{r}^{\top} \mathbf{h w}_{r}, \quad \mathbf{t}_{\perp}=\mathbf{t}-\mathbf{w}_{r}^{\top} \mathbf{t} \mathbf{w}_{r}如图所示： the score function is: f_{r}(\mathbf{h}, \mathbf{t})=\left\|\left(\mathbf{h}-\mathbf{w}_{r}^{\top} \mathbf{h w}_{r}\right)+\mathbf{d}_{r}-\left(\mathbf{t}-\mathbf{w}_{r}^{\top} \mathbf{t} \mathbf{w}_{r}\right)\right\|_{2}^{2}Trainingloss function consists of margin-based ranking loss and some constraints: \begin{aligned} \mathcal{L} &=\sum_{(h, r, t) \in \Delta\left(h^{\prime}, r^{\prime}, t^{\prime}\right) \in \Delta_{(h, r, t)}}\left[f_{r}(\mathbf{h}, \mathbf{t})+\gamma-f_{r^{\prime}}\left(\mathbf{h}^{\prime}, \mathbf{t}^{\prime}\right)\right]_{+} \\ &+C\left\{\sum_{e \in E}\left[\|\mathbf{e}\|_{2}^{2}-1\right]_{+}+\sum_{r \in R}\left[\frac{\left(\mathbf{w}_{r}^{\top} \mathbf{d}_{r}\right)^{2}}{\left\|\mathbf{d}_{r}\right\|_{2}^{2}}-\epsilon^{2}\right]_{+}\right\}, \text { (4) } \end{aligned}the constraints: \forall e \in E,\|\mathrm{e}\|_{2} \leq 1, // \text { scale }\\ \forall r \in R,\left|\mathbf{w}_{r}^{\top} \mathbf{d}_{r}\right| /\left\|\mathbf{d}_{r}\right\|_{2} \leq \epsilon, / / \text { orthogonal }\\ \forall r \in R,\left\|\mathbf{w}_{r}\right\|_{2}=1, / / \text { unit normal vector } the second grantees the translation vectot $d_r$ is in the hyperplane they project each $w_r$ to unit $l_2$-ball before visiting each mini-batch 既然transH可以完成将同一实体映射到不同的关系平面来获得不同的含义，那么我觉得 是不是不同代表同一含义的投影表示应该相同或者相似 这样是不是可以解决同一个实体的多义性问题。 Reducing Ralse Negative LabelsAuthors set different probabilities for replacing the head or tail entity depending on the mapping property of the relation (one-to-many, many-to-one, many-to-many) give more chance to replacing the head entity if the relation is one-to-many 分别统计每个头实体对应尾实体的数量（反之亦然），按占比进行生成负样例 通过这样的方式，例如one-many关系，替换头实体显然更不容易得到正样例（因为只有一种头实体是对的，然而替换尾实体因为对于头实体对应该关系的尾实体更多，说不定就有其他不在此many中的尾实体符合这个关系。 相比之下我认为在《Bootstrapping-Entity-Alignment-with-Knowledge-Graph-Embedding》采用的均匀截断负采样效果会更好一些 Experimentsthe detail can be seen in the paper Link predictionoutperform TransE in one-to-oneAuthors explain: entities are connected with relations so that better embeddings of some parts lead to better results on the whole. 我是觉得有些牵强，不过要是硬理解也是可以，毕竟通过投影相当于把实体和关系进行了一个联系，可能这个增强了效果。 Triplets ClassificationThis means FB13 is a very dense subgraph where strong correlations exist between entities Relational Fact Extraction from Text Actually, knowledge graph embedding is able to score a candidate fact, without observing any evidence from ex- ternal text corpus 可以看到从14年开始就有利用知识图谱来从文本抽取关系，最近这个应用好像又有起色，这个也可作为自己实验的一部分。 Reference https://blog.csdn.net/MonkeyDSummer/article/details/85273843]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>KG</category>
      </categories>
      <tags>
        <tag>KGE</tag>
        <tag>transH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Attention Is All You Need阅读笔记]]></title>
    <url>%2Fpost%2FAttention%20Is%20All%20You%20Need%2F</url>
    <content type="text"><![CDATA[transformer 是一个完全由注意力机制组成的搭建的模型，模型复杂度低，并可以进行并行计算，使得计算速度快。在翻译模型上取得了较好的效果。本篇论文属于经典必读论文，阅读笔记中对一些不清楚的地方进行了汉语解释，读完论文后阅读参考链接以加深理解。 论文下载地址 research objectivebased solely on attention mechanisms, increase parallezable computation and decrease train time Problem Statementrecurrent models hidden states depended on previous hidden state and the input for position precludes parallelization contribution Transformer, eschewing recurrence and instead relying entirely on an attention mechanism, solve the long dependency problem. draw global dependecies between input and output allow for significantly more parallelization Model ArchitectureThe Transformer uses stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. Encoder and Decoder StacksEncoder compose of a stack of N identical layers each layers has two sub-layers multi-head self-attention mechanism position-wise fully connected feed forward network employ a residual connection around each of the two sub-layers, followed by layer normalization the output of each sub-layer is $\text { LayerNorm }(x+\text { Sublayer }(x))$ encoder中的Q，K，V都是学出来的 Decoder composed of a stack of N identical layers has the same two sub-layers as the encoder the third sub-layer between the two sub-layers perform multi-head attention over the output of the encoder stack add a mask to modify the self-attention sub-layer to ensure that the predictions for position $i$ can depend only the known outputs at positions less than $i$ 除了第一子层中Q，K，V是自己学出来的，第二个子层利用了encoder中的K，V。 Attention Scaled Dot-Product Attentionthe calculation process as the left at the figure 2. formula： \text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V where $\sqrt{d_{k}}$ is to prevent value from getting too large, which will push the softmax function into regions where it has extremely small gradients. 因为量级太大，softmax后就非0即1了，不够“soft”了。也会导致softmax的梯度非常小。也就是让softmax结果不稀疏(问号脸，通常人们希望得到更稀疏的attention吧)。 $Q, K,V$ is a matrix needed to learn from input. Multi-Head Attentionhelps the encoder look at other words in the input sentence as it encodes a specific word in the figure 2 right. it’s beneficial to lineraly project the quries, keys and values $h$ times with different, learned projections to $d_k, d_k, d_v$ dimensions, respectively concatenate the output \begin{aligned} \text { MultiHead }(Q, K, V) &=\text { Concat (head }_{1}, \ldots, \text { head }_{h} ) W^{O} \\ \text { where head }_{i} &=\text { Attention }\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right) \end{aligned}where $W_{i}^{Q} \in \mathbb{R}^{d_{\text { model }} \times d_{k}}, W_{i}^{K} \in \mathbb{R}^{d_{\text { model }} \times d_{k}}, W_{i}^{V} \in \mathbb{R}^{d_{\text { model }} \times d_{v}}, W^{O} \in \mathbb{R}^{h d_{v} \times d_{\mathrm{model}}}$ Applications of Attention in our Model the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. mimicing the seq-to-seq self -attention can make that each position in the encoder can attend to all positions in the previous layer of the encoder We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. See Figure 2。即我们只能attend到前面已经翻译过的输出的词语，因为翻译过程我们当前还并不知道下一个输出词语，这是我们之后才会推测到的。即将$QK^T$中每行该单词之后的数值做处理，使得前面的单词看不到后面单词所占的重要性程度。 Position-wise Feed-Forward Networks applied to each position separately and identically feed-forward network consists of tow linear transformations with a ReLU activation. formula: \mathrm{FFN}(x)=\max \left(0, x W_{1}+b_{1}\right) W_{2}+b_{2} 小结 为什么叫强调position-wise? 解释一: 这里FFN层是每个position进行相同且独立的操作，所以叫position-wise。对每个position独立做FFN。 解释二：从卷积的角度解释，这里的FFN等价于kernel_size=1的卷积，这样每个position都是独立运算的。如果kernel_size=2，或者其他，position之间就具有依赖性了，貌似就不能叫做position-wise了 为什么要采用全连接层？ 目的: 增加非线性变换 如果不采用FFN呢？有什么替代的设计？ 为什么采用2层全连接，而且中间升维？ 这也是所谓的bottle neck，只不过低维在IO上，中间采用high rank Embeddings and SoftmaxSharing the same weight maatrix between the two embedding layers and the pre-softmax linear transformation Positional EncodingUsing sine and xosine functions of different frequencies: P E_{(p o s, 2 i)}=\sin \left(p o s / 10000^{2 i / d_{\text { model }}}\right) \\ P E_{(p o s, 2 i+1)}=\cos \left(p o s / 10000^{2 i / d_{\mathrm{model}}}\right) where $pos$ is the postiiton and $i$ is the dimension Authors hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$, $PE_{pos+k}$can be represented as a linear function of $PE_{pos}$ 但在语言中，相对位置也很重要，Google选择前述的位置向量公式的一个重要原因是：由于我们有$\sin (\alpha+\beta)=\sin \alpha \cos \beta+\cos \alpha \sin \beta$以及$\cos (\alpha+\beta)=\cos \alpha \cos \beta-\sin \alpha \sin \beta$，这表明位置$p+k$的向量可以表示成位置$p$的向量的线性变换，这提供了表达相对位置信息的可能性。 Compared with using learned positional embeddings, the sinusoidal version may allow the model to extrapolate to sequence lengths longer than the ones encountered during training. 注意由于该模型没有recurrence或convolution操作，所以没有明确的关于单词在源句子中位置的相对或绝对的信息，为了更好的让模型学习位置信息，所以添加了position encoding并将其叠加在word embedding上。 Why Self-Attention total computational complexity per layer the amount of computation that can be parallelized the path between long-range dependencies in the network self-attention|： $QK^TV$相乘，根据矩阵大小（分别为$nd, nd, nd$需要的复杂度为$O(n^2d2)$（忽略softmax） maximum path length：图说明了， 对于self-attention, target node (生成的那个点) 实际上和 输入中的任意一点的距离是相同的 convolutional: 每层有k个卷积和，对于input matix（$nd$)矩阵执行卷积需要运算复杂度是$nd*(d-m)$, m为卷积和宽度是一个比较小的常数，所以总复杂度为$O\left(k \cdot n \cdot d^{2}\right)$,作者提到可分离的卷基层暂时还不了解，可以以后查阅。 maximum path length: 正常卷积和的距离是$O(n/k)$, 但如果是堆叠卷积如图： 就可以减小到$O\left(\log _{k}(n)\right)$ recurrent: 计算是每个词向量乘隐藏权重($d*d$)，所以易得计算复杂度：$O\left(n \cdot d^{2}\right)$ maximum path length: 长度就是n。 操作步骤要从第一个到第n个为n步，是有顺序的。其他的都没有顺序要求 self-attentin(restricted) 相当于只输入r邻近的句子长度，自然可以得到如图结果 TrainOptimizer \text { lrate }=d_{\text { model }}^{-0.5} \cdot \min \left(\text {step}_{-} n u m^{-0.5}, \text { step }_{-} n u m \cdot \text { warmup steps }^{-1.5}\right) increasing the learning rate linearly for the first warmup_steps training steps decreasing it thereafter proportionally to the inverse square root of the step number RegularizationResidual Dropout apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks Label Smoothing This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score Resultmachine TranslationEven our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models Model Variations 缺点缺点在原文中没有提到，是后来在Universal Transformers中指出的，在这里加一下吧，主要是两点： 实践上：有些rnn轻易可以解决的问题transformer没做到，比如复制string，尤其是碰到比训练时的sequence更长的时 理论上：transformers非computationally universal（图灵完备），（我认为）因为无法实现“while”循环 总结Transformer是第一个用纯attention搭建的模型，不仅计算速度更快，在翻译任务上也获得了更好的结果。 Google现在的翻译应该是在此基础上做的，但是数据量大可能用transformer好一些，小的话还是继续用rnn-based model。 花了不少时间，算是理解了attention和transformer，对其中不是很清楚的点如attention的内部中Q，K，V具体是什么在self-attention和multi-head attention中大小是不同的，如何mask，如何计算复杂，等进行查阅资料弄懂了。总体来说还是收获很大的。准备在看一些代码讲解。 reference Attention机制详解（二）——Self-Attention与Transformer - 川陀学者的文章 - 知乎https://zhuanlan.zhihu.com/p/47282410 https://jalammar.github.io/illustrated-transformer/（这个讲的比较详细，建议看完论文后再看一遍这个会加深理解） 【NLP】Transformer详解 - 李如的文章 - 知乎https://zhuanlan.zhihu.com/p/44121378 https://blog.eson.org/pub/664e9bad/ https://mp.weixin.qq.com/s/J-anyCuwLd5UYjTsUFNT1g]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>classical</category>
      </categories>
      <tags>
        <tag>attention</tag>
        <tag>transformer</tag>
        <tag>translation</tag>
        <tag>classical</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Graph Neural Networks with Generated Parameters for Relation Extraction阅读笔记]]></title>
    <url>%2Fpost%2FGraph_Neural_Networks_with_Generated_Parameters_for_Relation%2F</url>
    <content type="text"><![CDATA[本文将GNNs应用到处理非结构化文本的（多跳）关系推理任务来进行关系抽取。采用从句子序列中获取的实体构建全链接图，应用编码（sequence model），传播（节点间信息）和分类（预测）三个模块来处理关系推理。本文提供了三个数据集。 problem statement existing relation extraction models fail to infer the relationship without multi-hop relational reasoning. existing GNNs can’t process multi-hop relational reasoning in natural language relational reasoning research objectiveenable GNNs to porcess relational reasoning on unstructed text inputs contribution extend a GNN with generated parameters, which could be applied to process relational reasoning on unstructured inputs verify GP-GNNs in the taks of relation extraction from text; present three datasets GP-GNNs construct a fully-connected graph with the entities in the sequence of text employs three models to process relational reasoning an encoding modul: enable edges to encode rich information from natural languages a propagation modul: propagates realtional information among various nodes a classification modul: make prediction with node representations As compared to tradtional GNNs, GP-GNNs could learn edges’ parameters from natural lanuages Related workGraph Neural Networks(GNNs) existing models still perfom message-passing on predefined graphs Learning Graphical State Transitions is most related introduecs a nove lnerual architecture to generate a graph based on the textal input dynamically update the relationship during the learning process relational reasoning existing models could not make full use of the multi-hop inference patterns among multiple entity pair and their relaitons within the sentence LEARNING GRAPHICAL STATE TRANSITIONS is the most related work the proposed model incorporates contextual relations with attention mechanism when predicting the relation of a target entity pair Graph Neural Network with Grenerated Parameters(GP-GNNs)The picture is overall architecture: encoding module, propagation module and classification module Encoding Moduleformula: \mathcal{A}_{i, j}^{(n)}=f\left(E\left(x_{0}^{i, j}\right), E\left(x_{1}^{i, j}\right), \cdots, E\left(x_{l-1}^{i, j}\right) ; \theta_{e}^{n}\right)where $f(\cdot)$ could be any model that could sequential(such as LSTMs); $E(\cdot)$ indicates an embedding function. $x^{i, j}$ is the word in sentence labeled( $i,j$) Porpagation Modulethe representations of layer n + 1 are calculated by: \mathbf{h}_{i}^{(n+1)}=\sum_{v_{j} \in \mathcal{N}\left(v_{i}\right)} \sigma\left(\mathcal{A}_{i, j}^{(n)} \mathbf{h}_{j}^{(n)}\right)where $\mathcal{N}\left(v_{i}\right)$ denotes the neighbors of node $v_i$ Classification Modulethe loss of GP-GNNs: \mathcal{L}=g\left(\mathbf{h}_{0 :|\mathcal{V}|-1}^{0}, \mathbf{h}_{0 :|\mathcal{V}|-1}^{1}, \ldots, \mathbf{h}_{0 :|\mathcal{V}|-1}^{K}, Y ; \theta_{c}\right)Relation Extraction with GP-GNNsAuthors introduce how to apply GP-GNNs to relation extraction Encoding Moduleencoding then context of entity pairs (or edges in the graph) E\left(x_{t}^{i, j}\right)=\left[\boldsymbol{x}_{t} ; \boldsymbol{p}_{t}^{i, j}\right]where $x_t$ denotes the word embedding; $\boldsymbol{p}_{t}^{i, j}$denotes the position embedding of word posistion t relative to the entity pair’s position $i, j$. position embeddingwe mark each token in the sentence as either belonging to the first entity $v_i$, the second entity $v_j$ or to neither of those Propagation Module the formula is the same as the front The Initial Embeddings of Nodes when extracting the relationship between entity $v_i$ and entity $v_j$, the initial embeddings of them are annotated as $\mathbf{h}_{v_{i}}^{(0)}=a_{\text { subject }}$, and $h_{v_{j}}^{(0)}=a_{\text { object }}$, while the intial embeddings of other entities are set to all zeros. In our experiments, we generalize the idea of Gated Graph Neural Networks (Li et al., 2016) by setting $a_{\text { subject }}=[1 ; 0]^{\top}$and $a_{\text { object }}=[0 ; 1]^{\top}$. classification ModuleAs the target entity pair $(v_i, v_j)$: \boldsymbol{r}_{v_{i}, v_{j}}=\left[\left[\boldsymbol{h}_{v_{i}}^{(1)} \odot \boldsymbol{h}_{v_{j}}^{(1)}\right]^{\top} ;\left[\boldsymbol{h}_{v_{i}}^{(2)} \odot \boldsymbol{h}_{v_{j}}^{(2)}\right]^{\top} ; \ldots ;\left[\boldsymbol{h}_{v_{i}}^{(K)} \odot \boldsymbol{h}_{v_{j}}^{(K)}\right]^{\top}\right]where $\odot$ represents element-wise multiplication classification: \mathbb{P}\left(r_{v_{i}, v_{j}} | h, t, s\right)=\operatorname{softmax}\left(M L P\left(\boldsymbol{r}_{v_{i}, v_{j}}\right)\right)loss: \mathcal{L}=\sum_{s \in S} \sum_{i \neq j} \log \mathbb{P}\left(r_{v_{i}, v_{j}} | i, j, s\right)Experimentsaim showing their best models could improve the performance of relation extraction under a variety of settings illlustrating that how the number of layers affect the performance of their model performing a qualitiative investigation to highlight the diference between their models and baseline models designas the first and second aim show that our models could improve instance-level relation extraction on a human annotated test set we will show that our models could also help enhance the performance of bag-level relation extraction on a distantly labeled test set split a subset of distantly labeled test set, where the number of entities and edges is large Datasetdistantly label set Sorokin and Gurevych (2017) proposed modify their dataset added reversed edges for all of the entity pairs with no relations, added “NA” labels to them Human annotated test set Sorokin and Gurevych (2017) select the distantly lablel pairs which all 5 annotaters are accepted. There are 350 sentences and 1,230 triples in this test set Dense distantly labeled test set criteria the number of entities should be strictly larger than 2 there must be at least one circle (with at least three entities) in the ground-truth label of the sentence There are 1,350 sentences and more than 17,915 triples and 7,906 relational facts in this test set. Models for comparison Context-aware RE Multi-Window CNN PCNN LSTM or GP-GNN with K = 1 layer GP-GNN with K = 2 or K = 3 layerss Evaluation DetailsTo evaluation models in bag-level: E\left(r | v_{i}, v_{j}, S\right)=\max _{s \in S} \mathbb{P}\left(r_{v_{i}, v_{j}} | i, j, s\right)result: Effectiveness of Reasoning Mechanism Context-Aware RE may introduce more noise, for it may mistakenly increase the probability of a relation with the similar topic with the context relations sentences from Wikipedia corpus are always complex, which may be hard to model for CNN and PCNN The Effectiveness of the Number of Layers the improvement of the third layer is much smaller on the overall distantly supervised test set than the one on the dense subset This observation reveals that the reasoning mechanism could help us identify relations especially on sentences where there are more entities as the number of layers grows, the curves get higher and higher precision, indicating considering more hops in reasoning leads to better performance Qualitative Results: Case Study Context-Aware RE makes a mistake by predicting (Kentucky, share boarder with, Ohio). As we have discussed before, this is due to its mechanism to model co-occurrence of multiple relations 思考文章是刘知远组的论文，针对的方向是关系抽取，在其中结合了关系推理，最近许多任务都在结合推理的思想。文章整体的结构，逻辑十分清晰，论述的也比较详细，属于标准论文。感觉文章中GP-GNNs结构图还可以画的更好一点，展现一下encoding module的层，可以更好理解。文章的精髓应该是这个propagation module的部分，还需要消化一下，不过这部分可能是有先前的知识支撑的。]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>GNNs</tag>
        <tag>relation extraction</tag>
        <tag>relation reasoning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[allennlp安装踩坑]]></title>
    <url>%2Fpost%2Fallennlp_install%2F</url>
    <content type="text"><![CDATA[安装allennlp的踩坑之路，踩了不少坑最后选择’Installing from source’的安装方法，排坑后下面方法亲测可用 Installing from source安装步骤： 1.下载GitHub文件git clone https://github.com/allenai/allennlp.git 2.创建conda环境conda create -n allennlp python=3.6 3.激活环境下载依赖文件 激活环境 source activate allennlp 进入github上下载的文件夹 下载依赖文件 pip install -r requirements.txt 遇到报错问题，参考下一小节，所欲问题解决。 4.安装allennlppip install --editable . 5.测试allennlp 成功后效果如下： $ allennlp 2019-05-22 21:58:42,297 - INFO - pytorch_pretrained_bert.modeling - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex . usage: allennlp Run AllenNLP optional arguments: -h, --help show this help message and exit --version show program's version number and exit Commands: configure Run the configuration wizard. train Train a model. evaluate Evaluate the specified model + dataset. predict Use a trained model to make predictions. make-vocab Create a vocabulary. elmo Create word vectors using a pretrained ELMo model. fine-tune Continue training a model on a new dataset. dry-run Create a vocabulary, compute dataset statistics and other training utilities. test-install Run the unit tests. find-lr Find a learning rate range. print-results Print results from allennlp serialization directories to the console. 遇到的问题问题1报错信息：ERROR: Failed building wheel for jsonnet 解决方法：conda install -c conda-forge jsonnet 问题2报错信息：报的都是某些包的版本问题 ERROR: botocore 1.12.152 has requirement urllib3=1.20; python_version >= "3.4", but you'll have urllib3 1.25.2 which is incompatible. ERROR: aws-sam-translator 1.11.0 has requirement jsonschema~=2.6, but you'll have jsonschema 3.0.1 which is incompatible. ERROR: cfn-lint 0.20.3 has requirement jsonschema~=2.6, but you'll have jsonschema 3.0.1 which is incompatible. ERROR: cfn-lint 0.20.3 has requirement requests=2.15.0, but you'll have requests 2.22.0 which is incompatible 解决方法根据报错信息下载相应安装包即可 问题3报错信息：ImportError: dlopen: cannot load any more object with static TLS ___________________________________________________________________________ Contents of /home/minelab/anaconda3/envs/allennlp/lib/python3.6/site-packages/sklearn/__check_build: __init__.py setup.py _check_build.cpython-36m-x86_64-linux-gnu.so __pycache__ ___________________________________________________________________________ It seems that scikit-learn has not been built correctly. If you have installed scikit-learn from source, please do not forget to build the package before using it: run `python setup.py install` or `make` in the source directory. If you have used an installer, please check that it is suited for your Python version, your operating system and your platform. 解决方法：下载更低版本的scikit-learn,例如 pip install scikit-learn=0.20.3 参考链接 https://github.com/pytorch/pytorch/issues/10443 https://github.com/pypa/pip/issues/4330 安装的启示环境问题 最基本的就是先去网上查这个错误的解决方法 网上的解决不了的，先猜猜大概率是哪方面的问题。 比如大概率是各种版本互相之间不适配的问题，那就调试版本，一般都会告诉你哪个有问题，比如上面的scikit-learn问题。]]></content>
      <categories>
        <category>install</category>
      </categories>
      <tags>
        <tag>allennlp</tag>
        <tag>包安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Triple Trustworthiness Measurement for Knowledge Graph阅读笔记]]></title>
    <url>%2Fpost%2FTriple%20Trustworthiness%20Measurement%20for%20Knowledge%20Graph%2F</url>
    <content type="text"><![CDATA[本文提出了一种通过计算triple trustworthiness来评估知识图谱的准确程度的方法。模型利用神经网络综合来自实体（借鉴Resource allocation）、关系（借鉴翻译模型的思想，如TransE）和KG全局（借鉴关系路径，RNN）三个层面的语义和全局信息，输出最后的 trustworthiness作为判断依据。 下载地址 SummaryThis paper proposed a method for estimating the accuracy of a knowledge graph by computing triple trustworthiness. The model uses neural network to synthesize semantic and global information from three levels: entity(resource allocation), relationship(translation model ideas, such as TransE)m and KG global(relationship path, RNN) and outputting the final trustworthiness as the basis for judgment. Problem statementpossible noises and conflicts are inevitably intoduced in the process of constructing the KG research objectivequantify the KG’s semantic correctness and the true degree of the facts expressed Contribution Knowledge graph triple trustworthiness measurement use the triple semantic information and globally inferring information three levels measurement and an intergration of confidence value experiment result verified the model valid on large-scale KG Freebase the KGTtm could be utilized in knowledge graph construction or improvement THE TRIPLE TRUSTWORTHINESS MEASUREMENT MODEL Longitudinally, the model can be divided into two level. the upper is a pool of multiple trustworthiness estimate cells(estimator) the output of these Estimator forms the input of lower-level fusion device(Fusioner) Viewed laterally, three progressive levels are be considered, as following. Is there a possible relationship between the entity pairs? ResourceRank: The algorithm assumes that the association between entity paires $(h,t)$ will be stronger, and more resource is passed from the head $h$ through all associated paths to the tail $t$ in a graph The amount of resource aggregated into $t$ ingeniously indicateds the association strength from $h$ to $t$. As pair $(e_1,e_2)$, there only one directed edge from $e_1$ to $e_2$ in the graph, where the different bandwidth of the edge indicates the number of the multiple relations. output: \left\{\begin{array}{c}{u=\alpha\left(W_{1} V+b_{1}\right)} \\ {R R(h, t)=W_{2} u+b_{2}}\end{array}\right.Authors constructed a $V$ vector by combining six characteristics. R (t | h); In-degree of head node ID(h); Out-degree of head node OD(h); In-degree of tail node ID(t); Out-degree of tail node OD(t); The depth from head node to tail node Dep As for 1. the formula: R(t | h)=(1-\theta) \sum_{e_{i} \in M_{t}} \frac{R\left(e_{i} | h\right) \cdot B W_{e_{i} t}}{O D\left(e_{i}\right)}+\frac{\theta}{N} $M_t$is the set of all nodes that have outgoing links to the node $t$, $OD (e_i)$ is the out-degree of the node eiand the $BW_{e_it}$ is the bandwidth from the $e_i$ to $t$. In order to improve the model fault-tolerance, we assume that the resource fow from each node may directly jump to a random node with the same probability θ Can the determined relationship $r$ occur between the entity pair $(h,t)$ ? Translation-based energy function (TEF)：depended on TransE $E(h, r, t)=|\mathbf{h}+\mathbf{r}-\mathbf{t}|$ output: P(E(h, r, t))=\frac{1}{1+e^{-\lambda\left(\delta_{r}-E(h, r, t)\right)}}Can the relevant triples in the KG infer that the triple is trustworthy? Reachable paths inference (RPI): There two challenges to exploit the reachable paths for inferring triple trustworthiness: reachable paths selectionSemantic distance-based path selection Reachable Paths Representationusing a RNN to deal with the embeddings of the three elements of each triple in the selected path Fusing the Estimatorsa classifer based on a multi-layer perceptron EXPERIMENTSdatasetFB15K Interpreting the Validity of the Trustworthiness The left picture shows that the positives examples are mainly concentrated in the upper region, vice versa. As for the right picture only if the value of a triple is higher than the threshold can it be considered trustworthy shows that the positive examples universally have higher confidence values Comparing With Other Models on The Knowledge Graph Error Detection Task Authors’ model has beter results in terms of accuracy and the F1-score than the other models. Analyzing the ability of models to tackle the three type noises. a higher recall shows that authors’ model can more accurately find the right from noisy triples higher average trustworthiness values show that authors’ model can better identify the correct instances and with high confidence the worst among the $(h, ?, t)$, because the various relations between a certain entity increase the difficulty of model judgment. Analyzing the Efects of Single Estimators It can be found that the accuracy obtained by each model is above 0.8, which proves the effectiveness of each Estimator 思考本文在方法上几乎没有什么创新，本质上就是一个老方法的多个组合。最大亮点就是作者能提出trustworthiness来把这个评价知识图谱准确度的问题进行了量化。这种能力比提出方法上的创新更加厉害，也是需要学习的地方。]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>KG</tag>
        <tag>Knowledge Graph</tag>
        <tag>Triple</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GloVe: Global Vectors for Word Representation阅读笔记]]></title>
    <url>%2Fpost%2FGloVe%3AGlobal%20Vectors%20for%20Word%20Representation%2F</url>
    <content type="text"><![CDATA[论文下载地址，GloVe是一个新的全球对数双线性回归模型，属于经典的词向量表示方法之一。 Introductionevaluate the intrinsic quality Most word vector methods rely on the distance or angle between pairs of word vectors Mikolov et al. (2013c) introduced word analogies that examines word vector’s various dimensions of difference. two main model families for learning vectors: global matrix factorization methods local context window methods Authors propose a specific weighted least squares model that trains on globla word-word co-occurrence counts and thus makes efficient use of statistics. Related WorkMatix Facroization MethodsThese methods utilize low-rank approximations to decompose large matrices that capture statistical information about a corpus. shortcomingthe most frequent words contribute a dispropoertionate amount to the similarity measure. Shallow Window-Based MethodsAnother approach is to learn word representations that aid in making predictins within local context windows. shortcomingdo not operate directly on the co-occurrence statistics of the corpus and fails to take advantage of the vast amount of repetition in the data. The GloVe ModelGloVe: Global Vectorsthe global corpus statistics are captured directly by the model the question about the model using the statistics of word occurrences in a corpus how meaning is generated from these statistics how the resulting word vectors might represent that meaning some notation$X_{ij}$ : the number of times word j occurs in the context of word i $X_i = \sum_{k} X_{i k}$ : the number of times any word appears in the context of word i $P_{i j}=P(j | i)=X_{i j} / X_{i}$: the probability that word j appear in the context of word i above that, werd vector learning should be with ratios of co-occurrence probabilities: $w \in \mathbb{R}^{d}$are word vectors and $\tilde{w} \in \mathbb{R}^{d}$are separate context word vectors For F, we should select a unique choice by enforcing a few desiderata. encoding the information present the ratio $P_{i k} / P_{j k}$ in the word vector space. Since vector spaces are inherently linear structures put F to be a compicated function parameterized, and avoiding bofuscating the linear structure the word-word co-occurrence matrices, we can exchange a word and a context word(because a word can also be a context word) F should be a homomorphism by Eqn.(3) F = exp or the Eqn(6) would have the exchange symmetry if not $\log \left(X_{i}\right)$ and $\log \left(X_{i}\right)$ is independent of k, so it can be absorbed into a bias $b_i$ for avoiding diverge, $\log \left(X_{i k}\right) \rightarrow \log \left(1+X_{i k}\right)$ a new weighted least squares regression model to address the problem that LSA wirhts all co-occuttences equally. cost function: Relationship to Other ModelsIn this subsection authors show how these models are related to their proposed model. the defect of cross entropy it has the unfortunate property that distributions with long tails are often modeled poorly with too much wieght given to the unlikely events. Complexity of the modelthe computational complexity of the model depends on the number of nonzero elects in the matrix $X$ some assumptions about the distribution of word co-occurrences the number of co-occurrences of word $i$ with word $j$, $X_{ij}$, can be modeled as a power-law function of the frequency rank of that word pair, $r_{ij}$: $X_{i j}=\frac{k}{\left(r_{i j}\right)^{\alpha}}$ ExperimentsEvaluation methodsauthors conduct experiments on the word analogy taks of Mikolov et al. (2013a) Word analogiesThe word analogy task consists of questions like, “a is to b as c is to ?” Word similarity Named entity recognitionResultsTable 2 shows the CloVe model performs significantly better than the other baslines, often with smaller vector sizes and smaller corpora. Table 3 shows results on five different word similarity datasets. Table 4 shows results on the NER task with the CRF-based model. Model Analysis: Vector Length and Context Size Model Analysis: Corpus Size On the syntactic subtask, larger corpora typically produce better statistics so that there is a monotonic increase in performance as the cor- pus size increases. But the same trend is not true for the semantic subtask, which is probably because of analogy dataset Model Analysis: Run-time Model Analysis: Comparison with word2vecFor the same corpus, vocabulary, window size, and training time, GloVe consistently outperforms word2vec 参考链接 https://blog.csdn.net/coderTC/article/details/73864097]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>word vector</tag>
        <tag>GloVe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep contextualized word representations 阅读笔记]]></title>
    <url>%2Fpost%2FDeep%20contextualized%20word%20representations%2F</url>
    <content type="text"><![CDATA[论文下载地址，ELMo事先用语言模型学好一个单词的 Word Embedding，此时多义词无法区分，不过这没关系。在我实际使用 Word Embedding 的时候，单词已经具备了特定的上下文了，这个时候我可以根据上下文单词的语义去调整单词的 Word Embedding 表示，这样经过调整后的 Word Embedding 更能表达在这个上下文中的具体含义，自然也就解决了多义词的问题了。所以 ELMO 本身是个根据当前上下文对 Word Embedding 动态调整的思路。 IntroductionELMo(Embedddings from Language Models):why call ELMo:Using vectors derived from a bidirectional LSTM that is trained with a coupled language model(LM) objective on a large text corups. characteristics ELMo representations are a function of all of the internal layers of the biLM. learn a linear combination of the vectors stacked above each input word for each end task the higher-level LSTM states capture context-dependent aspects of word meaning the lower-level states model aspects of syntax Extensive experiments EMLo representations can be easily added to existing models improve the state of art in every case ELMo outperform those derived from just the top layer of a LSTM Related work Some approaches for learning word vectors only allow a single context-independent representation for each word. to overcome some shortcomings of traditional word vectors: enriching them with subword information learning separate vectors for each word sense Authors uses subword units through the use of character convolutions, seamlessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes. context-depends representations Authors take full advantage of access to plentiful monolingual data Previous work also shown that different layers of deep biRNNs encode different types of information introducing multi-task syntactic supervision at the lower levels of a deep LSTM can improve overall performance of higher level tasks the top layer of an LSTM for encoding word context (Melamud et al., 2016) has been shown to learn representations of word sense. ELMo representations can also induce similar signals. ELMo: Embeddings from Language ModelsBidirectional language models model the probability of token $t_k$ given the history($t_1, … , t_{k-1}$): a backward LM: Authors’ formulation jointly maximizes the log likelihood of the forward and backward directions: ELMo For each token $t_k$, a L-layer biLM computes a set of 2L + 1 representations: For a downstream model, ELMo collapses all layers in R into a single vector. In the simplest case, ELMo just selects the top layer. For a task specific weighting of all biLM layers: $s^{task}$ are softmax-normalized weithts and the scalar parameter $γ^{task}$ allows the task model to scale the entire ELMo vector Using biLMs for supervised NLP tasks Given a pre-trained biLM and a supervised architecture for a target NLP task let the end task model learn a linear combination of these representations consider the lowest layers of th supervised model without the biLM add ELMo to the supervised model freeze the weights of the biLM concatenate the ELMo vector $ELMo^{task}_k$ with $x_k$ and pass the ELMo enhanced representation $[x_k,;ELMo^{task}_k ]$ into the task RNN. for some tasks, authors also include ELMo ar the output of task RNN by introducing another set of out put specific linear weights and replacing $h_k$ with $[h_k,;ELMo^{task}_k ]$ add a moderate amount of dropout to ELMo and in some case to regularize the ELMo weights Pre-trained bidirectional language model architecture the biLM provides three layers of representations for each input token, both directions and a residual connection between LSTM layers fine tuning the biLM on domain specific data Evaluationthe following picture shows the performance of ELMo in Question answering, Textual entailment, Semantic role labeling, Corefrence resolution, Named entity extraction, Sentiment analysis. In every task considered, simply adding ELMo establishes a new state-of-the-art result. AnalysisAlternate layer weighting schemes the following picture compares these alternatives. Including representations from all layers improves overall performance over just using the last layer, and including contextual representations from the last layer improves performace over the baseline. Also shows the $\lambda$ is important. Where to include ELMo?The ELMo can be included in both the input and output. the results show including the ELMo in both input and output can preform better. What information is captured by the biLM’s representations?Intuitively, the biLM must be disambiguating the meaning of words using their context. The GloVe can only capure the speech. but the biLM is able to disambiguate both the part of speech and word sense in the source sentence. Word sense disambiguationgiven a sentence, predicting the sense of a target word using a simple 1-nearst negihbor approach POS taggingto examine whether the biLM captures basic syntax. Sample efficiencyAdding ELMo to a model increases the sample efficiency considerably, both in terms of number of parameter updates to reach state-of-the-art performance and the overall training set size. Visualization of learned weights 参考链接 NAACL2018:高级词向量(ELMo)详解(超详细) 经典，这篇文章中阐述了一些使用的细节，并用图来表示，更加清晰。 ELMo算法介绍，这篇博客中自己对整个论文的概述和总结和好，需要学习。]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>embedding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Efficient Estimation of Word Representations in Vector Space》阅读笔记]]></title>
    <url>%2Fpost%2FEfficient%20Estimation%20of%20Word%20Representations%20in%20Vector%20Space%2F</url>
    <content type="text"><![CDATA[论文下载地址，该篇论文的大篇幅都在讨论实验结果的分析，模型的部分比较简单，没有详细分析，本来是想读一下CBOW和skip-gram的原始论文，发现并没有想象中的那么大的用处。 Goals of paper 开发了两种新模型，并保留了单词之间的线性规律 设计了一个新的综合测试集，用于测量句法和语义规律 讨论了训练时间和准确性如何取决于单词向量的维度和训练数据的数量 Model Architectures训练复杂度： 其中，E是训练次数，T是训练集单词数量，Q是模型结构。 Feedforward Neural Net Language Model (NNLM)它由输入，映射，隐藏和输出层组成。通过简化方法，Q= N x D x H Recurrent Neural Net Language Model (RNNLM)克服了模型需要固定的上下文长度的问题，并且只有输入，隐藏和输出层。 Q= H x H + H x V，其中H = D（单词表示），H x V 可以通过分级softmax被简化为H x log_2(V)。所以主要的复杂度来自于H x H。 Parallel Training of Neural Networks模型使用的DistBelief框架允许我们并行运行同一模型的多个副本，每个副本通过集中的服务器同步其梯度更新，该服务器保留所有参数 New Log-linear Models大多数复杂性是由于模型中的非线性隐藏层引起的。模型结构如下： Continuous Bag-of-Words Model(CBOW)第一个提出的体系结构类似于前馈NNLM，其中去除了非线性隐藏层，并且所有单词（不仅仅是投影矩阵）共享投影层。 因此，所有单词都被投射到相同的位置（它们的向量被平均）。 将这个架构称为词袋模型，因为历史中的单词顺序不会影响投影。 模型的复杂度：Q = N × D + D × log_2(V ) Continuous Skip-gram Model基于同一句子中的另一个单词最大化单词的分类。 更准确地说，使用每个当前单词作为具有连续投影层的对数线性分类器的输入，并预测当前单词之前和之后的特定范围内的单词。 模型的复杂度：Q = C × (D + D × log2(V ))，其中C是单词的最大距离。 实验任务描述为了度量词向量的质量，我们定义了一个复杂的测试集，它包括了五种类型的语义问题。九个类型的句法问题。包括每个类别的两个样本集在上表展示；总之，共拥有8869个语义问题和10675个句法问题 作者通过：最大化精确度 ，模型体系结构的比较，模型的大规模并行训练来证明提出模型的运速度和精确的优势。]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>word2vec</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shared Embedding Based Neural Networks for Knowledge Graph Completion阅读笔记]]></title>
    <url>%2Fpost%2FShared%20Embedding%20Based%20Neural%20Networks%20for%20Knowledge%20Graph%20Completion%2F</url>
    <content type="text"><![CDATA[原文下载链接，知识图谱补全（KGC，Knowledge Graph Completion)是一种自动建立图谱内部知识关联的工作。目标是补全知识图谱中三元组的缺失部分。主要方法为基于张量（或者矩阵）和基于翻译两类。在本文中，作者提出了一种基于共享嵌入的神经网络的模型（SENN）来处理KGC。 Contribulation 提出了SENN模型，该模型明确区分头实体、关系和为实体预测任务，并把它们整合到一个基于全连接神经网络框架中，该框架共享的实体和关系嵌入。 SENN提出了一个自适应全中损失机制，该方法可以很好的处理具有不同映射属性的三元组，并处理不同的预测任务。 由于关系预测通常比头尾实体预测具有更好的性能，我们把SENN应用到头尾实体预测，从而将SENN扩展到SENN+。 Related worksTensor/Matrix Based MethodsRESCAL是一个典型的方法，该方法基于三向张量因子分解的方法。 目标函数为： $M_r$是r的关系矩阵，大小为k x k。 ComlEx是最近提出的方法，该方法基于矩阵分解，并且它使用复数值来定义实体和关系的嵌入。 目标函数为： Re(x)返回x的实部。 Translation Based Methods代表模型为经典的TransE模型（这里不再赘述） Translation Based MethodsER-MLP使用多层感知器来捕获头实体，关系和尾实体之间的隐式交互。 目标函数为： ProjE使用具有组合层和投影层的神经网络来对头尾实体预测建模。 THE SENN METHOD模型结构如图所示： 作者将框架划分为以下四个部分： 三元组的批量预处理 知识图谱的Shared embeddings表示学习 独立的头尾实体及关系预测子模型训练与融合 联合损失函数构成 整个KGC的流程可以描述如下： 将训练数据中的完整三元组（知识图谱）划分批量后作为模型的输入 对于输入的三元组，分别训练得到实体（包括头尾实体）嵌入矩阵与关系嵌入矩阵（embeddings） 将头尾实体及关系embeddings分别输入到三个预测模型中（头实体预测（?, r, t），关系预测(h, ?, t)，尾实体预测(h, r, ?)） The Three Substructures预测子模型具有相似的结构如下图，模型输入关系向量与实体向量后，进入n层全连接层，得到预测向量，再经过一个sigmoid（或者softmax）层，输出预测标签向量。 头实体预测目标函数： f(x)= max(0,x). 预测标签： 其它两种与此头实体类似。 Model TrainingThe General Loss Function模型目标标签向量表示为： $I_h$是在训练集中给定r和t的所有有效头实体集。 三者的平滑向量表示为： 三个预测任务的损失函数为： 总损失函数为： The Adaptively Weighted Loss Mechanism.该方法的动机： 在知识图谱中的三元组有4种类型：1-TO-1, 1-TO-M, M-TO-1 and M-TO-M。所以预测在训练集中具有的有效实体/关系越多，它就越不确定。所以作者将对应于头部实体预测，关系预测和尾部实体预测的损失的权重与有效实体的数量相关联。 因为关系预测比实体预测更加容易。所以作者加大对头尾实体的错误预测的惩罚。 所以作者得到新的损失函数： 总损失函数变为： THE SENN+METHOD作者相信可以进一步利用关系预测的相当好的性能来辅助测试过程中的头部和尾部实体预测。 给定头部预测任务（？，r，t）并假设h是有效的头部实体。 如果我们采用SENN方法来预测h和t之间的关系，即执行关系预测任务（h，？，t），则关系r最有可能具有 预测标签高于其他关系，因此应排名高于其他关系。 其中Value（x，r）返回对应于关系r的向量x的条目; Rank（x，r）以降序返回对应于关系r的向量x的条目的等级。 最后SENN+种预测标签为： 其中 EXPERIMENTSDatasets Entity Prediction Relation Prediction 论文还进行了共享嵌入和自适应权重损失机制有效性的验证。 参考链接 http://blog.openkg.cn/%E8%AE%BA%E6%96%87%E6%B5%85%E5%B0%9D-%E9%9D%A2%E5%90%91%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E8%A1%A5%E5%85%A8%E7%9A%84%E5%85%B1%E4%BA%AB%E5%B5%8C%E5%85%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
        <tag>神经网络</tag>
        <tag>知识图谱补全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Bootstrapping Entity Alignment with Knowledge Graph Embedding》阅读笔记]]></title>
    <url>%2Fpost%2FBootstrapping%20Entity%20Alignment%20with%20Knowledge%20Graph%20Embedding%2F</url>
    <content type="text"><![CDATA[论文下载地址，采用了bootstrapping方法来解决缺乏训练数据的过程，提出了截断均匀负采样来提高负样例对于目标函数的贡献，采用基于限制的目标函数来按需调整正负样例的得分。 基于嵌入的实体对齐将不同的知识图谱（KG）表示为低维嵌入，并通过测量实体嵌入之间的相似性来查找实体对齐。其中，大量方法所面临的一个挑战是：缺乏足够的先前对齐作为标记的训练数据。 贡献 作者把实体对齐建模为一个分类问题，其基于KG嵌入来寻求最大化所有标记和未标记的实体对齐可能性 对于面向对齐的KG嵌入，作者提出了一种基于限制的目标函数；为了对不太可能区分的负三元组进行抽样，作者提出了一种截断均匀的负抽样方法。 作者提出了一个自举过程（bootstrapping）来克服缺乏足够训练数据，通过标记可能的对齐并迭代地将其添加到训练数据中来更新面向对齐的嵌入。 作者在三个跨语言和两个大型数据集上评估了所提出的方法，表明所提出的方法明显优于三种最先进的实体对齐方法。 问题描述最大似然准则指导选择实现最高对齐可能性的最佳θ 其中，L_x代表实体x的真实标签，1_[]是一个指示函数，表示给定命题的真值（0或1）。但是对于没有标签的实体，想要通过上述来得到theta就很困难。 模型面向对齐的KG嵌入作者提出了一个目标函数： 该目标函数有两个期望的属性： 预期正三元组得分较低，而负三元组得分较高。例如f(r)&lt;= r_1 并且 f(r’)&gt;=r_2，设置时r_2&gt;r_1,且r_1是一个小的正值。 仍然可以得到f(r’)-f(r)&gt;=r_2 - r_1，这表明所提出的目标函数仍然保留了基于边际排序损失的特征。 截断均匀负采样如果样例太容易区分，那么对整个的嵌入学习的贡献会很小。 所以，作者采用在嵌入空间中s最近的邻居作为候选集，剔除那些和实体x相似度过低的数据。 引导对齐（Bootstrapping Alignment）作者迭代地将可能的对齐标记作为训练数据，并使用它来进一步改进实体嵌入和对齐。 可能的对齐标签和编辑作者为了实现最大化对齐可能性并遵守一对一对齐约束，提出以下优化问题来标记第t次迭代： Y’_x = {y|y ∈ Y’ and π(y|x; Θ^(t)) &gt; γ3}代表标签x的候选集；ψ^(t)(·)是一个指示函数，只有当x在第t次迭代时标签为y时为1，其它情况为0。两个限制条件保证了一对一的标签。这时得到了一个新的标签对齐： 为了提高标签质量并满足一对一的对齐约束，在自举过程中，一旦被标记的实体可以在随后的标记中重新标记或变为未标记的实体。 当发生两个标签冲突时，我们通过计算下面的似然差异来确定保留哪个： 当该值大于0说明前者具有更大的对齐概率。 从整体角度学习为了获得标记和未标记实体的整理观察，作者定义了概率分布φx来描述所有x可能的概率分布。 由此，作者得到了最小化下面的似然函数来得到Θ： 因为，嵌入不仅应该捕获对齐可能性，还应该模拟KG的语义，所以作者最后定义联合目标函数： 实验数据集 DBP15K [Sun et al., 2017]包含三个跨语言数据集，这些数据集是从DBpedia的多语言版本构建的。DBPZH-EN(Chinese to English), DBPJA-EN(Japanese to English) and DBPFR-EN(French to English)每个数据集包含15，000个参考实体对齐。 DWY100K包含从DBpedia，Wikidata和YAGO3中提取的两个大型数据集，由DBP-WD和DBP-YG表示。 每个数据集都有10万个参考实体对齐 实验设置作者选取了三种最先进的基于嵌入的方法来实现实体对齐。 MTransE [Chen et al., 2017]，选取了第四种变体（表现最佳）。 IPTransE[Zhu et al., 2017]是一个迭代方法 JAPE [Sun et al., 2017]结合了实体对齐的关系和属性嵌入 AlignE面向对齐的KG嵌入模型的实现，具有截断的均匀负采样和参数交换，它优化了公式（3），但是没有自举 实验结果表2中我们观察到AlignE明显优于MTransE，IPTransE和JAPE，因为它采用面向对齐的嵌入。而BootEA显着改善了AlignE的结果，表明了自举的良好性能是由于其能够准确地将可能的对齐标记为训练数据。 分析截断均匀负抽样的有效性从图中可以看出，与MTransE，IPTransE和JAPE相比，具有均匀负采样的AlignE仍然获得了优异的结果，并且随着采样离x更加接近，效果呈上升趋势。 可能对齐的准确性可以看到以作者的标记方法S3表现最佳。这些结果证实作者的方法可以保证使用未标记数据的安全性。 对先前对准比例的敏感性 正如预期的那样，随着比例的增加，所有五个数据集的结果都变得更好，因为更多的先前对齐可以提供更多信息来对齐两个KG。 F1-score w.r.t. 关系三元数的分布 BootEA在所有时间间隔都优于MTransE，IPTransE和JAPE，这再次证实了BootEA的有效性。而且BootEA可以在稀疏数据上取得有希望的结果。]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
        <tag>知识图谱嵌入</tag>
        <tag>实体对齐</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Entity Alignment between Knowledge Graphs Using Attribute Embeddings》阅读笔记]]></title>
    <url>%2Fpost%2FEntity%20Alignment%20between%20Knowledge%20Graphs%20Using%20Attribute%20Embeddings%2F</url>
    <content type="text"><![CDATA[论文下载地址，知识图之间的实体对齐的任务旨在在代表相同现实世界实体的两个知识图中找到实体。本文最主要就是提出了属性字符嵌入(attribute character embeddings)的方法。 Abstract我们的模型利用知识图中存在的大量属性三元组(attribute triples)并生成属性字符嵌入。 属性字符嵌入(attribute character embeddings)通过基于实体的属性计算实体之间的相似性，将实体嵌入从两个知识图移位到同一空间中。我们使用传递规则来进一步丰富实体的属性数量以增强属性字符嵌入。 Contribution 提出了两个KG之间实体对齐的框架，它由谓词对齐模块，嵌入学习模块和实体对齐模块组成。 提出了一种新颖的嵌入模型，它将实体嵌入与属性嵌入集成在一起，以便为两个KG学习统一的嵌入空间。 我们在三个真正的KG对上评估建议的模型。结果表明，我们的模型在实体对齐任务上始终优于最先进的模型，hits@1超过50％。 模型模型总览predicate alignment, embedding learning, and entity alignment Predicate Alignment谓词对齐模块通过使用统一的命名方案重命名两个KG的谓词来合并两个KG，以便为关系嵌入提供统一的向量空间。dbp:bornIn vs. yago:wasBornIn 统一命名为 :bornIn。 为了找到部分匹配的谓词，作者计算谓词URI的最后部分的编辑距离（例如，bornIn与wasBornIn）并将0.95设置为相似性阈值。 Embedding LearningStructure Embedding作者采用TransE来学习对于实体的结构嵌入。与TransE不同的是，模型希望更关注已对齐的三元组，也就是包含对齐谓词的三元组。模型通过添加权重来实现这一目的。Structure embedding的目标函数如下： count(r)是关系r出现的数量。 Attribute Character Embedding对于属性字符嵌入，也参考TransE的思想，将谓词r解释为从头部实体h到属性a的转换。但是，相同的属性a可以在两个KG中以不同的形式出现，例如50.9989对50.9988888889作为实体的纬度; “Barack Obama”与“Barack Hussein Obama”作为人名等。因此，本文提出使用组合函数对属性值进行编码，并将属性三元组中每个元素的关系定义为h +r≈fa（a）。 这里，fa（a）是组合函数，a是属性值a = {c1，c2，c3，…，ct}的字符序列。 组合函数将属性值编码为单个向量，并将类似的属性值映射到类似的向量表示。 作者定义了三个组成函数如下。 Sum compositional function (SUM)存在问题：包含相同字符不同顺序的属性值会有相同的向量表示 LSTM-based compositional function (LSTM). N-gram-based compositional function (N-gram) 最后attribute character embedding目标函数： Joint Learning of Structure Embedding and Attribute Character Embedding作者使用属性字符嵌入通过最小化以下目标函数将结构嵌入移动到相同的向量空间： 本文整体损失函数： Entity Alignment在经过上述训练过程之后，来自不同KG的相似的实体将会有相似的向量表示，因此可通过 获得潜在实体对齐对。此外，模型设定相似度阈值来过滤潜在实体对齐对，得到最终的对齐结果。 Triple Enrichment via Transitivity Rule作者利用一阶逻辑传递关系来丰富三元组。即：存在和则可以推理出h_1+ (r_1.r_2) ≈ t_2 Database本文从 DBpedia (DBP)、LinkedGeoData (LGD)、Geonames (GEO) 和 YAGO 四个 KG 中抽取构建了三个数据集，分别是DBP-LGD、DBP-GEO和DBP-YAGO。具体的数据统计如下： ExperimentsEntity Alignment Results本文对比了三个相关的模型，分别是 TransE、MTransE 和 JAPE。试验结果表明，本文提出的模型在实体对齐任务上取得了全面的较大的提升，在三种组合函数中，N-gram函数的优势较为明显。此外，基于传递规则的三元组丰富模型对结果也有一定的提升。具体结果如下 Rule-based Entity Alignment Results为了进一步衡量 attribute character embedding 捕获实体间相似信息的能力，本文设计了基于规则的实体对齐模型。本实验对比了三种不同的模型：以label的字符串相似度作为基础模型；针对数据集特点，在基础模型的基础之上增加了坐标属性，以此作为第二个模型；第三个模型是把本文提出的模型作为附加模型，与基础模型相结合。具体结果如下： KG Completion Results本文还在KG补全任务上验证了模型的有效性。模型主要测试了链接预测和三元组分类两个标准任务，在这两个任务中，模型也取得了不错的效果。具体结果如下：]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
        <tag>知识图谱嵌入</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《RelNN A Deep Neural Model for Relational Learning》阅读笔记]]></title>
    <url>%2Fpost%2FRelNN%20A%20Deep%20Neural%20Model%20for%20Relational%20Learning%2F</url>
    <content type="text"><![CDATA[论文下载地址，这篇文章相当于结合了统计学习和深度神经网络。里面有些公式没有理解，应该是有许多先前论文需要阅读。但是本篇论文扩展了思路如何结合统计学和深度学习，并且基于其余数据来预测一个类中对象的一个属性，想法也比较好。 Introduction作者主要集中于基于其余数据来预测一个类中对象的一个属性。 Challenge当类中每个对象的属性依赖于不同数量的其他对象的属性和关系时，此问题具有挑战性。 在StarAI社区中，此问题称为聚合（aggregation）。 Relational Logistic Regression and Markov Logic NetworksStarAI模型旨在模拟对象之间关系的概率。 Relational logistic regression (RLR) (Kazemi et al. 2014)定义的概率公式如下： 上面定义的RLR模型仅适用于布尔值或多值父项。作者采用的是连续的原子（continuous atoms）（Fatemi, Kazemi, and Poole (2016)） Relational Neural Networks作者通过设计神经网络中线性层（LL），激活层（AL）和误差层（EL）的关系对应物，对具有分层架构的RLR / MLN模型进行编码。 关系神经网络（RelNN）是包含作为图形彼此连接的若干RLL和RAL的结构。 Motivations for hidden layers 使喜欢看动作电影的人数增加时，男性的概率变为[0, 1]重的任何数值，不至于直接变为0或者1。 因此，隐藏层通过使模型能够学习通用规则并相应地对对象进行分类，然后以不同方式处理不同类别的对象，从而提高了建模能力。 使用RLR表示不同类型的现有显式聚合器，然而有些情况需要使用2个RLLs和2个RALs Learning latent properties directly对象可能包含无法使用常规规则指定的潜在属性，但可以在训练期间直接从数据中学习。 考虑图2中的模型，让Latent（m）成为电影的数字潜在属性，其值将在训练期间学习。 From ConvNet Primitives to RelNNs我们解释为什么RelNN也可以被视为ConvNets的一个实例。 ConvNets的输入矩阵中的单元（例如，图像像素）具有空间相关性和空间冗余：彼此更接近的单元比更远的单元更依赖。 例如，如果M表示图像的输入通道，则M [i，j]和M [i + 1，j + 1]之间的依赖性可能远大于M [i，j]和M [i，j+20]之间的依赖性。 对于关系数据，输入矩阵中的依赖关系（关系）是不同的：同一行或列中的单元（即同一对象的关系）具有比不同行和列中的单元更高的依赖性（即不同对象的关系）。 因此，为了使ConvNets适应关系数据，我们需要矢量形状的过滤器，这些过滤器对行和列交换是不变的，并且更好地捕获关系依赖性和可交换性假设。 DatasetsMovielens 1M dataset (Harper and Konstan 2015)第一个数据集是Movielens 1M dataset (Harper and Konstan 2015)，忽略了实际的评级，只考虑电影是否被评级，只考虑动作和戏剧类型。 PAKDD15获取地址 all Chinese and Mexican restaurants in Yelp dataset challenge获取地址 Empirical Results作者提出了三个问题来进行实验： Q1：RelNN的性能与其他众所周知的关系学习算法相比如何？ Q2：基于数字和规则的潜在属性如何影响RelNN的性能?更改了RelNN中隐藏图层和数字潜在属性的数量，以查看它们如何影响性能。 请注意，添加图层只会添加一定数量的参数，但添加k个数字潜在属性会增加k * |Δm|参数。 Q3：RelNN如何推断出看不见的案例并解决指向的规模大小问题 （Poole et al.2014）?作者实施了两个实验： 我们在大量数据中训练一个RelNN，并在一小数据上进行测试：该实验可以看作每个模型受冷启动问题的严重程度 然后我们在一小数据上训练一个RelNN并在一大数据上进行测试：可以看作这些模型对更大群体的推断 Future作者将从数据中自动学习这些结构的问题留作未来的工作。]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>关系抽取</tag>
        <tag>统计学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Interaction Embeddings for Prediction and Explanation》阅读笔记]]></title>
    <url>%2Fpost%2FInteraction%20Embeddings%20for%20Prediction%20and%20Explanation%2F</url>
    <content type="text"><![CDATA[论文下载地址，此论文主要提出了实体和关系的交互作用对于知识图谱嵌入的影响，和提出了新的嵌入评估方案 - 搜索预测解释。 论文贡献 提出了CrossE，一种通过学习一个交互矩阵来给实体和关系的交互建模的新型知识图谱嵌入。 我们使用三个基准数据集评估CrossE与链接预测任务上的各种其他KGE的比较，并显示CrossE在具有适度参数大小的复杂且更具挑战性的数据集上实现最先进的结果。 我们提出了一种新的嵌入评估方案 - 搜索预测解释，并表明CrossE能够生成比其他方法更可靠的解释。 这表明交互嵌入更能在不同的三元组环境中捕捉实体和关系之间的相似性。 介绍给定知识图谱和一个要预测的三元组的头实体和关系，在预测尾实体的过程中，头实体和关系之间是有交叉交互的crossover interaction, 即关系决定了在预测的过程中哪些头实体的信息是有用的，而对预测有用的头实体的信息又决定了采用什么逻辑去推理出尾实体，文中通过一个模拟的知识图谱进行了说明如下图所示： 相关工作论文中在这部分对KGE（Knowledge graph embedding）进行了分类总结： KGEs with general embeddings KGEs with multiple embeddings. KGEs that utilize extra information. 这部分总结中对大量的方法进行描述，可以作为背景知识进行阅读。 CrossE模型基于对头实体和关系之间交叉交互的观察，本文提出了一个新的知识图谱表示学习模型CrossE. CrossE除了学习实体和关系的向量表示，同时还学习了一个交互矩阵C，C与关系相关，并且用于生成实体和关系经过交互之后的向量表示，所以在CrossE中实体和关系不仅仅有通用向量表示，同时还有很多交互向量表示。CrossE核心想法如下图： 目标函数粉四步生成： Interaction Embedding for Entities：根据头实体向量和交互矩阵（以关系确定的）来确定头实体的交互表示。 Interaction Embedding for Relations：根据头实体的交互表示和关系作用生成关系的交互表示 Combination Operator：将头实体的交互表示和关系的交互表示相结合，并进行非线性处理（tanh） Similarity Operator：计算结合后表示和尾实体表示之间的相似度。 最后分数函数： 损失函数：（这里就是一个交叉熵函数，但是写的有问题f(x)项应该在括号外） 对于预测的解释这部分作者描述了如何生成预测三元组的解释，并介绍了基于嵌入的路径搜索算法，主要步骤如下： Search for similar relations：修剪掉不合理路径 Search for paths between h and t：作者定义了6种路径（班汉一个或两个关系） Search similar entities：捕获实体之间的相似性方面越有能力，就越有可能存在（hs，r，ts） : Search for similar structures as supports：我们只将知识图中至少有一个支持的路径视为解释。 实验数据集 链接预测 从实验结果中我们可以看出，CrossE实现了较好的链接预测结果。我们去除CrossE中的头实体和关系的交叉交互，构造了模型 CrossES，CrossE 和 CrossES 的比较说明了交叉交互的有效性。 生成解释我们提出了一种基于相似结构通过知识图谱的表示学习结果生成预测结果解释的方法，并提出了两种衡量解释结果的指标，AvgSupport和Recall。Recall是指模型能给出解释的预测结果的占比，其介于0和1之间且值越大越好；AvgSupport是模型能给出解释的预测结果的平均support个数，AvgSupport是一个大于0的数且越大越好。可解释的评估结果如下： 链接预测和可解释的实验从两个不同的方面评估了知识图谱表示学习的效果，同时也说明了链接预测的准确性和可解释性没有必然联系，链接预测效果好的模型并不一定能够更好地提供解释，反之亦然。 参考链接 http://blog.openkg.cn/%E8%AE%BA%E6%96%87%E6%B5%85%E5%B0%9D-interaction-embeddings-for-prediction-and-explanation/]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
        <tag>知识图谱嵌入</tag>
        <tag>知识图谱推理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Differentiable Learning of Logical Rules for Knowledge Base Reasoning》阅读笔记]]></title>
    <url>%2Fpost%2FDifferentiable%20Learning%20of%20Logical%20Rules%20for%20Knowledge%20Base%20Reasoning%2F</url>
    <content type="text"><![CDATA[论文下载地址，本文研究用于于知识图谱推理的学习概率一阶逻辑规则的问题，提出了Neural Logic Programming（Neural-LP）框架，它结合了端到端可微分模型中一阶逻辑规则的参数和结构学习。为了在可微分的框架中同时学习参数和结构，作者设计了一个具有注意机制和记忆的神经控制器系统，以学习顺序组成TensorLog使用的原始可微操作。作者采用的注意机制是作为逻辑规则的置信度并且有寓意含义的。 下图展示了一个使用逻辑规则进行知识图谱推理的例子 使用概率逻辑的优点是通过为逻辑规则配备概率，可以更好地模拟统计复杂和噪声数据。 statistical relational learning（统计关系学习）：学习关系规则的集合 ==inductive logic programming（归纳逻辑规划）：==当学习涉及提出新的逻辑规则时。（这应该和我正在做的方向是相关的，都是带有归纳性质的，有新的东西产生）。 FrameworkKnowledge base reasoning为了推理知识库，对于每个查询我们都有兴趣学习以下形式的加权链式逻辑规则，类似于==随机逻辑程序==： 其中$\alpha$是和规则有关的置信度，R是知识库中的关系，query(Y,X) 表示一个三元组，query 表示一个关系。 TensorLog for KB reasoning将实体转换成one-hot变量；并用一个矩阵$M_R$表示关系，该矩阵只在（i，j）处为1，i、j为第i、j个实体。 结合两个操作，逻辑规则推理$R(Y,X) \gets P(Y,X) \bigwedge Q(Z,X)$可以被表示为：$M_P \cdot M_P \cdot v_x \doteq s$，向量s中为1的位置就是Y的答案。 对于一条查询，所有的逻辑规则的右边部分被表示为以下形式： 其中，l表示所有的可能规则的个数，$\alpha_l$是规则l的置信度，$\beta_l$是某特定关系里的有序关系列表，所以在inference时，给定实体$v_x​$，实体y的score等于向量s中的对应y的位置的值。对于推理，给定实体x，实体y的score等于向量s中的对应y的位置的值。 所以总结本文关心的优化问题如下： Learning the logical rules在上式的优化问题中，算法需要学习的部分分为两个：一个是规则的结构，即一个规则是由哪些条件组合而成的；另一个是规则的置信度。由于每一条规则的置信度都是依赖于具体的规则形式，而规则结构的组成也是一个离散化的过程，因此上式整体是不可微的。因此作者对前面的式子做了以下更改： 对比与式（2）：主要交换了连乘和累加的计算顺序，对预一个关系的相关的规则，为每个关系在每个步骤都学习了一个权重，即上式的 $a_t^k$。 由于上式固定了每个规则的长度都为 T，这显然是不合适的。为了能够学习到变长的规则，Neural LP中设计了记忆向量 $u_t$,表示每个步骤输出的答案—每个实体作为答案的概率分布，还设计了两个注意力向量：一个为记忆注意力向量 $b_t$ ——表示在步骤 t 时对于之前每个步骤的注意力；一个为算子注意力向量 $a_t$ ——表示在步骤 t 时对于每个关系算子的注意力。每个步骤的输出由下面三个式子生成： 其中$b_t$和$a_t$由以下公式通过RNN获得： 推理机的整体框架是： 其中memory存的就是每步的推理结果（实体），最后的输出（例如$u_{T+1}$，目标就是最大化 $logv_y^Tu$，加log是因为非线性能让效果变好。 整个算法如下： 实验（1） 两个标准数据集上的统计关系学习相关的实验 Unified Medical Language System (UMLS)：The entities are biomedical concepts (e.g. disease, antibiotic) and relations are like treats and diagnoses. Kinship：contains kinship relationships among members of the Alyawarra tribe from Central Australia [ （2） 在$16*16$的网格上的路径寻找的实验 （3） 知识库补全实验实验所用数据集信息： FB15KSelected：这是通过从FB15K中去除近似重复和反向关系而构造的 实验结果： 为了证明Neural LP的归纳推理的能力，本文还特别设计了一个实验，在训练数据集中去掉所有涉及测试集中包含的实体的三元组，然后训练并预测，得到结果如下： （4） 知识库问答的实验 总结本文提出了一个可微的规则学习模型，并强调了知识库中的规则应该是实体无关的，对于我目前在做的方向，本体论也是与实体无关的，这种规则学习有一定的借鉴性，但是好像所区别。这个规则推理也可以看成某些关系之间的包含关系3.1中举的HasOfficeInCity(New York,Uber) and CityInCountry(USA,New York)的例子，可以看作是2对于1有包含关系。并且可以看到本篇论文中，作者设计了丰富的实验。 参考链接 https://toutiao.io/posts/wrxf4z/preview https://zhuanlan.zhihu.com/p/46024825]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
        <tag>知识图谱推理</tag>
        <tag>规则学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《OK Google, What Is Your Ontology? Or/ Exploring Freebase Classification to Understand Google’s Knowledge Graph？》阅读笔记]]></title>
    <url>%2Fpost%2FOK%20Google%2C%20What%20Is%20Your%20Ontology%3F%20Or%2F%20Exploring%20Freebase%20Classification%20to%20Understand%20Google%E2%80%99s%20Knowledge%20Graph%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[本论文详细阐述Freebase中的数据格式，并进行了重构。通过考虑整体架构的三个部分：Freebase类型系统及其缺乏继承和依赖于不兼容性，允许表示值的不确定性的实现，以及合并和拆分对象的实现。来对本体进行阐述。论文下载地址 这篇论文重构了Freebase数据转储来理解谷歌语义搜索特征背后的本体。论文将会探索Freebase本体如何由许多力量塑造的，这些力量也通过深入研究本体论和一个小的相关性研究来形成分类系统。这些发现将会提供知识图谱专有黑盒的一瞥。 The structures found in the Freebase/Knowledge Graph ontology will be analyzed in light of the findings on classification systems in a key text by Bowker and Star (2000) [5]. 术语定义 ObjectFreebase对象是一个全局唯一的标识符，它是Freebase中世界上某种东西的表示。 TypeFreebase类型用来表达类的概念。 PropertyFreebase属性是描述对象如何链接到其他值或对象的关系。 Property Detail属性详细信息指的是可以通过属性链接的对象或值的约束。 RDF triple资源描述格式（RDF）是用于“三元组”（或N = 3元组）格式的数据表示的规范[17]。 Ontology对于本文，Freebase本体是类型，属性和属性详细信息的正式结构和描述，用于指定对象如何相互关联。 Architecture在本文中，架构指的是可以在本体中找到的一般模式和关系。 ==本体是否允许类（或Freebase用语中的类型）之间的继承？ 是否有与属性相关的默认值？ 如何处理“零”或空值？ 这些类型的问题不一定关注本体（飞机，火车或汽车）中具体表达的内容，而是关于本体表达方式的更多问题应该通过检查架构来解决。== Methodology作者把数据进行切分：按照RDF中三元组的谓语进行分类，例如： Freebase Ontology and Classification As Bowker and Star note, “Information infrastructure is a tricky thing to analyze…the easier they are to use, the harder they are to see.” [5]. What does the system make sense of? What is left out? What is privileged and by extension what is ignored by Google? 虽然Freebase本体可能不会立即看起来像一个分类系统，但类型（类）和属性的结构是一个基于对各种事物进行分类的系统。 作为对世界事物表征进行排序和分类的系统，将根据Bowker和Star的分类结果讨论Freebase本体。他们将对亚里士多德和原型分类（Aristotelian and prototype classification）进行了区分。 亚里士多德的分类“按照一组二元特征进行操作，被分类的物体呈现或不呈现”，而原型分类则认为“在我们心目中对于椅子是什么的广泛描述; 我们用隐喻和类比来扩展这张图片“ 5.1. Freebase’s Type System不兼容性的概念出现在Freebase系统中，用于表示对象如何具有某些类型，而这些类型必须将其排除在其他类型之外。 没有继承（not implement inheritance）：上述不兼容性在确保数据不表达可能在Google KP中提供的令人尴尬，有害或不正确的陈述方面发挥了足够强大的作用。 缺乏继承也可能是一种允许实体具有更大灵活性的特征。这里作者举了一个狗为电影演员的例子。 5.2. Has Value or Has No Value?三元组如何表达估计值，不确定值或空值？实际处理时用“Has Value” (HV) and “Has No Value” (HNV)来分别表达不确定值和空值。 以这种方式表达未知数和空值的有趣实现可能表明Freebase / KG最初并不是为了支持这种不确定性而建立的。Google的数据编码某些不确定性的概念并未向最终用户公开，尽管它肯定以这种独特的方式实现。 5.3. Dealing with Doppelgangers and Chimeras涉及Freebase如何处理“合并”重复对象（doppelgangers）和“拆分”混合对象（嵌合体）。 the property “/dataworld/gardening hint/replaced by” is used to implement merges be- tween various objects (e.g. by saying “/m/xyz123 - Replaced By - /m/abc123”). A Small Correlational Study主要探索这个问题：域的本体的复杂性（人物，电影等领域的类型，属性等）与表达与本体相关的事实（“知识库”）的三元组数量之间是否存在关联？ 对于本研究，通过考虑与域相关的属性详细信息量（多少描述，约束等）来实现“复杂性”和“成熟度”。 对于89个域中的每一个，获得了关于每个域的本体的以下统计： ==域中的类型和属性数== ==每种类型和属性的描述数== ==每种类型和属性的属性详细信息数== 通过获取域中每种类型和属性的平均描述数和属性详细信息来计算简单的复杂性分数。 所有域的RDF三元组计数与此复杂性得分之间的Pearson相关系数与0.2824呈正相关，简单线性回归的斜率为78,424.08（见图6）。 当排除异常音乐切片时，相关性和斜率分别变为0.6680和33,899.53。 虽然需要进一步的工作来探索这个研究问题，但这个小的相关性研究为进一步的实验提供了一些有希望的初步结果 discussion考虑整体架构的三个部分：Freebase类型系统及其缺乏继承和依赖于不兼容性，允许表示值的不确定性的实现，以及合并和拆分对象的实现。此外，还进行了一项小型相关研究，以检验基于Bowker和Star推动的预感的假设。在很大程度上，分类系统中的许多特征也可以在Freebase的本体和体系结构中找到。 本文具体而言，探讨了支持整个交付流程的基础结构（本体和体系结构），而不是Freebase / KG中表示的特定事实。 conclusion 应通过探索Freebase本体和体系结构的其他方面以及对Freebase进行更全面的实验分析来进行进一步的研究。]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
        <tag>Ontology</tag>
        <tag>freebase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式]]></title>
    <url>%2Fpost%2FRegular%20expression%2F</url>
    <content type="text"><![CDATA[本文参考了一些链接，记录了一些常用正则表达式的详细使用方法。 在自然语言处理中，很多时候我们都需要从文本或字符串中抽取出想要的信息，并进一步做语义理解或其它处理。在本文中，作者由基础到高级介绍了很多正则表达式，这些表达式或规则在很多编程语言中都是通用的。 书写正则表达式网站 正则表达式（regex 或 regexp）对于从文本中抽取信息极其有用，它一般会搜索匹配特定模式的语句，而这种模式及具体的 ASCII 序列或 Unicode 字符。从解析/替代字符串、预处理数据到网页爬取，正则表达式的应用范围非常广。 其中一个比较有意思的地方是，只要我们学会了正则表达式的语句，我们几乎可以将其应用于多有的编程语言，包括 JavaScript、Python、Ruby 和 Java 等。只不过对于各编程语言所支持的最高级特征与语法有细微的区别。 下面我们可以具体讨论一些案例与解释。 基本语句锚点：^ 和 $^The 匹配任何以“The”开头的字符串 end$ 匹配以“end”为结尾的字符串 ^The end$ 抽取匹配从“The”开始到“end”结束的字符串 roar 匹配任何带有文本“roar”的字符串 数量符：*、+、？和 {}**abc* 匹配在“ab”后面跟着零个或多个“c”的字符串 abc+ 匹配在“ab”后面跟着一个或多个“c”的字符串 abc? 匹配在“ab”后面跟着零个或一个“c”的字符串 abc{2} 匹配在“ab”后面跟着两个“c”的字符串 abc{2,} 匹配在“ab”后面跟着两个或更多“c”的字符串 abc{2,5} 匹配在“ab”后面跟着2到5个“c”的字符串 a(bc)* 匹配在“a”后面跟着零个或更多“bc”序列的字符串 a(bc){2,5} 匹配在“a”后面跟着2到5个“bc”序列的字符串 或运算符：| 、 []a(b|c) 匹配在“a”后面跟着“b”或“c”的字符串 a[bc] 匹配在“a”后面跟着“b”或“c”的字符串 字符类：\d、\w、\s 和 .**\d 匹配数字型的单个字符 \w 匹配单个词字（字母加下划线） \s 匹配单个空格字符（包括制表符和换行符） . 匹配任意字符 使用「.」运算符需要非常小心，因为常见类或排除型字符类都要更快与精确。\d、\w 和\s 同样有它们各自的排除型字符类，即\D、\W 和\S。例如\D 将执行与\d 完全相反的匹配方法： \D 匹配单个非数字型的字符 为了正确地匹配，我们必须使用转义符反斜杠「\」定义我们需要匹配的符号「^.[$()|*+?{\」，因为我们可能认为这些符号在原文本中有特殊的含义。 \$\d 匹配在单个数字前有符号“$”的字符串 -&gt; Try it! (https://regex101.com/r/cO8lqs/9) 注意我们同样能匹配 non-printable 字符，例如 Tab 符「\t」、换行符「\n」和回车符「\r」 Flags我们已经了解如何构建正则表达式，但仍然遗漏了一个非常基础的概念：flags。 正则表达式通常以/abc/这种形式出现，其中搜索模式由两个反斜杠「/」分离。而在模式的结尾，我们通常可以指定以下 flag 配置或它们的组合： g（global）在第一次完成匹配后并不会返回结果，它会继续搜索剩下的文本。 m（multi line）允许使用^和$匹配一行的开始和结尾，而不是整个序列。 i（insensitive）令整个表达式不区分大小写（例如/aBc/i 将匹配 AbC）。 中级语句分组和捕获：()a(bc) 圆括弧会创建一个捕获性分组，它会捕获匹配项“bc” a(?:bc)* 使用 “?:” 会使捕获分组失效，只需要匹配前面的“a” a(?&lt;foo&gt;bc) 使用 “?&lt;foo&gt;” 会为分组配置一个名称 捕获性圆括号 () 和非捕获性圆括弧 (?:) 对于从字符串或数据中抽取信息非常重要，我们可以使用 Python 等不同的编程语言实现这一功能。从多个分组中捕获的多个匹配项将以经典的数组形式展示：我们可以使用匹配结果的索引访问它们的值。 如果需要为分组添加名称（使用 (?…)），我们就能如字典那样使用匹配结果检索分组的值，其中字典的键为分组的名称。 方括弧表达式：[][abc] 匹配带有一个“a”、“ab”或“ac”的字符串 -&gt; 与 a|b|c 一样 [a-c] 匹配带有一个“a”、“ab”或“ac”的字符串 -&gt; 与 a|b|c 一样 [a-fA-F0-9] 匹配一个代表16进制数字的字符串，不区分大小写 [0-9]% 匹配在%符号前面带有0到9这几个字符的字符串 [^a-zA-Z] 匹配不带a到z或A到Z的字符串，其中^为否定表达式 记住在方括弧内，所有特殊字符（包括反斜杠\）都会失去它们应有的意义。 ==Greedy 和 Lazy 匹配==数量符（* + {}）是一种贪心运算符，所以它们会遍历给定的文本，并尽可能匹配。例如，&lt;.+&gt; 可以匹配文本「This is a simple div test」中的「simple div」。为了仅捕获 div 标签，我们需要使用「？」令贪心搜索变得 Lazy 一点： &lt;.+?&gt; 一次或多次匹配 “&lt;” 和 “&gt;” 里面的任何字符，可按需扩展 注意更好的解决方案应该需要避免使用「.」，这有利于实现更严格的正则表达式： &lt;[^&lt;&gt;]+&gt; 一次或多次匹配 “&lt;” 和 “&gt;” 里面的任何字符，除去 “&lt;” 或 “&gt;” 字符 高级语句 边界符：\b 和 \B \babc\b 执行整词匹配搜索 \b 如插入符号那样表示一个锚点（它与$和^相同）来匹配位置，其中一边是一个单词符号（如\w），另一边不是单词符号（例如它可能是字符串的起始点或空格符号）。 它同样能表达相反的非单词边界「\B」，它会匹配「\b」不会匹配的位置，如果我们希望找到被单词字符环绕的搜索模式，就可以使用它。 \Babc\B 只要是被单词字符环绕的模式就会匹配 前向匹配和后向匹配：(?=) 和 (?&lt;=)d(?=r) 只有在后面跟着“r”的时候才匹配“d”，但是“r”并不会成为整个正则表达式匹配的一部分 (?&lt;=r)d 只有在前面跟着“r”时才匹配“d”，但是“r”并不会成为整个正则表达式匹配的一部分 我们同样能使用否定运算子： d(?!r) 只有在后面不跟着“r”的时候才匹配“d”，但是“r”并不会成为整个正则表达式匹配的一部分 (?&lt;!r)d 只有在前面不跟着“r”时才匹配“d”，但是“r”并不会成为整个正则表达式匹配的一部分 结语 正如上文所示，正则表达式的应用领域非常广，很可能各位读者在开发的过程中已经遇到了它，下面是正则表达式常用的领域： 数据验证，例如检查时间字符串是否符合格式； 数据抓取，以特定顺序抓取包含特定文本或内容的网页； 数据包装，将数据从某种原格式转换为另外一种格式； 字符串解析，例如捕获所拥有 URL 的 GET 参数，或捕获一组圆括弧内的文本； 字符串替代，将字符串中的某个字符替换为其它字符。 参考链接https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650749657&amp;idx=4&amp;sn=da4852cb0c4919316d801fe19a64901d&amp;chksm=871afea7b06d77b1bda42ac134c5dddad5af24647f62a5a8e7bdcef4499f40fd4c97045a6f3d&amp;mpshare=1&amp;scene=23&amp;srcid=1009dbNFxJJahsQK6NGw4wS3%23rd python正则表达式的使用正则表达式match和search的区别]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019年计划]]></title>
    <url>%2Fpost%2F2019-plans%2F</url>
    <content type="text"><![CDATA[读个博吉他可以弹唱几首流行歌曲假期报个班，请老师指点一下 精读50篇论文平均下来，每周都需要读一篇，每一篇都要勾划，发博客。 发2篇paper上半年积累知识，做实验，暑假开始写 考托福过90上半年背单词等 买个微单学个拍照不急，现在没钱，下半年入手一个吧 自己出门旅行一次嗯，要是论文写完就去吧！目前计划去山东，顺便看一下我二姑。5天左右的旅行吧！ 学习滑板开始健身每周去三次健身房]]></content>
      <categories>
        <category>annual plans</category>
      </categories>
      <tags>
        <tag>plan</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《An Automatic Knowledge Graph Construction System for K-12 Education》阅读笔记]]></title>
    <url>%2Fpost%2FAn%20Automatic%20Knowledge%20Graph%20Construction%20System%20for%20K-12%20Education%2F</url>
    <content type="text"><![CDATA[论文原址。本篇文章主要提出了一个自动化构建数学领域知识图谱的系统，主要应用的事NER和数据挖掘技术，其中NER主要是抽取数学概念，概念间的关系是作者自己构建的（例如先修关系）。对于数据集，作者主要从the Chinese curriculum standards of mathematics上提取的概念实体，从自己的SLP平台上，通过对学生表现来提取关系（把这部分作为数据挖掘）。本篇文章实际上可以作为构建特定领域的知识图谱的一个参考。 challenges the desired educational concept entities are more abstract than real world entities like PERSON, ORGANIZATION, LOCATION the desired relations are more cognitive and implicit, so cannot be derived from the literal meanings of text like generic knowledge graphs contributions a novel but practical system entity recognition (NER) &amp; association rule mining algorithms demonstrate an exemplary case with constructing a knowledge graph for the subject of mathematics SYSTEM OVERVIEW Educational Concept Extraction Module: Implicit Relation Identification Module CONCEPT EXTRACTION 线性链式CRF模型 标签预测 RELATION IDENTIFICATION两种方法 support confidence From the perspective of prerequisite relation, if concept si is a prerequisite of concept sj, learners who do not master sivery likely do not master sj, and learners who master sjmost likely master si. EXEMPLARY CASE AND SYSTEM EVALUATIONConcept ExtractionDatasetthe Chinese curriculum standards of mathematics published by the ministry of education as the main data source Evaluation adopt precision, recall and F1- score The ground truth is manually labeled by two domain experts. Relation IdentificationDataset students’ performance data collected by our SLP platform. EvaluationThe ground truth of the prerequisite relations between selected 9 concepts are annotated manually by two domain experts.]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>NER</tag>
        <tag>知识图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《RECURRENT NEURAL NETWORK REGULARIZATION》阅读笔记]]></title>
    <url>%2Fpost%2FRECURRENT%20NEURAL%20NETWORK%20REGULARIZATION%2F</url>
    <content type="text"><![CDATA[论文链接。这篇论文提出了LSTM的dropout策略来防止过拟合，即只在非循环链接处采取dropout。在BasicLSTMCell的接口就是依据这篇论文实现的。 文章整体架构和重点 contribution present a simple regularization technique background dropout Srivastava(2013)，对于前向反馈网络最有力的正则化方法并不能很好的应用在RNNs上。这导致RNNs规模都很小，因为太大会过拟合。 Bayer et al. (2013)指出了卷积dropout不能在RNNs上很好工作的原因是循环会放大噪音。 model仿射变换（affine transform）关于仿射变换：线性变换加上平移，盗个知乎上的图（原文链接） 模型主体采用的是Graves et al. (2013) dropout策略 The main contribution of this paper is a recipe for applying dropout to LSTMs in a way that success- fully reduces overfitting. The main idea is to apply the dropout operator only to the non-recurrent connections. 观察公式，实际上就是通过在层间传递中应用dropout。如下图中虚线所示。 从上图中也可以看到，该dropout的次数只和网络深度有关（数值为网络深度+1）。 experiments实验部分作者做了4部分实验来证明自己采用的方法有效，分别为language modeling, speech recognition, machine translation, image caption generation。这部分没什么需要解释了，感兴趣可以自己看一下实验。]]></content>
      <categories>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Understanding LSTM Networks]]></title>
    <url>%2Fpost%2FUnderstanding%20LSTM%20Networks%2F</url>
    <content type="text"><![CDATA[原文链接。这篇文章很好很细的一步一步的分解讲解了LSTM，之前看过一篇翻译的博客，现在自己翻译一遍，感觉对LSTM的认识加深了许多，虽然还是对LSTM中存有一些问题，比如为什么用tanh，sigmoid，为什不采用其他的？，但是看过之后至少对LSTM没有那么畏惧，不觉得过于复杂了。 LSTMRecurrent Neural Networks 循环允许信息从一个网络传入下一个。 一个循环网络可以被认为是相同网络的多个复制，每一个网络都将信息传递给后继者。这种类似链的性质表明，递归神经网络与序列和列表密切相关。 The Problem of Long-Term DependenciesRNN的一个吸引力是他们可能能够将先前信息连接到当前任务。 有时，我们只需要查看最近的信息来执行当前任务。例如： If we are trying to predict the last word in “the clouds are in the sky,” we don’t need any further context – it’s pretty obvious the next word is going to be sky. 在这种情况下，如果相关信息与待预测地方之间的差距很小，RNN可以学习使用过去的信息。 但是，对于一些情况，我们需要更多的上下文信息。 Consider trying to predict the last word in the text “I grew up in France… I speak fluent French.” 这时，相关信息与需要变得非常大的点之间的差距完全有可能。不幸的是，随着差距的扩大，RNN无法学会连接信息。 The problem was explored in depth by Hochreiter (1991) [German] and Bengio, et al. (1994), who found some pretty fundamental reasons why it might be difficult. LSTM Networks LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn! 标准RNNs的重复模块只有一个简单的结构，比如tanh层。 LSTMs也有像这种链式结构，但是它的重复模块具有不同的结构。 有四个，而不是一个神经网络层，以一种非常特殊的方式进行交互。 基本符号如下： 在上图中，每一行都携带一个完整的向量，从一个节点的输出到其他节点的输入。 粉色圆圈表示逐点运算，如矢量加法，而黄色框表示神经网络层。 行合并表示连接，而行分叉表示其内容被复制，副本将转移到不同的位置。 The Core Idea Behind LSTMsLSTM的关键是单元状态，水平线贯穿图的顶部。 单元状态有点像传送带。 它直接沿着整个链运行，只有一些微小的线性相互作用。 信息很容易沿着它不变地流动。 LSTM确实能够移除或添加信息到细胞状态，由称为门的结构精心调节。 门是一种可选择通过信息的方式。 它们由Sigmoid神经网络层和逐点乘法运算组成。 sigmoid层输出0到1之间的数字，描述每个组件应该通过多少。 值为零意味着“不让任何东西通过”，而值为1则意味着“让一切都通过！” LSTM具有三个这样的门，用于保护和控制单元状态。 Step-by-Step LSTM Walk Through我们的第一步就是确定我们将从单元状态中丢弃的信息。这个决定是由一个称为“遗忘门层”的sigmoid层决定的。它查看$h_{t-1}$和$x_t$，并为单元状态$C_{t-1}$中的每一个数字输出一个介于0和1之间的数字。1代表“完全保留这个”，而0代表“完全舍弃这个”。 让我们回到我们的语言模型示例，试图根据以前的所有单词预测下一个单词。 在这样的问题中，单元状态可能包括当前受试者的性别，因此可以使用正确的代词。 当我们看到一个新主题时，我们想要忘记旧主题的性别。 下一步是确定我们将在单元状态中存储哪些新信息。 这有两个部分。 首先，称为“输入门层”的sigmoid层决定我们将更新哪些值。 接下来，tanh层创建可以添加到状态的新候选值$\tilde{C}_t$的向量。 在下一步中，我们将结合这两个来创建状态更新。 在我们的语言模型的例子中，我们想要将新主题的性别添加到单元格状态，以替换我们忘记的旧主题。 现在是时候将旧的单元状态$C_{T-1}$更新为新的单元状态$C_t$。 前面的步骤已经决定要做什么，我们只需要实际做到这一点。 我们将旧状态乘以$f_t$，忘记我们之前决定忘记的事情。 然后我们添加$i_t * \tilde{C}_t$。 这是新的候选值，根据我们决定更新每个状态的值来缩放。 在语言模型的情况下，我们实际上放弃了关于旧主题的性别的信息并添加新信息，正如我们在前面的步骤中所做的那样。 最后，我们需要决定我们要输出的内容。 此输出将基于我们的单元状态，但将是过滤版本。 首先，我们运行一个sigmoid层，它决定我们要输出的单元状态的哪些部分。 然后，我们将单元格状态设置为tanh（将值推到介于-1和1之间）并将其乘以sigmoid门的输出，以便我们只输出我们决定的部分。 对于语言模型示例，由于它只是看到一个主题，它可能想要输出与动词相关的信息，以防接下来会发生什么。 例如，它可能输出主语是单数还是复数，以便我们知道动词应该与什么形式共轭，如果接下来的话。 Variants on Long Short Term Memory到目前为止我所描述的是一个非常正常的LSTM。 但并非所有LSTM都与上述相同。 事实上，似乎几乎所有涉及LSTM的论文都使用略有不同的版本。 差异很小，但值得一提的是其中一些。 One popular LSTM variant, introduced by Gers &amp; Schmidhuber (2000), is adding “peephole connections.” This means that we let the gate layers look at the cell state. 上面的图表为所有门增加了窥视孔（peephole），但是许多论文会给一些窥视孔而不是其他的。 另一种变化是使用耦合的遗忘和输入门。 我们不是单独决定忘记什么以及应该添加新信息，而是一起做出这些决定。我们仅仅会当我们在当前位置将要输入时忘记。我们仅仅输入新的值到那些我们已经忘记旧的信息的那些状态 。 另一个改动较大的变体是 Gated Recurrent Unit (GRU)，这是由 Cho, et al. (2014) 提出。它将遗忘和输入门组合成一个“更新门”。它还合并了单元状态和隐藏状态，并进行了一些其他更改。 由此产生的模型比标准LSTM模型简单，并且越来越受欢迎。 这些只是最着名的LSTM变种中的一小部分。 还有很多其他的东西，如 Yao, et al. (2015) 提出的 Depth Gated RNN。 还有一些完全不同的解决长期依赖关系的方法，如 Koutnik, et al. (2014) 提出的 Clockwork RNN。 哪种变体最好？ 差异是否重要？ Greff, et al. (2015) 对流行变体进行了很好的比较，发现它们几乎完全相同。Jozefowicz, et al. (2015) 测试了超过一万个RNN架构，找到了一些在某些任务上比LSTM更好的架构。 Conclusion早些时候，我提到了人们用RNN取得的显着成果。基本上所有这些都是使用LSTM实现的。对于大多数任务来说，它们确实工作得更好！ 作为一组方程写下来，LSTM看起来非常令人生畏。希望，在这篇文章中逐步走过它们使他们更加平易近人。 LSTM是我们用RNN实现的重要一步。很自然地想知道：还有另一个重要的一步吗？研究人员的共同观点是：“是的！下一步是它的注意！“我们的想法是让RNN的每一步都从一些更大的信息集中选择信息。例如，如果您使用RNN创建描述图像的标题，则可能会选择图像的一部分来查看其输出的每个单词。 事实上， Xu, et al.(2015) 做到这一点 - 如果你想探索注意力，这可能是一个有趣的起点！使用注意力已经取得了许多非常令人兴奋的结果，似乎还有更多的事情即将来临…… 注意力不是RNN研究中唯一令人兴奋的问题。例如，Kalchbrenner, et al. (2015) 的Grid LSTMs似乎非常有希望。在生成模型中使用RNN工作 - 例如Gregor, et al. (2015), Chung, et al. (2015), 或者 Bayer &amp; Osendorfer (2015) 似乎也很有趣。过去几年对于反复出现的神经网络来说是一个激动人心的时刻，即将到来的那些承诺只会更加如此！]]></content>
      <categories>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation》阅读笔记]]></title>
    <url>%2Fpost%2FLearning_Phrase_Representations_using_RNN_Encoder%E2%80%93Decoder_for_Statistical_Machine_Translation%2F</url>
    <content type="text"><![CDATA[原文链接。该论文是Sequence to Sequence学习的最早原型，论文中提出一种崭新的RNN(GRU) Encoder-Decoder算法，虽然文章属于比较旧的文章，但作为seq2seq的基础原型，还是需要阅读了解一下的。文章写的比较详细，各部分细节都有讲解。 文章的主要结构 contribution a novel RNN Encoder–Decoder 能够处理变长序列 a novel hidden unit reset gate update gate RNN Encoder–Decoder模型结构图如下： 文中作者对齐进行总体概述为： From a probabilistic perspective, this new model is a general method to learn the conditional distribution over a variable-length sequence conditioned on yet another variable-length sequence 从概率的角度来看，这个新模型是学习在另一个可变长度序列条件下的可变长度序列上的条件分布的一般方法 Encoder这部分是一个RNN单元。每个时间步，我们向Encoder中输入一个字/词（一般为向量形式），直到我们输入这个句子的最后一个字/词$X_T$，然后输入整个句子的语义向量c。由于RNN的特带你就是把前面每一步的输入信息都考虑进来，所以理论上这个c就包含了整个句子的所有信息。我们可以把当成这个句子的一个语义表示。 DecoderDecoder是另一个RNN，其被训练出来以通过预测隐藏状态$h_t$的下一个符号$y_t$来生成输出序列。计算公式如下 h_t = f(h_{t-1},y_{t-1},c)下一个序列的计算公式如下： P(y_t|y_{t-1},y_{t-2},\dots,y_1,c)=g(h_t,y_{t-1},c)Hidden Unit该部分是对各部分具体的公式讲解，实际是GRU的具体公式算法，不再此详细叙述了。 reset gate In this formulation, when the reset gate is close to 0, the hidden state is forced to ignore the pre- vious hidden state and reset with the current input only. This effectively allows the hidden state to drop any information that is found to be irrelevant later in the future, thus, allowing a more compact representation. 这段原文主要讲解了复位门的作用：有效地允许隐藏状态丢弃在将来稍后发现不相关的任何信息，从而允许更紧凑的表示。 当捕获短期依赖时，复位门活跃 update gate the update gate controls how much information from the previous hidden state will carry over to the current hidden state. 更新门控制来自先前隐藏状态的多少信息将转移到当前隐藏状态。 当捕获长期依赖时，更新门活跃 Statistical Machine Translation(SMT) Experiments这部分作者主要做了量化分析和性质分析，主要就是说他的模型怎么厉害。。。（没有具体的数值指标，翻译的还不是中英翻译，想看的话可以去看一下，就不贴实验结果了）。 future这里作者提出了可以用decoder生成的目标短语来替换原句中短语的思路，如果没记错的话，这个想法好像对后面的机器翻译有很大的指导作用。]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>seq2seq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Differentiating Concepts and Instances for Knowledge Graph Embedding》阅读笔记]]></title>
    <url>%2Fpost%2Fread_Differentiating_Concepts_and_Instances_for_Knowledge_Graph_Embedding%2F</url>
    <content type="text"><![CDATA[论文获取地址。这篇文章最大的亮点就是把concept映射为一个球面，然后把instance映射为一个向量，通过这种空间关系来进行embedding。如果instance和concept满足InstanceOf的关系，则instance应该在球内；如果两个concept满足SubClassOf的关系，则一个球会在另一个球面内。 conceptA concept is a fundamental category of existence (Rosch, 1973) and can be reified by all of its actual or potential instances.Concepts, which represent a group of different instances sharing common properties, are essential information in knowledge representation. drawback of the previous method ignore to distinguish between concepts and instances will lead to two drawbacks: Insufficient concept representation： cannot explicitly represent the difference between concepts and instances Lack transitivity of both isA relations: instanceOf and subClassOf (generally known as isA)isA relations exhibit transitivity contributions the first to propose and formalize the problem of knowledge graph embedding which differentiates between concepts and instances a novel knowledge embedding method named TransC state-of-the-art on link prediction and triple classification Translation-based ModelsTransE triple (h, r, t) should satisfy h + r ≈ t loss function:$f_r(h,t) = ||h + r - t||^2_2$ suitable for 1-to-1 relations TransH It regards a relation vector r as a translation on a hyperplane with $w_r$ as the normal vector. loss function:$f_r(h,t) = ||h_{\bot} + r - t_{\bot}||^2_2$，其中$h_{\bot}=h-w^{\top}_r h w_r$，$t_{\bot}=t-w^{\top}_r t w_r$ suitable for 1-to-N, N-to-1, and N-to-N relations TransR/CTransR addresses the issue in TransE and TransH that some entities are similar in the entity space but comparably different in other specific aspects. loss function:$f_r(h,t) = ||M_rh +r -M_rt||^2_2$，$M_r$ for each relation r TransD considers the different types of entities and relations at the same time loss function:$f_r(h,t) = ||h_{\bot} + r - t_{\bot}||^2_2$，$h_{\bot} = M_{rh}h$和$t_{\bot} = M_{rt}t$，$M_{r,e}$ for each relation-entity pair (r, e) Bilinear ModelsRESCAL the first bilinear model It associates each entity with a vector to capture its latent semantics. Each relation is represented as a matrix which models pairwise interactions between latent factors. External Information Learning Models textual information entity descriptions Problem Formulation这部分中作者详细介绍了知识图谱的组成部分：概念和实例集、关系集（包括instanceOf、subClassOf和instance relation），三元组集（按照关系集同样分为三个部分）。为了能表达is A关系的传递性，作者将instanceOf和subClassof两种关系进行了精心的设计，也是该论文的重点。 For each concept c ∈ C, we learn a sphere s(p, m) with $p \in R^k$ and m denoting the sphere center and radius. TranCSpecifically, TransC encodes each concept in knowledge graph as a sphere and each instance as a vector in the same semantic space. InstanceOf Triple Representationloss function：$f_e(i,c) = ||i-p||_2 - m$，当该函数大于0时进行优化，使其小于零。 SubClassOf Triple Representation 如图所示，其中子图（a）为目标状态。两个概念的的圆心距离：$d = ||p_i - p_j||_2$。需要做到的就是$d-(m_j -m_i) \leq 0$并且$ (m_j &gt; m_i)$。 Relational Triple Representation这部分按照TranE的思路进行处理，$||h+r-t||^2_3$ train modelmargin based loss详解unit and bern Regarding the strategy of constructing negative labels, we use “unif” to denote the traditional way of replacing head or tail with equal probability, and use “bern.” to denote reducing false negative labels by replacing head or tail with different probabilities. the following research directions find a more expressive model instead of spheres to represent concepts A concept may have different meanings in different triples. use several typical vectors of instances as a concept’s centers to represent different meanings of a concept.]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Discourse-Level Named Entity Recognition and Relation Extraction Dataset for Chinese Literature Text 阅读笔记]]></title>
    <url>%2Fpost%2FREAD_A_Discourse-Level_Named_Entity_Recognition_and_Relation_Extraction_Dataset_for_Chinese_Literature_Text%2F</url>
    <content type="text"><![CDATA[该论文最主要的贡献就是这个数据，数据集地址。论文中提到的标标签过程也是一个创新点，运用了启发式和机器辅助标标签，这样可以提高准确度并减少标注人员工作。 contribution provide a new dataset for joint learning of NER and RE for Chinese literature text the proposed dataset is based on the discourse level which provides additional context information introduce some widely used models to conduct experiments tagging processtwo methods:one is a heuristic tagging method and another is a machine auxiliary tagging method. Step 1: First Tagging Processfind a problem of data inconsistency. Step 2: Heuristic Tagging Based on Generic disambiguating Rules For example, remove all adjective words and only tag “entity header” . re-annotate all articles and correct all inconsistency entities and relations based on the heuristic rules. Step 3: Machine Auxiliary Tagging The core idea is to train a model to learn annotation guidelines on the subset of the corpus and produce predicted tags on the rest data. CRF tagging set Annotation FormatEntityEach entity is identified by T tag, which takes several attributes. Id: a unique number identifying the entity within the document. It starts at 0, and is incremented every time a new entity is identified within the same document. Type: one of the entity tags. Begin Index: the begin index of an entity. It starts at 0, and is incremented every character. End Index: the end index of an entity. It starts at 0, and is incremented every character. Value: words being referred to an identifiable object. RelationEach relation is identified by R tag, which can take several attributes: Id: a unique number identifying the relation within the document. It starts at 0, and is incremented every time a new relation is identified within the same document. Arg1 and Arg2: two entities associated with a relation. Type: one of the relation tags.]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>NER</tag>
        <tag>RE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas的数据类型操作]]></title>
    <url>%2Fpost%2FPandas_data_type_manipulation%2F</url>
    <content type="text"><![CDATA[在原文链接中摘抄出部分信息作为记录形成本文。 数据类型 Pandas dtype Python 类型 NumPy 类型 用途 object str string_, unicode_ 文本 int64 int int_, int8, int16, int32, int64, uint8, uint16, uint32, uint64 整数 float64 float float_, float16, float32, float64 浮点数 bool bool bool_ 布尔值 datetime64 NA NA 日期时间 timedelta[ns] NA NA 时间差 category NA NA 有限长度的文本值列表 数据类型操作 使用df.dtypes可以显示数据所有列的类型 df.info（） 函数可以显示更有用的信息 使用 astype() 函数使用条件 数据是干净的，可以简单地解释为一个数字 你想要将一个数值转换为一个字符串对象 如果数据具有非数字字符或它们间不同质（homogeneous），那么 astype() 并不是类型转换的好选择。你需要进行额外的变换才能完成正确的类型转换。 使用方式为了真正修改原始 dataframe 中数据类型，记得把 astype() 函数的返回值重新赋值给 dataframe，因为 astype() 仅返回数据的副本而不原地修改。 参考链接 https://juejin.im/post/5acc36e66fb9a028d043c2a5]]></content>
      <categories>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>pandas</tag>
        <tag>python</tag>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬取中文网页时中文字符变英文的解决方法]]></title>
    <url>%2Fpost%2Fsolution_of_python_for_Chinese_characters_to_become_English_when_crawling_Chinese_web_pages%2F</url>
    <content type="text"><![CDATA[使用python的scrapy爬取网页时，源代码中的中文字符在爬取下来后变成了英文字符。 问题举例例如，原网页为： 爬取结果为： 解决方法修改请求头：在settings.py文件中找到下属代码： # Override the default request headers: #DEFAULT_REQUEST_HEADERS = { # 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', # 'Accept-Language': 'en', #} 改为： # Override the default request headers: DEFAULT_REQUEST_HEADERS = { 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Language': 'zh-CN', } 修改结果展示： 参考链接 https://blog.csdn.net/wuqili_1025/article/details/79690103]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python动态网页爬取之安装docker和splash]]></title>
    <url>%2Fpost%2FPython_dynamic_web_crawler_installed_docker_and_splash%2F</url>
    <content type="text"><![CDATA[利用python进行动态网页爬取时，在安装docker和splash时踩过的坑，记录了一下自己的安装过程。用的系统是mac os。 安装scrapy-splash 利用pip安装scrapy-splash库：$ pip install scrapy-splash 安装Docker==下面👇这样安不下去了== 如果是Mac的话需要使用brew安装，如下：brew install docker 报错： Error: Failure while executing; `git config --local --replace-all homebrew.private true` exited with 1. 解决方法： xcode-select --install 然后在执行： brew install docker 再继续： service docker start 报错： -bash: service: command not found上网上查一堆乱七八糟的解决方式，该路径啥的，真的不想改路径，怕把其他的改崩了。最后放弃这种方式，如果有兴趣也可以尝试解决。 ==尝试如下安装DOCKER方法== 去官网下载这种方法下载docker客户端需要从服务器下载，自己电脑下载12k/s，简直慢死了。 拉取镜像(pull the image)：docker pull scrapinghub/splash 用docker运行scrapinghub/splash： docker run -p 8050:8050 scrapinghub/splash 在浏览器中输入localhost:8050 ==安装成功== 参考链接 http://www.morecoder.com/article/1001249.html https://www.jianshu.com/p/e54a407c8a0a]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>splash</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用python3读写csv文档]]></title>
    <url>%2Fpost%2Fread%20the%20CSV%20document%20using%20python3%2F</url>
    <content type="text"><![CDATA[对于大多数的CSV格式的数据读写问题，都可以使用 csv 库。 例如：假设你在一个名叫stocks.csv文件中有一些股票市场数据，就像这样： Symbol,Price,Date,Time,Change,Volume "AA",39.48,"6/11/2007","9:36am",-0.18,181800 "AIG",71.38,"6/11/2007","9:36am",-0.15,195500 "AXP",62.58,"6/11/2007","9:36am",-0.46,935000 "BA",98.31,"6/11/2007","9:36am",+0.12,104800 "C",53.08,"6/11/2007","9:36am",-0.25,360900 "CAT",78.29,"6/11/2007","9:36am",-0.23,225400 csv文档的读取1. 常规读取下面向你展示如何将这些数据读取为一个元组的序列： import csv with open('stocks.csv') as f: f_csv = csv.reader(f) headers = next(f_csv) for row in f_csv: # Process row ... 在上面的代码中， row 会是一个列表。因此，为了访问某个字段，你需要使用下标，如 row[0]访问Symbol， row[4] 访问Change。==这样可以通过外建字典来存储读出的csv数据。== 2. 命名元组由于这种下标访问通常会引起混淆，你可以考虑使用==命名元组==。例如： from collections import namedtuple with open('stock.csv') as f: f_csv = csv.reader(f) headings = next(f_csv) Row = namedtuple('Row', headings) for r in f_csv: row = Row(*r) # Process row ... 它允许你使用列名如 row.Symbol 和 row.Change 代替下标访问。 需要注意的是这个只有在列名是合法的Python标识符的时候才生效。如果不是的话， 你可能需要修改下原始的列名(如将非标识符字符替换成下划线之类的)。 3. 字典另外一个选择就是将数据读取到一个字典序列中去。可以这样做： import csv with open('stocks.csv') as f: f_csv = csv.DictReader(f) for row in f_csv: # process row ... 在这个版本中，你可以使用列名去访问每一行的数据了。比如，row[&#39;Symbol&#39;] 或者 row[&#39;Change&#39;]。 fieldnames 是dict_reader的一个属性，表示CSV文档的数据名称。可以通过f_csv.fieldnames来访问数据名称那一行。 CSV文件写入为了写入CSV数据，你仍然可以使用csv模块，不过这时候先创建一个 writer 对象。例如: headers = ['Symbol','Price','Date','Time','Change','Volume'] rows = [('AA', 39.48, '6/11/2007', '9:36am', -0.18, 181800), ('AIG', 71.38, '6/11/2007', '9:36am', -0.15, 195500), ('AXP', 62.58, '6/11/2007', '9:36am', -0.46, 935000), ] with open('stocks.csv','w') as f: f_csv = csv.writer(f) f_csv.writerow(headers) f_csv.writerows(rows) 如果你有一个字典序列的数据，可以像这样做： headers = ['Symbol', 'Price', 'Date', 'Time', 'Change', 'Volume'] rows = [{'Symbol':'AA', 'Price':39.48, 'Date':'6/11/2007', 'Time':'9:36am', 'Change':-0.18, 'Volume':181800}, {'Symbol':'AIG', 'Price': 71.38, 'Date':'6/11/2007', 'Time':'9:36am', 'Change':-0.15, 'Volume': 195500}, {'Symbol':'AXP', 'Price': 62.58, 'Date':'6/11/2007', 'Time':'9:36am', 'Change':-0.46, 'Volume': 935000}, ] with open('stocks.csv','w') as f: f_csv = csv.DictWriter(f, headers) f_csv.writeheader() f_csv.writerows(rows) 其中f_csv.writeheader()也可以替换成f_csv.writerow(dict(zip(headers, headers))) 参考链接 https://python3-cookbook.readthedocs.io/zh_CN/latest/c06/p01_read_write_csv_data.html https://blog.csdn.net/guoziqing506/article/details/52014506]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>文件读取</tag>
        <tag>csv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neo4j初始化节点显示设置]]></title>
    <url>%2Fpost%2FNeo4j_initializes_the_node_display_Settings%2F</url>
    <content type="text"><![CDATA[问题描述：neo4j中有默认的初始化节点显示设置为300个节点，如果想要显示的节点多于300个，则会只显示300个，并给予以下提示语句： Not all return nodes are being displayed due to Initial Node Display setting. Only 300 of 300 nodes are being displayed. 解决方法：在如图所示initial Node Display处可以修改，在此处修改为300000.]]></content>
      <categories>
        <category>Neo4j</category>
      </categories>
      <tags>
        <tag>Neo4j</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Bidirectional LSTM-CRF Models for Sequence Tagging》阅读笔记]]></title>
    <url>%2Fpost%2Fread_Bidirectional_LSTM-CRF_Models_for_Sequence_Tagging%2F</url>
    <content type="text"><![CDATA[这篇论文可以作为一个RNN和LSTM学习的一个例子来看，有利于新手对LSTM的理解。对于NER的处理主要是作为一个序列标注问题。但是作为经典文章还是可以读一读了解一下的。 在本篇论文中，作者提出了4种模型：LSTM、BI-LSTM、LSTM-CRF和BI-LSTM-CRF。 contribution(贡献) 作者在NLP标注数据集上系统的对比了以上四个模型； 作者是首先提出把BI-LSTM-CRF模型用于NLP序列标注，并且达到了state-of-the-art的水平； 作者展示了BI-LSTM-CRF是robust，并且极少依赖于词向量。 model(模型)LSTM首先，作者先介绍了RNN的结构和工作原理，如图： 其中输入为句子：EU rejects German call to boycott British lamb。输出为标签：B-ORG O B-MISC O O O B-MISC O O，其中B-，I-表示实体开始和中间位置。标签种类为：other (O)和四种实体标签：Person (PER), Location (LOC), Organization (ORG), and Miscellaneous (MISC). 输入层表示在时间步 t 的特征。它们可以是 one-hot-encoding 的词特征，稠密或者稀疏的向量特征。输入层与特征有相同大小的维度。输出层表示在时间步 t 的标签上的概率分布，维度与标注数量相同。相比前馈神经网络，RNN 引入前一个隐藏状态和当前隐藏状态的结合，因此可以储存历史信息。 涉及公式为： 其中，U，W，V都是权重，函数f，g分别为sigmoid和softmax函数。 接下来，作者展示了LSTM的结构和原理，如图： 公式： 其中，σ是逻辑sigmoid函数，i, f, o 和 c分别是输入门，忘记门，输出门和细胞向量，所有的大小都和向量h一样。w权重的含义如其下表所示。 LSTM序列标注模型如图所示： 其中，中间的画斜线的格子即为图2中所示部分。 Bidirectional LSTM(双向LSTM)作者展示了双向LSTM的结构，如图所示： 双向LSTM网络可以有效利用过去特征和未来特征。在作者的实现中，对于整个句子的前向和后向操作，作者只需要在每个句子开始时将隐藏状态重置为0。作者采用批处理，使得可以同时处理多个句子。 CRF使用邻居标记信息预测当前标记有两种不同的方法： 预测每个时间步长的标签分布，然后使用波束式解码来找到最优的标签序列，代表方法：MEMMs 注重句子层次而不是个体位置，代表方法：CRF，输入和输出是直接相连的；如图： 研究表明，CRFs一般能够产生更高的标签精度。 LSTM-CRF作者展示了LSTM-CRF的结构，如图： 这网络可以有效地通过 LSTM 利用过去的输入特征和通过 CRF 利用句子级的标注信息。图中CRF层由连接连续输出层的线表示。CRF层有一个状态转移矩阵作为参数。 公式为： 函数f为网络的输出分数，[x]为输入， [fθ]i,t 为带有参数θ（句子x，第i 个标签，第t个单词）的网络输出； [A]i,j为转移分数，从连续的时间步i状态到j状态的转移分数。注意，该转换矩阵是位置无关的。 BI-LSTM-CRF作者展示了BI-LSTM-CRF的结构，如图所示： 作者在实验中展示了额外的未来特征可以提高标签的准确率。 训练过程本文使用的所有模型都共享一个通用SGD前向和后向训练过程。作者展示了BI-LSTM-CRF的算法，如图 作者设置了批次大小为100。 实验data作者在以下三个数据集上测试自己的模型：Penn TreeBank (PTB) POS tagging, CoNLL 2000 chunking, and CoNLL 2003 named entity tagging.数据集信息展示如下： Features作者从三个数据集中提取出其公共特征。特征可以分为拼写特征和上下文特征。最终，作者对于POS（词性标注）、chunking（组块）和NER（命名实体识别）分别提取401K，76K和341K个特征。 spelling features（拼写特征）除了小写字母特征之外，我们提取给定单词的以下特征。 context featurs（上下文特征）对于单词特征，作者使用unigram和bi-grams特征。对于在CoNLL2000数据集中的POS特征和在CoNLL2003数据集中的 POS &amp; CHUNK特征，作者使用了unigram，bi-gram和tri-gram特征。 词向量词向量在改进序列标注任务的表现方面起着至关重要的作用，我们使用 130K 词汇并且每个词汇的词向量维度是 50 维。我们将 one-hot-encoding词表示替换每个词对应的词向量。 Features connection tricks我们可以将拼写和上下文特征与单词特征一样对待。这样网络的输入包括单词，单词的拼写和上下文特征。然而，==我们发现将拼写和上下文特征与输出直接连接可以加速训练过程，同时也能保持标注的准确率，==如下图所示： 我们注意到，这种特征的使用与使用的最大熵特征类似。区别在于采用特征三列技术可能会发生特征冲突。由于序列标注数据集中的输出标签小于语言模型（通常为数十万），所以我们可以在特征和输出之间建立完整的连接，以避免潜在的特征冲突。 结果在相同的数据集上分别训练LSTM，BI-LSTM，CRF，LSTM-CRF和BI-LSTM-CRF模型，并且采用两种方式初始化word embedding：随机和Senna方式。模型的训练速率为0.1，隐藏层数量为300.不同模型在不同word embedding下的结果如表2所示，同时列出了之前最好模型Cov-CRF。 与Cov-CRF比较 实验中设置了3个基准模型，分别为LSTM、BI-LSTM和CRF，结果中LSTM在三个数据集中效果最差，BI-LSTM跟CRF在POS和chunking中效果接近，但是在NER中后者要优于前者。有趣的是表现最好的模型BI-LSTM-CRF相对于Cov-CRF来说对Senna embedding的依赖程度更小。 (robustness)模型鲁棒性 为验证模型的鲁棒性，对不同模型只采用word feature特征进行训练，训练结果如表3，括号中数字表示相比于全部特征，模型的结果下降数值。 与其他系统的比较 这里就不贴图了，总之就是阐述作者自己模型好。 结论总之作者的模型是基于之前模型的一些改进，主要运用了IBI-LSTM和CRF的结合。 论文下载地址]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>NER</tag>
        <tag>LSTM</tag>
        <tag>BI-LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CN-DBpedia A Never-Ending Chinese Knowledge Extraction SystemCN-DBpedia System]]></title>
    <url>%2Fpost%2Fessay%2F</url>
    <content type="text"><![CDATA[前言 本篇论文为2016年的一篇论文，主要介绍了作者构建中文知识图谱所遇到的一些问题和解决方法。 challenge 如何降低人力成本？ 如何保持知识库的新鲜度？ 贡献 在构建中文知识库中降低了人力成本： 重复利用已经存在的本体论 提出了一个不用人工监督的端到端的深度学习模型 提出了一个智能主动更新策略 系统结构 提高知识库质量： Normalization： normalize the attributes and values Enrichment：reuse the ontology Correction：two steps error detection: rule-based detection based on user feedbacks error correction crowd-sourcing 降低人力成本这部分作者采用了两种方法： 重复利用已经存在在知识库的本体论和类型化的中文实体 构建一个端到端提取器 Cross-Lingual Entity Typing（跨语言的实体类型） 第一步是通过用英文DBpedia类型来类型化中文实体。为了达到这个目的，作者提出了如下系统：系统建立了监督层次分类模型，系统输入为没有标记类型的中文实体，输出为在DB中所有有效的英文类型。作者将中文实体与共享相同中文标签名称的英语实体配对，这样中文实体以及配对英语实体的类型自然是标记样本。 用上述方法得到的训练集可能出现下面一些问题： 英文DBpedia实体类型在许多情况下可能不完全； 英文DBpedia实体类型在许多情况下可能是错误的； 中英文链接可能出错； 中文实体的特征通常不完整。 为了解决以上问题，作者提出了两种方法： 完善英文DBpedia实体类型； 设计一个过滤步骤来剔除错误样本。 infobox completion Infobox completion is a task to extract object for a given pair of entity and predicate from encyclopedia articles. 作者建模了一个seq2seq模型，输入为包含tokens的自然语言句子，输出为每个token的标签。对于标签为0或1。 对于建立一个有效的提取器有以下两个关键： 如何构建训练集：作者采用远程监督方法（利用Wikipedia） 如何选取期望的提取模型：LSTM-RNN，如图所示 知识库更新作者采用动态更新：识别新实体或可能包含新事实的旧实体 作者根据以下两方面来辨别这些实体： 近期热点新闻中提及的实体 在搜索引擎的流行搜索关键字或其他流行网页中提到的实体 对于如何从新闻标题和搜素指令中提取实体名字，作者采用简单的词分割方法，从百科全书中判断其是否为实体，并提出IDF值低的分割子串。 统计数据 论文下载链接]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
        <tag>机器学习</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ontology reasoning with deep neural networks]]></title>
    <url>%2Fpost%2Fessay%2F</url>
    <content type="text"><![CDATA[Ontology reasoning with deep neural networks（基于深度神经网络的本体推理）前言 本论文属于知识图谱的下游任务，也就是知识图谱的应用的一个例子。这篇论文的方法根据作者描述RRN是第一个基于深度学习的全面本体推理方法。 目标获得一个可以在不同的场景进行有效推理的模 问题描述基于机器学习的推理文章通常假设了一个特定的应用案例：自然语言或视觉输入的推理。作者采用一个不同的方法：将正式的推理问题作为起点。对于特定的问题选择：选择一种在表现力与另一方面复杂性之间取得适当平衡的方法通常是明智的。OWL RL本体推理是指一种常见的场景，在这种场景中，用于推理的推理规则（在此上下文中称为本体）与我们寻求推理的事实信息一起指定。 本体推理是一种非常灵活的工具，它允许对大量不同的场景进行建模，因此满足了我们对适用于各种应用的系统的需求。==首先引出了什么是本质推理，然后进一步阐述为什么要用机器学习== 今天用于推理的大多数KRR形式都植根于符号逻辑,这些方法在实践中会遇到许多问题：例如处理不完整，冲突或不确定数据的困难机器学习模型通常具有高度可扩展性，更能抵抗数据中的干扰，并且即使所提供的形式是错误的也能够提供预测。 作者的目标是通过采用尖端的深度学习技术，目标是在近似于形式方法的高度期望（理论）属性和另一方面利用机器学习的稳健性之间管理平衡行为。 对于用于推理的知识图谱：作者采用的是由个体、类和二元关系组成的信息构成，其中个体对应于顶点，关系对应于被标记的有向边缘，类对应于二进制顶点标签。关系是主体和客体之间的关系或者个人和类之间的关系。这与关系学习不同：在关系学习的背景下，知识图通常通过将类视为个人以及将成员视为普通关系来简化。然而，就作者的目的而言，明确区分类和关系是很重要的，因为在用于推理的知识图谱中类和关系可能不同。 模型总览整个模型是以RRN为基础进行构建的，每个RRN都针对特定的本体进行训练。当训练模型应用于一组特定的事实时，它分为如下两个步骤： 它为所有的步骤生成矢量表示，也就是嵌入在所考虑数据中出现的个体。 它仅基于这些生成向量计算查询预测 在图中， a中它考虑一个事实三元组，并根据数据集重复多次。 b中它每读取一个事实就获取三元组中的个体潜入，并将他们的反馈送入更新层，该层产生已提供的嵌入的更新版本，然后将其存储在前一个版本的位置。 c中从随机生成的向量开始，逐步更新嵌入，以便对关于它们所代表的个体的事实和推论进行编码。 评估作者在四个不同的数据集上训练和评估了RRN，其中两个是人工生成的玩具数据集，两个是从现实世界的数据库中提取的。这样做的原因： 玩具问题具有很大的优势，即很明显某些推论是多么困难，从而为我们提供了对模型能力的相当好的印象。 在现实环境中评估方法当然是性能不可或缺的衡量标准 作者为了评估真实世界数据的RRN模型，还从从两个著名的知识库DBpedia和Claros中提取了数据集。 结果 RRN能够有效地编码提供的关于类和关系的事实 对于关系的推理，可以看到DBpedia的准确度略低于98.9％，而其他数据集中的可导出关系在所有情况中至少99.6％被正确预测。 可以预测该模型在预测可推断类别方面比在关系方面表现更好，因为大多数这些都是仅依赖于单个三元组的推论。 为了评估作者提出的KRR方法常常遇到的问题，作者进行了如下实验： 对于缺少信息的问题，作者随机删除了一个无法通过每个样本的符号推理推断出的事实，并检查模型是否能够正确地重建它。结果：对于DBpedia来说，33.8％的失踪三元组就是这种情况，而对于Claros来说，38.4％被正确预测 对于冲突的问题，作者通过在每个测试样本中随机选择一个事实来测试模型解决冲突的能力，并添加相同的否定版本作为另一个事实。对于DBpedia，RRN正确解决了88.4％的引入冲突，而对于Claros，它甚至达到了96.2％。然而，最重要的是，对于任何一个损坏的数据集，之前报告的总精度都没有下降超过0.9。 所有RRN的查询预测都完全基于它为各个数据集中的个体生成的嵌入，这就是为什么仔细研究这样一组嵌入向量是有益的。 思考本论文属于知识图谱的下游任务，也就是知识图谱的应用的一个例子。这篇论文的方法根据作者描述RRN是第一个基于深度学习的全面本体推理方法。但是具体的操作方法论文中写的比较清晰，感觉自己是理解了。重点就是对于个体的嵌入表示，如果类比的话就是词向量，作者通过不断的处理更新这个词向量，最后通过所获的词向量进行推理。并且从这篇文章中可以看到作者使用的知识图谱和我之前在弄的关系三元组有所区别。 论文下载链接]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
        <tag>深度学习</tag>
        <tag>Ontology</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初次见面，你好NYSDY！]]></title>
    <url>%2Fpost%2Fessay%2F</url>
    <content type="text"></content>
  </entry>
</search>
