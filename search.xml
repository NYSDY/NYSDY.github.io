<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[2016 TransG : A Generative Model for Knowledge Graph Embeddingé˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FTransG_%3A_A_Generative_Model_for_Knowledge_Graph_Embedding%2F</url>
    <content type="text"><![CDATA[è®ºæ–‡ä¸‹è½½åœ°å€ è§£å†³é—®é¢˜multiple relation semanticsï¼ˆå¤šé‡å…³ç³»è¯­ä¹‰ï¼‰ï¼šä¸€ä¸ªå…³ç³»å¯èƒ½å…·æœ‰ä¸Žå¯¹åº”çš„ä¸‰å…ƒç»„å…³è”çš„å®žä½“å¯¹æ­ç¤ºçš„å¤šç§å«ä¹‰ã€‚ è¿™é‡Œä»¥TransEçš„å¯è§†åŒ–ä¸ºä¾‹ï¼Œè¡¨æ˜Žï¼šç‰¹å®šå…³ç³»æœ‰ä¸åŒçš„èšç±»ï¼Œå¹¶ä¸”ä¸åŒçš„èšç±»è¡¨ç¤ºä¸åŒçš„æ½œåœ¨è¯­ä¹‰ï¼Œè¯å®žäº†è¯¥é—®é¢˜çš„å­˜åœ¨æ€§ã€‚ è¯¥çŽ°è±¡äº§ç”ŸåŽŸå›  äººä¸ºç®€åŒ– çŸ¥è¯†åº“ç­–å±•äººä¸èƒ½æ¶‰åŠå¤ªå¤šç›¸ä¼¼å…³ç³»ï¼Œå› æ­¤å°†å¤šä¸ªç›¸ä¼¼å…³ç³»æŠ½è±¡ä¸ºä¸€ä¸ªç‰¹å®šå…³ç³»æ˜¯ä¸€ç§å¸¸è§çš„æŠ€å·§ çŸ¥è¯†æ€§è´¨ è¯­è¨€å’ŒçŸ¥è¯†è¡¨ç¤ºå½¢å¼å¸¸å¸¸æ¶‰åŠä¸æ˜Žç¡®çš„ä¿¡æ¯ã€‚ çŸ¥è¯†çš„æ¨¡ç³Šæ€§æ„å‘³ç€è¯­ä¹‰ä¸Šçš„æ··åˆ TransGåˆ©ç”¨è´å¶æ–¯éžå‚æ•°æ— é™æ··åˆæ¨¡åž‹é€šè¿‡ä¸ºå…³ç³»ç”Ÿæˆå¤šä¸ªç¿»è¯‘ç»„ä»¶æ¥å¤„ç†å¤šä¸ªå…³ç³»è¯­ä¹‰]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>2016</tag>
        <tag>ACL</tag>
        <tag>KGE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2016 Knowledge Graph Completion with Adaptive Sparse Transfer Matrixé˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2F2016_Knowledge_Graph_Completion_with_Adaptive_Sparse_Transfer_Matrix%2F</url>
    <content type="text"><![CDATA[ä¸ºè§£å†³heterogeneityå’Œimbalanceé—®é¢˜ï¼Œä½œè€…é’ˆå¯¹åŒä¸€å…³ç³»é“¾æŽ¥å®žä½“å¯¹çš„æ•°é‡å’ŒåŒä¸€å…³ç³»ä¸åŒå¤´å°¾å®žä½“æ•°é‡ï¼Œåˆ†åˆ«è®¾è®¡äº†ä¸åŒç¨€ç–ç¨‹åº¦çš„è½¬ç§»çŸ©é˜µã€‚å­˜åœ¨ç¼ºç‚¹ï¼šå¹¶æ²¡æœ‰åŒæ—¶è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ã€‚ è®ºæ–‡ä¸‹è½½åœ°å€ Problem Statement heterogeneityï¼ˆå¼‚è´¨æ€§ï¼‰ï¼šä¸€äº›å…³ç³»é“¾æŽ¥äº†å¾ˆå¤šå®žä½“å¯¹ï¼Œå¦ä¸€äº›æ²¡æœ‰ imbalanceï¼ˆä¸å¹³è¡¡ï¼‰ï¼š åœ¨ä¸€ç§å…³ç³»ä¸­ï¼Œå¤´å®žä½“å’Œå°¾å®žä½“çš„æ•°é‡ä¸åŒ Contribution ä½œè€…æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•è€ƒè™‘äº†å…ˆå‰æ¨¡åž‹ä¸­æœªä½¿ç”¨çš„å¼‚è´¨æ€§å’Œä¸å¹³è¡¡æ€§ï¼Œä»¥åµŒå…¥çŸ¥è¯†å›¾æ¥å®Œæˆå®ƒä»¬ï¼› ä½œè€…çš„æ–¹æ³•é«˜æ•ˆä¸”å‚æ•°è¾ƒå°‘ï¼Œå› æ­¤å¾ˆå®¹æ˜“æ‰©å±•åˆ°å¤§è§„æ¨¡çŸ¥è¯†å›¾ï¼› ä½œè€…ä¸ºè½¬ç§»çŸ©é˜µæä¾›äº†ä¸¤ç§ç¨€ç–æ¨¡å¼ï¼Œå¹¶åˆ†æžäº†å®ƒä»¬çš„ä¼˜ç¼ºç‚¹ï¼› åœ¨ä¸‰å…ƒç»„åˆ†ç±»å’Œé“¾æŽ¥é¢„æµ‹ä»»åŠ¡ä¸­ï¼Œä½œè€…çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ Sparse Matrixå®šä¹‰ç¨€ç–çŸ©é˜µæ˜¯æŒ‡å¤§å¤šæ•°æ¡ç›®ï¼ˆentriesï¼‰ä¸ºé›¶çš„çŸ©é˜µã€‚ é›¶å…ƒç´ å çŸ©é˜µå…ƒç´ æ€»æ•°çš„æ¯”ä¾‹ç§°ä¸ºç¨€ç–åº¦ï¼ˆsparse degreeï¼‰ã€‚ ç±»åˆ« ç»“æž„åŒ–çš„ éžç»“æž„åŒ–çš„ ä¸¤è€…é‡è¦åŒºåˆ« ç»“æž„åŒ–æ¨¡å¼æœ‰åˆ©äºŽçŸ©é˜µå‘é‡ä¹˜ç§¯è¿ç®—ã€‚ éžç»“æž„åŒ–å¾€å¾€å¯ä»¥å¸¦æ¥æ›´å¥½çš„å®žéªŒç»“æžœï¼šç”±äºŽæ›´åŠ çµæ´»åœ°è¿œèŒƒå›´çº¿æ€§ç»„åˆã€‚ Sparse Matrix vs Low-Rank Matrixç‰¹ç‚¹ low-rankçŸ©é˜µå¼ºåˆ¶ä¸€äº›å˜é‡è¦æ»¡è¶³ç‰¹å®šçš„çº¦æŸï¼Œå› æ­¤ï¼ŒçŸ©é˜µMæ— æ³•è‡ªç”±åœ°è¿›è¡Œèµ‹å€¼ã€‚ sparseçŸ©é˜µæ˜¯ä½œè€…ä»¤å…¶ä¸­çš„éƒ¨åˆ†å…ƒç´ å€¼ä¸º0ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸æ”¹å˜å®ƒçš„å€¼ï¼Œå…¶ä»–çš„éžé›¶å€¼è¿›è¡Œè®­ç»ƒã€‚ å¯¹æ¯” ç¨€ç–çŸ©é˜µæ¯”ä½Žç§©çŸ©é˜µæ›´çµæ´»ï¼Œå¯ä»¥æœ‰æ›´å¤§çš„è‡ªç”±åº¦ï¼šä½¿ç”¨ä½Žç§©çŸ©é˜µï¼Œé‚£ä¹ˆçŸ©é˜µçš„è‡ªç”±åº¦ä¼šå—åˆ°ä¸¥æ ¼çš„ç§©é™åˆ¶ã€‚ç„¶è€Œï¼Œsparse matrixçš„ç¨€ç–æ€§åªæ˜¯æŽ§åˆ¶çŸ©é˜µå…ƒç´ ä¸­é›¶å…ƒç´ çš„ä¸ªæ•°ã€‚ ç¨€ç–çŸ©é˜µæ¯”ä½Žç§©çŸ©é˜µæ›´æœ‰æ•ˆçŽ‡ï¼šåªæœ‰éžé›¶æ¡ç›®å‚ä¸Žè®¡ç®—ï¼Œæžå¤§åœ°å‡å°‘äº†è®¡ç®—é‡ ModelTranSpare(share) è§£å†³heterogeneityé—®é¢˜ ç‰¹ç‚¹ è½¬ç§»çŸ©é˜µçš„ç¨€ç–åº¦ç”±å…³ç³»é“¾æŽ¥çš„å®žä½“å¯¹çš„æ•°é‡ç¡®å®š å¹¶ä¸”å…³ç³»çš„ä¸¤ä¾§å…±äº«ç›¸åŒçš„è½¬ç§»çŸ©é˜µ å¯¹äºŽå¤æ‚å…³ç³»çš„è½¬ç§»çŸ©é˜µæ›´åŠ ç¨€ç– ä¸çŸ¥é“ä¸ºä»€ä¹ˆå¤æ‚è½¬ç§»çŸ©é˜µä¼šæ›´åŠ ç¨€ç–ï¼Ÿ è½¬ç§»çŸ©é˜µçš„ç¨€ç–ç¨‹åº¦ \theta_{r}=1-\left(1-\theta_{\min }\right) N_{r} / N_{r^{*}}å…¶ä¸­ï¼Œ$N_r$ä»£è¡¨é“¾æŽ¥å…³ç³»$r$çš„å®žä½“å¯¹çš„æ•°é‡ï¼Œ$r^$ä»£è¡¨é“¾æŽ¥æœ€å¤šå®žä½“å¯¹çš„å…³ç³»ï¼Œ$\theta_{\min }\left(0 \leq \theta_{\min } \leq 1\right)$æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œä»£è¡¨çŸ©é˜µ$M_{r^{}}$çš„æœ€å°ç³»æ•°ç¨‹åº¦ã€‚ ä¸ç†è§£è¿™é‡Œä¸ºä»€ä¹ˆæŠŠç¨€ç–ç¨‹åº¦å®šä¹‰æˆè¿™ä¹ˆéº»çƒ¦ï¼Œä¸ºä»€ä¸ç›´æŽ¥å®šä¹‰æˆ$\theta_{\min} N_{r} / N_{r^{*}}$ æ˜ å°„å‘é‡ \mathbf{h}_{p}=\mathbf{M}_{r}\left(\theta_{r}\right) \mathbf{h}, \quad \mathbf{t}_{p}=\mathbf{M}_{r}\left(\theta_{r}\right) \mathbf{t}TranSpare(separate) è§£å†³imbalanceé—®é¢˜ ç‰¹ç‚¹ æ¯ä¸ªå…³ç³»å…·æœ‰ä¸¤ä¸ªå•ç‹¬çš„ç¨€ç–ä¼ é€’çŸ©é˜µï¼Œä¸€ä¸ªç”¨äºŽå¤´å®žä½“ï¼Œå¦ä¸€ä¸ªç”¨äºŽå°¾å®žä½“ ç¨€ç–åº¦å–å†³äºŽé€šè¿‡å…³ç³»é“¾æŽ¥çš„å¤´ï¼ˆå°¾ï¼‰å®žä½“çš„æ•°é‡ è½¬ç§»çŸ©é˜µçš„ç¨€ç–ç¨‹åº¦ \theta_{r}^{l}=1-\left(1-\theta_{\min }\right) N_{r}^{l} / N_{r^{*}}^{l^{*}} \quad(l=h, t)å’Œshareç±»ä¼¼ï¼Œåªæ˜¯å¤´å°¾å®žä½“ä¸ç›¸åŒï¼Œå¢žåŠ læ¥ä»£è¡¨å¤´å°¾å®žä½“æ•°é‡ã€‚ æ˜ å°„å‘é‡ \theta_{r}^{l}=1-\left(1-\theta_{\min }\right) N_{r}^{l} / N_{r^{*}}^{l^{*}} \quad(l=h, t)åˆ†æ•°å‡½æ•°ä¸¤è€…çš„åˆ†æ•°å‡½æ•°ç›¸åŒå‡ä¸ºï¼š f_{r}(\mathbf{h}, \mathbf{t})=\left\|\mathbf{h}_{p}+\mathbf{r}-\mathbf{t}_{p}\right\|_{\ell_{1 / 2}}^{2}æ€»lossé‡‡ç”¨margin-based ranking loss $L=\sum_{(h, r, t) \in \Delta\left(h^{\prime}, r, t\right) \in \Delta^{\prime}}\left[\gamma+f_{r}(\mathbf{h}, \mathbf{t})-f_{r}\left(\mathbf{h}^{\prime}, \mathbf{t}^{\prime}\right)\right]_{+}$è®­ç»ƒè¿‡ç¨‹ä¸ºäº†åŠ é€Ÿè®­ç»ƒæ—¶æ”¶æ•›ä»¥åŠé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œä½œè€…ä½¿ç”¨TransEç®—æ³•çš„ç»“æžœè¿›è¡Œåˆå§‹åŒ–å®žä½“å’Œå…³ç³»çš„embeddingå‘é‡ï¼Œå¯¹äºŽè½¬åŒ–çŸ©é˜µï¼Œä½œè€…ä½¿ç”¨å•ä½çŸ©é˜µè¿›è¡Œåˆå§‹åŒ–ã€‚ä½†æ˜¯è¿™ä¸æ˜¯éžå¿…é¡»çš„ å¯¹äºŽè½¬åŒ–çŸ©é˜µ(å‡è®¾ä¸ºå•ä½çŸ©é˜µ)ï¼Œéžé›¶å‘é‡çš„ä¸ªæ•°$n z=\lfloor\theta \times n \times n\rfloor$ï¼Œç”±äºŽä½œè€…ä½¿ç”¨å•ä½å‘é‡åˆå§‹åŒ–ï¼Œæ‰€ä»¥é™¤äº†å¯¹è§’çº¿ä¸Šçš„éžé›¶å…ƒç´ ä¹‹å¤–ï¼Œå…¶ä»–éžé›¶å…ƒç´ çš„ä¸ªæ•°ä¸º$n z^{\prime}=n z-n$ï¼Œå¦‚æžœ$n z \leq n$ï¼Œé‚£ä¹ˆä½œè€…è®¾ç½®$n z^{\prime}=0$ã€‚ åœ¨æž„å»ºç»“æž„åŒ–çš„è½¬åŒ–çŸ©é˜µ$\mathbf{M}(\theta)$çš„æ—¶å€™ï¼Œä½œè€…è¦è®©$n z^{\prime}$éžé›¶å…ƒç´ å¯¹ç§°åˆ†å¸ƒåœ¨å¯¹è§’çº¿çš„ä¸¤è¾¹ï¼Œå¦‚æžœ$n z^{\prime}$ä¸èƒ½æ»¡è¶³è¦æ±‚ï¼Œé‚£ä¹ˆä½œè€…é€‰æ‹©å¦å¤–ä¸€ä¸ªæ•´æ•°ã€‚ åœ¨æž„å»ºéžç»“æž„åŒ–çš„è½¬åŒ–çŸ©é˜µ$\mathbf{M}(\theta)$çš„æ—¶å€™ï¼Œä½œè€…åªéšæœºæ•£å¸ƒ$\mathbf{M}(\theta)$ä¸­çš„$n z^{\prime}$éžé›¶å…ƒç´ ï¼ˆä½†ä¸åœ¨å¯¹è§’çº¿ä¸Šï¼‰ã€‚ åœ¨è®­ç»ƒå‰ï¼Œä½œè€…é¦–å…ˆè®¾ç½®è¶…å‚æ•°$\theta_{\min }$ï¼Œç„¶åŽè®¡ç®—æ¯ä¸ªè½¬åŒ–çŸ©é˜µçš„ç¨€ç–ç¨‹åº¦ï¼Œç„¶åŽï¼Œä½œè€…ä½¿ç”¨ç»“æž„åŒ–æˆ–éžç»“æž„åŒ–æ¨¡å¼æž„å»ºç¨€ç–è½¬åŒ–çŸ©é˜µã€‚ å®žéªŒä¸Žå¸¸è§„ä¸åŒçš„å°±æ˜¯åŠ äº†ä¸€ä¸ªå®žéªŒ]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>2016</tag>
        <tag>KGE</tag>
        <tag>AAAI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2015 TransA An Adaptive Approach for Knowledge Graph Embeddiné˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2F2015_TransA_An_Adaptive_Approach_for_Knowledge_Graph_Embeddin%2F</url>
    <content type="text"><![CDATA[è¿™ç¯‡æ–‡ç« å…·ä½“çš„å…¬å¼æ“ä½œæ¯”è¾ƒéš¾ä»¥ç†è§£ï¼Œä½†æ˜¯å¯¹äºŽæˆ‘æ¥è¯´ï¼Œå®ƒçš„æ¯ä¸€ç»´åº¦åŠ æƒå’Œæœ€åŽåˆ¤æ–­æ—¶ä¹Ÿåº”è¯¥è€ƒè™‘ä¸åŒç»´åº¦å·®å¼‚å’Œæˆ‘çš„æ€è·¯æ¯”è¾ƒåƒã€‚ è®ºæ–‡ä¸‹è½½åœ°å€ Problem Statementä¹‹å‰çš„æœºé‡ç¿»è¯‘çš„æ–¹æ³•è¿‡äºŽç®€åŒ–æŸå¤±åº¦é‡ï¼Œä½¿å¾—æ¨¡åž‹æ²¡æœ‰è¶³å¤Ÿçš„èƒ½åŠ›æ¥æ¨¡æ‹ŸçŸ¥è¯†åº“ä¸­çš„å„ç§å¤æ‚å®žä½“/å…³ç³»ã€‚ Introduction ç”±äºŽæŸå¤±åº¦é‡çš„ä¸çµæ´»æ€§ï¼Œå½“å‰åŸºäºŽç¿»è¯‘çš„æ–¹æ³•åº”ç”¨å…·æœ‰ä¸åŒåˆç†æ€§çš„çƒé¢ç­‰åŠ¿è¶…è¡¨é¢ï¼Œå…¶ä¸­ä¸‰å…ƒç»„æ›´é è¿‘ä¸­å¿ƒï¼Œä¸‰å…ƒç»„æ›´åˆç†ã€‚å¦‚å›¾1ï¼ˆaï¼‰æ‰€ç¤ºï¼Œçƒé¢ç­‰åŠ¿è¶…è¡¨é¢ä¸å¤Ÿçµæ´»ï¼Œä¸è¶³ä»¥è¡¨å¾æ‹“æ‰‘ç»“æž„ã€‚ å…¶æ¬¡ï¼Œç”±äºŽè¿‡åº¦ç®€åŒ–çš„æŸå¤±åº¦é‡ï¼Œå½“å‰åŸºäºŽç¿»è¯‘çš„æ–¹æ³•ç”¨åŒæ€§çš„æ¬§æ°è·ç¦»æ— æ³•ä½“çŽ°å‡ºå„ä¸ªç»´åº¦çš„é‡è¦æ€§ï¼Œè€Œæ˜¯å°†å„ä¸ªç»´åº¦çš„ç‰¹å¾éƒ½ç­‰åŒçœ‹å¾…ã€‚å¯¼è‡´å›¾2ä¸­å‡ºçŽ°çš„é—®é¢˜ï¼š ç”±äºŽå¯¹æ¯ä¸€ç»´åº¦å¤„ç†ç›¸åŒï¼Œä¸æ­£ç¡®çš„å®žä½“å°†è¢«åŒ¹é…ã€‚ä½†æ˜¯é€šè¿‡å¯¹ä¸åŒç»´åº¦è¿›è¡Œä¸åŒåŠ æƒå°†ä¼šé¿å…è¿™ç§ç”±äºŽè·ç¦»ç›¸åŒè¢«åŒ¹é…çš„é”™è¯¯ã€‚ Model ä½œè€…æå‡ºTransAï¼Œä¸€ç§åˆ©ç”¨è‡ªé€‚åº”å’Œçµæ´»åº¦é‡çš„åµŒå…¥æ–¹æ³•ï¼š TransAé‡‡ç”¨æ¤­åœ†å½¢è¡¨é¢è€Œä¸æ˜¯çƒå½¢è¡¨é¢ï¼šé€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¤æ‚å…³ç³»å¼•èµ·çš„å¤æ‚åµŒå…¥æ‹“æ‰‘ç»“æž„å¯ä»¥å¾—åˆ°æ›´å¥½çš„è¡¨çŽ°ã€‚ å¦‚â€œè‡ªé€‚åº”åº¦é‡æ–¹æ³•â€ä¸­æ‰€åˆ†æžçš„é‚£æ ·ï¼ŒTransAå¯ä»¥è¢«è§†ä¸ºåŠ æƒå˜æ¢ç‰¹å¾ç»´åº¦ã€‚ å› æ­¤ï¼Œæ¥è‡ªä¸ç›¸å…³å°ºå¯¸çš„å™ªå£°è¢«æŠ‘åˆ¶ã€‚ è‡ªé€‚åº”åº¦é‡åˆ†æ•°å‡½æ•°TransAé‡‡ç”¨è‡ªé€‚åº”Mahalanobisç»å¯¹æŸå¤±è·ç¦»ï¼š f_{r}(h, t)=(|\mathbf{h}+\mathbf{r}-\mathbf{t}|)^{\top} \mathbf{W}_{\mathbf{r}}(|\mathbf{h}+\mathbf{r}-\mathbf{t}|)å…¶ä¸­ |\mathbf{h}+\mathbf{r}-\mathbf{t}| \doteq\left(\left|h_{1}+r_{1}-t_{1}\right|,\left|h_{2}+r_{2}-t_{2}\right|, \ldots, | h_{n}+\right.\left.\left.r_{n}-t_{n}\right\rfloor\right)ï¼Œ$W_r$æ˜¯å¯¹åº”äºŽç‰¹å®šå…³ç³»çš„å¯¹ç§°éžè´Ÿæƒé‡çŸ©é˜µï¼Œä¹Ÿæ˜¯è‡ªé€‚åº”æƒé‡çŸ©é˜µã€‚ä¸Žä¼ ç»Ÿçš„å¾—åˆ†å‡½æ•°ä¸åŒï¼Œä½œè€…å–ç»å¯¹å€¼ï¼Œå› ä¸ºæƒ³è¦æµ‹é‡ï¼ˆh + rï¼‰å’Œtä¹‹é—´çš„ç»å¯¹æŸå¤±ã€‚åŽŸå› æœ‰ä»¥ä¸‹ä¸¤ç‚¹ï¼š ä½œè€…å°†çš„åˆ†æ•°å‡½æ•°æ‰©å±•ä¸ºè¯±å¯¼è§„èŒƒï¼š N_{r}(\mathbf{e})=\sqrt{f_{r}(h, t)}è¿™æ ·å¯ä»¥ç”¨ä»¥ä¸‹æ–¹å¼æ¥ç®€åŒ–è¿ç®—ï¼š \begin{array}{l}{\text { inequality } N_{r}\left(\mathbf{e}_{1}+\mathbf{e}_{2}\right)=\sqrt{\left|\mathbf{e}_{1}+\mathbf{e}_{2}\right|^{\top} \mathbf{W}_{\mathbf{r}}\left|\mathbf{e}_{\mathbf{1}}+\mathbf{e}_{\mathbf{2}}\right|} \leq} {\sqrt{\left|\mathbf{e}_{\mathbf{1}}\right|^{\top} \mathbf{W}_{\mathbf{r}}\left|\mathbf{e}_{\mathbf{1}}\right|}+\sqrt{\left|\mathbf{e}_{\mathbf{2}}\right|^{\top} \mathbf{W}_{\mathbf{r}}\left|\mathbf{e}_{\mathbf{2}}\right|}=N_{r}\left(\mathbf{e}_{\mathbf{1}}\right)+N_{r}\left(\mathbf{e}_{\mathbf{2}}\right)}\end{array} åœ¨å‡ ä½•ä¸­ï¼Œè´Ÿå€¼æˆ–æ­£å€¼è¡¨ç¤ºå‘ä¸‹æˆ–å‘ä¸Šçš„æ–¹å‘ã€‚è€Œåœ¨ä½œè€…çš„æ–¹æ³•ä¸­ï¼Œä½œè€…ä¸è€ƒè™‘è¿™ä¸ªå› ç´ ã€‚ è®©ä½œè€…çœ‹ä¸€ä¸‹å¦‚å›¾2æ‰€ç¤ºçš„å®žä¾‹ã€‚ å¯¹äºŽå®žä½“Goniffï¼Œå…¶æŸè€—å‘é‡çš„xè½´åˆ†é‡æ˜¯è´Ÿçš„ï¼Œå› æ­¤æ‰©å¤§è¯¥åˆ†é‡å°†ä½¿æ•´ä½“æŸå¤±æ›´å°ï¼Œè€Œè¿™ç§æƒ…å†µåº”è¯¥ä½¿æ•´ä½“æŸå¤±æ›´å¤§ã€‚ å› æ­¤ï¼Œç»å¯¹ç®—å­å¯¹ä½œè€…çš„æ–¹æ³•è‡³å…³é‡è¦ã€‚ å¯¹äºŽæ²¡æœ‰ç»å¯¹ç®—å­çš„æ•°å€¼ä¾‹å­ï¼Œå½“åµŒå…¥ç»´æ•°ä¸º2æ—¶ï¼Œæƒé‡çŸ©é˜µä¸º[0 1; 1 0]å’ŒæŸå¤±çŸ¢é‡ï¼ˆh + r - tï¼‰=ï¼ˆe1ï¼Œe2ï¼‰ï¼Œæ€»æŸå¤±ä¸º2e1e2ã€‚ å¦‚æžœe1â‰¥0ä¸”e2â‰¤0ï¼Œåˆ™ç»å¯¹æ›´å¤§çš„e2å°†å‡å°‘æ€»æŸè€—ï¼Œè¿™æ˜¯ä¸å¸Œæœ›çš„ ä»Žç­‰åŠ¿é¢çš„è§’åº¦å¯¹äºŽå…¶ä»–åŸºäºŽç¿»è¯‘çš„æ–¹æ³•ï¼Œç­‰åŠ¿è¶…æ›²é¢æ˜¯æ¬§å‡ é‡Œå¾·è·ç¦»å®šä¹‰çš„çƒä½“ï¼š \|(\mathbf{t}-\mathbf{h})-\mathbf{r}\|_{2}^{2}=\mathcal{C}å…¶ä¸­ï¼Œ$\mathcal{C}$è¡¨ç¤ºé˜ˆå€¼æˆ–ç­‰åŠ¿å€¼ã€‚ è€Œå¯¹äºŽTransAæ¥è¯´ï¼Œç­‰åŠ¿é¢æ˜¯æ¤­åœ† |(\mathbf{t}-\mathbf{h})-\mathbf{r}|^{\top} \mathbf{W}_{\mathbf{r}}|(\mathbf{t}-\mathbf{h})-\mathbf{r}|=\mathcal{C}â€‹ ç”±äºŽçŸ¥è¯†åº“æ˜¯å¤§è§„æ¨¡ä¸”éžå¸¸å¤æ‚çš„å®žé™…æƒ…å†µï¼ŒåµŒå…¥çš„æ‹“æ‰‘ç»“æž„ä¸èƒ½åƒçƒä½“é‚£æ ·å‡åŒ€åˆ†å¸ƒï¼Œå¦‚å›¾1æ‰€ç¤ºã€‚ å› æ­¤ï¼Œç”¨æ¤­åœ†å½¢æ›¿æ¢çƒé¢ç­‰åŠ¿è¶…æ›²é¢å°†å¢žå¼ºåµŒå…¥ ä»Žç‰¹å¾æƒé‡è§’åº¦TransAå¯ä»¥çœ‹åšæ˜¯å¸¦æœ‰æƒé‡çš„ç‰¹å¾å˜æ¢ï¼Œå‡è®¾æƒé‡çŸ©é˜µ$W_r$æ˜¯å¯¹ç§°çŸ©é˜µï¼Œé‚£ä¹ˆå¯ä»¥é€šè¿‡LDLåˆ†è§£å°†æƒé‡åˆ†è§£ä¸º \begin{array}{c}{\mathbf{W}_{\mathbf{r}}=\mathbf{L}_{\mathbf{r}}^{\top} \mathbf{D}_{\mathbf{r}} \mathbf{L}_{\mathbf{r}}} \\ {f_{r}=\left(\mathbf{L}_{\mathbf{r}}|\mathbf{h}+\mathbf{r}-\mathbf{t}|\right)^{\top} \mathbf{D}_{\mathbf{r}}\left(\mathbf{L}_{\mathbf{r}}|\mathbf{h}+\mathbf{r}-\mathbf{t}|\right)}\end{array}ç›¸å½“äºŽå¯¹losså‘é‡é€šè¿‡$L_r$è¿›è¡Œç‰¹å¾å˜æ¢ï¼Œå…¶ä¸­ï¼Œ$\mathbf{D}_{r}=\operatorname{diag}\left(w_{1}, w_{2}, \ldots\right)$æ˜¯ä¸€ä¸ªå¯¹è§’çŸ©é˜µï¼Œå¯¹è§’å…ƒç´ çš„å€¼å°±æ˜¯ä¸åŒåµŒå…¥ç»´åº¦çš„æƒå€¼ã€‚ å¯¹æ¯”ä¹‹å‰æ–¹æ³•ä¸Žå…³äºŽæ—‹è½¬å’Œç¼©æ”¾åµŒå…¥ç©ºé—´çš„TransRï¼ŒTransAå…·æœ‰ä¸¤ä¸ªä¼˜åŠ¿ã€‚ é¦–å…ˆï¼Œä½œè€…å¯¹ç‰¹å¾å°ºå¯¸è¿›è¡ŒåŠ æƒä»¥é¿å…å™ªéŸ³ã€‚ å…¶æ¬¡ï¼Œä½œè€…æ”¾æ¾äº†PSDæ¡ä»¶ä»¥èŽ·å¾—çµæ´»çš„è¡¨ç¤ºã€‚ ä¸Žä½¿ç”¨é¢„å…ˆè®¡ç®—çš„ç³»æ•°é‡æ–°æž„é€ å¯¹ç‰¹å¾å°ºå¯¸è¿›è¡ŒåŠ æƒçš„TransMï¼ŒTransAå…·æœ‰ä¸¤ä¸ªä¼˜åŠ¿ã€‚ é¦–å…ˆï¼Œä½œè€…ä»Žæ•°æ®ä¸­å­¦ä¹ æƒé‡ï¼Œè¿™ä½¿å¾—åˆ†æ•°å‡½æ•°æ›´å…·é€‚åº”æ€§ã€‚ å…¶æ¬¡ï¼Œä½œè€…åº”ç”¨ç‰¹å¾è½¬æ¢ï¼Œä½¿åµŒå…¥æ›´æœ‰æ•ˆ ç®—æ³•è®­ç»ƒlosså‡½æ•°ä¸ºï¼š \begin{array}{c}{\mathcal{L}=\sum_{(h, l, t) \in \Delta\left(h^{\prime}, l^{\prime}, t^{\prime}\right) \in \Delta^{\prime}}\left[\gamma+f_{r}(\mathrm{h}, \mathrm{t})-f_{r^{\prime}}\left(\mathrm{h}^{\prime}, \mathrm{t}^{\prime}\right)\right]_{+}+\lambda\left(\sum_{r \in R}\|\mathbf{W} r\|_{F}^{2}\right)+C\left(\sum_{e \in E}\|\mathbf{e}\|_{2}^{2}+\sum_{r \in R}\left\|\mathbf{r}_{2}^{2}\right\|\right)} \\ {\text { s.t. }\left[\mathbf{W}_{r}\right]_{i j} \geq 0}\end{array}ä¿è¯éžè´Ÿæ€§ï¼Œä½œè€…å°†æ‰€æœ‰å¦å®šæ¡ç›®æƒé‡çš„å€¼èµ‹ä¸º0 \mathbf{W}_{r}=-\sum_{(h, r, t) \in \Delta}\left(|\mathbf{h}+\mathbf{r}-\mathbf{t} \| \mathbf{h}+\mathbf{r}-\mathbf{t}|^{\top}\right)+\sum_{\left(h^{\prime}, r^{\prime}, t^{\prime}\right) \in \Delta^{\prime}}\left(\left|\mathbf{h}^{\prime}+\mathbf{r}^{\prime}-\mathbf{t}^{\prime}\right|\left|\mathbf{h}^{\prime}+\mathbf{r}^{\prime}-\mathbf{t}^{\prime}\right|^{\top}\right)æ¨¡åž‹å¤æ‚åº¦è‡³äºŽä½œè€…æ¨¡åž‹çš„å¤æ‚æ€§ï¼Œæƒé‡çŸ©é˜µå®Œå…¨ç”±çŽ°æœ‰çš„åµŒå…¥å‘é‡è®¡ç®—ï¼Œè¿™æ„å‘³ç€TransAå‡ ä¹Žå…·æœ‰ä¸ŽTransEç›¸åŒçš„è‡ªç”±å‚æ•°æ•°ã€‚ è‡³äºŽä½œè€…æ¨¡åž‹çš„æ•ˆçŽ‡ï¼Œæƒé‡çŸ©é˜µæœ‰ä¸€ä¸ªå°é—­çš„è§£å†³æ–¹æ¡ˆï¼Œè¿™åœ¨å¾ˆå¤§ç¨‹åº¦ä¸ŠåŠ å¿«äº†åŸ¹è®­è¿‡ç¨‹ å®žéªŒä½œè€…ç»Ÿè®¡äº†ä¸€ä¸ªATPEå€¼ï¼ˆAveraged Triple number Per Entity.ï¼‰è¯¥æ•°é‡è¡¡é‡æ•°æ®é›†çš„å¤šæ ·æ€§å’Œå¤æ‚æ€§ ä½œè€…å¯¹äºŽTransAåœ¨WN18ä¸Šè¡¨çŽ°ä¸å¥½è¿›è¡Œäº†åˆ†æžè§£é‡Šã€‚ TransAåœ¨WN18æ•°æ®é›†ä¸Šçš„å¹³å‡æŽ’åè¡¨çŽ°ä¸ä½³ã€‚ æ·±å…¥ç ”ç©¶è¯¦ç»†æƒ…å†µï¼Œæˆ‘ä»¬å‘çŽ°æœ‰27ä¸ªæµ‹è¯•ä¸‰å…ƒç»„ï¼ˆæµ‹è¯•é›†çš„0.54ï¼…ï¼‰ï¼Œå…¶æŽ’åè¶…è¿‡30,000ï¼Œè¿™å‡ ä¸ªæ¡ˆä¾‹å°†å¯¼è‡´çº¦162ä¸ªå¹³å‡ç­‰çº§æŸå¤±ã€‚ æ‰€æœ‰è¿™äº›ä¸‰å…ƒç»„çš„å°¾éƒ¨æˆ–å¤´éƒ¨å®žä½“ä»Žæœªä¸Žè®­ç»ƒé›†ä¸­çš„ç›¸åº”å…³ç³»å…±å­˜ã€‚ è®­ç»ƒæ•°æ®ä¸è¶³å¯¼è‡´æƒé‡çŸ©é˜µè¿‡åº¦æ‰­æ›²ï¼Œæƒé‡çŸ©é˜µè¿‡åº¦æ‰­æ›²å¯¼è‡´å¹³å‡æŽ’åä¸è‰¯ ä½œè€…è¿˜åšäº†ä¸€ä¸ªå®žéªŒ ä½œè€…è§£é‡Šï¼š ç²¾åº¦éšé‡é‡å·®å¼‚è€Œå˜åŒ–ï¼Œè¿™æ„å‘³ç€ç‰¹å¾åŠ æƒæœ‰åˆ©äºŽç²¾ç¡®åº¦ã€‚ è¿™è¯æ˜Žäº†TransAçš„ç†è®ºåˆ†æžå’Œæœ‰æ•ˆæ€§ ä½†æ˜¯æˆ‘å¹¶ä¸è®¤ä¸ºè¿™ä¸ªå¯ä»¥è¯´æ˜Žä»€ä¹ˆï¼Œé‚£æ˜¯ä¸æ˜¯è°ƒé«˜å…¶ä»–å…³ç³»çš„æƒé‡ä¼šå¸¦æ¥è¯¥å…³ç³»æ•ˆæžœçš„æå‡ï¼Ÿ]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
        <category>KG</category>
      </categories>
      <tags>
        <tag>KGE</tag>
        <tag>TransA</tag>
        <tag>2015</tag>
        <tag>arxiv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Transition-based Knowledge Graph Embedding with Relational Mapping Propertiesé˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FTransition-based_Knowledge_Graph_Embedding_with_Relational_Mapping_Properties%2F</url>
    <content type="text"><![CDATA[è®ºæ–‡ä¸‹è½½åœ°å€ Related WorkæŠŠçŸ¥è¯†å›¾è°±æ˜ å°„åˆ°ä½Žç»´å‘é‡çš„åŽŸå› ï¼šç¬¦å·å’Œç¦»æ•£çš„å­˜å‚¨ç»“æž„ä½¿æˆ‘ä»¬å¾ˆéš¾åˆ©ç”¨è¿™äº›çŸ¥è¯†æ¥å¢žå¼ºå…¶ä»–æ™ºèƒ½èŽ·å–çš„åº”ç”¨ç¨‹åºï¼ˆä¾‹å¦‚é—®ç­”ç³»ç»Ÿï¼‰ï¼Œå› ä¸ºè®¸å¤šä¸ŽAIç›¸å…³çš„ç®—æ³•æ›´å€¾å‘äºŽè¿›è¡Œå…³äºŽè¿žç»­æ•°æ®è®¡ç®—ã€‚ æ–‡ä¸­ç”¨ONE-TO-ONE (husband-to-wife), MANY-TO-ONE (children-to-father), ONE-TO- MANY (mother-to-children), MANY-TO-MANY (parents-to-children) æ¥è¿›è¡Œéžå•ä¸€å…³ç³»çš„ä¾‹å­è§‰å¾—å¾ˆå¥½ã€‚ ä»¥å‰ç®—æ³•çš„ç›®æ ‡å‡½æ•°å’Œå‚æ•°å¤æ‚åº¦ï¼š Modelç†è§£ä¸äº†æ˜¯å¦‚ä½•åŒºåˆ†1å¯¹å¤šå…³ç³»çš„ï¼Ÿï¼Ÿï¼Ÿï¼Ÿ ä½œè€…å°†æœ€ä¼˜å‡½æ•°å°†é€šè¿‡å¯¹åº”äºŽè¯¥å…³ç³»çš„é¢„å…ˆè®¡ç®—çš„æƒé‡ç»™å‡ºæ¯ä¸ªè®­ç»ƒä¸‰å…ƒç»„çš„ä¸åŒæ–¹é¢ã€‚ å®žé™…ä¸Šï¼Œå¤§çº¦åªæœ‰26.2ï¼…çš„ONE-TO-ONEä¸‰å…ƒç»„é€‚åˆç”±TransEå»ºæ¨¡ã€‚ å¦ä¸€æ–¹é¢ï¼Œå…¶ä½™ä¸‰å…ƒç»„ï¼ˆ73.8ï¼…ï¼‰å—åˆ°å½±å“ï¼Œå¦‚å›¾1å·¦ä¾§æ‰€ç¤ºã€‚ åŠ¨æœºæ ¹æ®ä½œè€…çš„è§‚å¯Ÿï¼Œä¸‰å…ƒç»„çš„æ˜ å°„å±žæ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºŽå®ƒçš„å…³ç³»ã€‚ ç›®æ ‡å‡½æ•°æƒé‡æ˜¯å…³ç³»ç‰¹å®šçš„ï¼Œä½œè€…ä¸ºä¸‰å…ƒç»„ï¼ˆhï¼Œrï¼Œtï¼‰æå‡ºçš„æ–°è¯„åˆ†å‡½æ•°æ˜¯ï¼š f_{r}(h, t)=w_{\mathbf{r}}\|\mathbf{h}+\mathbf{r}-\mathbf{t}\|_{L_{1} / L_{2}} w_{r}=\frac{1}{\log \left(h_{r} p t_{r}+t_{r} p h_{r}\right)} æµ‹é‡å…³ç³»çš„æ˜ å°„å±žæ€§ç¨‹åº¦çš„ä¸€ç§ç®€å•æ–¹æ³•æ˜¯è®¡ç®—æ¯ä¸ªä¸åŒå¤´éƒ¨å®žä½“çš„å°¾éƒ¨å®žä½“çš„å¹³å‡æ•°é‡ï¼Œåä¹‹äº¦ç„¶ã€‚ è¿™é‡Œè¿˜æ˜¯ä¸ç†è§£è¿™ä¸ªæƒé‡çš„æ„ä¹‰åœ¨å“ªé‡Œï¼ŸðŸ§ æŸå¤±å‡½æ•° \begin{array}{l}{\mathcal{L}=\min \sum_{(h, r, t) \in \Delta\left(h^{\prime}, r, t^{\prime}\right) \in \Delta_{(h, r, t)}^{\prime}}\left[\gamma+f_{r}(h, t)-f_{r}\left(h^{\prime}, t^{\prime}\right)\right]_{+}} \\ {\text {s.t. } \quad \forall e \in E,\|e\|_{2}=1}\end{array}å¯¹äºŽ$|e|_{2}=1$çš„è§£é‡Šï¼šçº¦æŸä½äºŽå•ä½çƒä¸Šçš„æ¯ä¸ªå®žä½“çš„åŽŸå› æ˜¯ä¸ºäº†ä¿è¯å®ƒä»¬å¯ä»¥ä»¥ç›¸åŒçš„æ¯”ä¾‹æ›´æ–°ï¼Œè€Œä¸æ˜¯å¤ªå¤§æˆ–å¤ªå°è€Œä¸èƒ½æ»¡è¶³æœ€ä½³ç›®æ ‡ã€‚ Experimentsè§è®ºæ–‡ï¼Œæ²¡ä»€ä¹ˆå¥½é˜è¿°çš„ã€‚ All the codes for the related models can be downloaded from https://github.com/glorotxa/SME]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
        <category>KG</category>
      </categories>
      <tags>
        <tag>KGE</tag>
        <tag>2014</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Embedding Edge-attributed Relational Hierarchiesé˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FEmbedding_Edge-attributed_Relational_Hierarchies%2F</url>
    <content type="text"><![CDATA[è®ºæ–‡ä¸‹è½½åœ°å€]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Knowledge Graph Embedding via Dynamic Mapping Matrixé˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FKnowledge_Graph_Embedding_via_Dynamic_Mapping_Matrix%2F</url>
    <content type="text"><![CDATA[è®ºæ–‡ä¸‹è½½åœ°å€ Problem Statement å¯¹äºŽç‰¹å®šçš„å…³ç³»$r$ï¼Œæ‰€æœ‰çš„å®žä½“éƒ½å…±äº«ç›¸åŒçš„æ˜ å°„çŸ©é˜µ$M_r$ã€‚ç„¶è€Œï¼Œç”±å…³ç³»é“¾æŽ¥çš„å®žä½“æ€»æ˜¯åŒ…å«å„ç§ç±»åž‹å’Œå±žæ€§ã€‚ æŠ•å½±æ˜¯å®žä½“å’Œå…³ç³»ä¹‹é—´çš„äº¤äº’è¿‡ç¨‹ï¼Œæ˜ å°„çŸ©é˜µåªèƒ½ç”±å…³ç³»å†³å®šæ˜¯ä¸åˆç†çš„ã€‚ çŸ©é˜µå‘é‡ä¹˜æ³•ä½¿å…¶å…·æœ‰å¤§é‡è®¡ç®—ï¼Œå¹¶ä¸”å½“å…³ç³»æ•°å¤§æ—¶ï¼Œå®ƒè¿˜å…·æœ‰æ¯”TransEå’ŒTransHæ›´å¤šçš„å‚æ•°ã€‚ ç”±äºŽå¤æ‚æ€§ï¼ŒTransR / CTransRéš¾ä»¥åº”ç”¨äºŽå¤§è§„æ¨¡çŸ¥è¯†å›¾ Contribution ä½œè€…æž„å»ºäº†ä¸€ä¸ªæ–°é¢–çš„æ¨¡åž‹TransDï¼Œé€šè¿‡åŒæ—¶è€ƒè™‘å®žä½“å’Œå…³ç³»çš„å¤šæ ·æ€§ï¼Œä¸ºæ¯ä¸€ä¸ªå®žä½“-å…³ç³»æž„å»ºåŠ¨æ€æ˜ å°„çŸ©é˜µã€‚å®ƒä¸ºå®žä½“è¡¨ç¤ºæ˜ å°„åˆ°å…³ç³»å‘é‡ç©ºé—´æä¾›çµæ´»çš„æ ·å¼ã€‚ ä¸ŽTransR / CTransRç›¸æ¯”ï¼ŒTransDå…·æœ‰æ›´å°‘çš„å‚æ•°å¹¶ä¸”æ²¡æœ‰çŸ©é˜µå‘é‡ä¹˜æ³• åœ¨å®žéªŒä¸­ï¼Œä½œè€…çš„æ–¹æ³•ä¼˜äºŽä¹‹å‰çš„æ¨¡åž‹ã€‚ Modelæ¨¡åž‹åœ¨TransDä¸­ï¼Œæ¯ä¸ªå‘½åçš„ç¬¦å·å¯¹è±¡ï¼ˆå®žä½“å’Œå…³ç³»ï¼‰ç”±ä¸¤ä¸ªå‘é‡è¡¨ç¤ºã€‚ ç¬¬ä¸€ä¸ªæ•èŽ·å®žä½“ï¼ˆå…³ç³»ï¼‰çš„å«ä¹‰ï¼Œå¦ä¸€ä¸ªç”¨äºŽæž„é€ æ˜ å°„çŸ©é˜µã€‚ å¯¹äºŽä¸€ä¸ªä¸‰å…ƒç»„$(h,r,t)$,å‘é‡ä¸€å…±æœ‰$h, h_p, r, r_p, t, t_p$,å…¶ä¸­å¸¦$p$çš„ä¸ºæ˜ å°„å‘é‡ï¼Œåˆ™æœ‰ \begin{aligned} \mathbf{M}_{r h} &=\mathbf{r}_{p} \mathbf{h}_{p}^{\top}+\mathbf{I}^{m \times n} \\ \mathbf{M}_{r t} &=\mathbf{r}_{p} \mathbf{t}_{p}^{\top}+\mathbf{I}^{m \times n} \end{aligned}æ•… \mathbf{h}_{\perp}=\mathbf{M}_{r h} \mathbf{h}, \quad \mathbf{t}_{\perp}=\mathbf{M}_{r t} \mathbf{t}å¯ä»¥ç»¼åˆä¸ºï¼š \begin{aligned} \mathbf{h}_{\perp} &=\mathbf{M}_{r h} \mathbf{h}=\mathbf{h}+\mathbf{h}_{p}^{\top} \mathbf{h} \mathbf{r}_{p} \\ \mathbf{t}_{\perp} &=\mathbf{M}_{r t} \mathbf{t}=\mathbf{t}+\mathbf{t}_{p}^{\top} \mathbf{t} \mathbf{r}_{p} \end{aligned}è¿™æ ·å°±æ²¡æœ‰çŸ©é˜µå’Œå‘é‡é—´çš„ä¹˜æ³•è¿ç®—ï¼Œå˜æˆå‘é‡é—´è¿ç®—ï¼Œæå‡è®¡ç®—é€Ÿåº¦ã€‚ Experiments and Results Analysiså¸¸è§„å®žéªŒï¼štriplets classification and link predictionä¸å†èµ˜è¿°ã€‚ ä½œè€…åœ¨å®žéªŒè¿‡ç¨‹ä¸­å…³æ³¨äº†ä¸€äº›å…·æœ‰æ›´ä½Žaccuracyçš„å…³ç³»ã€‚ åˆ†æžï¼š 1. å¯¹äºŽ$similar_to$å…³ç³»ä¸»è¦å› ä¸ºè®­ç»ƒæ•°æ®ä¸å……è¶³ï¼Œåªå äº†1.5%ã€‚ 2. å¯¹äºŽæœ€å³ä¾§çš„å›¾è¯´æ˜Žäº†bernæ–¹æ³•çš„æ•ˆæžœè¦å¥½äºŽunif Properties of Projection Vectorsä½œè€…è¿˜åšäº†case studyï¼Œé€šè¿‡ä¸åŒç±»åž‹å®žä½“å’Œå…³ç³»çš„æŠ•å½±å‘é‡çš„ç›¸ä¼¼æ€§è¡¨æ˜Žäº†ä½œè€…æ–¹æ³•çš„åˆç†æ€§]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>KGE</tag>
        <tag>TransD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[From Knowledge Graph Embedding to Ontology Embedding An Analysis of the Compatibility between Vector Space Representations and Rulesé˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FFrom_Knowledge_Graph_Embedding_to_Ontology_Embedding_An_Analysis_of_the_Compatibility_between_Vector_Space_Representations_and_Rules%2F</url>
    <content type="text"><![CDATA[è®ºæ–‡ä¸‹è½½åœ°å€]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Knowledge graph embedding with conceptsé˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FKnowledge_graph_embedding_with_concepts%2F</url>
    <content type="text"><![CDATA[è¿™ç¯‡è®ºæ–‡ï¼Œè¿ç”¨skip-gramæ–¹æ³•ï¼Œå°†å®žä½“å¯¹åº”ç›¸å…³æ¦‚å¿µå¼•å…¥å®žä½“å‘é‡è¡¨ç¤ºï¼Œä»¥å¢žå¼ºè¡¨ç¤ºæ•ˆæžœã€‚å®žä½“å’Œæ¦‚å¿µåœ¨åŒä¸€ç©ºé—´ä¸­ï¼Œä½†æ˜¯æ¦‚å¿µæ˜¯ç©ºé—´ä¸­çš„ä¸€ä¸ªè¶…å¹³é¢ï¼ˆç±»ä¼¼äºŽtransHï¼‰ã€‚æ–‡ä¸­ä¸¾ä¾‹å¾ˆå¤šä¾‹å­æ¥è¾…åŠ©è¯´æ˜Žï¼Œä½¿å¾—æ–‡ç« å¯è¯»æ€§å¤§å¹…æå‡ã€‚æ–‡ä¸­å®žéªŒæœ€åŽä¿©ä¸ªæ¯”è¾ƒæœ‰æ„æ€ã€‚æœ¬æ–‡å€¼å¾—æ€è€ƒå€Ÿé‰´çš„ä¸œè¥¿ä¸å°‘ï¼Œå€¼å¾—å†å¥½å¥½å›žé¡¾ã€‚ è®ºæ–‡ä¸‹è½½åœ°å€ import csv with open('stocks.csv') as f: f_csv = csv.DictReader(f) for row in f_csv: # process row problem statement å·²ç»å­˜åœ¨çš„KGEæ¨¡åž‹ä¸»è¦é›†ä¸­äºŽå®žä½“-å…³ç³»-å®žä½“ä¸‰å…ƒç»„æˆ–è€…æ–‡æœ¬è¯­æ–™äº¤äº’ã€‚ ä¸‰å…ƒç»„æ˜¯ç¼ºå°‘ä¿¡æ¯çš„ï¼Œå¹¶ä¸”åŸŸå†…æ–‡æœ¬ä¸æ€»æ˜¯å¯ä»¥èŽ·å¾—çš„â€”â€”å¯¼è‡´åµŒå…¥ç»“æžœåç¦»å®žé™… å¸¸è¯†æ¦‚å¿µçŸ¥è¯†å‘æŒ¥å¾ˆé‡è¦çš„ä½œç”¨ã€‚ background For example, for two triplets (Apple, Developer, IPhone) and (Apple, Developer, Samsung Mobile), it is quite difficult to distinguish which is the true triplet that contains fact triplets only, because â€˜â€˜IPhoneâ€™â€™ and â€˜â€˜Samsung Mobileâ€™â€™ both belong to mobile phones. However, in the concept graph, â€˜â€˜IPhoneâ€™â€™ has a concept â€˜â€˜apple deviceâ€™â€™, but â€˜â€˜Samsung Mobileâ€™â€™ does not. Thus, it is easy to infer the correct triplet by mapping â€˜â€˜IPhoneâ€™â€™ to the â€˜â€˜apple deviceâ€™â€™concept å¾ˆå¥½çš„ä¸€ä¸ªä¸¾ä¾‹å…³äºŽå¦‚ä½•è¿ç”¨conceptæ¥è¾…åŠ©å…³ç³»è¯†åˆ« Specifically, when a corpus about technology is provided, embedding methods with technical textual descriptions could easily infer the fact (Apple, Developer, IPhone), because the keywords â€˜â€˜hardware productsâ€™â€™ and â€˜â€˜iPhone smartphoneâ€™â€™ occur frequently in the textual description of â€˜â€˜Appleâ€™â€™. However, it is difficult to infer the fact (Apple, Taste, Sweet), which is irrelevant to textual descriptions of â€˜â€˜Appleâ€™â€™ about the specific topic of â€˜â€˜technology company è¿™é‡Œä½œè€…ä¸¾ä¾‹è¯´æ˜Žï¼šä¸Žå…·æœ‰æ–‡æœ¬ä¿¡æ¯çš„åµŒå…¥æ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰æ¦‚å¿µä¿¡æ¯çš„åµŒå…¥æ–¹æ³•åœ¨å…¶ä»»åŠ¡ä¸­æ›´åŠ é€šç”¨ï¼Œå¹¶ä¸”å®ƒä¸ä¾èµ–äºŽè¯­æ–™åº“çš„ä¸»é¢˜ã€‚ ä½œè€…æŠŠKGEåˆ†æˆäº†ä¸‰ç±»ï¼Œå¦‚ä¸‹ï¼š Embedding with symbolic tripletsï¼štransç³»åˆ—éƒ½æ”¾åˆ°äº†è¿™éƒ¨åˆ†ä¸­ Embedding with textual information Embedding with category information Methodologyconcept graph embeddingä½œè€…é‡‡ç”¨skip-gramæ¥å­¦ä¹ å¯ä»¥æ•èŽ·å…¶è¯­ä¹‰ç›¸å…³æ€§çš„æ¦‚å¿µå’Œå®žä½“çš„è¡¨ç¤ºã€‚ å…¶ä¸­ï¼Œæ¯ä¸ªå®žä½“å¯¹åº”å¤šä¸ªæ¦‚å¿µï¼Œæ¯ä¸ªæ¦‚å¿µåˆåŒ…å«å¤šä¸ªå®žä½“ï¼ˆè¿™äº›å®žä½“ä½œä¸ºå®žä½“çš„ä¸Šä¸‹æ–‡ï¼‰ã€‚ åˆ™ï¼Œskip-gramå‡½æ•°å¯ä»¥å†™ä¸ºï¼š \begin{array}{l}{P\left(e_{c} | e_{t}\right)=\frac{\exp \left(e_{c} \cdot e_{t}\right)}{\sum_{e \in E} \exp \left(e \cdot e_{t}\right)}} \\ {P\left(e_{c} | c_{i}\right)=\frac{\exp \left(e_{c} \cdot c_{i}\right)}{\sum_{e \in E} \exp \left(e \cdot c_{i}\right)}}\end{array}æ•…æŸå¤±å‡½æ•°ä¸ºï¼š L=\frac{1}{|D|} \sum_{\left(e_{c}, e_{t}\right) \in D}\left[\log P\left(e_{c} | e_{t}\right)+\sum_{c_{i} \in C\left(e_{t}\right)} \log P\left(e_{c} | c_{i}\right)\right]å­¦ä¹ çŽ‡è®¾ä¸ºï¼š Î± = starting_alphaÃ—(1âˆ’count_actual/(real)(iter Ã— total_size+1)) è¿™é‡Œä½œè€…è¯´ä¸ºäº†é¿å…è¿‡æ‹Ÿåˆï¼Œå¯¹ä¼˜åŒ–ç›®æ ‡é‡‡ç”¨â€œè´ŸæŠ½æ ·â€æ–¹æ³•ã€‚&quot;è´ŸæŠ½æ ·&quot;æ–¹æ³•è¿˜å¯ä»¥é¿å…è¿‡æ‹Ÿåˆï¼Ÿ knowledge graph embeddingå°†ç‰¹å®šä¸‰å…ƒç»„åµŒå…¥åˆ°æ¦‚å¿µå­ç©ºé—´ä¸­ï¼Œé¦–å…ˆæž„å»ºä¸€ä¸ªè¶…å¹³é¢ï¼Œå…¶ä¸­æ³•å‘é‡$c$ä¸ºæ¦‚å¿µå­ç©ºé—´ï¼š c=C\left(e_{h}, e_{t}\right)=\frac{e_{h}-e_{t}}{\left\|e_{h}-e_{t}\right\|_{2}^{2}}æ ¹æ®TransEä¸‰å…ƒç»„çš„åµŒå…¥æŸå¤±ä¸ºï¼š l=h+r-tæ‰€ä»¥ï¼Œå¯ä»¥è®¡ç®—å‡ºæ³•å‘é‡æ–¹å‘ä¸Šçš„æŸå¤±åˆ†é‡æ˜¯ï¼š \left(c^{T} l c\right)ç„¶åŽï¼ŒæŠ•å½±åˆ°è¶…å¹³é¢ä¸Šçš„å¦ä¸€ä¸ªæ­£äº¤åˆ†é‡æ˜¯ï¼š \left(l-c^{T} l c\right) å®šä¹‰æ€»æŸå¤±å‡½æ•°ï¼š f_{r}(h, t)=-\lambda\left\|l-c^{T} l c\right\|_{2}^{2}+\|l\|_{2}^{2}Model interpretation å¯ä»¥é€šè¿‡æ¦‚å¿µæ¥è¾…åŠ©ä¸‰å…ƒç»„è¯†åˆ«ï¼Œæ–‡ä¸­ä»¥(Christopher Plummer, /people/person/nationality, Canada)ä¸¾ä¾‹ å¯ä»¥è§£å†³åœ¨å½“ä¸¤ä¸ªå€™é€‰å®žä½“åœ¨KGEï¼Œä¸­è®¡ç®—lossç›¸ç­‰æ—¶è¾¨åˆ«è¿™ä¸¤ä¸ªå“ªä¸ªæ˜¯çœŸå®žçš„ã€‚æ–‡ä¸­ä»¥â€œwhich the director made the film â€˜â€˜WALL-Eâ€™â€™â€ä¸ºä¾‹æ¥è¿›è¡Œè¯´æ˜Ž éƒ½æ˜¯é€šè¿‡æŸ¥è¯¢å®žä½“å¯¹åº”æ¦‚å¿µæ¥è¿›è¡Œè¾…åŠ© Objectives and trainingmargin-based loss functionï¼š L=\sum_{(h, r, t) \in S} \sum_{\left(h^{\prime}, r, t^{\prime}\right) \in S_{(h, r, t)}^{\prime}}\left[\gamma+f_{r}(h, t)-f_{r^{\prime}}\left(h^{\prime}, t^{\prime}\right)\right]_{+}train å…ˆé¢„è®­ç»ƒæ¦‚å¿µå›¾æ¨¡åž‹åµŒå…¥ï¼ŒèŽ·å¾—åœ¨æ¦‚å¿µç©ºé—´ä¸­çš„å®žä½“å‘é‡ åˆ©ç”¨1ä¸­èŽ·å¾—çš„å®žä½“å‘é‡è¿›è¡Œæ›´æ–°ã€‚ datasets WN18 and FB15K Microsoft Concept Graph å…¶ä¸­ï¼Œrelationsè¡¨ç¤ºé¢‘çŽ‡ çœŸçš„æœ‰ç»Ÿè®¡é¢‘çŽ‡çš„è¿™ç§ ExperimentsKnowledge graph completionEntity classificationConcept relevance analysis è¿™ä¸ªå®žéªŒæ¯”è¾ƒæœ‰æ„æ€ï¼šæ¯ä¸ªå•å…ƒæ ¼ä¸­çš„æ•°å­—è¡¨ç¤ºåœ¨TransEä¸­æŽ’åå¤§äºŽmä¸”åœ¨æˆ‘ä»¬çš„æ¨¡åž‹ä¸­å°äºŽnçš„ä¸‰å…ƒç»„çš„æ•°é‡ã€‚ Precise semantic expression analysisæˆ‘ä»¬åœ¨é“¾æŽ¥é¢„æµ‹ï¼ˆæ¢å¥è¯è¯´ï¼Œè¿™äº›æ˜¯TransEçš„éš¾ä»¥è¯æ˜Žçš„ä¾‹å­ï¼‰ä¸­æ”¶é›†é‚£äº›å¾—åˆ†ç•¥é«˜äºŽçœŸå®žä¸‰å…ƒç»„ä½œä¸ºè´Ÿä¸‰å…ƒç»„ ç„¶åŽåœ¨KECä¸­å¯¹æ¯”ä¸¤è€…çš„åˆ†æ•°å·®å€¼ã€‚ å³è¾¹æ¡è¡¨ç¤ºKECåœ¨TransEå¤±è´¥æ—¶ä½œå‡ºæ­£ç¡®å†³å®šï¼Œå·¦è¾¹æ¡è¡¨ç¤ºKECå’ŒTransEéƒ½å¤±è´¥ã€‚]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
        <category>KG</category>
      </categories>
      <tags>
        <tag>KGE</tag>
        <tag>ontology</tag>
        <tag>concept</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Universal Representation Learning of Knowledge Bases by Jointly Embedding Instances and Ontological Conceptsé˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FUniversal_Representation_Learning_of_Knowledge_Bases_by_Jointly_Embedding_Instances_and_Ontological_Concepts%2F</url>
    <content type="text"><![CDATA[ è®ºæ–‡ä¸‹è½½åœ°å€ Problem StatementExisting KG embedding models merely focus on representing of an ontology view for abstract and commonsense concepts or an instance view for special entities that are instantiated from ontological concepts. Challenge mappings difficult :the semantic mappings from entities to concepts and from relations to meta-relations are complicated and difficult to be precisely captured by any current embedding models inadequate cross-view links: the known cross-view links inadequately cover a vast number of entities, which leads to insufficient information to align both views of the KB, and curtails discovering new cross-view links the scales and topological structures are different in ontological views and instance views Introduction ä»Žä¸¤ç§è§†å›¾æ¥å­¦ä¹ è¡¨ç¤ºæœ‰ä»¥ä¸‹ä¸¤ç‚¹å¥½å¤„ï¼š instance embeddings provide detailed and rich information for their corresponding ontological concepts. a concept embedding provides a high-level summary of its instances, which is extremely helpful when an instance is rarely observed. contribution a novel KG embedding model named JOIE, which jointly encodes both the ontology and instance views of a KB cross-view association model : a novel KG embedding model named JOIE, which jointly encodes both the ontology and instance views of a KB cross-view grouping technique : assumes that the two views can be forced into the same embedding space cross-view transformation technique : enables non-linear transformations from the instance embedding space to the ontology embedding space intra-view embedding model : characterizes the relational facts of ontology and instance views in two separate embedding spaces three state-of-the-art translational or similarity-based relational embedding techniques hierarchy-aware embedding: based on intra-view non- linear transformations to preserve ontologies hierarchical substructures. implement two experiments: the triple completion task : confirm the effectiveness of JOIE for populating knowledge in both ontology and instance-view KGs, which has significantly outperformed various baseline models. the entity typing task : show that JOIE is competent in discovering cross-view links to align the ontology-view and the instance-view KGs. Modeling Cross-view Association Model Cross-view Grouping (CG)è¯¥æ¨¡åž‹å¯ä»¥è§†ä¸ºgrouping-based regularizationï¼Œ å‡è®¾æœ¬ä½“è§†å›¾KGå’Œå®žä¾‹è§†å›¾KGå¯ä»¥è¢«åµŒå…¥åˆ°åŒä¸€ç©ºé—´ä¸­ï¼Œå¹¶å¼ºåˆ¶ä½¿å®žä¾‹å‘é‡é è¿‘ä¸Žå®ƒç›¸å…³è”çš„æ¦‚å¿µå‘é‡ï¼Œå¦‚å›¾3(a)æ‰€ç¤º å®šä¹‰æŸå¤±å‡½æ•°å¦‚ä¸‹ï¼š J_{\text { Cross }}^{\mathrm{CG}}=\frac{1}{|\mathcal{S}|} \sum_{(e, c) \in \mathcal{S}}\left[\|\mathbf{c}-\mathbf{e}\|_{2}-\gamma^{\mathrm{CG}}\right]_{+}Cross-view Transformation (CT)è¯•å›¾åœ¨å®žä½“åµŒå…¥ç©ºé—´å’Œæ¦‚å¿µç©ºé—´ä¹‹é—´è½¬æ¢ä¿¡æ¯ï¼Œå¦‚å›¾3(b)æ‰€ç¤º å®šä¹‰æ˜ å°„å‡½æ•°ï¼Œå°†å®žä¾‹æ˜ å°„åˆ°æœ¬ä½“è§†å›¾ç©ºé—´ï¼Œè¯¥æ˜ å°„åŽå‘é‡åº”è¯¥é è¿‘å®ƒçš„ç›¸å…³è”æ¦‚å¿µï¼š \mathbf{c} \leftarrow f_{\mathrm{CT}}(\mathbf{e}), \forall(e, c) \in \mathcal{S}å…¶ä¸­ï¼Œ f_{\mathrm{CT}}(\mathbf{e})=\sigma\left(\mathbf{W}_{\mathrm{ct}} \cdot \mathbf{e}+\mathbf{b}_{\mathrm{ct}}\right)æ•´ä¸ªCTçš„æŸå¤±å‡½æ•°ä¸ºï¼š J_{\text { Cross }}^{\mathrm{CT}}=\frac{1}{|\mathcal{S}|} \sum_{(e, c) \in \mathcal{S} \atop \wedge\left(e, c^{\prime}\right) \in \mathcal{S}}\left[\gamma^{\mathrm{CT}}+\left\|\mathbf{c}-f_{\mathrm{CT}}(\mathbf{e})\right\|_{2}-\left\|\mathbf{c}^{\prime}-f_{\mathrm{CT}}(\mathbf{e})\right\|_{2}\right]_{+}Intra-view Modelè¯¥æ¨¡åž‹çš„ç›®çš„ï¼šåœ¨ä¸¤ä¸ªåµŒå…¥ç©ºé—´ä¸­åˆ†åˆ«ä¿ç•™KBçš„æ¯ä¸ªè§†å›¾ä¸­çš„åŽŸå§‹ç»“æž„ä¿¡æ¯ã€‚ Default Intra-view Modelä½œè€…é‡‡ç”¨ä¸‰ç§æ–¹å¼ï¼š \begin{aligned} f_{\text { TransE }}(\mathbf{h}, \mathbf{r}, \mathbf{t}) &=-\|\mathbf{h}+\mathbf{r}-\mathbf{t}\|_{2} \\ f_{\text { Mult }}(\mathbf{h}, \mathbf{r}, \mathbf{t}) &=(\mathbf{h} \circ \mathbf{t}) \cdot \mathbf{r} \\ f_{\text { HolE }}(\mathbf{h}, \mathbf{r}, \mathbf{t}) &=(\mathbf{h} \star \mathbf{t}) \cdot \mathbf{r} \end{aligned}æŸå¤±å‡½æ•°ï¼š J_{\text { Intra }}^{G}=\frac{1}{|\mathcal{G}|} \sum_{(h, r, t) \in \mathcal{G}}\left[\gamma^{\mathcal{G}}+f\left(\mathbf{h}^{\prime}, \mathbf{r}, \mathbf{t}^{\prime}\right)-f(\mathbf{h}, \mathbf{r}, \mathbf{t})\right]_{+}intraæŸå¤±å‡½æ•°ï¼š J_{\text { Intra }}=J_{\text { Intra }}^{\mathcal{G}_{I}}+\alpha_{1} \cdot J_{\text { Intra }}^{\mathcal{G}_{O}}Hierarchy-Aware Intra-view Model for the Ontologyè¿›ä¸€æ­¥åŒºåˆ†äº†æž„æˆæœ¬ä½“å±‚æ¬¡ç»“æž„çš„å…ƒå…³ç³»å’Œè§†å›¾å†…æ¨¡åž‹ä¸­çš„è§„åˆ™è¯­ä¹‰å…³ç³»(å¦‚â€œrelated_toâ€)ã€‚ ç»™å®šæ¦‚å¿µå¯¹ï¼ˆclï¼Œchï¼‰ï¼Œæˆ‘ä»¬å°†è¿™ç§å±‚æ¬¡ç»“æž„å»ºæ¨¡ä¸ºç²—ç•¥æ¦‚å¿µå’Œç›¸å…³æ›´ç²¾ç»†æ¦‚å¿µä¹‹é—´çš„éžçº¿æ€§å˜æ¢: g_{\mathrm{HA}}\left(\mathbf{c}_{h}\right)=\sigma\left(\mathbf{W}_{\mathrm{HA}} \cdot \mathbf{c}_{l}+\mathbf{b}_{\mathrm{HA}}\right)æŸå¤±å‡½æ•°ä¸ºï¼š J_{\text { Intra }}^{\mathrm{HA}}=\frac{1}{|\mathcal{T}|} \sum_{\left(c_{l}, c_{h}\right) \in \mathcal{T}}\left[\gamma^{\mathrm{HA}}+\left\|\mathbf{c}_{h}-g\left(\mathbf{c}_{l}\right)\right\|_{2}-\left\|\mathbf{c}_{\mathrm{h}}^{\prime}-g\left(\mathbf{c}_{1}\right)\right\|_{2}\right]_{+}æ•…ï¼Œè¯¥éƒ¨åˆ†æŸå¤±å‡½æ•°ä¸ºï¼š J_{\text { Intra }}=J_{\text { Intra }}^{G_{I}}+\alpha_{1} \cdot J_{\text { Intra }}^{\mathcal{G} o \backslash \mathcal{T}}+\alpha_{2} \cdot J_{\text { Intra }}^{\mathrm{HA}} $J_{\text { Intra }}^{\mathcal{G} o} \backslash \mathcal{T}$: é»˜è®¤çš„è§†å›¾å†…æ¨¡åž‹çš„ä¸¢å¤±ï¼Œè¯¥æ¨¡åž‹ä»…åœ¨å…·æœ‰è§„åˆ™è¯­ä¹‰å…³ç³»çš„ä¸‰å…ƒç»„ä¸Šè®­ç»ƒ $J_{\text { Intra }}^{\mathrm{HA}}$æ˜Žç¡®è®­ç»ƒä¸‰å…ƒç»„ä¸Žå½¢æˆæœ¬ä½“å±‚æ¬¡ç»“æž„çš„å…ƒå…³ç³» æ„Ÿè§‰è¿™éƒ¨åˆ†å°±æ˜¯ä¼ é€’å…³ç³»ï¼Œç±»ä¼¼æŽ¨ç†æ€§è´¨çš„ã€‚ æ²¡æ˜Žç™½ä¸¤ç§ontologyå…³ç³»çš„åŒºåˆ†ç‚¹ Joint Training on Two-View KBsè”åˆæŸå¤±å‡½æ•°ï¼š J=J_{\text { Intra }}+\omega \cdot J_{\text { Cross }}ä½œè€…å¹¶ä¸ç›´æŽ¥æ›´æ–°$J$ï¼Œè€Œæ˜¯äº¤æ›¿æ›´æ–°$J_{\text { Intra }}^{\mathcal{G}_{I}}, J_{\text { Intra }}^{\mathcal{G} O} \text { and } J_{\text { Cross }}$. EXPERIMENTSå…·ä½“ç»†èŠ‚ç›´æŽ¥è§è®ºæ–‡ datasetæ•°æ®é›†æ˜¯ä½œè€…è‡ªå·±æž„å»ºçš„ï¼Œä¿¡æ¯å¦‚ä¸Šå›¾æ‰€ç¤ºã€‚ Case StudyOntology Populationä½œè€…æƒ³é¢„æµ‹åœ¨å…ƒå…³ç³»è¯è¡¨ä¸­å¹¶ä¸å­˜åœ¨çš„å…ƒå…³ç³»ï¼Œä¾‹å¦‚ï¼šé¢„æµ‹(â€œOffice Holderâ€, ?r, â€œCountryâ€) è¿™é‡Œï¼Œä½œè€…é‡‡å–çš„æ–¹å¼æ˜¯å°†æ¦‚å¿µé€šè¿‡ä¹‹å‰æåˆ°çš„å®žä½“ç©ºé—´åˆ°æ¦‚å¿µç©ºé—´çš„æ˜ å°„æ¥è¿›è¡Œåæ˜ å°„ã€‚ç„¶åŽæŒ‰ç…§$f_{\mathrm{CT}}^{\mathrm{inv}}\left(\mathbf{c}_{\text { country }}\right)-f_{\mathrm{CT}}^{\mathrm{inv}}\left(\mathbf{c}_{\text { office }}\right)$æ¥åœ¨å®žä½“åµŒå…¥ç©ºé—´è¿›è¡Œæœç´¢ä¸Žä¹‹ç›¸è¿‘çš„å®žä½“é—´å…³ç³»ã€‚ Long-tail entity typing In KGs, the frequency of entities and relations often follow a long-tail distribution (Zipfâ€™s law) ä½œè€…æŠ½å–äº†ä½Žé¢‘æ¬¡å®žä½“è¿›è¡Œäº†è®­ç»ƒï¼Œå‘çŽ°JOIEæ¨¡åž‹çš„æ•ˆæžœè™½ç„¶æœ‰ä¸‹é™ï¼Œä½†å°šåœ¨å¯ä»¥æŽ¥å—çš„ç¨‹åº¦å†…ã€‚ FUTURE WORK Particularly, instead of optimizing structure loss with triples (first-order neighborhood) locally, we plan to adopt more complex embedding models which leverage information from higher order neighborhood, logic paths or even global knowledge graph structures. We also plan to explore the alignment on relations and meta-relations like entity-concept. exploring different triple encoding techniques Note that we are also aware of the fact that there are more comprehensive properties of relations and meta-relations in the two views such as logical rules of relations and entity types. Incorporating such properties into the learning process is left as future work. æ€è€ƒè¿™ç¯‡è®ºæ–‡å’Œä¹‹å‰è·Ÿå¼ è€å¸ˆå®šçš„æˆ‘çš„è®ºæ–‡çš„æ€è·¯åŸºæœ¬ä¸€è‡´ï¼Œé¢ï¼Œæœ‰ç‚¹æ„Ÿè§‰æœ‰ç‚¹å—æ‰“å‡»ã€‚è¿™ç¯‡æ–‡ç« ä¹Ÿæ˜¯è¯¥ä½œè€…åšå£«æ¯•ä¸šè®ºæ–‡ä¸­çš„ä¸€éƒ¨åˆ†ï¼Œæ‰€ä»¥åº”è¯¥æ˜¯è¿™ä¸ªä½œè€…æ—©å°±æœ‰è¿™ä¸ªæ€è·¯äº†ï¼Œæ‰€ä»¥ä¹Ÿæ²¡ä»€ä¹ˆå¥½çº ç»“çš„ã€‚è¿™ç¯‡æ–‡ç« ä¹Ÿæ˜¯èµ°çš„transçš„è·¯çº¿ï¼Œå’Œåˆ˜çš„è®ºæ–‡åˆä¸ä¸€æ ·çš„æ€è·¯ï¼Œä½†æ˜¯éƒ½æ˜¯æ¦‚å¿µæœ¬ä½“è¿™ç±»çš„ã€‚å…¶ä¸­æœ‰ä¸€ç‚¹ä¸ä¸€æ ·ï¼Œå°±æ˜¯is_aå…³ç³»å¯èƒ½ä¸¤ç¯‡è®ºæ–‡ç”¨çš„ä¸ä¸€æ ·ã€‚è¿™ç¯‡è®ºæ–‡ä¸­æåˆ°äº†æ•°æ®é›†å¼€æºï¼Œå¯æ˜¯githubçš„é“¾æŽ¥ä¸­å¹¶æ²¡æœ‰æ•°æ®é›†ã€‚è™½ç„¶è½®æ–‡ä¸­è¯´ä»–ç»“åˆäº†æ¦‚å¿µå’Œå®žä¾‹çš„è§†å›¾ï¼Œä½†æ˜¯å…¶å®žåƒåˆ˜çš„è®ºæ–‡å°±å·²ç»æå‡ºç»“åˆäº†æ¦‚å¿µå’Œå®žä¾‹çš„è§’åº¦äº†ã€‚]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
        <category>KG</category>
      </categories>
      <tags>
        <tag>KGE</tag>
        <tag>ontology</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DocRED A Large-Scale Document-Level Relation Extraction Dataseté˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FDocRED_A_Large-Scale_Document-Level_Relation_Extraction_Dataset%2F</url>
    <content type="text"><![CDATA[è¿™æ˜¯ä¸€ä¸ªä»‹ç»æ•°æ®é›†çš„è®ºæ–‡ï¼Œä¸»è¦æ˜¯æ–‡æ¡£çº§åˆ«çš„å…³ç³»æŠ½å–æ•°æ®é›†ã€‚ è®ºæ–‡ä¸‹è½½åœ°å€ Problem Statementexisting datasets for document-level RE either only have a small number of manually-annotated relations and entities, or exhibit noisy annotations from distant supervision, or serve specific domains or approaches. Contribution (DocRED) constructed from Wikipedia and Wikidata DocRED contains 132, 375 entities and 56, 354 relational facts annotated on 5, 053 Wikipedia documents As at least 40.7% of the relational facts in DocRED can only be extracted from multiple sentences also provide large-scale distantly supervised data to support weakly supervised RE research indicate the existing methods deal with the taks document level RE is more difficult sentence-level RE. data]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>RE</tag>
        <tag>dataset</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Learning Entity and Relation Embeddings for Knowledge Graph Completioné˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FLearning_Entity_and_Relation_Embeddings_for_Knowledge_Graph_Completion%2F</url>
    <content type="text"><![CDATA[TransR embeds entities and relations in distinct entity space and relation space, and learns embeddings via translation between projected entities.CTransR models internal complicated correlations within each relation type. è®ºæ–‡ä¸‹è½½åœ°å€ Problem StatementIn fact, an entity may have multiple aspects and various relaitons may focus on different aspects of entities, which makes a common space insurficient for modeling. Contribution propose a TransR model which models entities and relations in distinct spaces CTransR models internal complicated correlations within each relation type. experiment on benchmark datasets of WordNet and Freebase and gain consistent improvements compared to state-of-the-art models Future work Existing models including TransR consider each relational fact separately. relation transitive explore a unified embedding model of both text side and knowledge graph modeling internal correlations within each relation type TransR for each triple$(h, r, t)$, entities embeddings are set as $\mathbf{h}, \mathbf{t} \in \mathbb{R}^{k}$ and relation embedding is set as $\mathbf{r} \in \mathbb{R}^{d}$, $k \neq d$ for each relation $r$, set a projection matrix $\mathbf{M}_{r} \in\mathbb{R}^{k \times d}$ projects entities from entity space to relation space projected vectors of entities as \mathbf{h}_{r}=\mathbf{h} \mathbf{M}_{r}, \quad \mathbf{t}_{r}=\mathbf{t} \mathbf{M}_{r} score function: f_{r}(h, t)=\left\|\mathbf{h}_{r}+\mathbf{r}-\mathbf{t}_{r}\right\|_{2}^{2} Cluster-based TransR (CTransR)why propose CTransRTransE, TransH and TransR, learn a unique vector for each relation, which may be under-representative to fit all entity pairs under this relation, because these relations are usually rather diverse. basic idea incorporate the idea of piecewise linear regression Ritzema and others 1994 segment input instances into several groups process for a specific relation r, all entity pairs (h, t) in the training data are clustered into multiple groups, and entity pairs in each group are expected to exhibit similar r relation. All entity pairs (h, t) are represented with their vector offsets (h âˆ’ t) for clustering, where h and t are obtained with TransE. learn a separate relation vector $r_c$for each cluster and matrix $M_r$ for each relation, respectively projected vectors of entities as $\mathbf{h}_{r, c}=\mathbf{h} \mathbf{M}_{r} \text { and } \mathbf{t}_{r, c}=\mathbf{t} \mathbf{M}_{r}$ sorce fuction f_{r}(h, t)=\left\|\mathbf{h}_{r, c}+\mathbf{r}_{c}-\mathbf{t}_{r, c}\right\|_{2}^{2}+\alpha\left\|\mathbf{r}_{c}-\mathbf{r}\right\|_{2}^{2}the later item aims to ensure cluster-specific relation vector rcnot too far away from the original relation vector r dataseté‡‡ç”¨å’Œå‰äººæ‰€ç”¨ä¸€æ ·çš„æ•°æ®é›† Dataset #Rel #Ent #Train #Valid # Test WN18 18 40,943 141,442 5,000 5,000 FB15K 1,345 14,951 483,142 50,000 59,071 WN11 11 38,696 112,581 2,609 10,544 FB13 13 75,043 316,232 5,908 23,733 FB40K 1,336 39528 370,648 67,946 96,678 Experimentä½œè€…é‡‡å–å¸¸è§„å®žéªŒ Link Predictionè¿™é‡Œä½œè€…å¯¹å…³ç³»ä¸­èšç±»è¿›è¡Œäº†å±•ç¤ºï¼š æˆ‘è§‰å¾—è¿™ç§æ–¹å¼æ˜¯å€¼å¾—å°è¯•çš„ã€‚ Triple classificationMoreover, the â€œbernâ€ sampling technique improves the performance of TransE, TransH and TransR on all three data sets. berné‡‡æ ·æ–¹æ³•éœ€è¦æŽŒæ¡ã€‚ Relation Extraction from Text]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>KGE</tag>
        <tag>KGR</tag>
        <tag>TransR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neural Relation Extraction with Selective Attention over Instancesé˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FNeural_Relation_Extraction_with_Selective_Attention_over_Instances%2F</url>
    <content type="text"><![CDATA[è¿™ç¯‡æ–‡ç« ä¹‹å‰çœ‹è¿‡ðŸ˜‚ã€‚ è®ºä¸‹è½½åœ°å€ Problem Statementâ€‹ Distant supervision inevitably accompanies with the wrong labelling problem, and thse noisy data will substantially hurt the performance of relation extraction. Contribution As compared to existing neural relation extraction model, our model can make full use of all informative sentences of each entity pair. To address the wrong labelling problem in distant supervision, we propose selective attention to de-emphasize those noisy instances. In the experiments, we show that selective attention is beneficial to two kinds of CNN models in the task of relation extraction. Methodologyæ¨¡åž‹æ•´ä½“æž¶æž„å¦‚ä¸‹æ‰€ç¤ºï¼š]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>RE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Learning as the Unsupervised Alignment of Conceptual Systemsé˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FLearning_as_the_Unsupervised_Alignment_of_Conceptual_Systems%2F</url>
    <content type="text"><![CDATA[è¿™ç¯‡æ–‡ç« æ²¡æ€Žä¹ˆçœ‹æ‡‚ï¼Œä¸»è¦æ€æƒ³åº”è¯¥æ˜¯ä»£è¡¨åŒæ—¶æ¦‚å¿µçš„ä¸åŒå½¢å¼ï¼ˆæ–‡æœ¬ï¼Œå›¾åƒï¼Œè¯­éŸ³ç­‰ï¼‰åº”è¯¥å…·æœ‰ç›¸ä¼¼çš„åˆ†å¸ƒï¼Œä»¥æ­¤æ¥è¿›è¡Œæ— ç›‘ç£çš„æ¦‚å¿µå¯¹é½ã€‚è¿™ç§æ€è·¯æŒºä¸é”™çš„ï¼Œä¸è¿‡è¿˜æ²¡æœ‰æ·±å…¥çš„æƒ³æ³•ï¼Œç®—æ˜¯æ‹“å±•è§†é‡Žå§ï¼ KEYThe key insight is that each concept has a unique signature within one conceptual system (e.g., images) that is recapitulated in other systems (e.g., text or audio) Problem Statement For supervised approaches, as the number of concepts grows, so does the number of required training examples V. W. Quine argued, even supervised instruction contains a substantial amount of ambiguity (Quine, 1960).Quine suggested that meaning may derive from somethingâ€™s place within a conceptual system. ModelIn order to solve Quinneâ€™s problem, we align a system of word labels and a system of visual semantics that both refer to the same underlying reality and therefore have related structure that can be discovered by unsupervised means (Figure 1ï¼‰]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ERNIE Enhanced Language Representation with Informative Entitiesé˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FERNIE_Enhanced_Language_Representation_with_Informative_Entities%2F</url>
    <content type="text"><![CDATA[è¯¥ç¯‡è®ºæ–‡å€Ÿé‰´BERTï¼Œè¯•å›¾å°†å®žä½“ä¿¡æ¯ï¼ˆTransEï¼‰èžå…¥token(singal word)ä¸­ï¼Œé€šè¿‡ç±»ä¼¼å®žä½“å¯¹é½çš„æ–¹æ³•å°†å®žä½“ä¸Žtokenå¯¹é½ï¼ˆå¹¶é‡‡å–maskæ–¹å¼è¿›è¡Œé¢„è®­ç»ƒï¼‰ï¼Œé€šè¿‡infromation fusion å°†tokenä¸Žå®žä½“èžåˆæ˜ å°„å…¥ç›¸å…³è”çš„ä¸¤ä¸ªå‘é‡ç©ºé—´ã€‚ è®ºæ–‡ä¸‹è½½åœ°å€ Problem Statementthe existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. ChallengeFor incorporating external knowledge into language representation models Structured Knowledge Encoding regarding to the given text, how to effectively extract and encode its related informative facts in KGs for language representation models Heterogeneous Information Fusion how to design a special pre-training objective to fuse the lexical, syntactic, and knowledge information is another challenge. Methodology Model ArchitectureERNIE the underlying textual encoder (T-Encoder)è´Ÿè´£ä»Žæ–‡æœ¬ä¸­æ•èŽ·åŸºæœ¬çš„è¯æ³•å’Œè¯­æ³•ä¿¡æ¯ \left\{\boldsymbol{w}_{1}, \ldots, \boldsymbol{w}_{n}\right\}=\mathrm{T}-\operatorname{Encoder}\left(\left\{w_{1}, \ldots, w_{n}\right\}\right)T-Encoder(Â·) is a multi-layer bidirectional Transformer encoder the upper knowledgeable encoder (K-Encoder) entity embeddings are pre-trained by TransEè´Ÿè´£å°†çŸ¥è¯†å›¾è°±é›†æˆåˆ°åº•å±‚çš„æ–‡æœ¬ä¿¡æ¯ä¸­ Knowledgeable Encoder the knowledgeable encoder K-Encoder consists of stacked aggregators designed for encoding both tokens and entities as well as fusing their heterogeneous features. In the i-th aggregator the input: token embeddings: $\left\{\boldsymbol{w}_{1}^{(i-1)}, \ldots, \boldsymbol{w}_{n}^{(i-1)}\right\}$ entity embeddings :$\left\{\boldsymbol{e}_{1}^{(i-1)}, \ldots, \boldsymbol{e}_{m}^{(i-1)}\right\}$ fed into two multi-head self-attentions(MH-ATTs) $\left\{\tilde{\boldsymbol{w}}_{1}^{(i)}, \ldots, \tilde{\boldsymbol{w}}_{n}^{(i)}\right\}=\mathrm{MH}-\operatorname{ATT}\left(\left\{\boldsymbol{w}_{1}^{(i-1)}, \ldots, \boldsymbol{w}_{n}^{(i-1)}\right\}\right)$ $\left\{\tilde{\boldsymbol{e}}_{1}^{(i)}, \ldots, \tilde{\boldsymbol{e}}_{m}^{(i)}\right\}=\mathrm{MH}-\operatorname{ATT}\left(\left\{\boldsymbol{e}_{1}^{(i-1)}, \ldots, \boldsymbol{e}_{m}^{(i-1)}\right\}\right)$ an information fusion layer \begin{aligned} \boldsymbol{h}_{j} &=\sigma\left(\tilde{\boldsymbol{W}}_{t}^{(i)} \tilde{\boldsymbol{w}}_{j}^{(i)}+\tilde{\boldsymbol{W}}_{e}^{(i)} \tilde{\boldsymbol{e}}_{k}^{(i)}+\tilde{\boldsymbol{b}}^{(i)}\right) \\ \boldsymbol{w}_{j}^{(i)} &=\sigma\left(\boldsymbol{W}_{t}^{(i)} \boldsymbol{h}_{j}+\boldsymbol{b}_{t}^{(i)}\right) \\ \boldsymbol{e}_{k}^{(i)} &=\sigma\left(\boldsymbol{W}_{e}^{(i)} \boldsymbol{h}_{j}+\boldsymbol{b}_{e}^{(i)}\right) \end{aligned} $h_j$ is the inner hidden state For the tokens without corresponding entities \begin{aligned} \boldsymbol{h}_{j} &=\sigma\left(\tilde{\boldsymbol{W}}_{t}^{(i)} \tilde{\boldsymbol{w}}_{j}^{(i)}+\tilde{\boldsymbol{b}}^{(i)}\right) \\ \boldsymbol{w}_{j}^{(i)} &=\sigma\left(\boldsymbol{W}_{t}^{(i)} \boldsymbol{h}_{j}+\boldsymbol{b}_{t}^{(i)}\right) \end{aligned}Pre-training for Injecting KnowledgeIn order to inject knowledge into language rep- resentation by informative entities. Randomly masks some token-entity alignments and then requires the system to predict all corresponding entities based on aligned tokens. denoising entity auto-encoder (dEA) define the aligned entity distribution for the token $w_i$ as follows: p\left(e_{j} | w_{i}\right)=\frac{\exp \left(\text { linear }\left(\boldsymbol{w}_{i}^{o}\right) \cdot \boldsymbol{e}_{j}\right)}{\sum_{k=1}^{m} \exp \left(\text { linear }\left(\boldsymbol{w}_{i}^{o}\right) \cdot \boldsymbol{e}_{k}\right)} linear(Â·) is a linear layer For dEA, perform the following operations: in 5% of the time, replace the entity with another random aims to train model to correct the errors that the token is aligned with a wrong entity; In 15% of the time, mask token-entity alignments aims to train model to correct the errors that entity alignment system doesnâ€™t extract all existing alignments; in the rest of the time, keep token-entity alignments unchanged aims to encourage our model to integrate the entity information into token representations for better language understanding. Fine-tuning for Specific Tasks We can take the final output embedding for the first token, which corresponds to the special [CLS] token, as the representation of the input sequence for specific tasks. For some knowledge-driven tasks, we design special fine-tuning procedure: relation classification design different tokens [HD] and [TL] for head entities and tail entities respectively a similar role like position embeddings in the conventional relation classification models (Zeng et al., 2015) entity typing the mention mark token [ENT] è¿™é‡Œçš„CLSä¸çŸ¥é“æœ‰ä»€ä¹ˆä½œç”¨ï¼Œæ‰€æœ‰çš„ä»»åŠ¡éƒ½æœ‰ï¼Œæ˜¯ä¸åŒçš„ä»»åŠ¡é‡CLSçš„embeddingæœ‰æ‰€ä¸åŒå—ï¼Ÿä¸ªäººç›®å‰è§‰å¾—æ˜¯è¿™æ ·çš„ã€‚ ä½œè€…è¿™é‡Œé‡‡ç”¨çš„mark tokençš„æ–¹æ³•ä»£æ›¿position embeddingï¼Œä¸çŸ¥é“ä¸¤ä¸ªå¯¹æ¯”é‚£ç§æ•ˆæžœä¼šæ›´å¥½ä¸€äº›ã€‚ç›´è§‚è§‰å¾—éƒ½æ˜¯æ ‡è®°ä½ç½®ä¿¡æ¯ã€‚ ExperimentsPre-training Dataset we use English Wikipedia as our pre-training corpus and align text to Wiki-data 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities before pre-training ERINE, entity embeddings by TransE sample part of Wikidata which contains 5, 040, 986 entities and 24, 267, 796 fact triples Training Details We also fine-tune ERNIE on the distant supervised dataset, i.e., FIGER (Ling et al., 2015) we use TAGME (Ferragina and Scaiella, 2010) to extract the entity mentions in the sentences and link them to their corresponding entities in KGs Entity Typingdatasettwo well-established datasets FIGER (Ling et al., 2015) and Open Entity (Choi et al., 2018). The training set of FIGER is labeled with distant supervision, and its test set is annotated by human. Open Entity is a completely manually-annotated dataset. Comparble model NFGEC NFGEC is a hybrid model proposed by Shimaoka et al. (2016) UFET (Choi et al., 2018) The results on FIGER:However, BERT has lower accuracy than the best NFGEC model. As strict accuracy is the ratio of instances whose predictions are identical to human annotations, it illustrates some wrong labels from distant supervision are learned by BERT due to its powerful fitting ability. Relation Classificationdatasettwo well-established datasets FewRel (Han et al., 2018b) and TACRED (Zhang et al., 2017). FewRel As FewRel does not have any null instance where there isnâ€™t any relation between entities, we adopt macro averaged metrics to present the model performances. Since FewRel is built by checking whether the sentences contain facts in Wiki-data, we drop the related facts in KGs before pretraining for fair comparison TACRED In TACRED, there are nearly 80% null instances so that we follow the previous work (Zhang et al., 2017) to adopt micro averaged metrics to represent the model performances instead of the macro Comparble model CNN:(Zeng et al., 2015). PA-LSTM C-GCN :Zhang et al. (2018) adopt the graph convolution operations to model dependency trees for relation classification. GLUEThe General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is a collection of diverse natural language understanding tasks Ablation Studyexplore the effects of the informative entities and the knowledgeable pretraining task (dEA) for ERNIE using FewRel dataset å®žéªŒéƒ¨åˆ†åšçš„å¾ˆä¸°å¯Œï¼Œæ—¢æœ‰ä¸¤ä¸ªä»»åŠ¡çš„å¯¹æ¯”å®žéªŒï¼Œä¹Ÿæœ‰å¯¹è‡ªèº«æ¨¡å—çš„å¯¹æ¯”å®žéªŒï¼Œå¹¶ä¸”è¿˜å¯¹æ¯”äº†bertæ¥æ£€æµ‹è‡ªå·±æ¨¡åž‹æ˜¯å¦å¯¹GLUEä»»åŠ¡æ•ˆæžœæœ‰é™ä½Žã€‚ future research1) inject knowledge into feature-based pre-training models such as ELMo (Peters et al., 2018); (2) introduce diverse structured knowledge into language representation models such as ConceptNet (Speer and Havasi, 2012) which is different from world knowledge database Wikidata; (3) annotate more real-world corpora heuristically for larger pre-training data å‚è€ƒé“¾æŽ¥ https://blog.csdn.net/summerhmh/article/details/91042273]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
        <category>KG</category>
      </categories>
      <tags>
        <tag>KGR</tag>
        <tag>BERT</tag>
        <tag>KG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Incorporating Literals into Knowledge Graph Embeddingsé˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FIncorporating_Literals_into_Knowledge_Graph_Embeddings%2F</url>
    <content type="text"><![CDATA[è¯»å®Œäº†å‰ä¸¤ç« ï¼Œç®€å•çš„çœ‹äº†ä¸€ä¸‹ä½œè€…æå‡ºçš„æ¨¡åž‹ï¼Œæ„Ÿè§‰å¹¶æ²¡æœ‰å¤ªå¤§ä»·å€¼ï¼Œå°±æ˜¯ç»™å®žä½“è¾“å…¥å¤šåŠ å…¥äº†ä¸€ä¸ªliteralçš„ä¿¡æ¯ï¼ˆåŠ å…¥æ–¹æ³•å¯ä»¥é‡‡ç”¨çº¿æ€§ã€éžçº¿æ€§æˆ–è€…ç¥žç»ç½‘ç»œï¼‰ã€‚ è¯»è®ºæ–‡å‰éœ€è¦å…ˆç†Ÿæ‚‰DistMultã€ComlLExå’ŒConvEæ¨¡åž‹ï¼Œæ­¤è®ºæ–‡æ–¹æ³•æ˜¯æ·»åŠ åœ¨è¿™äº›æ–¹æ³•ä¸Šçš„ã€‚]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
        <category>KG</category>
      </categories>
      <tags>
        <tag>KGE</tag>
        <tag>link prediction</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Learning Knowledge Embeddings by Combining Limit-based Scoring Lossé˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FLearning_Knowledge_Embeddings_by_Combining_Limit-based_Scoring_Loss%2F</url>
    <content type="text"><![CDATA[æ­¤ç¯‡æ–‡ç« æœ€ä¸ºé‡è¦çš„å°±æ˜¯ä½œè€…è®¾è®¡çš„ margin-based ranking loss çš„æ”¹è¿›ï¼Œå¯¹ä¸¤ä¸ªè¶…å‚æ•°$\lambda$å’Œ$\gamma$çš„å®žéªŒï¼Œå¯¹äºŽå®žéªŒç»“æžœæœ‰å¾ˆå¤šå€¼å¾—åˆ†æžä¸Žæ€è€ƒçš„åœ°æ–¹ã€‚ è®ºæ–‡ä¸‹è½½åœ°å€ Problem StatementThe margin-based ranking loss function cannot ensure the fact that the scoring of correct triplets must be low enough to fulfill the translation. research objectivereduce the scoring of correct triplets to fulfill the translation by mending the margin-based ranking loss function Contributions proposing a limit-based ranking loss item combined with margin-based ranking loss extending TransE and TransH to TransE-RS and TransH-RS ModelMargin-based Tanking Lossformula: L_{R}=\sum_{(h, r, t) \in \Delta} \sum_{\left(h^{\prime}, r, t^{\prime}\right) \in \Delta^{\prime}}\left[\gamma_{1}+f_{r}(h, t)-f_{r}\left(h^{\prime}, t^{\prime}\right)\right) ]_{+} The margin-based ranking loss function aims to make the score $f_{r}\left(h^{\prime}, t^{\prime}\right)$ of corrupted triplet higher by at least $\gamma_{1}$ than of positive triplet. cannot be proved $f_{r}(h, t)&lt;\varepsilon$ Limit-based Scoring Lossformula: L_{S}=\sum_{(h, r, t) \in \Delta}\left[f_{r}(h, t)-\gamma_{2}\right]_{+}Finally lossformula: L_{R S}=L_{R}+\lambda L_{S}, \quad(\lambda>0)detail is : \begin{array}{c}{L_{R S}=\sum_{(h, r, t) \in \Delta} \sum_{\left(h^{\prime}, r, t^{\prime}\right) \in \Delta^{\prime}}\left\{\left[\gamma_{1}+f_{r}(h, t)-f_{r}\left(h^{\prime}, t^{\prime}\right)\right]_{+}\right.} \\ {+\lambda\left[f_{r}(h, t)-\gamma_{2}\right]_{+} \}}\end{array}Experimentsdataset Link prediction æ€è€ƒ ä½œè€…åªæ˜¯å¯¹è¡¨æ ¼çš„æ•°æ®è¿›è¡Œäº†é™ˆè¿°ï¼Œæœ‰ä¸€äº›é—®é¢˜å¹¶æ²¡æœ‰è¿›è¡Œåˆ†æžè§£é‡Š å¹¶æ²¡æœ‰åˆ†æžæ¯”å¦‚è¯´ä¸ºä»€ä¹ˆæ”¹è¿›lossåŽçš„transEä¸ºä»€ä¹ˆä¼šæ¯”TransHï¼ˆRã€Dï¼‰æ•ˆæžœè¦å¥½ï¼Ÿ ä¸ºä»€ä¹ˆåœ¨n-to-1ä¸­çš„è¡¨çŽ°æ•ˆæžœæ²¡æœ‰è¾¾åˆ°æœ€å¥½ï¼ˆå…¶ä»–çš„éƒ½è¾¾åˆ°äº†æœ€å¥½ï¼‰ï¼Ÿ é€šè¿‡è¿™ç§æ”¹è¿›å¯ä»¥å‘çŽ°ï¼ŒtransHç›¸æ¯”äºŽTransEå¹¶æ²¡æœ‰æ˜¾è‘—æå‡ï¼ŒåŽŸå› æ˜¯ä»€ä¹ˆï¼Ÿ Triple Classification TransE-RS and TransH-RS have same parameter and operation complexities as TransE and TransH, which is lower than TransR and TransD. Our models randomly initial the entities, not use the learned embeddings by TransE as TransR and TransD. It means that our models have much better ability to overcome the problem of overfitting Distributions of Tripletsâ€™ Scoresaimanalyze the difference between $L_R$ Loss and our $L_RS$ Loss Parameters æ€è€ƒï¼š å¯¹äºŽæˆ‘è‡ªå·±æ­£åœ¨åšçš„å®žéªŒï¼šæ˜¯ä¸æ˜¯æˆ‘è‡ªå·±ç”¨çš„é—´éš”å¤ªå°äº† result æ€è€ƒ è¿™éƒ¨åˆ†çš„å®žéªŒå€¼å¾—å€Ÿé‰´ï¼Œå®ƒå¯ä»¥ç›¸å¯¹äºŽç›´è§‚çš„å¯ä»¥å±•ç¤ºå‡ºä¸ºä»€ä¹ˆæ•ˆæžœä¼šå¥½ã€‚ æ¯”å¦‚å¯¹äºŽä¸Šè¿°ä¸ºä»€ä¹ˆæ”¹è¿›åŽçš„transEçš„æ•ˆæžœä¼šæ›´å¥½ çœ‹åˆ°æœ€åŽçš„åˆ†æ•°åˆ†å¸ƒtransE-RSçš„åˆ†å¸ƒæ•ˆæžœå’ŒTrans-Hçš„ååˆ†æŽ¥è¿‘ï¼Œ è€ŒtransEçš„æ¨¡åž‹è¾ƒä¸ºç®€å•ï¼Œå¯èƒ½æœ€ç»ˆlossæœ€å°åŒ–ä¼šä½¿å¾—æ¨¡åž‹å……åˆ†è¡¨è¾¾ï¼Œè€Œå…¶ä»–æ¨¡åž‹å¼•å…¥äº†æ›´å¤šçš„å‡è®¾å¯èƒ½ä¼šå¸¦æ¥æ›´å¤šçš„å™ªå£° ä¹Ÿå¯èƒ½å½“losså¾ˆå°æ—¶ï¼Œå…¶ä»–çš„å‡è®¾æ¡ä»¶å‘æŒ¥ä½œç”¨çš„å¾ˆå°ï¼ˆè‡³å°‘ä»Žå®žéªŒç»“æžœæ¥çœ‹æ˜¯çš„ï¼Œä½†æ˜¯è¿˜æœ‰å¾…äºŽè¿›ä¸€æ­¥è®¾è®¡å®žéªŒéªŒè¯ï¼‰ Discussion of ParametersDiscussion on Î³1 and Î³2. We find that Î³2 = 3Î³1 or Î³2 = 4Î³1 is better for link prediction, but for triplet classification there are not obvious characteristics on Î³1 and Î³2. a lower Î³2 is expected to ensure the golden condition $\mathbf{h}+\mathbf{r} \approx \mathbf{t}$ for positive triplets, but an entity needs to satisfy many golden coditions at the same time. æ€è€ƒ æ—¢ç„¶å¦‚ä½œè€…è¯´ï¼Œé‚£ä¹ˆç†è®ºä¸ŠtransHçš„æ•ˆæžœåº”è¯¥å¾ˆå¥½æ‰å¯¹ï¼Œä½†æ˜¯ç»“æžœå¹¶ä¸æ˜¯è¿™æ ·çš„ï¼Œè¿™åˆäº§ç”ŸçŸ›ç›¾ã€‚ Discussion on Î» æ€è€ƒ çœ‹åˆ°Î»å¹¶æ²¡æœ‰å¯¹æ¨¡åž‹å½±å“å¹¶æ²¡å¾ˆå¤§ Î»åœ¨1å·¦å³æ˜¯æ•ˆæžœä¼šæ¯”è¾ƒå¥½ Î»å’Œmarginä¼šä¸ä¼šäº§ç”Ÿå…³è”ï¼Ÿ]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
        <category>KG</category>
      </categories>
      <tags>
        <tag>KG</tag>
        <tag>transH</tag>
        <tag>margin loss</tag>
        <tag>transE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Knowledge Graph Embedding by Translating on Hyperplanesé˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FKnowledge%20Graph%20Embedding%20by%20Translating%20on%20Hyperplanes%2F</url>
    <content type="text"><![CDATA[ä½œä¸ºtransç³»åˆ—ç»å…¸æ–‡çŒ®ï¼Œå¿…è¯»ã€‚æ–‡ç« ä¸»è¦ç²¾åŽåœ¨äºŽè¿™ç§è¶…å¹³é¢æƒ³æ³•çš„ç”±æ¥è§£å†³äº†åŒä¸€å®žä½“çš„å¤šå…³ç³»é—®é¢˜ã€‚ Authors proposed TransH which models a relation as a hyperplane together with a translation operation on it. It solves the problem of multi-relation and makes a good trade-off between model capacity and efficiency. æŽ¨æµ‹transHçš„æƒ³æ³•æ¥æº æ—¢ç„¶å®žé™…æ˜¯è¡¨è¾¾åŒä¸€å…³ç³»ä¸åŒå®žä½“æœ€åŽé€šè¿‡TransEåŽä¼šè¶‹äºŽä¸€è‡´ï¼Œé‚£ä¹ˆæˆ‘ç›´æŽ¥é€šè¿‡ä¸€ä¸ªä¸­ä»‹æ¥è¿›è¡Œæ˜ å°„å°†åŒä¸€è¡¨ç¤ºæ˜ å°„æˆä¸åŒå‘é‡è¡¨ç¤ºï¼Œé‚£ä¹ˆè¿™äº›å‘é‡è¡¨ç¤ºå°±å¯ä»¥ä»£è¡¨ä¸åŒçš„å®žä½“ï¼Œå°±è¾¾åˆ°äº†ä¸åŒå®žä½“æ‹¥æœ‰ä¸åŒè¡¨ç¤ºçš„ç›®çš„ã€‚å› ä¸ºå…³ç³»æ˜¯ä¸å˜çš„æ‰€ä»¥æƒ³åˆ°äº†å°†å…³ç³»ä½œä¸ºæ˜ å°„å¹³é¢ï¼Œè®©å®žä½“å‘é‡å‘å…¶ä¸­æ˜ å°„ã€‚ research objective solves the problem of multi-relation makes a good trade-off between model capacity and efficiency Problem Statement TransE canâ€™t deal with reflexive, one-to-many, many-to-many and many -to-one relations some complex model sacrifice efficiency in the process(although can deal with transEâ€™s problem) Contribution proposing a method named translation on hyperplanes(TransH) interpreting a relation as a translating operation on a hyperplane proposing a simple trick to reduce the chance of false negative labeling Embedding by Translating on HyperplanesRelationsâ€™ Mapping Properties in EmbeddingtransE the representation of an entity is the same when involved in any relations, ignoring distributed representations of entities when invovled in different relaions Translating on Hyperplanes (TransH)åŒä¸€ä¸ªå®žä½“åœ¨ä¸åŒå…³ç³»ä¸­çš„æ„ä¹‰ä¸åŒï¼ŒåŒæ—¶ä¸åŒå®žä½“ï¼Œåœ¨åŒä¸€å…³ç³»ä¸­çš„æ„ä¹‰ï¼Œä¹Ÿå¯ä»¥ç›¸åŒã€‚ å°†æ¯ä¸ªå…³ç³»å®šä¹‰åœ¨ä¸€ä¸ªç‹¬ç‰¹çš„å¹³é¢å‘¢ï¼Œåœ¨è¯¥å¹³é¢å†…æœ‰ç¬¦åˆè¯¥å…³ç³»çš„transEçš„è¡¨ç¤ºï¼ˆh,r,t)ï¼Œå¤šåŠ å…¥çš„ä»£è¡¨è¯¥å¹³é¢çš„æ³•å‘é‡å®Œæˆäº†å°†ä¸åŒå®žä½“å‘å¹³é¢å†…å’Œhï¼Œtè½¬åŒ–çš„ä»»åŠ¡ï¼Œä½¿å¾—åŒä¸€å…³ç³»çš„ä¸åŒå®žä½“æ‹¥æœ‰ä¸åŒçš„è¡¨ç¤ºï¼Œä½†æ˜¯åœ¨å…³ç³»å¹³é¢å†…çš„æŠ•å½±ç›¸åŒï¼›åŒä¸€å®žä½“å¯ä»¥åœ¨ä¸åŒçš„å…³ç³»å¹³é¢å†…æ‹¥æœ‰ä¸åŒçš„å«ä¹‰ï¼ˆå¹³é¢å†…çš„æŠ•å½±ï¼‰ å¦‚å›¾æ‰€ç¤ºï¼Œå¯¹äºŽæ­£ç¡®çš„ä¸‰å…ƒç»„æ¥è¯´$(h, r, t) \in \Delta$ï¼Œæ‰€éœ€æ»¡è¶³çš„å…³ç³»å¦‚å›¾æ‰€ç¤ºã€‚é‚£ä¹ˆå¯¹äºŽä¸€ä¸ªå®žä½“$hâ€™â€™$å¦‚æžœæ»¡è¶³$\left(h^{\prime \prime}, r, t\right) \in \Delta $ï¼Œåœ¨transEä¸­æ˜¯éœ€è¦$hâ€™â€™=h$ï¼Œè€Œåœ¨transHä¸­åˆ™å°†çº¦æŸæ”¾å®½åˆ°$h,hâ€™â€™$åœ¨$W_r$ä¸Šçš„æŠ•å½±ç›¸åŒå°±å¯ä»¥äº†ï¼Œä¹Ÿå¯ä»¥å®žçŽ°å°†$h,hâ€™â€™$åŒºåˆ†å¼€å¹¶ä¸”å…·æœ‰ä¸åŒçš„è¡¨ç¤ºã€‚ ç›®æ ‡å‡½æ•°scoring functionï¼š d(h+r, t)=f_{r}(h, t)=\left\|h_{\perp}+d_{r}-t_{\perp}\right\|_{2}^{2}As the hyperplane $W_r$, the $w_r$ is the normal vector of it, and $\left|w_{r}\right|_{2}^{2}=1$, so the projection $h$ in $w_r$ is: h_{w_{r}}=w_r^{T} h w_rå…¶ä¸­ï¼Œ$w_r^{T} h=|w_r||h| \cos \theta$å¯ä»¥è¡¨ç¤º$h$åœ¨$w_r$ä¸Šçš„æŠ•å½±çš„é•¿åº¦å’Œ$w_r$é•¿åº¦çš„ä¹˜ç§¯ï¼Œå› ä¸º$\left|w_{r}\right|_{2}^{2}=1$,æ‰€ä»¥å¯ä»¥ä»£è¡¨æŠ•å½±çš„é•¿åº¦ï¼Œå†ä¹˜ä¸Šå•ä½å‘é‡å³å¯è¡¨ç¤ºæŠ•å½±å‘é‡ã€‚æ‰€ä»¥ï¼š \mathbf{h}_{\perp}=\mathbf{h}-\mathbf{w}_{r}^{\top} \mathbf{h w}_{r}, \quad \mathbf{t}_{\perp}=\mathbf{t}-\mathbf{w}_{r}^{\top} \mathbf{t} \mathbf{w}_{r}å¦‚å›¾æ‰€ç¤ºï¼š the score function is: f_{r}(\mathbf{h}, \mathbf{t})=\left\|\left(\mathbf{h}-\mathbf{w}_{r}^{\top} \mathbf{h w}_{r}\right)+\mathbf{d}_{r}-\left(\mathbf{t}-\mathbf{w}_{r}^{\top} \mathbf{t} \mathbf{w}_{r}\right)\right\|_{2}^{2}Trainingloss function consists of margin-based ranking loss and some constraints: \begin{aligned} \mathcal{L} &=\sum_{(h, r, t) \in \Delta\left(h^{\prime}, r^{\prime}, t^{\prime}\right) \in \Delta_{(h, r, t)}}\left[f_{r}(\mathbf{h}, \mathbf{t})+\gamma-f_{r^{\prime}}\left(\mathbf{h}^{\prime}, \mathbf{t}^{\prime}\right)\right]_{+} \\ &+C\left\{\sum_{e \in E}\left[\|\mathbf{e}\|_{2}^{2}-1\right]_{+}+\sum_{r \in R}\left[\frac{\left(\mathbf{w}_{r}^{\top} \mathbf{d}_{r}\right)^{2}}{\left\|\mathbf{d}_{r}\right\|_{2}^{2}}-\epsilon^{2}\right]_{+}\right\}, \text { (4) } \end{aligned}the constraints: \forall e \in E,\|\mathrm{e}\|_{2} \leq 1, // \text { scale }\\ \forall r \in R,\left|\mathbf{w}_{r}^{\top} \mathbf{d}_{r}\right| /\left\|\mathbf{d}_{r}\right\|_{2} \leq \epsilon, / / \text { orthogonal }\\ \forall r \in R,\left\|\mathbf{w}_{r}\right\|_{2}=1, / / \text { unit normal vector } the second grantees the translation vectot $d_r$ is in the hyperplane they project each $w_r$ to unit $l_2$-ball before visiting each mini-batch æ—¢ç„¶transHå¯ä»¥å®Œæˆå°†åŒä¸€å®žä½“æ˜ å°„åˆ°ä¸åŒçš„å…³ç³»å¹³é¢æ¥èŽ·å¾—ä¸åŒçš„å«ä¹‰ï¼Œé‚£ä¹ˆæˆ‘è§‰å¾— æ˜¯ä¸æ˜¯ä¸åŒä»£è¡¨åŒä¸€å«ä¹‰çš„æŠ•å½±è¡¨ç¤ºåº”è¯¥ç›¸åŒæˆ–è€…ç›¸ä¼¼ è¿™æ ·æ˜¯ä¸æ˜¯å¯ä»¥è§£å†³åŒä¸€ä¸ªå®žä½“çš„å¤šä¹‰æ€§é—®é¢˜ã€‚ Reducing Ralse Negative LabelsAuthors set different probabilities for replacing the head or tail entity depending on the mapping property of the relation (one-to-many, many-to-one, many-to-many) give more chance to replacing the head entity if the relation is one-to-many åˆ†åˆ«ç»Ÿè®¡æ¯ä¸ªå¤´å®žä½“å¯¹åº”å°¾å®žä½“çš„æ•°é‡ï¼ˆåä¹‹äº¦ç„¶ï¼‰ï¼ŒæŒ‰å æ¯”è¿›è¡Œç”Ÿæˆè´Ÿæ ·ä¾‹ é€šè¿‡è¿™æ ·çš„æ–¹å¼ï¼Œä¾‹å¦‚one-manyå…³ç³»ï¼Œæ›¿æ¢å¤´å®žä½“æ˜¾ç„¶æ›´ä¸å®¹æ˜“å¾—åˆ°æ­£æ ·ä¾‹ï¼ˆå› ä¸ºåªæœ‰ä¸€ç§å¤´å®žä½“æ˜¯å¯¹çš„ï¼Œç„¶è€Œæ›¿æ¢å°¾å®žä½“å› ä¸ºå¯¹äºŽå¤´å®žä½“å¯¹åº”è¯¥å…³ç³»çš„å°¾å®žä½“æ›´å¤šï¼Œè¯´ä¸å®šå°±æœ‰å…¶ä»–ä¸åœ¨æ­¤manyä¸­çš„å°¾å®žä½“ç¬¦åˆè¿™ä¸ªå…³ç³»ã€‚ ç›¸æ¯”ä¹‹ä¸‹æˆ‘è®¤ä¸ºåœ¨ã€ŠBootstrapping-Entity-Alignment-with-Knowledge-Graph-Embeddingã€‹é‡‡ç”¨çš„å‡åŒ€æˆªæ–­è´Ÿé‡‡æ ·æ•ˆæžœä¼šæ›´å¥½ä¸€äº› Experimentsthe detail can be seen in the paper Link predictionoutperform TransE in one-to-oneAuthors explain: entities are connected with relations so that better embeddings of some parts lead to better results on the whole. æˆ‘æ˜¯è§‰å¾—æœ‰äº›ç‰µå¼ºï¼Œä¸è¿‡è¦æ˜¯ç¡¬ç†è§£ä¹Ÿæ˜¯å¯ä»¥ï¼Œæ¯•ç«Ÿé€šè¿‡æŠ•å½±ç›¸å½“äºŽæŠŠå®žä½“å’Œå…³ç³»è¿›è¡Œäº†ä¸€ä¸ªè”ç³»ï¼Œå¯èƒ½è¿™ä¸ªå¢žå¼ºäº†æ•ˆæžœã€‚ Triplets ClassificationThis means FB13 is a very dense subgraph where strong correlations exist between entities Relational Fact Extraction from Text Actually, knowledge graph embedding is able to score a candidate fact, without observing any evidence from ex- ternal text corpus å¯ä»¥çœ‹åˆ°ä»Ž14å¹´å¼€å§‹å°±æœ‰åˆ©ç”¨çŸ¥è¯†å›¾è°±æ¥ä»Žæ–‡æœ¬æŠ½å–å…³ç³»ï¼Œæœ€è¿‘è¿™ä¸ªåº”ç”¨å¥½åƒåˆæœ‰èµ·è‰²ï¼Œè¿™ä¸ªä¹Ÿå¯ä½œä¸ºè‡ªå·±å®žéªŒçš„ä¸€éƒ¨åˆ†ã€‚ Reference https://blog.csdn.net/MonkeyDSummer/article/details/85273843]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
        <category>KG</category>
      </categories>
      <tags>
        <tag>KGE</tag>
        <tag>transH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Attention Is All You Needé˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FAttention%20Is%20All%20You%20Need%2F</url>
    <content type="text"><![CDATA[transformer æ˜¯ä¸€ä¸ªå®Œå…¨ç”±æ³¨æ„åŠ›æœºåˆ¶ç»„æˆçš„æ­å»ºçš„æ¨¡åž‹ï¼Œæ¨¡åž‹å¤æ‚åº¦ä½Žï¼Œå¹¶å¯ä»¥è¿›è¡Œå¹¶è¡Œè®¡ç®—ï¼Œä½¿å¾—è®¡ç®—é€Ÿåº¦å¿«ã€‚åœ¨ç¿»è¯‘æ¨¡åž‹ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ•ˆæžœã€‚æœ¬ç¯‡è®ºæ–‡å±žäºŽç»å…¸å¿…è¯»è®ºæ–‡ï¼Œé˜…è¯»ç¬”è®°ä¸­å¯¹ä¸€äº›ä¸æ¸…æ¥šçš„åœ°æ–¹è¿›è¡Œäº†æ±‰è¯­è§£é‡Šï¼Œè¯»å®Œè®ºæ–‡åŽé˜…è¯»å‚è€ƒé“¾æŽ¥ä»¥åŠ æ·±ç†è§£ã€‚ è®ºæ–‡ä¸‹è½½åœ°å€ research objectivebased solely on attention mechanisms, increase parallezable computation and decrease train time Problem Statementrecurrent models hidden states depended on previous hidden state and the input for position precludes parallelization contribution Transformer, eschewing recurrence and instead relying entirely on an attention mechanism, solve the long dependency problem. draw global dependecies between input and output allow for significantly more parallelization Model ArchitectureThe Transformer uses stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. Encoder and Decoder StacksEncoder compose of a stack of N identical layers each layers has two sub-layers multi-head self-attention mechanism position-wise fully connected feed forward network employ a residual connection around each of the two sub-layers, followed by layer normalization the output of each sub-layer is $\text { LayerNorm }(x+\text { Sublayer }(x))$ encoderä¸­çš„Qï¼ŒKï¼ŒVéƒ½æ˜¯å­¦å‡ºæ¥çš„ Decoder composed of a stack of N identical layers has the same two sub-layers as the encoder the third sub-layer between the two sub-layers perform multi-head attention over the output of the encoder stack add a mask to modify the self-attention sub-layer to ensure that the predictions for position $i$ can depend only the known outputs at positions less than $i$ é™¤äº†ç¬¬ä¸€å­å±‚ä¸­Qï¼ŒKï¼ŒVæ˜¯è‡ªå·±å­¦å‡ºæ¥çš„ï¼Œç¬¬äºŒä¸ªå­å±‚åˆ©ç”¨äº†encoderä¸­çš„Kï¼ŒVã€‚ Attention Scaled Dot-Product Attentionthe calculation process as the left at the figure 2. formulaï¼š \text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V where $\sqrt{d_{k}}$ is to prevent value from getting too large, which will push the softmax function into regions where it has extremely small gradients. å› ä¸ºé‡çº§å¤ªå¤§ï¼ŒsoftmaxåŽå°±éž0å³1äº†ï¼Œä¸å¤Ÿâ€œsoftâ€äº†ã€‚ä¹Ÿä¼šå¯¼è‡´softmaxçš„æ¢¯åº¦éžå¸¸å°ã€‚ä¹Ÿå°±æ˜¯è®©softmaxç»“æžœä¸ç¨€ç–(é—®å·è„¸ï¼Œé€šå¸¸äººä»¬å¸Œæœ›å¾—åˆ°æ›´ç¨€ç–çš„attentionå§)ã€‚ $Q, K,V$ is a matrix needed to learn from input. Multi-Head Attentionhelps the encoder look at other words in the input sentence as it encodes a specific word in the figure 2 right. itâ€™s beneficial to lineraly project the quries, keys and values $h$ times with different, learned projections to $d_k, d_k, d_v$ dimensions, respectively concatenate the output \begin{aligned} \text { MultiHead }(Q, K, V) &=\text { Concat (head }_{1}, \ldots, \text { head }_{h} ) W^{O} \\ \text { where head }_{i} &=\text { Attention }\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right) \end{aligned}where $W_{i}^{Q} \in \mathbb{R}^{d_{\text { model }} \times d_{k}}, W_{i}^{K} \in \mathbb{R}^{d_{\text { model }} \times d_{k}}, W_{i}^{V} \in \mathbb{R}^{d_{\text { model }} \times d_{v}}, W^{O} \in \mathbb{R}^{h d_{v} \times d_{\mathrm{model}}}$ Applications of Attention in our Model the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. mimicing the seq-to-seq self -attention can make that each position in the encoder can attend to all positions in the previous layer of the encoder We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to âˆ’âˆž) all values in the input of the softmax which correspond to illegal connections. See Figure 2ã€‚å³æˆ‘ä»¬åªèƒ½attendåˆ°å‰é¢å·²ç»ç¿»è¯‘è¿‡çš„è¾“å‡ºçš„è¯è¯­ï¼Œå› ä¸ºç¿»è¯‘è¿‡ç¨‹æˆ‘ä»¬å½“å‰è¿˜å¹¶ä¸çŸ¥é“ä¸‹ä¸€ä¸ªè¾“å‡ºè¯è¯­ï¼Œè¿™æ˜¯æˆ‘ä»¬ä¹‹åŽæ‰ä¼šæŽ¨æµ‹åˆ°çš„ã€‚å³å°†$QK^T$ä¸­æ¯è¡Œè¯¥å•è¯ä¹‹åŽçš„æ•°å€¼åšå¤„ç†ï¼Œä½¿å¾—å‰é¢çš„å•è¯çœ‹ä¸åˆ°åŽé¢å•è¯æ‰€å çš„é‡è¦æ€§ç¨‹åº¦ã€‚ Position-wise Feed-Forward Networks applied to each position separately and identically feed-forward network consists of tow linear transformations with a ReLU activation. formula: \mathrm{FFN}(x)=\max \left(0, x W_{1}+b_{1}\right) W_{2}+b_{2} å°ç»“ ä¸ºä»€ä¹ˆå«å¼ºè°ƒposition-wise? è§£é‡Šä¸€: è¿™é‡ŒFFNå±‚æ˜¯æ¯ä¸ªpositionè¿›è¡Œç›¸åŒä¸”ç‹¬ç«‹çš„æ“ä½œï¼Œæ‰€ä»¥å«position-wiseã€‚å¯¹æ¯ä¸ªpositionç‹¬ç«‹åšFFNã€‚ è§£é‡ŠäºŒï¼šä»Žå·ç§¯çš„è§’åº¦è§£é‡Šï¼Œè¿™é‡Œçš„FFNç­‰ä»·äºŽkernel_size=1çš„å·ç§¯ï¼Œè¿™æ ·æ¯ä¸ªpositionéƒ½æ˜¯ç‹¬ç«‹è¿ç®—çš„ã€‚å¦‚æžœkernel_size=2ï¼Œæˆ–è€…å…¶ä»–ï¼Œpositionä¹‹é—´å°±å…·æœ‰ä¾èµ–æ€§äº†ï¼Œè²Œä¼¼å°±ä¸èƒ½å«åšposition-wiseäº† ä¸ºä»€ä¹ˆè¦é‡‡ç”¨å…¨è¿žæŽ¥å±‚ï¼Ÿ ç›®çš„: å¢žåŠ éžçº¿æ€§å˜æ¢ å¦‚æžœä¸é‡‡ç”¨FFNå‘¢ï¼Ÿæœ‰ä»€ä¹ˆæ›¿ä»£çš„è®¾è®¡ï¼Ÿ ä¸ºä»€ä¹ˆé‡‡ç”¨2å±‚å…¨è¿žæŽ¥ï¼Œè€Œä¸”ä¸­é—´å‡ç»´ï¼Ÿ è¿™ä¹Ÿæ˜¯æ‰€è°“çš„bottle neckï¼Œåªä¸è¿‡ä½Žç»´åœ¨IOä¸Šï¼Œä¸­é—´é‡‡ç”¨high rank Embeddings and SoftmaxSharing the same weight maatrix between the two embedding layers and the pre-softmax linear transformation Positional EncodingUsing sine and xosine functions of different frequencies: P E_{(p o s, 2 i)}=\sin \left(p o s / 10000^{2 i / d_{\text { model }}}\right) \\ P E_{(p o s, 2 i+1)}=\cos \left(p o s / 10000^{2 i / d_{\mathrm{model}}}\right) where $pos$ is the postiiton and $i$ is the dimension Authors hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$, $PE_{pos+k}$can be represented as a linear function of $PE_{pos}$ ä½†åœ¨è¯­è¨€ä¸­ï¼Œç›¸å¯¹ä½ç½®ä¹Ÿå¾ˆé‡è¦ï¼ŒGoogleé€‰æ‹©å‰è¿°çš„ä½ç½®å‘é‡å…¬å¼çš„ä¸€ä¸ªé‡è¦åŽŸå› æ˜¯ï¼šç”±äºŽæˆ‘ä»¬æœ‰$\sin (\alpha+\beta)=\sin \alpha \cos \beta+\cos \alpha \sin \beta$ä»¥åŠ$\cos (\alpha+\beta)=\cos \alpha \cos \beta-\sin \alpha \sin \beta$ï¼Œè¿™è¡¨æ˜Žä½ç½®$p+k$çš„å‘é‡å¯ä»¥è¡¨ç¤ºæˆä½ç½®$p$çš„å‘é‡çš„çº¿æ€§å˜æ¢ï¼Œè¿™æä¾›äº†è¡¨è¾¾ç›¸å¯¹ä½ç½®ä¿¡æ¯çš„å¯èƒ½æ€§ã€‚ Compared with using learned positional embeddings, the sinusoidal version may allow the model to extrapolate to sequence lengths longer than the ones encountered during training. æ³¨æ„ç”±äºŽè¯¥æ¨¡åž‹æ²¡æœ‰recurrenceæˆ–convolutionæ“ä½œï¼Œæ‰€ä»¥æ²¡æœ‰æ˜Žç¡®çš„å…³äºŽå•è¯åœ¨æºå¥å­ä¸­ä½ç½®çš„ç›¸å¯¹æˆ–ç»å¯¹çš„ä¿¡æ¯ï¼Œä¸ºäº†æ›´å¥½çš„è®©æ¨¡åž‹å­¦ä¹ ä½ç½®ä¿¡æ¯ï¼Œæ‰€ä»¥æ·»åŠ äº†position encodingå¹¶å°†å…¶å åŠ åœ¨word embeddingä¸Šã€‚ Why Self-Attention total computational complexity per layer the amount of computation that can be parallelized the path between long-range dependencies in the network self-attention|ï¼š $QK^TV$ç›¸ä¹˜ï¼Œæ ¹æ®çŸ©é˜µå¤§å°ï¼ˆåˆ†åˆ«ä¸º$nd, nd, nd$éœ€è¦çš„å¤æ‚åº¦ä¸º$O(n^2d2)$ï¼ˆå¿½ç•¥softmaxï¼‰ maximum path lengthï¼šå›¾è¯´æ˜Žäº†ï¼Œ å¯¹äºŽself-attention, target node (ç”Ÿæˆçš„é‚£ä¸ªç‚¹) å®žé™…ä¸Šå’Œ è¾“å…¥ä¸­çš„ä»»æ„ä¸€ç‚¹çš„è·ç¦»æ˜¯ç›¸åŒçš„ convolutional: æ¯å±‚æœ‰kä¸ªå·ç§¯å’Œï¼Œå¯¹äºŽinput matixï¼ˆ$nd$)çŸ©é˜µæ‰§è¡Œå·ç§¯éœ€è¦è¿ç®—å¤æ‚åº¦æ˜¯$nd*(d-m)$, mä¸ºå·ç§¯å’Œå®½åº¦æ˜¯ä¸€ä¸ªæ¯”è¾ƒå°çš„å¸¸æ•°ï¼Œæ‰€ä»¥æ€»å¤æ‚åº¦ä¸º$O\left(k \cdot n \cdot d^{2}\right)$,ä½œè€…æåˆ°å¯åˆ†ç¦»çš„å·åŸºå±‚æš‚æ—¶è¿˜ä¸äº†è§£ï¼Œå¯ä»¥ä»¥åŽæŸ¥é˜…ã€‚ maximum path length: æ­£å¸¸å·ç§¯å’Œçš„è·ç¦»æ˜¯$O(n/k)$, ä½†å¦‚æžœæ˜¯å †å å·ç§¯å¦‚å›¾ï¼š å°±å¯ä»¥å‡å°åˆ°$O\left(\log _{k}(n)\right)$ recurrent: è®¡ç®—æ˜¯æ¯ä¸ªè¯å‘é‡ä¹˜éšè—æƒé‡($d*d$)ï¼Œæ‰€ä»¥æ˜“å¾—è®¡ç®—å¤æ‚åº¦ï¼š$O\left(n \cdot d^{2}\right)$ maximum path length: é•¿åº¦å°±æ˜¯nã€‚ æ“ä½œæ­¥éª¤è¦ä»Žç¬¬ä¸€ä¸ªåˆ°ç¬¬nä¸ªä¸ºnæ­¥ï¼Œæ˜¯æœ‰é¡ºåºçš„ã€‚å…¶ä»–çš„éƒ½æ²¡æœ‰é¡ºåºè¦æ±‚ self-attentin(restricted) ç›¸å½“äºŽåªè¾“å…¥ré‚»è¿‘çš„å¥å­é•¿åº¦ï¼Œè‡ªç„¶å¯ä»¥å¾—åˆ°å¦‚å›¾ç»“æžœ TrainOptimizer \text { lrate }=d_{\text { model }}^{-0.5} \cdot \min \left(\text {step}_{-} n u m^{-0.5}, \text { step }_{-} n u m \cdot \text { warmup steps }^{-1.5}\right) increasing the learning rate linearly for the first warmup_steps training steps decreasing it thereafter proportionally to the inverse square root of the step number RegularizationResidual Dropout apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks Label Smoothing This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score Resultmachine TranslationEven our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models Model Variations ç¼ºç‚¹ç¼ºç‚¹åœ¨åŽŸæ–‡ä¸­æ²¡æœ‰æåˆ°ï¼Œæ˜¯åŽæ¥åœ¨Universal Transformersä¸­æŒ‡å‡ºçš„ï¼Œåœ¨è¿™é‡ŒåŠ ä¸€ä¸‹å§ï¼Œä¸»è¦æ˜¯ä¸¤ç‚¹ï¼š å®žè·µä¸Šï¼šæœ‰äº›rnnè½»æ˜“å¯ä»¥è§£å†³çš„é—®é¢˜transformeræ²¡åšåˆ°ï¼Œæ¯”å¦‚å¤åˆ¶stringï¼Œå°¤å…¶æ˜¯ç¢°åˆ°æ¯”è®­ç»ƒæ—¶çš„sequenceæ›´é•¿çš„æ—¶ ç†è®ºä¸Šï¼štransformerséžcomputationally universalï¼ˆå›¾çµå®Œå¤‡ï¼‰ï¼Œï¼ˆæˆ‘è®¤ä¸ºï¼‰å› ä¸ºæ— æ³•å®žçŽ°â€œwhileâ€å¾ªçŽ¯ æ€»ç»“Transformeræ˜¯ç¬¬ä¸€ä¸ªç”¨çº¯attentionæ­å»ºçš„æ¨¡åž‹ï¼Œä¸ä»…è®¡ç®—é€Ÿåº¦æ›´å¿«ï¼Œåœ¨ç¿»è¯‘ä»»åŠ¡ä¸Šä¹ŸèŽ·å¾—äº†æ›´å¥½çš„ç»“æžœã€‚ GoogleçŽ°åœ¨çš„ç¿»è¯‘åº”è¯¥æ˜¯åœ¨æ­¤åŸºç¡€ä¸Šåšçš„ï¼Œä½†æ˜¯æ•°æ®é‡å¤§å¯èƒ½ç”¨transformerå¥½ä¸€äº›ï¼Œå°çš„è¯è¿˜æ˜¯ç»§ç»­ç”¨rnn-based modelã€‚ èŠ±äº†ä¸å°‘æ—¶é—´ï¼Œç®—æ˜¯ç†è§£äº†attentionå’Œtransformerï¼Œå¯¹å…¶ä¸­ä¸æ˜¯å¾ˆæ¸…æ¥šçš„ç‚¹å¦‚attentionçš„å†…éƒ¨ä¸­Qï¼ŒKï¼ŒVå…·ä½“æ˜¯ä»€ä¹ˆåœ¨self-attentionå’Œmulti-head attentionä¸­å¤§å°æ˜¯ä¸åŒçš„ï¼Œå¦‚ä½•maskï¼Œå¦‚ä½•è®¡ç®—å¤æ‚ï¼Œç­‰è¿›è¡ŒæŸ¥é˜…èµ„æ–™å¼„æ‡‚äº†ã€‚æ€»ä½“æ¥è¯´è¿˜æ˜¯æ”¶èŽ·å¾ˆå¤§çš„ã€‚å‡†å¤‡åœ¨çœ‹ä¸€äº›ä»£ç è®²è§£ã€‚ reference Attentionæœºåˆ¶è¯¦è§£ï¼ˆäºŒï¼‰â€”â€”Self-Attentionä¸ŽTransformer - å·é™€å­¦è€…çš„æ–‡ç«  - çŸ¥ä¹Žhttps://zhuanlan.zhihu.com/p/47282410 https://jalammar.github.io/illustrated-transformer/ï¼ˆè¿™ä¸ªè®²çš„æ¯”è¾ƒè¯¦ç»†ï¼Œå»ºè®®çœ‹å®Œè®ºæ–‡åŽå†çœ‹ä¸€éè¿™ä¸ªä¼šåŠ æ·±ç†è§£ï¼‰ ã€NLPã€‘Transformerè¯¦è§£ - æŽå¦‚çš„æ–‡ç«  - çŸ¥ä¹Žhttps://zhuanlan.zhihu.com/p/44121378 https://blog.eson.org/pub/664e9bad/ https://mp.weixin.qq.com/s/J-anyCuwLd5UYjTsUFNT1g]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
        <category>classical</category>
      </categories>
      <tags>
        <tag>attention</tag>
        <tag>transformer</tag>
        <tag>translation</tag>
        <tag>classical</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Graph Neural Networks with Generated Parameters for Relation Extractioné˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FGraph_Neural_Networks_with_Generated_Parameters_for_Relation%2F</url>
    <content type="text"><![CDATA[æœ¬æ–‡å°†GNNsåº”ç”¨åˆ°å¤„ç†éžç»“æž„åŒ–æ–‡æœ¬çš„ï¼ˆå¤šè·³ï¼‰å…³ç³»æŽ¨ç†ä»»åŠ¡æ¥è¿›è¡Œå…³ç³»æŠ½å–ã€‚é‡‡ç”¨ä»Žå¥å­åºåˆ—ä¸­èŽ·å–çš„å®žä½“æž„å»ºå…¨é“¾æŽ¥å›¾ï¼Œåº”ç”¨ç¼–ç ï¼ˆsequence modelï¼‰ï¼Œä¼ æ’­ï¼ˆèŠ‚ç‚¹é—´ä¿¡æ¯ï¼‰å’Œåˆ†ç±»ï¼ˆé¢„æµ‹ï¼‰ä¸‰ä¸ªæ¨¡å—æ¥å¤„ç†å…³ç³»æŽ¨ç†ã€‚æœ¬æ–‡æä¾›äº†ä¸‰ä¸ªæ•°æ®é›†ã€‚ problem statement existing relation extraction models fail to infer the relationship without multi-hop relational reasoning. existing GNNs canâ€™t process multi-hop relational reasoning in natural language relational reasoning research objectiveenable GNNs to porcess relational reasoning on unstructed text inputs contribution extend a GNN with generated parameters, which could be applied to process relational reasoning on unstructured inputs verify GP-GNNs in the taks of relation extraction from text; present three datasets GP-GNNs construct a fully-connected graph with the entities in the sequence of text employs three models to process relational reasoning an encoding modul: enable edges to encode rich information from natural languages a propagation modul: propagates realtional information among various nodes a classification modul: make prediction with node representations As compared to tradtional GNNs, GP-GNNs could learn edgesâ€™ parameters from natural lanuages Related workGraph Neural Networks(GNNs) existing models still perfom message-passing on predefined graphs Learning Graphical State Transitions is most related introduecs a nove lnerual architecture to generate a graph based on the textal input dynamically update the relationship during the learning process relational reasoning existing models could not make full use of the multi-hop inference patterns among multiple entity pair and their relaitons within the sentence LEARNING GRAPHICAL STATE TRANSITIONS is the most related work the proposed model incorporates contextual relations with attention mechanism when predicting the relation of a target entity pair Graph Neural Network with Grenerated Parameters(GP-GNNs)The picture is overall architecture: encoding module, propagation module and classification module Encoding Moduleformula: \mathcal{A}_{i, j}^{(n)}=f\left(E\left(x_{0}^{i, j}\right), E\left(x_{1}^{i, j}\right), \cdots, E\left(x_{l-1}^{i, j}\right) ; \theta_{e}^{n}\right)where $f(\cdot)$ could be any model that could sequential(such as LSTMs); $E(\cdot)$ indicates an embedding function. $x^{i, j}$ is the word in sentence labeled( $i,j$) Porpagation Modulethe representations of layer n + 1 are calculated by: \mathbf{h}_{i}^{(n+1)}=\sum_{v_{j} \in \mathcal{N}\left(v_{i}\right)} \sigma\left(\mathcal{A}_{i, j}^{(n)} \mathbf{h}_{j}^{(n)}\right)where $\mathcal{N}\left(v_{i}\right)$ denotes the neighbors of node $v_i$ Classification Modulethe loss of GP-GNNs: \mathcal{L}=g\left(\mathbf{h}_{0 :|\mathcal{V}|-1}^{0}, \mathbf{h}_{0 :|\mathcal{V}|-1}^{1}, \ldots, \mathbf{h}_{0 :|\mathcal{V}|-1}^{K}, Y ; \theta_{c}\right)Relation Extraction with GP-GNNsAuthors introduce how to apply GP-GNNs to relation extraction Encoding Moduleencoding then context of entity pairs (or edges in the graph) E\left(x_{t}^{i, j}\right)=\left[\boldsymbol{x}_{t} ; \boldsymbol{p}_{t}^{i, j}\right]where $x_t$ denotes the word embedding; $\boldsymbol{p}_{t}^{i, j}$denotes the position embedding of word posistion t relative to the entity pairâ€™s position $i, j$. position embeddingwe mark each token in the sentence as either belonging to the first entity $v_i$, the second entity $v_j$ or to neither of those Propagation Module the formula is the same as the front The Initial Embeddings of Nodes when extracting the relationship between entity $v_i$ and entity $v_j$, the initial embeddings of them are annotated as $\mathbf{h}_{v_{i}}^{(0)}=a_{\text { subject }}$, and $h_{v_{j}}^{(0)}=a_{\text { object }}$, while the intial embeddings of other entities are set to all zeros. In our experiments, we generalize the idea of Gated Graph Neural Networks (Li et al., 2016) by setting $a_{\text { subject }}=[1 ; 0]^{\top}$and $a_{\text { object }}=[0 ; 1]^{\top}$. classification ModuleAs the target entity pair $(v_i, v_j)$: \boldsymbol{r}_{v_{i}, v_{j}}=\left[\left[\boldsymbol{h}_{v_{i}}^{(1)} \odot \boldsymbol{h}_{v_{j}}^{(1)}\right]^{\top} ;\left[\boldsymbol{h}_{v_{i}}^{(2)} \odot \boldsymbol{h}_{v_{j}}^{(2)}\right]^{\top} ; \ldots ;\left[\boldsymbol{h}_{v_{i}}^{(K)} \odot \boldsymbol{h}_{v_{j}}^{(K)}\right]^{\top}\right]where $\odot$ represents element-wise multiplication classification: \mathbb{P}\left(r_{v_{i}, v_{j}} | h, t, s\right)=\operatorname{softmax}\left(M L P\left(\boldsymbol{r}_{v_{i}, v_{j}}\right)\right)loss: \mathcal{L}=\sum_{s \in S} \sum_{i \neq j} \log \mathbb{P}\left(r_{v_{i}, v_{j}} | i, j, s\right)Experimentsaim showing their best models could improve the performance of relation extraction under a variety of settings illlustrating that how the number of layers affect the performance of their model performing a qualitiative investigation to highlight the diference between their models and baseline models designas the first and second aim show that our models could improve instance-level relation extraction on a human annotated test set we will show that our models could also help enhance the performance of bag-level relation extraction on a distantly labeled test set split a subset of distantly labeled test set, where the number of entities and edges is large Datasetdistantly label set Sorokin and Gurevych (2017) proposed modify their dataset added reversed edges for all of the entity pairs with no relations, added â€œNAâ€ labels to them Human annotated test set Sorokin and Gurevych (2017) select the distantly lablel pairs which all 5 annotaters are accepted. There are 350 sentences and 1,230 triples in this test set Dense distantly labeled test set criteria the number of entities should be strictly larger than 2 there must be at least one circle (with at least three entities) in the ground-truth label of the sentence There are 1,350 sentences and more than 17,915 triples and 7,906 relational facts in this test set. Models for comparison Context-aware RE Multi-Window CNN PCNN LSTM or GP-GNN with K = 1 layer GP-GNN with K = 2 or K = 3 layerss Evaluation DetailsTo evaluation models in bag-level: E\left(r | v_{i}, v_{j}, S\right)=\max _{s \in S} \mathbb{P}\left(r_{v_{i}, v_{j}} | i, j, s\right)result: Effectiveness of Reasoning Mechanism Context-Aware RE may introduce more noise, for it may mistakenly increase the probability of a relation with the similar topic with the context relations sentences from Wikipedia corpus are always complex, which may be hard to model for CNN and PCNN The Effectiveness of the Number of Layers the improvement of the third layer is much smaller on the overall distantly supervised test set than the one on the dense subset This observation reveals that the reasoning mechanism could help us identify relations especially on sentences where there are more entities as the number of layers grows, the curves get higher and higher precision, indicating considering more hops in reasoning leads to better performance Qualitative Results: Case Study Context-Aware RE makes a mistake by predicting (Kentucky, share boarder with, Ohio). As we have discussed before, this is due to its mechanism to model co-occurrence of multiple relations æ€è€ƒæ–‡ç« æ˜¯åˆ˜çŸ¥è¿œç»„çš„è®ºæ–‡ï¼Œé’ˆå¯¹çš„æ–¹å‘æ˜¯å…³ç³»æŠ½å–ï¼Œåœ¨å…¶ä¸­ç»“åˆäº†å…³ç³»æŽ¨ç†ï¼Œæœ€è¿‘è®¸å¤šä»»åŠ¡éƒ½åœ¨ç»“åˆæŽ¨ç†çš„æ€æƒ³ã€‚æ–‡ç« æ•´ä½“çš„ç»“æž„ï¼Œé€»è¾‘ååˆ†æ¸…æ™°ï¼Œè®ºè¿°çš„ä¹Ÿæ¯”è¾ƒè¯¦ç»†ï¼Œå±žäºŽæ ‡å‡†è®ºæ–‡ã€‚æ„Ÿè§‰æ–‡ç« ä¸­GP-GNNsç»“æž„å›¾è¿˜å¯ä»¥ç”»çš„æ›´å¥½ä¸€ç‚¹ï¼Œå±•çŽ°ä¸€ä¸‹encoding moduleçš„å±‚ï¼Œå¯ä»¥æ›´å¥½ç†è§£ã€‚æ–‡ç« çš„ç²¾é«“åº”è¯¥æ˜¯è¿™ä¸ªpropagation moduleçš„éƒ¨åˆ†ï¼Œè¿˜éœ€è¦æ¶ˆåŒ–ä¸€ä¸‹ï¼Œä¸è¿‡è¿™éƒ¨åˆ†å¯èƒ½æ˜¯æœ‰å…ˆå‰çš„çŸ¥è¯†æ”¯æ’‘çš„ã€‚]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>GNNs</tag>
        <tag>relation extraction</tag>
        <tag>relation reasoning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[allennlpå®‰è£…è¸©å‘]]></title>
    <url>%2Fpost%2Fallennlp_install%2F</url>
    <content type="text"><![CDATA[å®‰è£…allennlpçš„è¸©å‘ä¹‹è·¯ï¼Œè¸©äº†ä¸å°‘å‘æœ€åŽé€‰æ‹©â€™Installing from sourceâ€™çš„å®‰è£…æ–¹æ³•ï¼ŒæŽ’å‘åŽä¸‹é¢æ–¹æ³•äº²æµ‹å¯ç”¨ Installing from sourceå®‰è£…æ­¥éª¤ï¼š 1.ä¸‹è½½GitHubæ–‡ä»¶git clone https://github.com/allenai/allennlp.git 2.åˆ›å»ºcondaçŽ¯å¢ƒconda create -n allennlp python=3.6 3.æ¿€æ´»çŽ¯å¢ƒä¸‹è½½ä¾èµ–æ–‡ä»¶ æ¿€æ´»çŽ¯å¢ƒ source activate allennlp è¿›å…¥githubä¸Šä¸‹è½½çš„æ–‡ä»¶å¤¹ ä¸‹è½½ä¾èµ–æ–‡ä»¶ pip install -r requirements.txt é‡åˆ°æŠ¥é”™é—®é¢˜ï¼Œå‚è€ƒä¸‹ä¸€å°èŠ‚ï¼Œæ‰€æ¬²é—®é¢˜è§£å†³ã€‚ 4.å®‰è£…allennlppip install --editable . 5.æµ‹è¯•allennlp æˆåŠŸåŽæ•ˆæžœå¦‚ä¸‹ï¼š $ allennlp 2019-05-22 21:58:42,297 - INFO - pytorch_pretrained_bert.modeling - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex . usage: allennlp Run AllenNLP optional arguments: -h, --help show this help message and exit --version show program's version number and exit Commands: configure Run the configuration wizard. train Train a model. evaluate Evaluate the specified model + dataset. predict Use a trained model to make predictions. make-vocab Create a vocabulary. elmo Create word vectors using a pretrained ELMo model. fine-tune Continue training a model on a new dataset. dry-run Create a vocabulary, compute dataset statistics and other training utilities. test-install Run the unit tests. find-lr Find a learning rate range. print-results Print results from allennlp serialization directories to the console. é‡åˆ°çš„é—®é¢˜é—®é¢˜1æŠ¥é”™ä¿¡æ¯ï¼šERROR: Failed building wheel for jsonnet è§£å†³æ–¹æ³•ï¼šconda install -c conda-forge jsonnet é—®é¢˜2æŠ¥é”™ä¿¡æ¯ï¼šæŠ¥çš„éƒ½æ˜¯æŸäº›åŒ…çš„ç‰ˆæœ¬é—®é¢˜ ERROR: botocore 1.12.152 has requirement urllib3=1.20; python_version >= "3.4", but you'll have urllib3 1.25.2 which is incompatible. ERROR: aws-sam-translator 1.11.0 has requirement jsonschema~=2.6, but you'll have jsonschema 3.0.1 which is incompatible. ERROR: cfn-lint 0.20.3 has requirement jsonschema~=2.6, but you'll have jsonschema 3.0.1 which is incompatible. ERROR: cfn-lint 0.20.3 has requirement requests=2.15.0, but you'll have requests 2.22.0 which is incompatible è§£å†³æ–¹æ³•æ ¹æ®æŠ¥é”™ä¿¡æ¯ä¸‹è½½ç›¸åº”å®‰è£…åŒ…å³å¯ é—®é¢˜3æŠ¥é”™ä¿¡æ¯ï¼šImportError: dlopen: cannot load any more object with static TLS ___________________________________________________________________________ Contents of /home/minelab/anaconda3/envs/allennlp/lib/python3.6/site-packages/sklearn/__check_build: __init__.py setup.py _check_build.cpython-36m-x86_64-linux-gnu.so __pycache__ ___________________________________________________________________________ It seems that scikit-learn has not been built correctly. If you have installed scikit-learn from source, please do not forget to build the package before using it: run `python setup.py install` or `make` in the source directory. If you have used an installer, please check that it is suited for your Python version, your operating system and your platform. è§£å†³æ–¹æ³•ï¼šä¸‹è½½æ›´ä½Žç‰ˆæœ¬çš„scikit-learn,ä¾‹å¦‚ pip install scikit-learn=0.20.3 å‚è€ƒé“¾æŽ¥ https://github.com/pytorch/pytorch/issues/10443 https://github.com/pypa/pip/issues/4330 å®‰è£…çš„å¯ç¤ºçŽ¯å¢ƒé—®é¢˜ æœ€åŸºæœ¬çš„å°±æ˜¯å…ˆåŽ»ç½‘ä¸ŠæŸ¥è¿™ä¸ªé”™è¯¯çš„è§£å†³æ–¹æ³• ç½‘ä¸Šçš„è§£å†³ä¸äº†çš„ï¼Œå…ˆçŒœçŒœå¤§æ¦‚çŽ‡æ˜¯å“ªæ–¹é¢çš„é—®é¢˜ã€‚ æ¯”å¦‚å¤§æ¦‚çŽ‡æ˜¯å„ç§ç‰ˆæœ¬äº’ç›¸ä¹‹é—´ä¸é€‚é…çš„é—®é¢˜ï¼Œé‚£å°±è°ƒè¯•ç‰ˆæœ¬ï¼Œä¸€èˆ¬éƒ½ä¼šå‘Šè¯‰ä½ å“ªä¸ªæœ‰é—®é¢˜ï¼Œæ¯”å¦‚ä¸Šé¢çš„scikit-learné—®é¢˜ã€‚]]></content>
      <categories>
        <category>install</category>
      </categories>
      <tags>
        <tag>allennlp</tag>
        <tag>åŒ…å®‰è£…</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Triple Trustworthiness Measurement for Knowledge Graphé˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FTriple%20Trustworthiness%20Measurement%20for%20Knowledge%20Graph%2F</url>
    <content type="text"><![CDATA[æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡è®¡ç®—triple trustworthinessæ¥è¯„ä¼°çŸ¥è¯†å›¾è°±çš„å‡†ç¡®ç¨‹åº¦çš„æ–¹æ³•ã€‚æ¨¡åž‹åˆ©ç”¨ç¥žç»ç½‘ç»œç»¼åˆæ¥è‡ªå®žä½“ï¼ˆå€Ÿé‰´Resource allocationï¼‰ã€å…³ç³»ï¼ˆå€Ÿé‰´ç¿»è¯‘æ¨¡åž‹çš„æ€æƒ³ï¼Œå¦‚TransEï¼‰å’ŒKGå…¨å±€ï¼ˆå€Ÿé‰´å…³ç³»è·¯å¾„ï¼ŒRNNï¼‰ä¸‰ä¸ªå±‚é¢çš„è¯­ä¹‰å’Œå…¨å±€ä¿¡æ¯ï¼Œè¾“å‡ºæœ€åŽçš„ trustworthinessä½œä¸ºåˆ¤æ–­ä¾æ®ã€‚ ä¸‹è½½åœ°å€ SummaryThis paper proposed a method for estimating the accuracy of a knowledge graph by computing triple trustworthiness. The model uses neural network to synthesize semantic and global information from three levels: entity(resource allocation), relationship(translation model ideas, such as TransE)m and KG global(relationship path, RNN) and outputting the final trustworthiness as the basis for judgment. Problem statementpossible noises and conflicts are inevitably intoduced in the process of constructing the KG research objectivequantify the KGâ€™s semantic correctness and the true degree of the facts expressed Contribution Knowledge graph triple trustworthiness measurement use the triple semantic information and globally inferring information three levels measurement and an intergration of confidence value experiment result verified the model valid on large-scale KG Freebase the KGTtm could be utilized in knowledge graph construction or improvement THE TRIPLE TRUSTWORTHINESS MEASUREMENT MODEL Longitudinally, the model can be divided into two level. the upper is a pool of multiple trustworthiness estimate cells(estimator) the output of these Estimator forms the input of lower-level fusion device(Fusioner) Viewed laterally, three progressive levels are be considered, as following. Is there a possible relationship between the entity pairs? ResourceRank: The algorithm assumes that the association between entity paires $(h,t)$ will be stronger, and more resource is passed from the head $h$ through all associated paths to the tail $t$ in a graph The amount of resource aggregated into $t$ ingeniously indicateds the association strength from $h$ to $t$. As pair $(e_1,e_2)$, there only one directed edge from $e_1$ to $e_2$ in the graph, where the different bandwidth of the edge indicates the number of the multiple relations. output: \left\{\begin{array}{c}{u=\alpha\left(W_{1} V+b_{1}\right)} \\ {R R(h, t)=W_{2} u+b_{2}}\end{array}\right.Authors constructed a $V$ vector by combining six characteristics. R (t | h); In-degree of head node ID(h); Out-degree of head node OD(h); In-degree of tail node ID(t); Out-degree of tail node OD(t); The depth from head node to tail node Dep As for 1. the formula: R(t | h)=(1-\theta) \sum_{e_{i} \in M_{t}} \frac{R\left(e_{i} | h\right) \cdot B W_{e_{i} t}}{O D\left(e_{i}\right)}+\frac{\theta}{N} $M_t$is the set of all nodes that have outgoing links to the node $t$, $OD (e_i)$ is the out-degree of the node eiand the $BW_{e_it}$ is the bandwidth from the $e_i$ to $t$. In order to improve the model fault-tolerance, we assume that the resource fow from each node may directly jump to a random node with the same probability Î¸ Can the determined relationship $r$ occur between the entity pair $(h,t)$ ? Translation-based energy function (TEF)ï¼šdepended on TransE $E(h, r, t)=|\mathbf{h}+\mathbf{r}-\mathbf{t}|$ output: P(E(h, r, t))=\frac{1}{1+e^{-\lambda\left(\delta_{r}-E(h, r, t)\right)}}Can the relevant triples in the KG infer that the triple is trustworthy? Reachable paths inference (RPI): There two challenges to exploit the reachable paths for inferring triple trustworthiness: reachable paths selectionSemantic distance-based path selection Reachable Paths Representationusing a RNN to deal with the embeddings of the three elements of each triple in the selected path Fusing the Estimatorsa classifer based on a multi-layer perceptron EXPERIMENTSdatasetFB15K Interpreting the Validity of the Trustworthiness The left picture shows that the positives examples are mainly concentrated in the upper region, vice versa. As for the right picture only if the value of a triple is higher than the threshold can it be considered trustworthy shows that the positive examples universally have higher confidence values Comparing With Other Models on The Knowledge Graph Error Detection Task Authorsâ€™ model has beter results in terms of accuracy and the F1-score than the other models. Analyzing the ability of models to tackle the three type noises. a higher recall shows that authorsâ€™ model can more accurately find the right from noisy triples higher average trustworthiness values show that authorsâ€™ model can better identify the correct instances and with high confidence the worst among the $(h, ?, t)$, because the various relations between a certain entity increase the difficulty of model judgment. Analyzing the Efects of Single Estimators It can be found that the accuracy obtained by each model is above 0.8, which proves the effectiveness of each Estimator æ€è€ƒæœ¬æ–‡åœ¨æ–¹æ³•ä¸Šå‡ ä¹Žæ²¡æœ‰ä»€ä¹ˆåˆ›æ–°ï¼Œæœ¬è´¨ä¸Šå°±æ˜¯ä¸€ä¸ªè€æ–¹æ³•çš„å¤šä¸ªç»„åˆã€‚æœ€å¤§äº®ç‚¹å°±æ˜¯ä½œè€…èƒ½æå‡ºtrustworthinessæ¥æŠŠè¿™ä¸ªè¯„ä»·çŸ¥è¯†å›¾è°±å‡†ç¡®åº¦çš„é—®é¢˜è¿›è¡Œäº†é‡åŒ–ã€‚è¿™ç§èƒ½åŠ›æ¯”æå‡ºæ–¹æ³•ä¸Šçš„åˆ›æ–°æ›´åŠ åŽ‰å®³ï¼Œä¹Ÿæ˜¯éœ€è¦å­¦ä¹ çš„åœ°æ–¹ã€‚]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>KG</tag>
        <tag>Knowledge Graph</tag>
        <tag>Triple</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GloVe: Global Vectors for Word Representationé˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FGloVe%3AGlobal%20Vectors%20for%20Word%20Representation%2F</url>
    <content type="text"><![CDATA[è®ºæ–‡ä¸‹è½½åœ°å€ï¼ŒGloVeæ˜¯ä¸€ä¸ªæ–°çš„å…¨çƒå¯¹æ•°åŒçº¿æ€§å›žå½’æ¨¡åž‹ï¼Œå±žäºŽç»å…¸çš„è¯å‘é‡è¡¨ç¤ºæ–¹æ³•ä¹‹ä¸€ã€‚ Introductionevaluate the intrinsic quality Most word vector methods rely on the distance or angle between pairs of word vectors Mikolov et al. (2013c) introduced word analogies that examines word vectorâ€™s various dimensions of difference. two main model families for learning vectors: global matrix factorization methods local context window methods Authors propose a specific weighted least squares model that trains on globla word-word co-occurrence counts and thus makes efficient use of statistics. Related WorkMatix Facroization MethodsThese methods utilize low-rank approximations to decompose large matrices that capture statistical information about a corpus. shortcomingthe most frequent words contribute a dispropoertionate amount to the similarity measure. Shallow Window-Based MethodsAnother approach is to learn word representations that aid in making predictins within local context windows. shortcomingdo not operate directly on the co-occurrence statistics of the corpus and fails to take advantage of the vast amount of repetition in the data. The GloVe ModelGloVe: Global Vectorsthe global corpus statistics are captured directly by the model the question about the model using the statistics of word occurrences in a corpus how meaning is generated from these statistics how the resulting word vectors might represent that meaning some notation$X_{ij}$ : the number of times word j occurs in the context of word i $X_i = \sum_{k} X_{i k}$ : the number of times any word appears in the context of word i $P_{i j}=P(j | i)=X_{i j} / X_{i}$: the probability that word j appear in the context of word i above that, werd vector learning should be with ratios of co-occurrence probabilities: $w \in \mathbb{R}^{d}$are word vectors and $\tilde{w} \in \mathbb{R}^{d}$are separate context word vectors For F, we should select a unique choice by enforcing a few desiderata. encoding the information present the ratio $P_{i k} / P_{j k}$ in the word vector space. Since vector spaces are inherently linear structures put F to be a compicated function parameterized, and avoiding bofuscating the linear structure the word-word co-occurrence matrices, we can exchange a word and a context word(because a word can also be a context word) F should be a homomorphism by Eqn.(3) F = exp or the Eqn(6) would have the exchange symmetry if not $\log \left(X_{i}\right)$ and $\log \left(X_{i}\right)$ is independent of k, so it can be absorbed into a bias $b_i$ for avoiding diverge, $\log \left(X_{i k}\right) \rightarrow \log \left(1+X_{i k}\right)$ a new weighted least squares regression model to address the problem that LSA wirhts all co-occuttences equally. cost function: Relationship to Other ModelsIn this subsection authors show how these models are related to their proposed model. the defect of cross entropy it has the unfortunate property that distributions with long tails are often modeled poorly with too much wieght given to the unlikely events. Complexity of the modelthe computational complexity of the model depends on the number of nonzero elects in the matrix $X$ some assumptions about the distribution of word co-occurrences the number of co-occurrences of word $i$ with word $j$, $X_{ij}$, can be modeled as a power-law function of the frequency rank of that word pair, $r_{ij}$: $X_{i j}=\frac{k}{\left(r_{i j}\right)^{\alpha}}$ ExperimentsEvaluation methodsauthors conduct experiments on the word analogy taks of Mikolov et al. (2013a) Word analogiesThe word analogy task consists of questions like, â€œa is to b as c is to ?â€ Word similarity Named entity recognitionResultsTable 2 shows the CloVe model performs significantly better than the other baslines, often with smaller vector sizes and smaller corpora. Table 3 shows results on five different word similarity datasets. Table 4 shows results on the NER task with the CRF-based model. Model Analysis: Vector Length and Context Size Model Analysis: Corpus Size On the syntactic subtask, larger corpora typically produce better statistics so that there is a monotonic increase in performance as the cor- pus size increases. But the same trend is not true for the semantic subtask, which is probably because of analogy dataset Model Analysis: Run-time Model Analysis: Comparison with word2vecFor the same corpus, vocabulary, window size, and training time, GloVe consistently outperforms word2vec å‚è€ƒé“¾æŽ¥ https://blog.csdn.net/coderTC/article/details/73864097]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>word vector</tag>
        <tag>GloVe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep contextualized word representations é˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FDeep%20contextualized%20word%20representations%2F</url>
    <content type="text"><![CDATA[è®ºæ–‡ä¸‹è½½åœ°å€ï¼ŒELMoäº‹å…ˆç”¨è¯­è¨€æ¨¡åž‹å­¦å¥½ä¸€ä¸ªå•è¯çš„ Word Embeddingï¼Œæ­¤æ—¶å¤šä¹‰è¯æ— æ³•åŒºåˆ†ï¼Œä¸è¿‡è¿™æ²¡å…³ç³»ã€‚åœ¨æˆ‘å®žé™…ä½¿ç”¨ Word Embedding çš„æ—¶å€™ï¼Œå•è¯å·²ç»å…·å¤‡äº†ç‰¹å®šçš„ä¸Šä¸‹æ–‡äº†ï¼Œè¿™ä¸ªæ—¶å€™æˆ‘å¯ä»¥æ ¹æ®ä¸Šä¸‹æ–‡å•è¯çš„è¯­ä¹‰åŽ»è°ƒæ•´å•è¯çš„ Word Embedding è¡¨ç¤ºï¼Œè¿™æ ·ç»è¿‡è°ƒæ•´åŽçš„ Word Embedding æ›´èƒ½è¡¨è¾¾åœ¨è¿™ä¸ªä¸Šä¸‹æ–‡ä¸­çš„å…·ä½“å«ä¹‰ï¼Œè‡ªç„¶ä¹Ÿå°±è§£å†³äº†å¤šä¹‰è¯çš„é—®é¢˜äº†ã€‚æ‰€ä»¥ ELMO æœ¬èº«æ˜¯ä¸ªæ ¹æ®å½“å‰ä¸Šä¸‹æ–‡å¯¹ Word Embedding åŠ¨æ€è°ƒæ•´çš„æ€è·¯ã€‚ IntroductionELMo(Embedddings from Language Models):why call ELMo:Using vectors derived from a bidirectional LSTM that is trained with a coupled language model(LM) objective on a large text corups. characteristics ELMo representations are a function of all of the internal layers of the biLM. learn a linear combination of the vectors stacked above each input word for each end task the higher-level LSTM states capture context-dependent aspects of word meaning the lower-level states model aspects of syntax Extensive experiments EMLo representations can be easily added to existing models improve the state of art in every case ELMo outperform those derived from just the top layer of a LSTM Related work Some approaches for learning word vectors only allow a single context-independent representation for each word. to overcome some shortcomings of traditional word vectors: enriching them with subword information learning separate vectors for each word sense Authors uses subword units through the use of character convolutions, seamlessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes. context-depends representations Authors take full advantage of access to plentiful monolingual data Previous work also shown that different layers of deep biRNNs encode different types of information introducing multi-task syntactic supervision at the lower levels of a deep LSTM can improve overall performance of higher level tasks the top layer of an LSTM for encoding word context (Melamud et al., 2016) has been shown to learn representations of word sense. ELMo representations can also induce similar signals. ELMo: Embeddings from Language ModelsBidirectional language models model the probability of token $t_k$ given the history($t_1, â€¦ , t_{k-1}$): a backward LM: Authorsâ€™ formulation jointly maximizes the log likelihood of the forward and backward directions: ELMo For each token $t_k$, a L-layer biLM computes a set of 2L + 1 representations: For a downstream model, ELMo collapses all layers in R into a single vector. In the simplest case, ELMo just selects the top layer. For a task specific weighting of all biLM layers: $s^{task}$ are softmax-normalized weithts and the scalar parameter $Î³^{task}$ allows the task model to scale the entire ELMo vector Using biLMs for supervised NLP tasks Given a pre-trained biLM and a supervised architecture for a target NLP task let the end task model learn a linear combination of these representations consider the lowest layers of th supervised model without the biLM add ELMo to the supervised model freeze the weights of the biLM concatenate the ELMo vector $ELMo^{task}_k$ with $x_k$ and pass the ELMo enhanced representation $[x_k,;ELMo^{task}_k ]$ into the task RNN. for some tasks, authors also include ELMo ar the output of task RNN by introducing another set of out put specific linear weights and replacing $h_k$ with $[h_k,;ELMo^{task}_k ]$ add a moderate amount of dropout to ELMo and in some case to regularize the ELMo weights Pre-trained bidirectional language model architecture the biLM provides three layers of representations for each input token, both directions and a residual connection between LSTM layers fine tuning the biLM on domain specific data Evaluationthe following picture shows the performance of ELMo in Question answering, Textual entailment, Semantic role labeling, Corefrence resolution, Named entity extraction, Sentiment analysis. In every task considered, simply adding ELMo establishes a new state-of-the-art result. AnalysisAlternate layer weighting schemes the following picture compares these alternatives. Including representations from all layers improves overall performance over just using the last layer, and including contextual representations from the last layer improves performace over the baseline. Also shows the $\lambda$ is important. Where to include ELMo?The ELMo can be included in both the input and output. the results show including the ELMo in both input and output can preform better. What information is captured by the biLMâ€™s representations?Intuitively, the biLM must be disambiguating the meaning of words using their context. The GloVe can only capure the speech. but the biLM is able to disambiguate both the part of speech and word sense in the source sentence. Word sense disambiguationgiven a sentence, predicting the sense of a target word using a simple 1-nearst negihbor approach POS taggingto examine whether the biLM captures basic syntax. Sample efficiencyAdding ELMo to a model increases the sample efficiency considerably, both in terms of number of parameter updates to reach state-of-the-art performance and the overall training set size. Visualization of learned weights å‚è€ƒé“¾æŽ¥ NAACL2018:é«˜çº§è¯å‘é‡(ELMo)è¯¦è§£(è¶…è¯¦ç»†) ç»å…¸ï¼Œè¿™ç¯‡æ–‡ç« ä¸­é˜è¿°äº†ä¸€äº›ä½¿ç”¨çš„ç»†èŠ‚ï¼Œå¹¶ç”¨å›¾æ¥è¡¨ç¤ºï¼Œæ›´åŠ æ¸…æ™°ã€‚ ELMoç®—æ³•ä»‹ç»ï¼Œè¿™ç¯‡åšå®¢ä¸­è‡ªå·±å¯¹æ•´ä¸ªè®ºæ–‡çš„æ¦‚è¿°å’Œæ€»ç»“å’Œå¥½ï¼Œéœ€è¦å­¦ä¹ ã€‚]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>embedding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ã€ŠEfficient Estimation of Word Representations in Vector Spaceã€‹é˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FEfficient%20Estimation%20of%20Word%20Representations%20in%20Vector%20Space%2F</url>
    <content type="text"><![CDATA[è®ºæ–‡ä¸‹è½½åœ°å€ï¼Œè¯¥ç¯‡è®ºæ–‡çš„å¤§ç¯‡å¹…éƒ½åœ¨è®¨è®ºå®žéªŒç»“æžœçš„åˆ†æžï¼Œæ¨¡åž‹çš„éƒ¨åˆ†æ¯”è¾ƒç®€å•ï¼Œæ²¡æœ‰è¯¦ç»†åˆ†æžï¼Œæœ¬æ¥æ˜¯æƒ³è¯»ä¸€ä¸‹CBOWå’Œskip-gramçš„åŽŸå§‹è®ºæ–‡ï¼Œå‘çŽ°å¹¶æ²¡æœ‰æƒ³è±¡ä¸­çš„é‚£ä¹ˆå¤§çš„ç”¨å¤„ã€‚ Goals of paper å¼€å‘äº†ä¸¤ç§æ–°æ¨¡åž‹ï¼Œå¹¶ä¿ç•™äº†å•è¯ä¹‹é—´çš„çº¿æ€§è§„å¾‹ è®¾è®¡äº†ä¸€ä¸ªæ–°çš„ç»¼åˆæµ‹è¯•é›†ï¼Œç”¨äºŽæµ‹é‡å¥æ³•å’Œè¯­ä¹‰è§„å¾‹ è®¨è®ºäº†è®­ç»ƒæ—¶é—´å’Œå‡†ç¡®æ€§å¦‚ä½•å–å†³äºŽå•è¯å‘é‡çš„ç»´åº¦å’Œè®­ç»ƒæ•°æ®çš„æ•°é‡ Model Architecturesè®­ç»ƒå¤æ‚åº¦ï¼š å…¶ä¸­ï¼ŒEæ˜¯è®­ç»ƒæ¬¡æ•°ï¼ŒTæ˜¯è®­ç»ƒé›†å•è¯æ•°é‡ï¼ŒQæ˜¯æ¨¡åž‹ç»“æž„ã€‚ Feedforward Neural Net Language Model (NNLM)å®ƒç”±è¾“å…¥ï¼Œæ˜ å°„ï¼Œéšè—å’Œè¾“å‡ºå±‚ç»„æˆã€‚é€šè¿‡ç®€åŒ–æ–¹æ³•ï¼ŒQ= N x D x H Recurrent Neural Net Language Model (RNNLM)å…‹æœäº†æ¨¡åž‹éœ€è¦å›ºå®šçš„ä¸Šä¸‹æ–‡é•¿åº¦çš„é—®é¢˜ï¼Œå¹¶ä¸”åªæœ‰è¾“å…¥ï¼Œéšè—å’Œè¾“å‡ºå±‚ã€‚ Q= H x H + H x Vï¼Œå…¶ä¸­H = Dï¼ˆå•è¯è¡¨ç¤ºï¼‰ï¼ŒH x V å¯ä»¥é€šè¿‡åˆ†çº§softmaxè¢«ç®€åŒ–ä¸ºH x log_2(V)ã€‚æ‰€ä»¥ä¸»è¦çš„å¤æ‚åº¦æ¥è‡ªäºŽH x Hã€‚ Parallel Training of Neural Networksæ¨¡åž‹ä½¿ç”¨çš„DistBeliefæ¡†æž¶å…è®¸æˆ‘ä»¬å¹¶è¡Œè¿è¡ŒåŒä¸€æ¨¡åž‹çš„å¤šä¸ªå‰¯æœ¬ï¼Œæ¯ä¸ªå‰¯æœ¬é€šè¿‡é›†ä¸­çš„æœåŠ¡å™¨åŒæ­¥å…¶æ¢¯åº¦æ›´æ–°ï¼Œè¯¥æœåŠ¡å™¨ä¿ç•™æ‰€æœ‰å‚æ•° New Log-linear Modelså¤§å¤šæ•°å¤æ‚æ€§æ˜¯ç”±äºŽæ¨¡åž‹ä¸­çš„éžçº¿æ€§éšè—å±‚å¼•èµ·çš„ã€‚æ¨¡åž‹ç»“æž„å¦‚ä¸‹ï¼š Continuous Bag-of-Words Model(CBOW)ç¬¬ä¸€ä¸ªæå‡ºçš„ä½“ç³»ç»“æž„ç±»ä¼¼äºŽå‰é¦ˆNNLMï¼Œå…¶ä¸­åŽ»é™¤äº†éžçº¿æ€§éšè—å±‚ï¼Œå¹¶ä¸”æ‰€æœ‰å•è¯ï¼ˆä¸ä»…ä»…æ˜¯æŠ•å½±çŸ©é˜µï¼‰å…±äº«æŠ•å½±å±‚ã€‚ å› æ­¤ï¼Œæ‰€æœ‰å•è¯éƒ½è¢«æŠ•å°„åˆ°ç›¸åŒçš„ä½ç½®ï¼ˆå®ƒä»¬çš„å‘é‡è¢«å¹³å‡ï¼‰ã€‚ å°†è¿™ä¸ªæž¶æž„ç§°ä¸ºè¯è¢‹æ¨¡åž‹ï¼Œå› ä¸ºåŽ†å²ä¸­çš„å•è¯é¡ºåºä¸ä¼šå½±å“æŠ•å½±ã€‚ æ¨¡åž‹çš„å¤æ‚åº¦ï¼šQ = N Ã— D + D Ã— log_2(V ) Continuous Skip-gram ModelåŸºäºŽåŒä¸€å¥å­ä¸­çš„å¦ä¸€ä¸ªå•è¯æœ€å¤§åŒ–å•è¯çš„åˆ†ç±»ã€‚ æ›´å‡†ç¡®åœ°è¯´ï¼Œä½¿ç”¨æ¯ä¸ªå½“å‰å•è¯ä½œä¸ºå…·æœ‰è¿žç»­æŠ•å½±å±‚çš„å¯¹æ•°çº¿æ€§åˆ†ç±»å™¨çš„è¾“å…¥ï¼Œå¹¶é¢„æµ‹å½“å‰å•è¯ä¹‹å‰å’Œä¹‹åŽçš„ç‰¹å®šèŒƒå›´å†…çš„å•è¯ã€‚ æ¨¡åž‹çš„å¤æ‚åº¦ï¼šQ = C Ã— (D + D Ã— log2(V ))ï¼Œå…¶ä¸­Cæ˜¯å•è¯çš„æœ€å¤§è·ç¦»ã€‚ å®žéªŒä»»åŠ¡æè¿°ä¸ºäº†åº¦é‡è¯å‘é‡çš„è´¨é‡ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªå¤æ‚çš„æµ‹è¯•é›†ï¼Œå®ƒåŒ…æ‹¬äº†äº”ç§ç±»åž‹çš„è¯­ä¹‰é—®é¢˜ã€‚ä¹ä¸ªç±»åž‹çš„å¥æ³•é—®é¢˜ã€‚åŒ…æ‹¬æ¯ä¸ªç±»åˆ«çš„ä¸¤ä¸ªæ ·æœ¬é›†åœ¨ä¸Šè¡¨å±•ç¤ºï¼›æ€»ä¹‹ï¼Œå…±æ‹¥æœ‰8869ä¸ªè¯­ä¹‰é—®é¢˜å’Œ10675ä¸ªå¥æ³•é—®é¢˜ ä½œè€…é€šè¿‡ï¼šæœ€å¤§åŒ–ç²¾ç¡®åº¦ ï¼Œæ¨¡åž‹ä½“ç³»ç»“æž„çš„æ¯”è¾ƒï¼Œæ¨¡åž‹çš„å¤§è§„æ¨¡å¹¶è¡Œè®­ç»ƒæ¥è¯æ˜Žæå‡ºæ¨¡åž‹çš„è¿é€Ÿåº¦å’Œç²¾ç¡®çš„ä¼˜åŠ¿ã€‚]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>word2vec</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shared Embedding Based Neural Networks for Knowledge Graph Completioné˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FShared%20Embedding%20Based%20Neural%20Networks%20for%20Knowledge%20Graph%20Completion%2F</url>
    <content type="text"><![CDATA[åŽŸæ–‡ä¸‹è½½é“¾æŽ¥ï¼ŒçŸ¥è¯†å›¾è°±è¡¥å…¨ï¼ˆKGCï¼ŒKnowledge Graph Completion)æ˜¯ä¸€ç§è‡ªåŠ¨å»ºç«‹å›¾è°±å†…éƒ¨çŸ¥è¯†å…³è”çš„å·¥ä½œã€‚ç›®æ ‡æ˜¯è¡¥å…¨çŸ¥è¯†å›¾è°±ä¸­ä¸‰å…ƒç»„çš„ç¼ºå¤±éƒ¨åˆ†ã€‚ä¸»è¦æ–¹æ³•ä¸ºåŸºäºŽå¼ é‡ï¼ˆæˆ–è€…çŸ©é˜µï¼‰å’ŒåŸºäºŽç¿»è¯‘ä¸¤ç±»ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºŽå…±äº«åµŒå…¥çš„ç¥žç»ç½‘ç»œçš„æ¨¡åž‹ï¼ˆSENNï¼‰æ¥å¤„ç†KGCã€‚ Contribulation æå‡ºäº†SENNæ¨¡åž‹ï¼Œè¯¥æ¨¡åž‹æ˜Žç¡®åŒºåˆ†å¤´å®žä½“ã€å…³ç³»å’Œä¸ºå®žä½“é¢„æµ‹ä»»åŠ¡ï¼Œå¹¶æŠŠå®ƒä»¬æ•´åˆåˆ°ä¸€ä¸ªåŸºäºŽå…¨è¿žæŽ¥ç¥žç»ç½‘ç»œæ¡†æž¶ä¸­ï¼Œè¯¥æ¡†æž¶å…±äº«çš„å®žä½“å’Œå…³ç³»åµŒå…¥ã€‚ SENNæå‡ºäº†ä¸€ä¸ªè‡ªé€‚åº”å…¨ä¸­æŸå¤±æœºåˆ¶ï¼Œè¯¥æ–¹æ³•å¯ä»¥å¾ˆå¥½çš„å¤„ç†å…·æœ‰ä¸åŒæ˜ å°„å±žæ€§çš„ä¸‰å…ƒç»„ï¼Œå¹¶å¤„ç†ä¸åŒçš„é¢„æµ‹ä»»åŠ¡ã€‚ ç”±äºŽå…³ç³»é¢„æµ‹é€šå¸¸æ¯”å¤´å°¾å®žä½“é¢„æµ‹å…·æœ‰æ›´å¥½çš„æ€§èƒ½ï¼Œæˆ‘ä»¬æŠŠSENNåº”ç”¨åˆ°å¤´å°¾å®žä½“é¢„æµ‹ï¼Œä»Žè€Œå°†SENNæ‰©å±•åˆ°SENN+ã€‚ Related worksTensor/Matrix Based MethodsRESCALæ˜¯ä¸€ä¸ªå…¸åž‹çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŸºäºŽä¸‰å‘å¼ é‡å› å­åˆ†è§£çš„æ–¹æ³•ã€‚ ç›®æ ‡å‡½æ•°ä¸ºï¼š $M_r$æ˜¯rçš„å…³ç³»çŸ©é˜µï¼Œå¤§å°ä¸ºk x kã€‚ ComlExæ˜¯æœ€è¿‘æå‡ºçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŸºäºŽçŸ©é˜µåˆ†è§£ï¼Œå¹¶ä¸”å®ƒä½¿ç”¨å¤æ•°å€¼æ¥å®šä¹‰å®žä½“å’Œå…³ç³»çš„åµŒå…¥ã€‚ ç›®æ ‡å‡½æ•°ä¸ºï¼š Re(x)è¿”å›žxçš„å®žéƒ¨ã€‚ Translation Based Methodsä»£è¡¨æ¨¡åž‹ä¸ºç»å…¸çš„TransEæ¨¡åž‹ï¼ˆè¿™é‡Œä¸å†èµ˜è¿°ï¼‰ Translation Based MethodsER-MLPä½¿ç”¨å¤šå±‚æ„ŸçŸ¥å™¨æ¥æ•èŽ·å¤´å®žä½“ï¼Œå…³ç³»å’Œå°¾å®žä½“ä¹‹é—´çš„éšå¼äº¤äº’ã€‚ ç›®æ ‡å‡½æ•°ä¸ºï¼š ProjEä½¿ç”¨å…·æœ‰ç»„åˆå±‚å’ŒæŠ•å½±å±‚çš„ç¥žç»ç½‘ç»œæ¥å¯¹å¤´å°¾å®žä½“é¢„æµ‹å»ºæ¨¡ã€‚ THE SENN METHODæ¨¡åž‹ç»“æž„å¦‚å›¾æ‰€ç¤ºï¼š ä½œè€…å°†æ¡†æž¶åˆ’åˆ†ä¸ºä»¥ä¸‹å››ä¸ªéƒ¨åˆ†ï¼š ä¸‰å…ƒç»„çš„æ‰¹é‡é¢„å¤„ç† çŸ¥è¯†å›¾è°±çš„Shared embeddingsè¡¨ç¤ºå­¦ä¹  ç‹¬ç«‹çš„å¤´å°¾å®žä½“åŠå…³ç³»é¢„æµ‹å­æ¨¡åž‹è®­ç»ƒä¸Žèžåˆ è”åˆæŸå¤±å‡½æ•°æž„æˆ æ•´ä¸ªKGCçš„æµç¨‹å¯ä»¥æè¿°å¦‚ä¸‹ï¼š å°†è®­ç»ƒæ•°æ®ä¸­çš„å®Œæ•´ä¸‰å…ƒç»„ï¼ˆçŸ¥è¯†å›¾è°±ï¼‰åˆ’åˆ†æ‰¹é‡åŽä½œä¸ºæ¨¡åž‹çš„è¾“å…¥ å¯¹äºŽè¾“å…¥çš„ä¸‰å…ƒç»„ï¼Œåˆ†åˆ«è®­ç»ƒå¾—åˆ°å®žä½“ï¼ˆåŒ…æ‹¬å¤´å°¾å®žä½“ï¼‰åµŒå…¥çŸ©é˜µä¸Žå…³ç³»åµŒå…¥çŸ©é˜µï¼ˆembeddingsï¼‰ å°†å¤´å°¾å®žä½“åŠå…³ç³»embeddingsåˆ†åˆ«è¾“å…¥åˆ°ä¸‰ä¸ªé¢„æµ‹æ¨¡åž‹ä¸­ï¼ˆå¤´å®žä½“é¢„æµ‹ï¼ˆ?, r, tï¼‰ï¼Œå…³ç³»é¢„æµ‹(h, ?, t)ï¼Œå°¾å®žä½“é¢„æµ‹(h, r, ?)ï¼‰ The Three Substructuresé¢„æµ‹å­æ¨¡åž‹å…·æœ‰ç›¸ä¼¼çš„ç»“æž„å¦‚ä¸‹å›¾ï¼Œæ¨¡åž‹è¾“å…¥å…³ç³»å‘é‡ä¸Žå®žä½“å‘é‡åŽï¼Œè¿›å…¥nå±‚å…¨è¿žæŽ¥å±‚ï¼Œå¾—åˆ°é¢„æµ‹å‘é‡ï¼Œå†ç»è¿‡ä¸€ä¸ªsigmoidï¼ˆæˆ–è€…softmaxï¼‰å±‚ï¼Œè¾“å‡ºé¢„æµ‹æ ‡ç­¾å‘é‡ã€‚ å¤´å®žä½“é¢„æµ‹ç›®æ ‡å‡½æ•°ï¼š f(x)= max(0,x). é¢„æµ‹æ ‡ç­¾ï¼š å…¶å®ƒä¸¤ç§ä¸Žæ­¤å¤´å®žä½“ç±»ä¼¼ã€‚ Model TrainingThe General Loss Functionæ¨¡åž‹ç›®æ ‡æ ‡ç­¾å‘é‡è¡¨ç¤ºä¸ºï¼š $I_h$æ˜¯åœ¨è®­ç»ƒé›†ä¸­ç»™å®šrå’Œtçš„æ‰€æœ‰æœ‰æ•ˆå¤´å®žä½“é›†ã€‚ ä¸‰è€…çš„å¹³æ»‘å‘é‡è¡¨ç¤ºä¸ºï¼š ä¸‰ä¸ªé¢„æµ‹ä»»åŠ¡çš„æŸå¤±å‡½æ•°ä¸ºï¼š æ€»æŸå¤±å‡½æ•°ä¸ºï¼š The Adaptively Weighted Loss Mechanism.è¯¥æ–¹æ³•çš„åŠ¨æœºï¼š åœ¨çŸ¥è¯†å›¾è°±ä¸­çš„ä¸‰å…ƒç»„æœ‰4ç§ç±»åž‹ï¼š1-TO-1, 1-TO-M, M-TO-1 and M-TO-Mã€‚æ‰€ä»¥é¢„æµ‹åœ¨è®­ç»ƒé›†ä¸­å…·æœ‰çš„æœ‰æ•ˆå®žä½“/å…³ç³»è¶Šå¤šï¼Œå®ƒå°±è¶Šä¸ç¡®å®šã€‚æ‰€ä»¥ä½œè€…å°†å¯¹åº”äºŽå¤´éƒ¨å®žä½“é¢„æµ‹ï¼Œå…³ç³»é¢„æµ‹å’Œå°¾éƒ¨å®žä½“é¢„æµ‹çš„æŸå¤±çš„æƒé‡ä¸Žæœ‰æ•ˆå®žä½“çš„æ•°é‡ç›¸å…³è”ã€‚ å› ä¸ºå…³ç³»é¢„æµ‹æ¯”å®žä½“é¢„æµ‹æ›´åŠ å®¹æ˜“ã€‚æ‰€ä»¥ä½œè€…åŠ å¤§å¯¹å¤´å°¾å®žä½“çš„é”™è¯¯é¢„æµ‹çš„æƒ©ç½šã€‚ æ‰€ä»¥ä½œè€…å¾—åˆ°æ–°çš„æŸå¤±å‡½æ•°ï¼š æ€»æŸå¤±å‡½æ•°å˜ä¸ºï¼š THE SENN+METHODä½œè€…ç›¸ä¿¡å¯ä»¥è¿›ä¸€æ­¥åˆ©ç”¨å…³ç³»é¢„æµ‹çš„ç›¸å½“å¥½çš„æ€§èƒ½æ¥è¾…åŠ©æµ‹è¯•è¿‡ç¨‹ä¸­çš„å¤´éƒ¨å’Œå°¾éƒ¨å®žä½“é¢„æµ‹ã€‚ ç»™å®šå¤´éƒ¨é¢„æµ‹ä»»åŠ¡ï¼ˆï¼Ÿï¼Œrï¼Œtï¼‰å¹¶å‡è®¾hæ˜¯æœ‰æ•ˆçš„å¤´éƒ¨å®žä½“ã€‚ å¦‚æžœæˆ‘ä»¬é‡‡ç”¨SENNæ–¹æ³•æ¥é¢„æµ‹hå’Œtä¹‹é—´çš„å…³ç³»ï¼Œå³æ‰§è¡Œå…³ç³»é¢„æµ‹ä»»åŠ¡ï¼ˆhï¼Œï¼Ÿï¼Œtï¼‰ï¼Œåˆ™å…³ç³»ræœ€æœ‰å¯èƒ½å…·æœ‰ é¢„æµ‹æ ‡ç­¾é«˜äºŽå…¶ä»–å…³ç³»ï¼Œå› æ­¤åº”æŽ’åé«˜äºŽå…¶ä»–å…³ç³»ã€‚ å…¶ä¸­Valueï¼ˆxï¼Œrï¼‰è¿”å›žå¯¹åº”äºŽå…³ç³»rçš„å‘é‡xçš„æ¡ç›®; Rankï¼ˆxï¼Œrï¼‰ä»¥é™åºè¿”å›žå¯¹åº”äºŽå…³ç³»rçš„å‘é‡xçš„æ¡ç›®çš„ç­‰çº§ã€‚ æœ€åŽSENN+ç§é¢„æµ‹æ ‡ç­¾ä¸ºï¼š å…¶ä¸­ EXPERIMENTSDatasets Entity Prediction Relation Prediction è®ºæ–‡è¿˜è¿›è¡Œäº†å…±äº«åµŒå…¥å’Œè‡ªé€‚åº”æƒé‡æŸå¤±æœºåˆ¶æœ‰æ•ˆæ€§çš„éªŒè¯ã€‚ å‚è€ƒé“¾æŽ¥ http://blog.openkg.cn/%E8%AE%BA%E6%96%87%E6%B5%85%E5%B0%9D-%E9%9D%A2%E5%90%91%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E8%A1%A5%E5%85%A8%E7%9A%84%E5%85%B1%E4%BA%AB%E5%B5%8C%E5%85%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>çŸ¥è¯†å›¾è°±</tag>
        <tag>ç¥žç»ç½‘ç»œ</tag>
        <tag>çŸ¥è¯†å›¾è°±è¡¥å…¨</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ã€ŠBootstrapping Entity Alignment with Knowledge Graph Embeddingã€‹é˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FBootstrapping%20Entity%20Alignment%20with%20Knowledge%20Graph%20Embedding%2F</url>
    <content type="text"><![CDATA[è®ºæ–‡ä¸‹è½½åœ°å€ï¼Œé‡‡ç”¨äº†bootstrappingæ–¹æ³•æ¥è§£å†³ç¼ºä¹è®­ç»ƒæ•°æ®çš„è¿‡ç¨‹ï¼Œæå‡ºäº†æˆªæ–­å‡åŒ€è´Ÿé‡‡æ ·æ¥æé«˜è´Ÿæ ·ä¾‹å¯¹äºŽç›®æ ‡å‡½æ•°çš„è´¡çŒ®ï¼Œé‡‡ç”¨åŸºäºŽé™åˆ¶çš„ç›®æ ‡å‡½æ•°æ¥æŒ‰éœ€è°ƒæ•´æ­£è´Ÿæ ·ä¾‹çš„å¾—åˆ†ã€‚ åŸºäºŽåµŒå…¥çš„å®žä½“å¯¹é½å°†ä¸åŒçš„çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰è¡¨ç¤ºä¸ºä½Žç»´åµŒå…¥ï¼Œå¹¶é€šè¿‡æµ‹é‡å®žä½“åµŒå…¥ä¹‹é—´çš„ç›¸ä¼¼æ€§æ¥æŸ¥æ‰¾å®žä½“å¯¹é½ã€‚å…¶ä¸­ï¼Œå¤§é‡æ–¹æ³•æ‰€é¢ä¸´çš„ä¸€ä¸ªæŒ‘æˆ˜æ˜¯ï¼šç¼ºä¹è¶³å¤Ÿçš„å…ˆå‰å¯¹é½ä½œä¸ºæ ‡è®°çš„è®­ç»ƒæ•°æ®ã€‚ è´¡çŒ® ä½œè€…æŠŠå®žä½“å¯¹é½å»ºæ¨¡ä¸ºä¸€ä¸ªåˆ†ç±»é—®é¢˜ï¼Œå…¶åŸºäºŽKGåµŒå…¥æ¥å¯»æ±‚æœ€å¤§åŒ–æ‰€æœ‰æ ‡è®°å’Œæœªæ ‡è®°çš„å®žä½“å¯¹é½å¯èƒ½æ€§ å¯¹äºŽé¢å‘å¯¹é½çš„KGåµŒå…¥ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºŽé™åˆ¶çš„ç›®æ ‡å‡½æ•°ï¼›ä¸ºäº†å¯¹ä¸å¤ªå¯èƒ½åŒºåˆ†çš„è´Ÿä¸‰å…ƒç»„è¿›è¡ŒæŠ½æ ·ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æˆªæ–­å‡åŒ€çš„è´ŸæŠ½æ ·æ–¹æ³•ã€‚ ä½œè€…æå‡ºäº†ä¸€ä¸ªè‡ªä¸¾è¿‡ç¨‹ï¼ˆbootstrappingï¼‰æ¥å…‹æœç¼ºä¹è¶³å¤Ÿè®­ç»ƒæ•°æ®ï¼Œé€šè¿‡æ ‡è®°å¯èƒ½çš„å¯¹é½å¹¶è¿­ä»£åœ°å°†å…¶æ·»åŠ åˆ°è®­ç»ƒæ•°æ®ä¸­æ¥æ›´æ–°é¢å‘å¯¹é½çš„åµŒå…¥ã€‚ ä½œè€…åœ¨ä¸‰ä¸ªè·¨è¯­è¨€å’Œä¸¤ä¸ªå¤§åž‹æ•°æ®é›†ä¸Šè¯„ä¼°äº†æ‰€æå‡ºçš„æ–¹æ³•ï¼Œè¡¨æ˜Žæ‰€æå‡ºçš„æ–¹æ³•æ˜Žæ˜¾ä¼˜äºŽä¸‰ç§æœ€å…ˆè¿›çš„å®žä½“å¯¹é½æ–¹æ³•ã€‚ é—®é¢˜æè¿°æœ€å¤§ä¼¼ç„¶å‡†åˆ™æŒ‡å¯¼é€‰æ‹©å®žçŽ°æœ€é«˜å¯¹é½å¯èƒ½æ€§çš„æœ€ä½³Î¸ å…¶ä¸­ï¼ŒL_xä»£è¡¨å®žä½“xçš„çœŸå®žæ ‡ç­¾ï¼Œ1_[]æ˜¯ä¸€ä¸ªæŒ‡ç¤ºå‡½æ•°ï¼Œè¡¨ç¤ºç»™å®šå‘½é¢˜çš„çœŸå€¼ï¼ˆ0æˆ–1ï¼‰ã€‚ä½†æ˜¯å¯¹äºŽæ²¡æœ‰æ ‡ç­¾çš„å®žä½“ï¼Œæƒ³è¦é€šè¿‡ä¸Šè¿°æ¥å¾—åˆ°thetaå°±å¾ˆå›°éš¾ã€‚ æ¨¡åž‹é¢å‘å¯¹é½çš„KGåµŒå…¥ä½œè€…æå‡ºäº†ä¸€ä¸ªç›®æ ‡å‡½æ•°ï¼š è¯¥ç›®æ ‡å‡½æ•°æœ‰ä¸¤ä¸ªæœŸæœ›çš„å±žæ€§ï¼š é¢„æœŸæ­£ä¸‰å…ƒç»„å¾—åˆ†è¾ƒä½Žï¼Œè€Œè´Ÿä¸‰å…ƒç»„å¾—åˆ†è¾ƒé«˜ã€‚ä¾‹å¦‚f(r)&lt;= r_1 å¹¶ä¸” f(râ€™)&gt;=r_2ï¼Œè®¾ç½®æ—¶r_2&gt;r_1,ä¸”r_1æ˜¯ä¸€ä¸ªå°çš„æ­£å€¼ã€‚ ä»ç„¶å¯ä»¥å¾—åˆ°f(râ€™)-f(r)&gt;=r_2 - r_1ï¼Œè¿™è¡¨æ˜Žæ‰€æå‡ºçš„ç›®æ ‡å‡½æ•°ä»ç„¶ä¿ç•™äº†åŸºäºŽè¾¹é™…æŽ’åºæŸå¤±çš„ç‰¹å¾ã€‚ æˆªæ–­å‡åŒ€è´Ÿé‡‡æ ·å¦‚æžœæ ·ä¾‹å¤ªå®¹æ˜“åŒºåˆ†ï¼Œé‚£ä¹ˆå¯¹æ•´ä¸ªçš„åµŒå…¥å­¦ä¹ çš„è´¡çŒ®ä¼šå¾ˆå°ã€‚ æ‰€ä»¥ï¼Œä½œè€…é‡‡ç”¨åœ¨åµŒå…¥ç©ºé—´ä¸­sæœ€è¿‘çš„é‚»å±…ä½œä¸ºå€™é€‰é›†ï¼Œå‰”é™¤é‚£äº›å’Œå®žä½“xç›¸ä¼¼åº¦è¿‡ä½Žçš„æ•°æ®ã€‚ å¼•å¯¼å¯¹é½ï¼ˆBootstrapping Alignmentï¼‰ä½œè€…è¿­ä»£åœ°å°†å¯èƒ½çš„å¯¹é½æ ‡è®°ä½œä¸ºè®­ç»ƒæ•°æ®ï¼Œå¹¶ä½¿ç”¨å®ƒæ¥è¿›ä¸€æ­¥æ”¹è¿›å®žä½“åµŒå…¥å’Œå¯¹é½ã€‚ å¯èƒ½çš„å¯¹é½æ ‡ç­¾å’Œç¼–è¾‘ä½œè€…ä¸ºäº†å®žçŽ°æœ€å¤§åŒ–å¯¹é½å¯èƒ½æ€§å¹¶éµå®ˆä¸€å¯¹ä¸€å¯¹é½çº¦æŸï¼Œæå‡ºä»¥ä¸‹ä¼˜åŒ–é—®é¢˜æ¥æ ‡è®°ç¬¬tæ¬¡è¿­ä»£ï¼š Yâ€™_x = {y|y âˆˆ Yâ€™ and Ï€(y|x; Î˜^(t)) &gt; Î³3}ä»£è¡¨æ ‡ç­¾xçš„å€™é€‰é›†ï¼›Ïˆ^(t)(Â·)æ˜¯ä¸€ä¸ªæŒ‡ç¤ºå‡½æ•°ï¼Œåªæœ‰å½“xåœ¨ç¬¬tæ¬¡è¿­ä»£æ—¶æ ‡ç­¾ä¸ºyæ—¶ä¸º1ï¼Œå…¶å®ƒæƒ…å†µä¸º0ã€‚ä¸¤ä¸ªé™åˆ¶æ¡ä»¶ä¿è¯äº†ä¸€å¯¹ä¸€çš„æ ‡ç­¾ã€‚è¿™æ—¶å¾—åˆ°äº†ä¸€ä¸ªæ–°çš„æ ‡ç­¾å¯¹é½ï¼š ä¸ºäº†æé«˜æ ‡ç­¾è´¨é‡å¹¶æ»¡è¶³ä¸€å¯¹ä¸€çš„å¯¹é½çº¦æŸï¼Œåœ¨è‡ªä¸¾è¿‡ç¨‹ä¸­ï¼Œä¸€æ—¦è¢«æ ‡è®°çš„å®žä½“å¯ä»¥åœ¨éšåŽçš„æ ‡è®°ä¸­é‡æ–°æ ‡è®°æˆ–å˜ä¸ºæœªæ ‡è®°çš„å®žä½“ã€‚ å½“å‘ç”Ÿä¸¤ä¸ªæ ‡ç­¾å†²çªæ—¶ï¼Œæˆ‘ä»¬é€šè¿‡è®¡ç®—ä¸‹é¢çš„ä¼¼ç„¶å·®å¼‚æ¥ç¡®å®šä¿ç•™å“ªä¸ªï¼š å½“è¯¥å€¼å¤§äºŽ0è¯´æ˜Žå‰è€…å…·æœ‰æ›´å¤§çš„å¯¹é½æ¦‚çŽ‡ã€‚ ä»Žæ•´ä½“è§’åº¦å­¦ä¹ ä¸ºäº†èŽ·å¾—æ ‡è®°å’Œæœªæ ‡è®°å®žä½“çš„æ•´ç†è§‚å¯Ÿï¼Œä½œè€…å®šä¹‰äº†æ¦‚çŽ‡åˆ†å¸ƒÏ†xæ¥æè¿°æ‰€æœ‰xå¯èƒ½çš„æ¦‚çŽ‡åˆ†å¸ƒã€‚ ç”±æ­¤ï¼Œä½œè€…å¾—åˆ°äº†æœ€å°åŒ–ä¸‹é¢çš„ä¼¼ç„¶å‡½æ•°æ¥å¾—åˆ°Î˜ï¼š å› ä¸ºï¼ŒåµŒå…¥ä¸ä»…åº”è¯¥æ•èŽ·å¯¹é½å¯èƒ½æ€§ï¼Œè¿˜åº”è¯¥æ¨¡æ‹ŸKGçš„è¯­ä¹‰ï¼Œæ‰€ä»¥ä½œè€…æœ€åŽå®šä¹‰è”åˆç›®æ ‡å‡½æ•°ï¼š å®žéªŒæ•°æ®é›† DBP15K [Sun et al., 2017]åŒ…å«ä¸‰ä¸ªè·¨è¯­è¨€æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†æ˜¯ä»ŽDBpediaçš„å¤šè¯­è¨€ç‰ˆæœ¬æž„å»ºçš„ã€‚DBPZH-EN(Chinese to English), DBPJA-EN(Japanese to English) and DBPFR-EN(French to English)æ¯ä¸ªæ•°æ®é›†åŒ…å«15ï¼Œ000ä¸ªå‚è€ƒå®žä½“å¯¹é½ã€‚ DWY100KåŒ…å«ä»ŽDBpediaï¼ŒWikidataå’ŒYAGO3ä¸­æå–çš„ä¸¤ä¸ªå¤§åž‹æ•°æ®é›†ï¼Œç”±DBP-WDå’ŒDBP-YGè¡¨ç¤ºã€‚ æ¯ä¸ªæ•°æ®é›†éƒ½æœ‰10ä¸‡ä¸ªå‚è€ƒå®žä½“å¯¹é½ å®žéªŒè®¾ç½®ä½œè€…é€‰å–äº†ä¸‰ç§æœ€å…ˆè¿›çš„åŸºäºŽåµŒå…¥çš„æ–¹æ³•æ¥å®žçŽ°å®žä½“å¯¹é½ã€‚ MTransE [Chen et al., 2017]ï¼Œé€‰å–äº†ç¬¬å››ç§å˜ä½“ï¼ˆè¡¨çŽ°æœ€ä½³ï¼‰ã€‚ IPTransE[Zhu et al., 2017]æ˜¯ä¸€ä¸ªè¿­ä»£æ–¹æ³• JAPE [Sun et al., 2017]ç»“åˆäº†å®žä½“å¯¹é½çš„å…³ç³»å’Œå±žæ€§åµŒå…¥ AlignEé¢å‘å¯¹é½çš„KGåµŒå…¥æ¨¡åž‹çš„å®žçŽ°ï¼Œå…·æœ‰æˆªæ–­çš„å‡åŒ€è´Ÿé‡‡æ ·å’Œå‚æ•°äº¤æ¢ï¼Œå®ƒä¼˜åŒ–äº†å…¬å¼ï¼ˆ3ï¼‰ï¼Œä½†æ˜¯æ²¡æœ‰è‡ªä¸¾ å®žéªŒç»“æžœè¡¨2ä¸­æˆ‘ä»¬è§‚å¯Ÿåˆ°AlignEæ˜Žæ˜¾ä¼˜äºŽMTransEï¼ŒIPTransEå’ŒJAPEï¼Œå› ä¸ºå®ƒé‡‡ç”¨é¢å‘å¯¹é½çš„åµŒå…¥ã€‚è€ŒBootEAæ˜¾ç€æ”¹å–„äº†AlignEçš„ç»“æžœï¼Œè¡¨æ˜Žäº†è‡ªä¸¾çš„è‰¯å¥½æ€§èƒ½æ˜¯ç”±äºŽå…¶èƒ½å¤Ÿå‡†ç¡®åœ°å°†å¯èƒ½çš„å¯¹é½æ ‡è®°ä¸ºè®­ç»ƒæ•°æ®ã€‚ åˆ†æžæˆªæ–­å‡åŒ€è´ŸæŠ½æ ·çš„æœ‰æ•ˆæ€§ä»Žå›¾ä¸­å¯ä»¥çœ‹å‡ºï¼Œä¸ŽMTransEï¼ŒIPTransEå’ŒJAPEç›¸æ¯”ï¼Œå…·æœ‰å‡åŒ€è´Ÿé‡‡æ ·çš„AlignEä»ç„¶èŽ·å¾—äº†ä¼˜å¼‚çš„ç»“æžœï¼Œå¹¶ä¸”éšç€é‡‡æ ·ç¦»xæ›´åŠ æŽ¥è¿‘ï¼Œæ•ˆæžœå‘ˆä¸Šå‡è¶‹åŠ¿ã€‚ å¯èƒ½å¯¹é½çš„å‡†ç¡®æ€§å¯ä»¥çœ‹åˆ°ä»¥ä½œè€…çš„æ ‡è®°æ–¹æ³•S3è¡¨çŽ°æœ€ä½³ã€‚è¿™äº›ç»“æžœè¯å®žä½œè€…çš„æ–¹æ³•å¯ä»¥ä¿è¯ä½¿ç”¨æœªæ ‡è®°æ•°æ®çš„å®‰å…¨æ€§ã€‚ å¯¹å…ˆå‰å¯¹å‡†æ¯”ä¾‹çš„æ•æ„Ÿæ€§ æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œéšç€æ¯”ä¾‹çš„å¢žåŠ ï¼Œæ‰€æœ‰äº”ä¸ªæ•°æ®é›†çš„ç»“æžœéƒ½å˜å¾—æ›´å¥½ï¼Œå› ä¸ºæ›´å¤šçš„å…ˆå‰å¯¹é½å¯ä»¥æä¾›æ›´å¤šä¿¡æ¯æ¥å¯¹é½ä¸¤ä¸ªKGã€‚ F1-score w.r.t. å…³ç³»ä¸‰å…ƒæ•°çš„åˆ†å¸ƒ BootEAåœ¨æ‰€æœ‰æ—¶é—´é—´éš”éƒ½ä¼˜äºŽMTransEï¼ŒIPTransEå’ŒJAPEï¼Œè¿™å†æ¬¡è¯å®žäº†BootEAçš„æœ‰æ•ˆæ€§ã€‚è€Œä¸”BootEAå¯ä»¥åœ¨ç¨€ç–æ•°æ®ä¸Šå–å¾—æœ‰å¸Œæœ›çš„ç»“æžœã€‚]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>çŸ¥è¯†å›¾è°±</tag>
        <tag>çŸ¥è¯†å›¾è°±åµŒå…¥</tag>
        <tag>å®žä½“å¯¹é½</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ã€ŠEntity Alignment between Knowledge Graphs Using Attribute Embeddingsã€‹é˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FEntity%20Alignment%20between%20Knowledge%20Graphs%20Using%20Attribute%20Embeddings%2F</url>
    <content type="text"><![CDATA[è®ºæ–‡ä¸‹è½½åœ°å€ï¼ŒçŸ¥è¯†å›¾ä¹‹é—´çš„å®žä½“å¯¹é½çš„ä»»åŠ¡æ—¨åœ¨åœ¨ä»£è¡¨ç›¸åŒçŽ°å®žä¸–ç•Œå®žä½“çš„ä¸¤ä¸ªçŸ¥è¯†å›¾ä¸­æ‰¾åˆ°å®žä½“ã€‚æœ¬æ–‡æœ€ä¸»è¦å°±æ˜¯æå‡ºäº†å±žæ€§å­—ç¬¦åµŒå…¥(attribute character embeddings)çš„æ–¹æ³•ã€‚ Abstractæˆ‘ä»¬çš„æ¨¡åž‹åˆ©ç”¨çŸ¥è¯†å›¾ä¸­å­˜åœ¨çš„å¤§é‡å±žæ€§ä¸‰å…ƒç»„(attribute triples)å¹¶ç”Ÿæˆå±žæ€§å­—ç¬¦åµŒå…¥ã€‚ å±žæ€§å­—ç¬¦åµŒå…¥(attribute character embeddings)é€šè¿‡åŸºäºŽå®žä½“çš„å±žæ€§è®¡ç®—å®žä½“ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œå°†å®žä½“åµŒå…¥ä»Žä¸¤ä¸ªçŸ¥è¯†å›¾ç§»ä½åˆ°åŒä¸€ç©ºé—´ä¸­ã€‚æˆ‘ä»¬ä½¿ç”¨ä¼ é€’è§„åˆ™æ¥è¿›ä¸€æ­¥ä¸°å¯Œå®žä½“çš„å±žæ€§æ•°é‡ä»¥å¢žå¼ºå±žæ€§å­—ç¬¦åµŒå…¥ã€‚ Contribution æå‡ºäº†ä¸¤ä¸ªKGä¹‹é—´å®žä½“å¯¹é½çš„æ¡†æž¶ï¼Œå®ƒç”±è°“è¯å¯¹é½æ¨¡å—ï¼ŒåµŒå…¥å­¦ä¹ æ¨¡å—å’Œå®žä½“å¯¹é½æ¨¡å—ç»„æˆã€‚ æå‡ºäº†ä¸€ç§æ–°é¢–çš„åµŒå…¥æ¨¡åž‹ï¼Œå®ƒå°†å®žä½“åµŒå…¥ä¸Žå±žæ€§åµŒå…¥é›†æˆåœ¨ä¸€èµ·ï¼Œä»¥ä¾¿ä¸ºä¸¤ä¸ªKGå­¦ä¹ ç»Ÿä¸€çš„åµŒå…¥ç©ºé—´ã€‚ æˆ‘ä»¬åœ¨ä¸‰ä¸ªçœŸæ­£çš„KGå¯¹ä¸Šè¯„ä¼°å»ºè®®çš„æ¨¡åž‹ã€‚ç»“æžœè¡¨æ˜Žï¼Œæˆ‘ä»¬çš„æ¨¡åž‹åœ¨å®žä½“å¯¹é½ä»»åŠ¡ä¸Šå§‹ç»ˆä¼˜äºŽæœ€å…ˆè¿›çš„æ¨¡åž‹ï¼Œhits@1è¶…è¿‡50ï¼…ã€‚ æ¨¡åž‹æ¨¡åž‹æ€»è§ˆpredicate alignment, embedding learning, and entity alignment Predicate Alignmentè°“è¯å¯¹é½æ¨¡å—é€šè¿‡ä½¿ç”¨ç»Ÿä¸€çš„å‘½åæ–¹æ¡ˆé‡å‘½åä¸¤ä¸ªKGçš„è°“è¯æ¥åˆå¹¶ä¸¤ä¸ªKGï¼Œä»¥ä¾¿ä¸ºå…³ç³»åµŒå…¥æä¾›ç»Ÿä¸€çš„å‘é‡ç©ºé—´ã€‚dbp:bornIn vs. yago:wasBornIn ç»Ÿä¸€å‘½åä¸º :bornInã€‚ ä¸ºäº†æ‰¾åˆ°éƒ¨åˆ†åŒ¹é…çš„è°“è¯ï¼Œä½œè€…è®¡ç®—è°“è¯URIçš„æœ€åŽéƒ¨åˆ†çš„ç¼–è¾‘è·ç¦»ï¼ˆä¾‹å¦‚ï¼ŒbornInä¸ŽwasBornInï¼‰å¹¶å°†0.95è®¾ç½®ä¸ºç›¸ä¼¼æ€§é˜ˆå€¼ã€‚ Embedding LearningStructure Embeddingä½œè€…é‡‡ç”¨TransEæ¥å­¦ä¹ å¯¹äºŽå®žä½“çš„ç»“æž„åµŒå…¥ã€‚ä¸ŽTransEä¸åŒçš„æ˜¯ï¼Œæ¨¡åž‹å¸Œæœ›æ›´å…³æ³¨å·²å¯¹é½çš„ä¸‰å…ƒç»„ï¼Œä¹Ÿå°±æ˜¯åŒ…å«å¯¹é½è°“è¯çš„ä¸‰å…ƒç»„ã€‚æ¨¡åž‹é€šè¿‡æ·»åŠ æƒé‡æ¥å®žçŽ°è¿™ä¸€ç›®çš„ã€‚Structure embeddingçš„ç›®æ ‡å‡½æ•°å¦‚ä¸‹ï¼š count(r)æ˜¯å…³ç³»rå‡ºçŽ°çš„æ•°é‡ã€‚ Attribute Character Embeddingå¯¹äºŽå±žæ€§å­—ç¬¦åµŒå…¥ï¼Œä¹Ÿå‚è€ƒTransEçš„æ€æƒ³ï¼Œå°†è°“è¯rè§£é‡Šä¸ºä»Žå¤´éƒ¨å®žä½“håˆ°å±žæ€§açš„è½¬æ¢ã€‚ä½†æ˜¯ï¼Œç›¸åŒçš„å±žæ€§aå¯ä»¥åœ¨ä¸¤ä¸ªKGä¸­ä»¥ä¸åŒçš„å½¢å¼å‡ºçŽ°ï¼Œä¾‹å¦‚50.9989å¯¹50.9988888889ä½œä¸ºå®žä½“çš„çº¬åº¦; â€œBarack Obamaâ€ä¸Žâ€œBarack Hussein Obamaâ€ä½œä¸ºäººåç­‰ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºä½¿ç”¨ç»„åˆå‡½æ•°å¯¹å±žæ€§å€¼è¿›è¡Œç¼–ç ï¼Œå¹¶å°†å±žæ€§ä¸‰å…ƒç»„ä¸­æ¯ä¸ªå…ƒç´ çš„å…³ç³»å®šä¹‰ä¸ºh +râ‰ˆfaï¼ˆaï¼‰ã€‚ è¿™é‡Œï¼Œfaï¼ˆaï¼‰æ˜¯ç»„åˆå‡½æ•°ï¼Œaæ˜¯å±žæ€§å€¼a = {c1ï¼Œc2ï¼Œc3ï¼Œâ€¦ï¼Œct}çš„å­—ç¬¦åºåˆ—ã€‚ ç»„åˆå‡½æ•°å°†å±žæ€§å€¼ç¼–ç ä¸ºå•ä¸ªå‘é‡ï¼Œå¹¶å°†ç±»ä¼¼çš„å±žæ€§å€¼æ˜ å°„åˆ°ç±»ä¼¼çš„å‘é‡è¡¨ç¤ºã€‚ ä½œè€…å®šä¹‰äº†ä¸‰ä¸ªç»„æˆå‡½æ•°å¦‚ä¸‹ã€‚ Sum compositional function (SUM)å­˜åœ¨é—®é¢˜ï¼šåŒ…å«ç›¸åŒå­—ç¬¦ä¸åŒé¡ºåºçš„å±žæ€§å€¼ä¼šæœ‰ç›¸åŒçš„å‘é‡è¡¨ç¤º LSTM-based compositional function (LSTM). N-gram-based compositional function (N-gram) æœ€åŽattribute character embeddingç›®æ ‡å‡½æ•°ï¼š Joint Learning of Structure Embedding and Attribute Character Embeddingä½œè€…ä½¿ç”¨å±žæ€§å­—ç¬¦åµŒå…¥é€šè¿‡æœ€å°åŒ–ä»¥ä¸‹ç›®æ ‡å‡½æ•°å°†ç»“æž„åµŒå…¥ç§»åŠ¨åˆ°ç›¸åŒçš„å‘é‡ç©ºé—´ï¼š æœ¬æ–‡æ•´ä½“æŸå¤±å‡½æ•°ï¼š Entity Alignmentåœ¨ç»è¿‡ä¸Šè¿°è®­ç»ƒè¿‡ç¨‹ä¹‹åŽï¼Œæ¥è‡ªä¸åŒKGçš„ç›¸ä¼¼çš„å®žä½“å°†ä¼šæœ‰ç›¸ä¼¼çš„å‘é‡è¡¨ç¤ºï¼Œå› æ­¤å¯é€šè¿‡ èŽ·å¾—æ½œåœ¨å®žä½“å¯¹é½å¯¹ã€‚æ­¤å¤–ï¼Œæ¨¡åž‹è®¾å®šç›¸ä¼¼åº¦é˜ˆå€¼æ¥è¿‡æ»¤æ½œåœ¨å®žä½“å¯¹é½å¯¹ï¼Œå¾—åˆ°æœ€ç»ˆçš„å¯¹é½ç»“æžœã€‚ Triple Enrichment via Transitivity Ruleä½œè€…åˆ©ç”¨ä¸€é˜¶é€»è¾‘ä¼ é€’å…³ç³»æ¥ä¸°å¯Œä¸‰å…ƒç»„ã€‚å³ï¼šå­˜åœ¨å’Œåˆ™å¯ä»¥æŽ¨ç†å‡ºh_1+ (r_1.r_2) â‰ˆ t_2 Databaseæœ¬æ–‡ä»Ž DBpedia (DBP)ã€LinkedGeoData (LGD)ã€Geonames (GEO) å’Œ YAGO å››ä¸ª KG ä¸­æŠ½å–æž„å»ºäº†ä¸‰ä¸ªæ•°æ®é›†ï¼Œåˆ†åˆ«æ˜¯DBP-LGDã€DBP-GEOå’ŒDBP-YAGOã€‚å…·ä½“çš„æ•°æ®ç»Ÿè®¡å¦‚ä¸‹ï¼š ExperimentsEntity Alignment Resultsæœ¬æ–‡å¯¹æ¯”äº†ä¸‰ä¸ªç›¸å…³çš„æ¨¡åž‹ï¼Œåˆ†åˆ«æ˜¯ TransEã€MTransE å’Œ JAPEã€‚è¯•éªŒç»“æžœè¡¨æ˜Žï¼Œæœ¬æ–‡æå‡ºçš„æ¨¡åž‹åœ¨å®žä½“å¯¹é½ä»»åŠ¡ä¸Šå–å¾—äº†å…¨é¢çš„è¾ƒå¤§çš„æå‡ï¼Œåœ¨ä¸‰ç§ç»„åˆå‡½æ•°ä¸­ï¼ŒN-gramå‡½æ•°çš„ä¼˜åŠ¿è¾ƒä¸ºæ˜Žæ˜¾ã€‚æ­¤å¤–ï¼ŒåŸºäºŽä¼ é€’è§„åˆ™çš„ä¸‰å…ƒç»„ä¸°å¯Œæ¨¡åž‹å¯¹ç»“æžœä¹Ÿæœ‰ä¸€å®šçš„æå‡ã€‚å…·ä½“ç»“æžœå¦‚ä¸‹ Rule-based Entity Alignment Resultsä¸ºäº†è¿›ä¸€æ­¥è¡¡é‡ attribute character embedding æ•èŽ·å®žä½“é—´ç›¸ä¼¼ä¿¡æ¯çš„èƒ½åŠ›ï¼Œæœ¬æ–‡è®¾è®¡äº†åŸºäºŽè§„åˆ™çš„å®žä½“å¯¹é½æ¨¡åž‹ã€‚æœ¬å®žéªŒå¯¹æ¯”äº†ä¸‰ç§ä¸åŒçš„æ¨¡åž‹ï¼šä»¥labelçš„å­—ç¬¦ä¸²ç›¸ä¼¼åº¦ä½œä¸ºåŸºç¡€æ¨¡åž‹ï¼›é’ˆå¯¹æ•°æ®é›†ç‰¹ç‚¹ï¼Œåœ¨åŸºç¡€æ¨¡åž‹çš„åŸºç¡€ä¹‹ä¸Šå¢žåŠ äº†åæ ‡å±žæ€§ï¼Œä»¥æ­¤ä½œä¸ºç¬¬äºŒä¸ªæ¨¡åž‹ï¼›ç¬¬ä¸‰ä¸ªæ¨¡åž‹æ˜¯æŠŠæœ¬æ–‡æå‡ºçš„æ¨¡åž‹ä½œä¸ºé™„åŠ æ¨¡åž‹ï¼Œä¸ŽåŸºç¡€æ¨¡åž‹ç›¸ç»“åˆã€‚å…·ä½“ç»“æžœå¦‚ä¸‹ï¼š KG Completion Resultsæœ¬æ–‡è¿˜åœ¨KGè¡¥å…¨ä»»åŠ¡ä¸ŠéªŒè¯äº†æ¨¡åž‹çš„æœ‰æ•ˆæ€§ã€‚æ¨¡åž‹ä¸»è¦æµ‹è¯•äº†é“¾æŽ¥é¢„æµ‹å’Œä¸‰å…ƒç»„åˆ†ç±»ä¸¤ä¸ªæ ‡å‡†ä»»åŠ¡ï¼Œåœ¨è¿™ä¸¤ä¸ªä»»åŠ¡ä¸­ï¼Œæ¨¡åž‹ä¹Ÿå–å¾—äº†ä¸é”™çš„æ•ˆæžœã€‚å…·ä½“ç»“æžœå¦‚ä¸‹ï¼š]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>çŸ¥è¯†å›¾è°±</tag>
        <tag>çŸ¥è¯†å›¾è°±åµŒå…¥</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ã€ŠRelNN A Deep Neural Model for Relational Learningã€‹é˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FRelNN%20A%20Deep%20Neural%20Model%20for%20Relational%20Learning%2F</url>
    <content type="text"><![CDATA[è®ºæ–‡ä¸‹è½½åœ°å€ï¼Œè¿™ç¯‡æ–‡ç« ç›¸å½“äºŽç»“åˆäº†ç»Ÿè®¡å­¦ä¹ å’Œæ·±åº¦ç¥žç»ç½‘ç»œã€‚é‡Œé¢æœ‰äº›å…¬å¼æ²¡æœ‰ç†è§£ï¼Œåº”è¯¥æ˜¯æœ‰è®¸å¤šå…ˆå‰è®ºæ–‡éœ€è¦é˜…è¯»ã€‚ä½†æ˜¯æœ¬ç¯‡è®ºæ–‡æ‰©å±•äº†æ€è·¯å¦‚ä½•ç»“åˆç»Ÿè®¡å­¦å’Œæ·±åº¦å­¦ä¹ ï¼Œå¹¶ä¸”åŸºäºŽå…¶ä½™æ•°æ®æ¥é¢„æµ‹ä¸€ä¸ªç±»ä¸­å¯¹è±¡çš„ä¸€ä¸ªå±žæ€§ï¼Œæƒ³æ³•ä¹Ÿæ¯”è¾ƒå¥½ã€‚ Introductionä½œè€…ä¸»è¦é›†ä¸­äºŽåŸºäºŽå…¶ä½™æ•°æ®æ¥é¢„æµ‹ä¸€ä¸ªç±»ä¸­å¯¹è±¡çš„ä¸€ä¸ªå±žæ€§ã€‚ Challengeå½“ç±»ä¸­æ¯ä¸ªå¯¹è±¡çš„å±žæ€§ä¾èµ–äºŽä¸åŒæ•°é‡çš„å…¶ä»–å¯¹è±¡çš„å±žæ€§å’Œå…³ç³»æ—¶ï¼Œæ­¤é—®é¢˜å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ åœ¨StarAIç¤¾åŒºä¸­ï¼Œæ­¤é—®é¢˜ç§°ä¸ºèšåˆï¼ˆaggregationï¼‰ã€‚ Relational Logistic Regression and Markov Logic NetworksStarAIæ¨¡åž‹æ—¨åœ¨æ¨¡æ‹Ÿå¯¹è±¡ä¹‹é—´å…³ç³»çš„æ¦‚çŽ‡ã€‚ Relational logistic regression (RLR) (Kazemi et al. 2014)å®šä¹‰çš„æ¦‚çŽ‡å…¬å¼å¦‚ä¸‹ï¼š ä¸Šé¢å®šä¹‰çš„RLRæ¨¡åž‹ä»…é€‚ç”¨äºŽå¸ƒå°”å€¼æˆ–å¤šå€¼çˆ¶é¡¹ã€‚ä½œè€…é‡‡ç”¨çš„æ˜¯è¿žç»­çš„åŽŸå­ï¼ˆcontinuous atomsï¼‰ï¼ˆFatemi, Kazemi, and Poole (2016)ï¼‰ Relational Neural Networksä½œè€…é€šè¿‡è®¾è®¡ç¥žç»ç½‘ç»œä¸­çº¿æ€§å±‚ï¼ˆLLï¼‰ï¼Œæ¿€æ´»å±‚ï¼ˆALï¼‰å’Œè¯¯å·®å±‚ï¼ˆELï¼‰çš„å…³ç³»å¯¹åº”ç‰©ï¼Œå¯¹å…·æœ‰åˆ†å±‚æž¶æž„çš„RLR / MLNæ¨¡åž‹è¿›è¡Œç¼–ç ã€‚ å…³ç³»ç¥žç»ç½‘ç»œï¼ˆRelNNï¼‰æ˜¯åŒ…å«ä½œä¸ºå›¾å½¢å½¼æ­¤è¿žæŽ¥çš„è‹¥å¹²RLLå’ŒRALçš„ç»“æž„ã€‚ Motivations for hidden layers ä½¿å–œæ¬¢çœ‹åŠ¨ä½œç”µå½±çš„äººæ•°å¢žåŠ æ—¶ï¼Œç”·æ€§çš„æ¦‚çŽ‡å˜ä¸º[0, 1]é‡çš„ä»»ä½•æ•°å€¼ï¼Œä¸è‡³äºŽç›´æŽ¥å˜ä¸º0æˆ–è€…1ã€‚ å› æ­¤ï¼Œéšè—å±‚é€šè¿‡ä½¿æ¨¡åž‹èƒ½å¤Ÿå­¦ä¹ é€šç”¨è§„åˆ™å¹¶ç›¸åº”åœ°å¯¹å¯¹è±¡è¿›è¡Œåˆ†ç±»ï¼Œç„¶åŽä»¥ä¸åŒæ–¹å¼å¤„ç†ä¸åŒç±»åˆ«çš„å¯¹è±¡ï¼Œä»Žè€Œæé«˜äº†å»ºæ¨¡èƒ½åŠ›ã€‚ ä½¿ç”¨RLRè¡¨ç¤ºä¸åŒç±»åž‹çš„çŽ°æœ‰æ˜¾å¼èšåˆå™¨ï¼Œç„¶è€Œæœ‰äº›æƒ…å†µéœ€è¦ä½¿ç”¨2ä¸ªRLLså’Œ2ä¸ªRALs Learning latent properties directlyå¯¹è±¡å¯èƒ½åŒ…å«æ— æ³•ä½¿ç”¨å¸¸è§„è§„åˆ™æŒ‡å®šçš„æ½œåœ¨å±žæ€§ï¼Œä½†å¯ä»¥åœ¨è®­ç»ƒæœŸé—´ç›´æŽ¥ä»Žæ•°æ®ä¸­å­¦ä¹ ã€‚ è€ƒè™‘å›¾2ä¸­çš„æ¨¡åž‹ï¼Œè®©Latentï¼ˆmï¼‰æˆä¸ºç”µå½±çš„æ•°å­—æ½œåœ¨å±žæ€§ï¼Œå…¶å€¼å°†åœ¨è®­ç»ƒæœŸé—´å­¦ä¹ ã€‚ From ConvNet Primitives to RelNNsæˆ‘ä»¬è§£é‡Šä¸ºä»€ä¹ˆRelNNä¹Ÿå¯ä»¥è¢«è§†ä¸ºConvNetsçš„ä¸€ä¸ªå®žä¾‹ã€‚ ConvNetsçš„è¾“å…¥çŸ©é˜µä¸­çš„å•å…ƒï¼ˆä¾‹å¦‚ï¼Œå›¾åƒåƒç´ ï¼‰å…·æœ‰ç©ºé—´ç›¸å…³æ€§å’Œç©ºé—´å†—ä½™ï¼šå½¼æ­¤æ›´æŽ¥è¿‘çš„å•å…ƒæ¯”æ›´è¿œçš„å•å…ƒæ›´ä¾èµ–ã€‚ ä¾‹å¦‚ï¼Œå¦‚æžœMè¡¨ç¤ºå›¾åƒçš„è¾“å…¥é€šé“ï¼Œåˆ™M [iï¼Œj]å’ŒM [i + 1ï¼Œj + 1]ä¹‹é—´çš„ä¾èµ–æ€§å¯èƒ½è¿œå¤§äºŽM [iï¼Œj]å’ŒM [iï¼Œj+20]ä¹‹é—´çš„ä¾èµ–æ€§ã€‚ å¯¹äºŽå…³ç³»æ•°æ®ï¼Œè¾“å…¥çŸ©é˜µä¸­çš„ä¾èµ–å…³ç³»ï¼ˆå…³ç³»ï¼‰æ˜¯ä¸åŒçš„ï¼šåŒä¸€è¡Œæˆ–åˆ—ä¸­çš„å•å…ƒï¼ˆå³åŒä¸€å¯¹è±¡çš„å…³ç³»ï¼‰å…·æœ‰æ¯”ä¸åŒè¡Œå’Œåˆ—ä¸­çš„å•å…ƒæ›´é«˜çš„ä¾èµ–æ€§ï¼ˆå³ä¸åŒå¯¹è±¡çš„å…³ç³»ï¼‰ã€‚ å› æ­¤ï¼Œä¸ºäº†ä½¿ConvNetsé€‚åº”å…³ç³»æ•°æ®ï¼Œæˆ‘ä»¬éœ€è¦çŸ¢é‡å½¢çŠ¶çš„è¿‡æ»¤å™¨ï¼Œè¿™äº›è¿‡æ»¤å™¨å¯¹è¡Œå’Œåˆ—äº¤æ¢æ˜¯ä¸å˜çš„ï¼Œå¹¶ä¸”æ›´å¥½åœ°æ•èŽ·å…³ç³»ä¾èµ–æ€§å’Œå¯äº¤æ¢æ€§å‡è®¾ã€‚ DatasetsMovielens 1M dataset (Harper and Konstan 2015)ç¬¬ä¸€ä¸ªæ•°æ®é›†æ˜¯Movielens 1M dataset (Harper and Konstan 2015)ï¼Œå¿½ç•¥äº†å®žé™…çš„è¯„çº§ï¼Œåªè€ƒè™‘ç”µå½±æ˜¯å¦è¢«è¯„çº§ï¼Œåªè€ƒè™‘åŠ¨ä½œå’Œæˆå‰§ç±»åž‹ã€‚ PAKDD15èŽ·å–åœ°å€ all Chinese and Mexican restaurants in Yelp dataset challengeèŽ·å–åœ°å€ Empirical Resultsä½œè€…æå‡ºäº†ä¸‰ä¸ªé—®é¢˜æ¥è¿›è¡Œå®žéªŒï¼š Q1ï¼šRelNNçš„æ€§èƒ½ä¸Žå…¶ä»–ä¼—æ‰€å‘¨çŸ¥çš„å…³ç³»å­¦ä¹ ç®—æ³•ç›¸æ¯”å¦‚ä½•ï¼Ÿ Q2ï¼šåŸºäºŽæ•°å­—å’Œè§„åˆ™çš„æ½œåœ¨å±žæ€§å¦‚ä½•å½±å“RelNNçš„æ€§èƒ½?æ›´æ”¹äº†RelNNä¸­éšè—å›¾å±‚å’Œæ•°å­—æ½œåœ¨å±žæ€§çš„æ•°é‡ï¼Œä»¥æŸ¥çœ‹å®ƒä»¬å¦‚ä½•å½±å“æ€§èƒ½ã€‚ è¯·æ³¨æ„ï¼Œæ·»åŠ å›¾å±‚åªä¼šæ·»åŠ ä¸€å®šæ•°é‡çš„å‚æ•°ï¼Œä½†æ·»åŠ kä¸ªæ•°å­—æ½œåœ¨å±žæ€§ä¼šå¢žåŠ k * |Î”m|å‚æ•°ã€‚ Q3ï¼šRelNNå¦‚ä½•æŽ¨æ–­å‡ºçœ‹ä¸è§çš„æ¡ˆä¾‹å¹¶è§£å†³æŒ‡å‘çš„è§„æ¨¡å¤§å°é—®é¢˜ ï¼ˆPoole et al.2014ï¼‰?ä½œè€…å®žæ–½äº†ä¸¤ä¸ªå®žéªŒï¼š æˆ‘ä»¬åœ¨å¤§é‡æ•°æ®ä¸­è®­ç»ƒä¸€ä¸ªRelNNï¼Œå¹¶åœ¨ä¸€å°æ•°æ®ä¸Šè¿›è¡Œæµ‹è¯•ï¼šè¯¥å®žéªŒå¯ä»¥çœ‹ä½œæ¯ä¸ªæ¨¡åž‹å—å†·å¯åŠ¨é—®é¢˜çš„ä¸¥é‡ç¨‹åº¦ ç„¶åŽæˆ‘ä»¬åœ¨ä¸€å°æ•°æ®ä¸Šè®­ç»ƒä¸€ä¸ªRelNNå¹¶åœ¨ä¸€å¤§æ•°æ®ä¸Šè¿›è¡Œæµ‹è¯•ï¼šå¯ä»¥çœ‹ä½œè¿™äº›æ¨¡åž‹å¯¹æ›´å¤§ç¾¤ä½“çš„æŽ¨æ–­ Futureä½œè€…å°†ä»Žæ•°æ®ä¸­è‡ªåŠ¨å­¦ä¹ è¿™äº›ç»“æž„çš„é—®é¢˜ç•™ä½œæœªæ¥çš„å·¥ä½œã€‚]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>å…³ç³»æŠ½å–</tag>
        <tag>ç»Ÿè®¡å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ã€ŠInteraction Embeddings for Prediction and Explanationã€‹é˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FInteraction%20Embeddings%20for%20Prediction%20and%20Explanation%2F</url>
    <content type="text"><![CDATA[è®ºæ–‡ä¸‹è½½åœ°å€ï¼Œæ­¤è®ºæ–‡ä¸»è¦æå‡ºäº†å®žä½“å’Œå…³ç³»çš„äº¤äº’ä½œç”¨å¯¹äºŽçŸ¥è¯†å›¾è°±åµŒå…¥çš„å½±å“ï¼Œå’Œæå‡ºäº†æ–°çš„åµŒå…¥è¯„ä¼°æ–¹æ¡ˆ - æœç´¢é¢„æµ‹è§£é‡Šã€‚ è®ºæ–‡è´¡çŒ® æå‡ºäº†CrossEï¼Œä¸€ç§é€šè¿‡å­¦ä¹ ä¸€ä¸ªäº¤äº’çŸ©é˜µæ¥ç»™å®žä½“å’Œå…³ç³»çš„äº¤äº’å»ºæ¨¡çš„æ–°åž‹çŸ¥è¯†å›¾è°±åµŒå…¥ã€‚ æˆ‘ä»¬ä½¿ç”¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†è¯„ä¼°CrossEä¸Žé“¾æŽ¥é¢„æµ‹ä»»åŠ¡ä¸Šçš„å„ç§å…¶ä»–KGEçš„æ¯”è¾ƒï¼Œå¹¶æ˜¾ç¤ºCrossEåœ¨å…·æœ‰é€‚åº¦å‚æ•°å¤§å°çš„å¤æ‚ä¸”æ›´å…·æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šå®žçŽ°æœ€å…ˆè¿›çš„ç»“æžœã€‚ æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åµŒå…¥è¯„ä¼°æ–¹æ¡ˆ - æœç´¢é¢„æµ‹è§£é‡Šï¼Œå¹¶è¡¨æ˜ŽCrossEèƒ½å¤Ÿç”Ÿæˆæ¯”å…¶ä»–æ–¹æ³•æ›´å¯é çš„è§£é‡Šã€‚ è¿™è¡¨æ˜Žäº¤äº’åµŒå…¥æ›´èƒ½åœ¨ä¸åŒçš„ä¸‰å…ƒç»„çŽ¯å¢ƒä¸­æ•æ‰å®žä½“å’Œå…³ç³»ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚ ä»‹ç»ç»™å®šçŸ¥è¯†å›¾è°±å’Œä¸€ä¸ªè¦é¢„æµ‹çš„ä¸‰å…ƒç»„çš„å¤´å®žä½“å’Œå…³ç³»ï¼Œåœ¨é¢„æµ‹å°¾å®žä½“çš„è¿‡ç¨‹ä¸­ï¼Œå¤´å®žä½“å’Œå…³ç³»ä¹‹é—´æ˜¯æœ‰äº¤å‰äº¤äº’çš„crossover interaction, å³å…³ç³»å†³å®šäº†åœ¨é¢„æµ‹çš„è¿‡ç¨‹ä¸­å“ªäº›å¤´å®žä½“çš„ä¿¡æ¯æ˜¯æœ‰ç”¨çš„ï¼Œè€Œå¯¹é¢„æµ‹æœ‰ç”¨çš„å¤´å®žä½“çš„ä¿¡æ¯åˆå†³å®šäº†é‡‡ç”¨ä»€ä¹ˆé€»è¾‘åŽ»æŽ¨ç†å‡ºå°¾å®žä½“ï¼Œæ–‡ä¸­é€šè¿‡ä¸€ä¸ªæ¨¡æ‹Ÿçš„çŸ¥è¯†å›¾è°±è¿›è¡Œäº†è¯´æ˜Žå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š ç›¸å…³å·¥ä½œè®ºæ–‡ä¸­åœ¨è¿™éƒ¨åˆ†å¯¹KGEï¼ˆKnowledge graph embeddingï¼‰è¿›è¡Œäº†åˆ†ç±»æ€»ç»“ï¼š KGEs with general embeddings KGEs with multiple embeddings. KGEs that utilize extra information. è¿™éƒ¨åˆ†æ€»ç»“ä¸­å¯¹å¤§é‡çš„æ–¹æ³•è¿›è¡Œæè¿°ï¼Œå¯ä»¥ä½œä¸ºèƒŒæ™¯çŸ¥è¯†è¿›è¡Œé˜…è¯»ã€‚ CrossEæ¨¡åž‹åŸºäºŽå¯¹å¤´å®žä½“å’Œå…³ç³»ä¹‹é—´äº¤å‰äº¤äº’çš„è§‚å¯Ÿï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„çŸ¥è¯†å›¾è°±è¡¨ç¤ºå­¦ä¹ æ¨¡åž‹CrossE. CrossEé™¤äº†å­¦ä¹ å®žä½“å’Œå…³ç³»çš„å‘é‡è¡¨ç¤ºï¼ŒåŒæ—¶è¿˜å­¦ä¹ äº†ä¸€ä¸ªäº¤äº’çŸ©é˜µCï¼ŒCä¸Žå…³ç³»ç›¸å…³ï¼Œå¹¶ä¸”ç”¨äºŽç”Ÿæˆå®žä½“å’Œå…³ç³»ç»è¿‡äº¤äº’ä¹‹åŽçš„å‘é‡è¡¨ç¤ºï¼Œæ‰€ä»¥åœ¨CrossEä¸­å®žä½“å’Œå…³ç³»ä¸ä»…ä»…æœ‰é€šç”¨å‘é‡è¡¨ç¤ºï¼ŒåŒæ—¶è¿˜æœ‰å¾ˆå¤šäº¤äº’å‘é‡è¡¨ç¤ºã€‚CrossEæ ¸å¿ƒæƒ³æ³•å¦‚ä¸‹å›¾ï¼š ç›®æ ‡å‡½æ•°ç²‰å››æ­¥ç”Ÿæˆï¼š Interaction Embedding for Entitiesï¼šæ ¹æ®å¤´å®žä½“å‘é‡å’Œäº¤äº’çŸ©é˜µï¼ˆä»¥å…³ç³»ç¡®å®šçš„ï¼‰æ¥ç¡®å®šå¤´å®žä½“çš„äº¤äº’è¡¨ç¤ºã€‚ Interaction Embedding for Relationsï¼šæ ¹æ®å¤´å®žä½“çš„äº¤äº’è¡¨ç¤ºå’Œå…³ç³»ä½œç”¨ç”Ÿæˆå…³ç³»çš„äº¤äº’è¡¨ç¤º Combination Operatorï¼šå°†å¤´å®žä½“çš„äº¤äº’è¡¨ç¤ºå’Œå…³ç³»çš„äº¤äº’è¡¨ç¤ºç›¸ç»“åˆï¼Œå¹¶è¿›è¡Œéžçº¿æ€§å¤„ç†ï¼ˆtanhï¼‰ Similarity Operatorï¼šè®¡ç®—ç»“åˆåŽè¡¨ç¤ºå’Œå°¾å®žä½“è¡¨ç¤ºä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚ æœ€åŽåˆ†æ•°å‡½æ•°ï¼š æŸå¤±å‡½æ•°ï¼šï¼ˆè¿™é‡Œå°±æ˜¯ä¸€ä¸ªäº¤å‰ç†µå‡½æ•°ï¼Œä½†æ˜¯å†™çš„æœ‰é—®é¢˜f(x)é¡¹åº”è¯¥åœ¨æ‹¬å·å¤–ï¼‰ å¯¹äºŽé¢„æµ‹çš„è§£é‡Šè¿™éƒ¨åˆ†ä½œè€…æè¿°äº†å¦‚ä½•ç”Ÿæˆé¢„æµ‹ä¸‰å…ƒç»„çš„è§£é‡Šï¼Œå¹¶ä»‹ç»äº†åŸºäºŽåµŒå…¥çš„è·¯å¾„æœç´¢ç®—æ³•ï¼Œä¸»è¦æ­¥éª¤å¦‚ä¸‹ï¼š Search for similar relationsï¼šä¿®å‰ªæŽ‰ä¸åˆç†è·¯å¾„ Search for paths between h and tï¼šä½œè€…å®šä¹‰äº†6ç§è·¯å¾„ï¼ˆç­æ±‰ä¸€ä¸ªæˆ–ä¸¤ä¸ªå…³ç³»ï¼‰ Search similar entitiesï¼šæ•èŽ·å®žä½“ä¹‹é—´çš„ç›¸ä¼¼æ€§æ–¹é¢è¶Šæœ‰èƒ½åŠ›ï¼Œå°±è¶Šæœ‰å¯èƒ½å­˜åœ¨ï¼ˆhsï¼Œrï¼Œtsï¼‰ : Search for similar structures as supportsï¼šæˆ‘ä»¬åªå°†çŸ¥è¯†å›¾ä¸­è‡³å°‘æœ‰ä¸€ä¸ªæ”¯æŒçš„è·¯å¾„è§†ä¸ºè§£é‡Šã€‚ å®žéªŒæ•°æ®é›† é“¾æŽ¥é¢„æµ‹ ä»Žå®žéªŒç»“æžœä¸­æˆ‘ä»¬å¯ä»¥çœ‹å‡ºï¼ŒCrossEå®žçŽ°äº†è¾ƒå¥½çš„é“¾æŽ¥é¢„æµ‹ç»“æžœã€‚æˆ‘ä»¬åŽ»é™¤CrossEä¸­çš„å¤´å®žä½“å’Œå…³ç³»çš„äº¤å‰äº¤äº’ï¼Œæž„é€ äº†æ¨¡åž‹ CrossESï¼ŒCrossE å’Œ CrossES çš„æ¯”è¾ƒè¯´æ˜Žäº†äº¤å‰äº¤äº’çš„æœ‰æ•ˆæ€§ã€‚ ç”Ÿæˆè§£é‡Šæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºŽç›¸ä¼¼ç»“æž„é€šè¿‡çŸ¥è¯†å›¾è°±çš„è¡¨ç¤ºå­¦ä¹ ç»“æžœç”Ÿæˆé¢„æµ‹ç»“æžœè§£é‡Šçš„æ–¹æ³•ï¼Œå¹¶æå‡ºäº†ä¸¤ç§è¡¡é‡è§£é‡Šç»“æžœçš„æŒ‡æ ‡ï¼ŒAvgSupportå’ŒRecallã€‚Recallæ˜¯æŒ‡æ¨¡åž‹èƒ½ç»™å‡ºè§£é‡Šçš„é¢„æµ‹ç»“æžœçš„å æ¯”ï¼Œå…¶ä»‹äºŽ0å’Œ1ä¹‹é—´ä¸”å€¼è¶Šå¤§è¶Šå¥½ï¼›AvgSupportæ˜¯æ¨¡åž‹èƒ½ç»™å‡ºè§£é‡Šçš„é¢„æµ‹ç»“æžœçš„å¹³å‡supportä¸ªæ•°ï¼ŒAvgSupportæ˜¯ä¸€ä¸ªå¤§äºŽ0çš„æ•°ä¸”è¶Šå¤§è¶Šå¥½ã€‚å¯è§£é‡Šçš„è¯„ä¼°ç»“æžœå¦‚ä¸‹ï¼š é“¾æŽ¥é¢„æµ‹å’Œå¯è§£é‡Šçš„å®žéªŒä»Žä¸¤ä¸ªä¸åŒçš„æ–¹é¢è¯„ä¼°äº†çŸ¥è¯†å›¾è°±è¡¨ç¤ºå­¦ä¹ çš„æ•ˆæžœï¼ŒåŒæ—¶ä¹Ÿè¯´æ˜Žäº†é“¾æŽ¥é¢„æµ‹çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§æ²¡æœ‰å¿…ç„¶è”ç³»ï¼Œé“¾æŽ¥é¢„æµ‹æ•ˆæžœå¥½çš„æ¨¡åž‹å¹¶ä¸ä¸€å®šèƒ½å¤Ÿæ›´å¥½åœ°æä¾›è§£é‡Šï¼Œåä¹‹äº¦ç„¶ã€‚ å‚è€ƒé“¾æŽ¥ http://blog.openkg.cn/%E8%AE%BA%E6%96%87%E6%B5%85%E5%B0%9D-interaction-embeddings-for-prediction-and-explanation/]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>çŸ¥è¯†å›¾è°±</tag>
        <tag>çŸ¥è¯†å›¾è°±åµŒå…¥</tag>
        <tag>çŸ¥è¯†å›¾è°±æŽ¨ç†</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ã€ŠDifferentiable Learning of Logical Rules for Knowledge Base Reasoningã€‹é˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FDifferentiable%20Learning%20of%20Logical%20Rules%20for%20Knowledge%20Base%20Reasoning%2F</url>
    <content type="text"><![CDATA[è®ºæ–‡ä¸‹è½½åœ°å€ï¼Œæœ¬æ–‡ç ”ç©¶ç”¨äºŽäºŽçŸ¥è¯†å›¾è°±æŽ¨ç†çš„å­¦ä¹ æ¦‚çŽ‡ä¸€é˜¶é€»è¾‘è§„åˆ™çš„é—®é¢˜ï¼Œæå‡ºäº†Neural Logic Programmingï¼ˆNeural-LPï¼‰æ¡†æž¶ï¼Œå®ƒç»“åˆäº†ç«¯åˆ°ç«¯å¯å¾®åˆ†æ¨¡åž‹ä¸­ä¸€é˜¶é€»è¾‘è§„åˆ™çš„å‚æ•°å’Œç»“æž„å­¦ä¹ ã€‚ä¸ºäº†åœ¨å¯å¾®åˆ†çš„æ¡†æž¶ä¸­åŒæ—¶å­¦ä¹ å‚æ•°å’Œç»“æž„ï¼Œä½œè€…è®¾è®¡äº†ä¸€ä¸ªå…·æœ‰æ³¨æ„æœºåˆ¶å’Œè®°å¿†çš„ç¥žç»æŽ§åˆ¶å™¨ç³»ç»Ÿï¼Œä»¥å­¦ä¹ é¡ºåºç»„æˆTensorLogä½¿ç”¨çš„åŽŸå§‹å¯å¾®æ“ä½œã€‚ä½œè€…é‡‡ç”¨çš„æ³¨æ„æœºåˆ¶æ˜¯ä½œä¸ºé€»è¾‘è§„åˆ™çš„ç½®ä¿¡åº¦å¹¶ä¸”æœ‰å¯“æ„å«ä¹‰çš„ã€‚ ä¸‹å›¾å±•ç¤ºäº†ä¸€ä¸ªä½¿ç”¨é€»è¾‘è§„åˆ™è¿›è¡ŒçŸ¥è¯†å›¾è°±æŽ¨ç†çš„ä¾‹å­ ä½¿ç”¨æ¦‚çŽ‡é€»è¾‘çš„ä¼˜ç‚¹æ˜¯é€šè¿‡ä¸ºé€»è¾‘è§„åˆ™é…å¤‡æ¦‚çŽ‡ï¼Œå¯ä»¥æ›´å¥½åœ°æ¨¡æ‹Ÿç»Ÿè®¡å¤æ‚å’Œå™ªå£°æ•°æ®ã€‚ statistical relational learningï¼ˆç»Ÿè®¡å…³ç³»å­¦ä¹ ï¼‰ï¼šå­¦ä¹ å…³ç³»è§„åˆ™çš„é›†åˆ ==inductive logic programmingï¼ˆå½’çº³é€»è¾‘è§„åˆ’ï¼‰ï¼š==å½“å­¦ä¹ æ¶‰åŠæå‡ºæ–°çš„é€»è¾‘è§„åˆ™æ—¶ã€‚ï¼ˆè¿™åº”è¯¥å’Œæˆ‘æ­£åœ¨åšçš„æ–¹å‘æ˜¯ç›¸å…³çš„ï¼Œéƒ½æ˜¯å¸¦æœ‰å½’çº³æ€§è´¨çš„ï¼Œæœ‰æ–°çš„ä¸œè¥¿äº§ç”Ÿï¼‰ã€‚ FrameworkKnowledge base reasoningä¸ºäº†æŽ¨ç†çŸ¥è¯†åº“ï¼Œå¯¹äºŽæ¯ä¸ªæŸ¥è¯¢æˆ‘ä»¬éƒ½æœ‰å…´è¶£å­¦ä¹ ä»¥ä¸‹å½¢å¼çš„åŠ æƒé“¾å¼é€»è¾‘è§„åˆ™ï¼Œç±»ä¼¼äºŽ==éšæœºé€»è¾‘ç¨‹åº==ï¼š å…¶ä¸­$\alpha$æ˜¯å’Œè§„åˆ™æœ‰å…³çš„ç½®ä¿¡åº¦ï¼ŒRæ˜¯çŸ¥è¯†åº“ä¸­çš„å…³ç³»ï¼Œquery(Y,X) è¡¨ç¤ºä¸€ä¸ªä¸‰å…ƒç»„ï¼Œquery è¡¨ç¤ºä¸€ä¸ªå…³ç³»ã€‚ TensorLog for KB reasoningå°†å®žä½“è½¬æ¢æˆone-hotå˜é‡ï¼›å¹¶ç”¨ä¸€ä¸ªçŸ©é˜µ$M_R$è¡¨ç¤ºå…³ç³»ï¼Œè¯¥çŸ©é˜µåªåœ¨ï¼ˆiï¼Œjï¼‰å¤„ä¸º1ï¼Œiã€jä¸ºç¬¬iã€jä¸ªå®žä½“ã€‚ ç»“åˆä¸¤ä¸ªæ“ä½œï¼Œé€»è¾‘è§„åˆ™æŽ¨ç†$R(Y,X) \gets P(Y,X) \bigwedge Q(Z,X)$å¯ä»¥è¢«è¡¨ç¤ºä¸ºï¼š$M_P \cdot M_P \cdot v_x \doteq s$ï¼Œå‘é‡sä¸­ä¸º1çš„ä½ç½®å°±æ˜¯Yçš„ç­”æ¡ˆã€‚ å¯¹äºŽä¸€æ¡æŸ¥è¯¢ï¼Œæ‰€æœ‰çš„é€»è¾‘è§„åˆ™çš„å³è¾¹éƒ¨åˆ†è¢«è¡¨ç¤ºä¸ºä»¥ä¸‹å½¢å¼ï¼š å…¶ä¸­ï¼Œlè¡¨ç¤ºæ‰€æœ‰çš„å¯èƒ½è§„åˆ™çš„ä¸ªæ•°ï¼Œ$\alpha_l$æ˜¯è§„åˆ™lçš„ç½®ä¿¡åº¦ï¼Œ$\beta_l$æ˜¯æŸç‰¹å®šå…³ç³»é‡Œçš„æœ‰åºå…³ç³»åˆ—è¡¨ï¼Œæ‰€ä»¥åœ¨inferenceæ—¶ï¼Œç»™å®šå®žä½“$v_xâ€‹$ï¼Œå®žä½“yçš„scoreç­‰äºŽå‘é‡sä¸­çš„å¯¹åº”yçš„ä½ç½®çš„å€¼ã€‚å¯¹äºŽæŽ¨ç†ï¼Œç»™å®šå®žä½“xï¼Œå®žä½“yçš„scoreç­‰äºŽå‘é‡sä¸­çš„å¯¹åº”yçš„ä½ç½®çš„å€¼ã€‚ æ‰€ä»¥æ€»ç»“æœ¬æ–‡å…³å¿ƒçš„ä¼˜åŒ–é—®é¢˜å¦‚ä¸‹ï¼š Learning the logical rulesåœ¨ä¸Šå¼çš„ä¼˜åŒ–é—®é¢˜ä¸­ï¼Œç®—æ³•éœ€è¦å­¦ä¹ çš„éƒ¨åˆ†åˆ†ä¸ºä¸¤ä¸ªï¼šä¸€ä¸ªæ˜¯è§„åˆ™çš„ç»“æž„ï¼Œå³ä¸€ä¸ªè§„åˆ™æ˜¯ç”±å“ªäº›æ¡ä»¶ç»„åˆè€Œæˆçš„ï¼›å¦ä¸€ä¸ªæ˜¯è§„åˆ™çš„ç½®ä¿¡åº¦ã€‚ç”±äºŽæ¯ä¸€æ¡è§„åˆ™çš„ç½®ä¿¡åº¦éƒ½æ˜¯ä¾èµ–äºŽå…·ä½“çš„è§„åˆ™å½¢å¼ï¼Œè€Œè§„åˆ™ç»“æž„çš„ç»„æˆä¹Ÿæ˜¯ä¸€ä¸ªç¦»æ•£åŒ–çš„è¿‡ç¨‹ï¼Œå› æ­¤ä¸Šå¼æ•´ä½“æ˜¯ä¸å¯å¾®çš„ã€‚å› æ­¤ä½œè€…å¯¹å‰é¢çš„å¼å­åšäº†ä»¥ä¸‹æ›´æ”¹ï¼š å¯¹æ¯”ä¸Žå¼ï¼ˆ2ï¼‰ï¼šä¸»è¦äº¤æ¢äº†è¿žä¹˜å’Œç´¯åŠ çš„è®¡ç®—é¡ºåºï¼Œå¯¹é¢„ä¸€ä¸ªå…³ç³»çš„ç›¸å…³çš„è§„åˆ™ï¼Œä¸ºæ¯ä¸ªå…³ç³»åœ¨æ¯ä¸ªæ­¥éª¤éƒ½å­¦ä¹ äº†ä¸€ä¸ªæƒé‡ï¼Œå³ä¸Šå¼çš„ $a_t^k$ã€‚ ç”±äºŽä¸Šå¼å›ºå®šäº†æ¯ä¸ªè§„åˆ™çš„é•¿åº¦éƒ½ä¸º Tï¼Œè¿™æ˜¾ç„¶æ˜¯ä¸åˆé€‚çš„ã€‚ä¸ºäº†èƒ½å¤Ÿå­¦ä¹ åˆ°å˜é•¿çš„è§„åˆ™ï¼ŒNeural LPä¸­è®¾è®¡äº†è®°å¿†å‘é‡ $u_t$,è¡¨ç¤ºæ¯ä¸ªæ­¥éª¤è¾“å‡ºçš„ç­”æ¡ˆâ€”æ¯ä¸ªå®žä½“ä½œä¸ºç­”æ¡ˆçš„æ¦‚çŽ‡åˆ†å¸ƒï¼Œè¿˜è®¾è®¡äº†ä¸¤ä¸ªæ³¨æ„åŠ›å‘é‡ï¼šä¸€ä¸ªä¸ºè®°å¿†æ³¨æ„åŠ›å‘é‡ $b_t$ â€”â€”è¡¨ç¤ºåœ¨æ­¥éª¤ t æ—¶å¯¹äºŽä¹‹å‰æ¯ä¸ªæ­¥éª¤çš„æ³¨æ„åŠ›ï¼›ä¸€ä¸ªä¸ºç®—å­æ³¨æ„åŠ›å‘é‡ $a_t$ â€”â€”è¡¨ç¤ºåœ¨æ­¥éª¤ t æ—¶å¯¹äºŽæ¯ä¸ªå…³ç³»ç®—å­çš„æ³¨æ„åŠ›ã€‚æ¯ä¸ªæ­¥éª¤çš„è¾“å‡ºç”±ä¸‹é¢ä¸‰ä¸ªå¼å­ç”Ÿæˆï¼š å…¶ä¸­$b_t$å’Œ$a_t$ç”±ä»¥ä¸‹å…¬å¼é€šè¿‡RNNèŽ·å¾—ï¼š æŽ¨ç†æœºçš„æ•´ä½“æ¡†æž¶æ˜¯ï¼š å…¶ä¸­memoryå­˜çš„å°±æ˜¯æ¯æ­¥çš„æŽ¨ç†ç»“æžœï¼ˆå®žä½“ï¼‰ï¼Œæœ€åŽçš„è¾“å‡ºï¼ˆä¾‹å¦‚$u_{T+1}$ï¼Œç›®æ ‡å°±æ˜¯æœ€å¤§åŒ– $logv_y^Tu$ï¼ŒåŠ logæ˜¯å› ä¸ºéžçº¿æ€§èƒ½è®©æ•ˆæžœå˜å¥½ã€‚ æ•´ä¸ªç®—æ³•å¦‚ä¸‹ï¼š å®žéªŒï¼ˆ1ï¼‰ ä¸¤ä¸ªæ ‡å‡†æ•°æ®é›†ä¸Šçš„ç»Ÿè®¡å…³ç³»å­¦ä¹ ç›¸å…³çš„å®žéªŒ Unified Medical Language System (UMLS)ï¼šThe entities are biomedical concepts (e.g. disease, antibiotic) and relations are like treats and diagnoses. Kinshipï¼šcontains kinship relationships among members of the Alyawarra tribe from Central Australia [ ï¼ˆ2ï¼‰ åœ¨$16*16$çš„ç½‘æ ¼ä¸Šçš„è·¯å¾„å¯»æ‰¾çš„å®žéªŒ ï¼ˆ3ï¼‰ çŸ¥è¯†åº“è¡¥å…¨å®žéªŒå®žéªŒæ‰€ç”¨æ•°æ®é›†ä¿¡æ¯ï¼š FB15KSelectedï¼šè¿™æ˜¯é€šè¿‡ä»ŽFB15Kä¸­åŽ»é™¤è¿‘ä¼¼é‡å¤å’Œåå‘å…³ç³»è€Œæž„é€ çš„ å®žéªŒç»“æžœï¼š ä¸ºäº†è¯æ˜ŽNeural LPçš„å½’çº³æŽ¨ç†çš„èƒ½åŠ›ï¼Œæœ¬æ–‡è¿˜ç‰¹åˆ«è®¾è®¡äº†ä¸€ä¸ªå®žéªŒï¼Œåœ¨è®­ç»ƒæ•°æ®é›†ä¸­åŽ»æŽ‰æ‰€æœ‰æ¶‰åŠæµ‹è¯•é›†ä¸­åŒ…å«çš„å®žä½“çš„ä¸‰å…ƒç»„ï¼Œç„¶åŽè®­ç»ƒå¹¶é¢„æµ‹ï¼Œå¾—åˆ°ç»“æžœå¦‚ä¸‹ï¼š ï¼ˆ4ï¼‰ çŸ¥è¯†åº“é—®ç­”çš„å®žéªŒ æ€»ç»“æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå¯å¾®çš„è§„åˆ™å­¦ä¹ æ¨¡åž‹ï¼Œå¹¶å¼ºè°ƒäº†çŸ¥è¯†åº“ä¸­çš„è§„åˆ™åº”è¯¥æ˜¯å®žä½“æ— å…³çš„ï¼Œå¯¹äºŽæˆ‘ç›®å‰åœ¨åšçš„æ–¹å‘ï¼Œæœ¬ä½“è®ºä¹Ÿæ˜¯ä¸Žå®žä½“æ— å…³çš„ï¼Œè¿™ç§è§„åˆ™å­¦ä¹ æœ‰ä¸€å®šçš„å€Ÿé‰´æ€§ï¼Œä½†æ˜¯å¥½åƒæ‰€åŒºåˆ«ã€‚è¿™ä¸ªè§„åˆ™æŽ¨ç†ä¹Ÿå¯ä»¥çœ‹æˆæŸäº›å…³ç³»ä¹‹é—´çš„åŒ…å«å…³ç³»3.1ä¸­ä¸¾çš„HasOfficeInCity(New York,Uber) and CityInCountry(USA,New York)çš„ä¾‹å­ï¼Œå¯ä»¥çœ‹ä½œæ˜¯2å¯¹äºŽ1æœ‰åŒ…å«å…³ç³»ã€‚å¹¶ä¸”å¯ä»¥çœ‹åˆ°æœ¬ç¯‡è®ºæ–‡ä¸­ï¼Œä½œè€…è®¾è®¡äº†ä¸°å¯Œçš„å®žéªŒã€‚ å‚è€ƒé“¾æŽ¥ https://toutiao.io/posts/wrxf4z/preview https://zhuanlan.zhihu.com/p/46024825]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>çŸ¥è¯†å›¾è°±</tag>
        <tag>çŸ¥è¯†å›¾è°±æŽ¨ç†</tag>
        <tag>è§„åˆ™å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ã€ŠOK Google, What Is Your Ontology? Or/ Exploring Freebase Classification to Understand Googleâ€™s Knowledge Graphï¼Ÿã€‹é˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FOK%20Google%2C%20What%20Is%20Your%20Ontology%3F%20Or%2F%20Exploring%20Freebase%20Classification%20to%20Understand%20Google%E2%80%99s%20Knowledge%20Graph%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[æœ¬è®ºæ–‡è¯¦ç»†é˜è¿°Freebaseä¸­çš„æ•°æ®æ ¼å¼ï¼Œå¹¶è¿›è¡Œäº†é‡æž„ã€‚é€šè¿‡è€ƒè™‘æ•´ä½“æž¶æž„çš„ä¸‰ä¸ªéƒ¨åˆ†ï¼šFreebaseç±»åž‹ç³»ç»ŸåŠå…¶ç¼ºä¹ç»§æ‰¿å’Œä¾èµ–äºŽä¸å…¼å®¹æ€§ï¼Œå…è®¸è¡¨ç¤ºå€¼çš„ä¸ç¡®å®šæ€§çš„å®žçŽ°ï¼Œä»¥åŠåˆå¹¶å’Œæ‹†åˆ†å¯¹è±¡çš„å®žçŽ°ã€‚æ¥å¯¹æœ¬ä½“è¿›è¡Œé˜è¿°ã€‚è®ºæ–‡ä¸‹è½½åœ°å€ è¿™ç¯‡è®ºæ–‡é‡æž„äº†Freebaseæ•°æ®è½¬å‚¨æ¥ç†è§£è°·æ­Œè¯­ä¹‰æœç´¢ç‰¹å¾èƒŒåŽçš„æœ¬ä½“ã€‚è®ºæ–‡å°†ä¼šæŽ¢ç´¢Freebaseæœ¬ä½“å¦‚ä½•ç”±è®¸å¤šåŠ›é‡å¡‘é€ çš„ï¼Œè¿™äº›åŠ›é‡ä¹Ÿé€šè¿‡æ·±å…¥ç ”ç©¶æœ¬ä½“è®ºå’Œä¸€ä¸ªå°çš„ç›¸å…³æ€§ç ”ç©¶æ¥å½¢æˆåˆ†ç±»ç³»ç»Ÿã€‚è¿™äº›å‘çŽ°å°†ä¼šæä¾›çŸ¥è¯†å›¾è°±ä¸“æœ‰é»‘ç›’çš„ä¸€çž¥ã€‚ The structures found in the Freebase/Knowledge Graph ontology will be analyzed in light of the findings on classification systems in a key text by Bowker and Star (2000) [5]. æœ¯è¯­å®šä¹‰ ObjectFreebaseå¯¹è±¡æ˜¯ä¸€ä¸ªå…¨å±€å”¯ä¸€çš„æ ‡è¯†ç¬¦ï¼Œå®ƒæ˜¯Freebaseä¸­ä¸–ç•Œä¸ŠæŸç§ä¸œè¥¿çš„è¡¨ç¤ºã€‚ TypeFreebaseç±»åž‹ç”¨æ¥è¡¨è¾¾ç±»çš„æ¦‚å¿µã€‚ PropertyFreebaseå±žæ€§æ˜¯æè¿°å¯¹è±¡å¦‚ä½•é“¾æŽ¥åˆ°å…¶ä»–å€¼æˆ–å¯¹è±¡çš„å…³ç³»ã€‚ Property Detailå±žæ€§è¯¦ç»†ä¿¡æ¯æŒ‡çš„æ˜¯å¯ä»¥é€šè¿‡å±žæ€§é“¾æŽ¥çš„å¯¹è±¡æˆ–å€¼çš„çº¦æŸã€‚ RDF tripleèµ„æºæè¿°æ ¼å¼ï¼ˆRDFï¼‰æ˜¯ç”¨äºŽâ€œä¸‰å…ƒç»„â€ï¼ˆæˆ–N = 3å…ƒç»„ï¼‰æ ¼å¼çš„æ•°æ®è¡¨ç¤ºçš„è§„èŒƒ[17]ã€‚ Ontologyå¯¹äºŽæœ¬æ–‡ï¼ŒFreebaseæœ¬ä½“æ˜¯ç±»åž‹ï¼Œå±žæ€§å’Œå±žæ€§è¯¦ç»†ä¿¡æ¯çš„æ­£å¼ç»“æž„å’Œæè¿°ï¼Œç”¨äºŽæŒ‡å®šå¯¹è±¡å¦‚ä½•ç›¸äº’å…³è”ã€‚ Architectureåœ¨æœ¬æ–‡ä¸­ï¼Œæž¶æž„æŒ‡çš„æ˜¯å¯ä»¥åœ¨æœ¬ä½“ä¸­æ‰¾åˆ°çš„ä¸€èˆ¬æ¨¡å¼å’Œå…³ç³»ã€‚ ==æœ¬ä½“æ˜¯å¦å…è®¸ç±»ï¼ˆæˆ–Freebaseç”¨è¯­ä¸­çš„ç±»åž‹ï¼‰ä¹‹é—´çš„ç»§æ‰¿ï¼Ÿ æ˜¯å¦æœ‰ä¸Žå±žæ€§ç›¸å…³çš„é»˜è®¤å€¼ï¼Ÿ å¦‚ä½•å¤„ç†â€œé›¶â€æˆ–ç©ºå€¼ï¼Ÿ è¿™äº›ç±»åž‹çš„é—®é¢˜ä¸ä¸€å®šå…³æ³¨æœ¬ä½“ï¼ˆé£žæœºï¼Œç«è½¦æˆ–æ±½è½¦ï¼‰ä¸­å…·ä½“è¡¨è¾¾çš„å†…å®¹ï¼Œè€Œæ˜¯å…³äºŽæœ¬ä½“è¡¨è¾¾æ–¹å¼çš„æ›´å¤šé—®é¢˜åº”è¯¥é€šè¿‡æ£€æŸ¥æž¶æž„æ¥è§£å†³ã€‚== Methodologyä½œè€…æŠŠæ•°æ®è¿›è¡Œåˆ‡åˆ†ï¼šæŒ‰ç…§RDFä¸­ä¸‰å…ƒç»„çš„è°“è¯­è¿›è¡Œåˆ†ç±»ï¼Œä¾‹å¦‚ï¼š Freebase Ontology and Classification As Bowker and Star note, â€œInformation infrastructure is a tricky thing to analyzeâ€¦the easier they are to use, the harder they are to see.â€ [5]. What does the system make sense of? What is left out? What is privileged and by extension what is ignored by Google? è™½ç„¶Freebaseæœ¬ä½“å¯èƒ½ä¸ä¼šç«‹å³çœ‹èµ·æ¥åƒä¸€ä¸ªåˆ†ç±»ç³»ç»Ÿï¼Œä½†ç±»åž‹ï¼ˆç±»ï¼‰å’Œå±žæ€§çš„ç»“æž„æ˜¯ä¸€ä¸ªåŸºäºŽå¯¹å„ç§äº‹ç‰©è¿›è¡Œåˆ†ç±»çš„ç³»ç»Ÿã€‚ ä½œä¸ºå¯¹ä¸–ç•Œäº‹ç‰©è¡¨å¾è¿›è¡ŒæŽ’åºå’Œåˆ†ç±»çš„ç³»ç»Ÿï¼Œå°†æ ¹æ®Bowkerå’ŒStarçš„åˆ†ç±»ç»“æžœè®¨è®ºFreebaseæœ¬ä½“ã€‚ä»–ä»¬å°†å¯¹äºšé‡Œå£«å¤šå¾·å’ŒåŽŸåž‹åˆ†ç±»ï¼ˆAristotelian and prototype classificationï¼‰è¿›è¡Œäº†åŒºåˆ†ã€‚ äºšé‡Œå£«å¤šå¾·çš„åˆ†ç±»â€œæŒ‰ç…§ä¸€ç»„äºŒå…ƒç‰¹å¾è¿›è¡Œæ“ä½œï¼Œè¢«åˆ†ç±»çš„ç‰©ä½“å‘ˆçŽ°æˆ–ä¸å‘ˆçŽ°â€ï¼Œè€ŒåŽŸåž‹åˆ†ç±»åˆ™è®¤ä¸ºâ€œåœ¨æˆ‘ä»¬å¿ƒç›®ä¸­å¯¹äºŽæ¤…å­æ˜¯ä»€ä¹ˆçš„å¹¿æ³›æè¿°; æˆ‘ä»¬ç”¨éšå–»å’Œç±»æ¯”æ¥æ‰©å±•è¿™å¼ å›¾ç‰‡â€œ 5.1. Freebaseâ€™s Type Systemä¸å…¼å®¹æ€§çš„æ¦‚å¿µå‡ºçŽ°åœ¨Freebaseç³»ç»Ÿä¸­ï¼Œç”¨äºŽè¡¨ç¤ºå¯¹è±¡å¦‚ä½•å…·æœ‰æŸäº›ç±»åž‹ï¼Œè€Œè¿™äº›ç±»åž‹å¿…é¡»å°†å…¶æŽ’é™¤åœ¨å…¶ä»–ç±»åž‹ä¹‹å¤–ã€‚ æ²¡æœ‰ç»§æ‰¿ï¼ˆnot implement inheritanceï¼‰ï¼šä¸Šè¿°ä¸å…¼å®¹æ€§åœ¨ç¡®ä¿æ•°æ®ä¸è¡¨è¾¾å¯èƒ½åœ¨Google KPä¸­æä¾›çš„ä»¤äººå°´å°¬ï¼Œæœ‰å®³æˆ–ä¸æ­£ç¡®çš„é™ˆè¿°æ–¹é¢å‘æŒ¥äº†è¶³å¤Ÿå¼ºå¤§çš„ä½œç”¨ã€‚ ç¼ºä¹ç»§æ‰¿ä¹Ÿå¯èƒ½æ˜¯ä¸€ç§å…è®¸å®žä½“å…·æœ‰æ›´å¤§çµæ´»æ€§çš„ç‰¹å¾ã€‚è¿™é‡Œä½œè€…ä¸¾äº†ä¸€ä¸ªç‹—ä¸ºç”µå½±æ¼”å‘˜çš„ä¾‹å­ã€‚ 5.2. Has Value or Has No Value?ä¸‰å…ƒç»„å¦‚ä½•è¡¨è¾¾ä¼°è®¡å€¼ï¼Œä¸ç¡®å®šå€¼æˆ–ç©ºå€¼ï¼Ÿå®žé™…å¤„ç†æ—¶ç”¨â€œHas Valueâ€ (HV) and â€œHas No Valueâ€ (HNV)æ¥åˆ†åˆ«è¡¨è¾¾ä¸ç¡®å®šå€¼å’Œç©ºå€¼ã€‚ ä»¥è¿™ç§æ–¹å¼è¡¨è¾¾æœªçŸ¥æ•°å’Œç©ºå€¼çš„æœ‰è¶£å®žçŽ°å¯èƒ½è¡¨æ˜ŽFreebase / KGæœ€åˆå¹¶ä¸æ˜¯ä¸ºäº†æ”¯æŒè¿™ç§ä¸ç¡®å®šæ€§è€Œå»ºç«‹çš„ã€‚Googleçš„æ•°æ®ç¼–ç æŸäº›ä¸ç¡®å®šæ€§çš„æ¦‚å¿µå¹¶æœªå‘æœ€ç»ˆç”¨æˆ·å…¬å¼€ï¼Œå°½ç®¡å®ƒè‚¯å®šä»¥è¿™ç§ç‹¬ç‰¹çš„æ–¹å¼å®žçŽ°ã€‚ 5.3. Dealing with Doppelgangers and Chimerasæ¶‰åŠFreebaseå¦‚ä½•å¤„ç†â€œåˆå¹¶â€é‡å¤å¯¹è±¡ï¼ˆdoppelgangersï¼‰å’Œâ€œæ‹†åˆ†â€æ··åˆå¯¹è±¡ï¼ˆåµŒåˆä½“ï¼‰ã€‚ the property â€œ/dataworld/gardening hint/replaced byâ€ is used to implement merges be- tween various objects (e.g. by saying â€œ/m/xyz123 - Replaced By - /m/abc123â€). A Small Correlational Studyä¸»è¦æŽ¢ç´¢è¿™ä¸ªé—®é¢˜ï¼šåŸŸçš„æœ¬ä½“çš„å¤æ‚æ€§ï¼ˆäººç‰©ï¼Œç”µå½±ç­‰é¢†åŸŸçš„ç±»åž‹ï¼Œå±žæ€§ç­‰ï¼‰ä¸Žè¡¨è¾¾ä¸Žæœ¬ä½“ç›¸å…³çš„äº‹å®žï¼ˆâ€œçŸ¥è¯†åº“â€ï¼‰çš„ä¸‰å…ƒç»„æ•°é‡ä¹‹é—´æ˜¯å¦å­˜åœ¨å…³è”ï¼Ÿ å¯¹äºŽæœ¬ç ”ç©¶ï¼Œé€šè¿‡è€ƒè™‘ä¸ŽåŸŸç›¸å…³çš„å±žæ€§è¯¦ç»†ä¿¡æ¯é‡ï¼ˆå¤šå°‘æè¿°ï¼Œçº¦æŸç­‰ï¼‰æ¥å®žçŽ°â€œå¤æ‚æ€§â€å’Œâ€œæˆç†Ÿåº¦â€ã€‚ å¯¹äºŽ89ä¸ªåŸŸä¸­çš„æ¯ä¸€ä¸ªï¼ŒèŽ·å¾—äº†å…³äºŽæ¯ä¸ªåŸŸçš„æœ¬ä½“çš„ä»¥ä¸‹ç»Ÿè®¡ï¼š ==åŸŸä¸­çš„ç±»åž‹å’Œå±žæ€§æ•°== ==æ¯ç§ç±»åž‹å’Œå±žæ€§çš„æè¿°æ•°== ==æ¯ç§ç±»åž‹å’Œå±žæ€§çš„å±žæ€§è¯¦ç»†ä¿¡æ¯æ•°== é€šè¿‡èŽ·å–åŸŸä¸­æ¯ç§ç±»åž‹å’Œå±žæ€§çš„å¹³å‡æè¿°æ•°å’Œå±žæ€§è¯¦ç»†ä¿¡æ¯æ¥è®¡ç®—ç®€å•çš„å¤æ‚æ€§åˆ†æ•°ã€‚ æ‰€æœ‰åŸŸçš„RDFä¸‰å…ƒç»„è®¡æ•°ä¸Žæ­¤å¤æ‚æ€§å¾—åˆ†ä¹‹é—´çš„Pearsonç›¸å…³ç³»æ•°ä¸Ž0.2824å‘ˆæ­£ç›¸å…³ï¼Œç®€å•çº¿æ€§å›žå½’çš„æ–œçŽ‡ä¸º78,424.08ï¼ˆè§å›¾6ï¼‰ã€‚ å½“æŽ’é™¤å¼‚å¸¸éŸ³ä¹åˆ‡ç‰‡æ—¶ï¼Œç›¸å…³æ€§å’Œæ–œçŽ‡åˆ†åˆ«å˜ä¸º0.6680å’Œ33,899.53ã€‚ è™½ç„¶éœ€è¦è¿›ä¸€æ­¥çš„å·¥ä½œæ¥æŽ¢ç´¢è¿™ä¸ªç ”ç©¶é—®é¢˜ï¼Œä½†è¿™ä¸ªå°çš„ç›¸å…³æ€§ç ”ç©¶ä¸ºè¿›ä¸€æ­¥çš„å®žéªŒæä¾›äº†ä¸€äº›æœ‰å¸Œæœ›çš„åˆæ­¥ç»“æžœ discussionè€ƒè™‘æ•´ä½“æž¶æž„çš„ä¸‰ä¸ªéƒ¨åˆ†ï¼šFreebaseç±»åž‹ç³»ç»ŸåŠå…¶ç¼ºä¹ç»§æ‰¿å’Œä¾èµ–äºŽä¸å…¼å®¹æ€§ï¼Œå…è®¸è¡¨ç¤ºå€¼çš„ä¸ç¡®å®šæ€§çš„å®žçŽ°ï¼Œä»¥åŠåˆå¹¶å’Œæ‹†åˆ†å¯¹è±¡çš„å®žçŽ°ã€‚æ­¤å¤–ï¼Œè¿˜è¿›è¡Œäº†ä¸€é¡¹å°åž‹ç›¸å…³ç ”ç©¶ï¼Œä»¥æ£€éªŒåŸºäºŽBowkerå’ŒStaræŽ¨åŠ¨çš„é¢„æ„Ÿçš„å‡è®¾ã€‚åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šï¼Œåˆ†ç±»ç³»ç»Ÿä¸­çš„è®¸å¤šç‰¹å¾ä¹Ÿå¯ä»¥åœ¨Freebaseçš„æœ¬ä½“å’Œä½“ç³»ç»“æž„ä¸­æ‰¾åˆ°ã€‚ æœ¬æ–‡å…·ä½“è€Œè¨€ï¼ŒæŽ¢è®¨äº†æ”¯æŒæ•´ä¸ªäº¤ä»˜æµç¨‹çš„åŸºç¡€ç»“æž„ï¼ˆæœ¬ä½“å’Œä½“ç³»ç»“æž„ï¼‰ï¼Œè€Œä¸æ˜¯Freebase / KGä¸­è¡¨ç¤ºçš„ç‰¹å®šäº‹å®žã€‚ conclusion åº”é€šè¿‡æŽ¢ç´¢Freebaseæœ¬ä½“å’Œä½“ç³»ç»“æž„çš„å…¶ä»–æ–¹é¢ä»¥åŠå¯¹Freebaseè¿›è¡Œæ›´å…¨é¢çš„å®žéªŒåˆ†æžæ¥è¿›è¡Œè¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>çŸ¥è¯†å›¾è°±</tag>
        <tag>Ontology</tag>
        <tag>freebase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ­£åˆ™è¡¨è¾¾å¼]]></title>
    <url>%2Fpost%2FRegular%20expression%2F</url>
    <content type="text"><![CDATA[æœ¬æ–‡å‚è€ƒäº†ä¸€äº›é“¾æŽ¥ï¼Œè®°å½•äº†ä¸€äº›å¸¸ç”¨æ­£åˆ™è¡¨è¾¾å¼çš„è¯¦ç»†ä½¿ç”¨æ–¹æ³•ã€‚ åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼Œå¾ˆå¤šæ—¶å€™æˆ‘ä»¬éƒ½éœ€è¦ä»Žæ–‡æœ¬æˆ–å­—ç¬¦ä¸²ä¸­æŠ½å–å‡ºæƒ³è¦çš„ä¿¡æ¯ï¼Œå¹¶è¿›ä¸€æ­¥åšè¯­ä¹‰ç†è§£æˆ–å…¶å®ƒå¤„ç†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œä½œè€…ç”±åŸºç¡€åˆ°é«˜çº§ä»‹ç»äº†å¾ˆå¤šæ­£åˆ™è¡¨è¾¾å¼ï¼Œè¿™äº›è¡¨è¾¾å¼æˆ–è§„åˆ™åœ¨å¾ˆå¤šç¼–ç¨‹è¯­è¨€ä¸­éƒ½æ˜¯é€šç”¨çš„ã€‚ ä¹¦å†™æ­£åˆ™è¡¨è¾¾å¼ç½‘ç«™ æ­£åˆ™è¡¨è¾¾å¼ï¼ˆregex æˆ– regexpï¼‰å¯¹äºŽä»Žæ–‡æœ¬ä¸­æŠ½å–ä¿¡æ¯æžå…¶æœ‰ç”¨ï¼Œå®ƒä¸€èˆ¬ä¼šæœç´¢åŒ¹é…ç‰¹å®šæ¨¡å¼çš„è¯­å¥ï¼Œè€Œè¿™ç§æ¨¡å¼åŠå…·ä½“çš„ ASCII åºåˆ—æˆ– Unicode å­—ç¬¦ã€‚ä»Žè§£æž/æ›¿ä»£å­—ç¬¦ä¸²ã€é¢„å¤„ç†æ•°æ®åˆ°ç½‘é¡µçˆ¬å–ï¼Œæ­£åˆ™è¡¨è¾¾å¼çš„åº”ç”¨èŒƒå›´éžå¸¸å¹¿ã€‚ å…¶ä¸­ä¸€ä¸ªæ¯”è¾ƒæœ‰æ„æ€çš„åœ°æ–¹æ˜¯ï¼Œåªè¦æˆ‘ä»¬å­¦ä¼šäº†æ­£åˆ™è¡¨è¾¾å¼çš„è¯­å¥ï¼Œæˆ‘ä»¬å‡ ä¹Žå¯ä»¥å°†å…¶åº”ç”¨äºŽå¤šæœ‰çš„ç¼–ç¨‹è¯­è¨€ï¼ŒåŒ…æ‹¬ JavaScriptã€Pythonã€Ruby å’Œ Java ç­‰ã€‚åªä¸è¿‡å¯¹äºŽå„ç¼–ç¨‹è¯­è¨€æ‰€æ”¯æŒçš„æœ€é«˜çº§ç‰¹å¾ä¸Žè¯­æ³•æœ‰ç»†å¾®çš„åŒºåˆ«ã€‚ ä¸‹é¢æˆ‘ä»¬å¯ä»¥å…·ä½“è®¨è®ºä¸€äº›æ¡ˆä¾‹ä¸Žè§£é‡Šã€‚ åŸºæœ¬è¯­å¥é”šç‚¹ï¼š^ å’Œ $^The åŒ¹é…ä»»ä½•ä»¥â€œTheâ€å¼€å¤´çš„å­—ç¬¦ä¸² end$ åŒ¹é…ä»¥â€œendâ€ä¸ºç»“å°¾çš„å­—ç¬¦ä¸² ^The end$ æŠ½å–åŒ¹é…ä»Žâ€œTheâ€å¼€å§‹åˆ°â€œendâ€ç»“æŸçš„å­—ç¬¦ä¸² roar åŒ¹é…ä»»ä½•å¸¦æœ‰æ–‡æœ¬â€œroarâ€çš„å­—ç¬¦ä¸² æ•°é‡ç¬¦ï¼š*ã€+ã€ï¼Ÿå’Œ {}**abc* åŒ¹é…åœ¨â€œabâ€åŽé¢è·Ÿç€é›¶ä¸ªæˆ–å¤šä¸ªâ€œcâ€çš„å­—ç¬¦ä¸² abc+ åŒ¹é…åœ¨â€œabâ€åŽé¢è·Ÿç€ä¸€ä¸ªæˆ–å¤šä¸ªâ€œcâ€çš„å­—ç¬¦ä¸² abc? åŒ¹é…åœ¨â€œabâ€åŽé¢è·Ÿç€é›¶ä¸ªæˆ–ä¸€ä¸ªâ€œcâ€çš„å­—ç¬¦ä¸² abc{2} åŒ¹é…åœ¨â€œabâ€åŽé¢è·Ÿç€ä¸¤ä¸ªâ€œcâ€çš„å­—ç¬¦ä¸² abc{2,} åŒ¹é…åœ¨â€œabâ€åŽé¢è·Ÿç€ä¸¤ä¸ªæˆ–æ›´å¤šâ€œcâ€çš„å­—ç¬¦ä¸² abc{2,5} åŒ¹é…åœ¨â€œabâ€åŽé¢è·Ÿç€2åˆ°5ä¸ªâ€œcâ€çš„å­—ç¬¦ä¸² a(bc)* åŒ¹é…åœ¨â€œaâ€åŽé¢è·Ÿç€é›¶ä¸ªæˆ–æ›´å¤šâ€œbcâ€åºåˆ—çš„å­—ç¬¦ä¸² a(bc){2,5} åŒ¹é…åœ¨â€œaâ€åŽé¢è·Ÿç€2åˆ°5ä¸ªâ€œbcâ€åºåˆ—çš„å­—ç¬¦ä¸² æˆ–è¿ç®—ç¬¦ï¼š| ã€ []a(b|c) åŒ¹é…åœ¨â€œaâ€åŽé¢è·Ÿç€â€œbâ€æˆ–â€œcâ€çš„å­—ç¬¦ä¸² a[bc] åŒ¹é…åœ¨â€œaâ€åŽé¢è·Ÿç€â€œbâ€æˆ–â€œcâ€çš„å­—ç¬¦ä¸² å­—ç¬¦ç±»ï¼š\dã€\wã€\s å’Œ .**\d åŒ¹é…æ•°å­—åž‹çš„å•ä¸ªå­—ç¬¦ \w åŒ¹é…å•ä¸ªè¯å­—ï¼ˆå­—æ¯åŠ ä¸‹åˆ’çº¿ï¼‰ \s åŒ¹é…å•ä¸ªç©ºæ ¼å­—ç¬¦ï¼ˆåŒ…æ‹¬åˆ¶è¡¨ç¬¦å’Œæ¢è¡Œç¬¦ï¼‰ . åŒ¹é…ä»»æ„å­—ç¬¦ ä½¿ç”¨ã€Œ.ã€è¿ç®—ç¬¦éœ€è¦éžå¸¸å°å¿ƒï¼Œå› ä¸ºå¸¸è§ç±»æˆ–æŽ’é™¤åž‹å­—ç¬¦ç±»éƒ½è¦æ›´å¿«ä¸Žç²¾ç¡®ã€‚\dã€\w å’Œ\s åŒæ ·æœ‰å®ƒä»¬å„è‡ªçš„æŽ’é™¤åž‹å­—ç¬¦ç±»ï¼Œå³\Dã€\W å’Œ\Sã€‚ä¾‹å¦‚\D å°†æ‰§è¡Œä¸Ž\d å®Œå…¨ç›¸åçš„åŒ¹é…æ–¹æ³•ï¼š \D åŒ¹é…å•ä¸ªéžæ•°å­—åž‹çš„å­—ç¬¦ ä¸ºäº†æ­£ç¡®åœ°åŒ¹é…ï¼Œæˆ‘ä»¬å¿…é¡»ä½¿ç”¨è½¬ä¹‰ç¬¦åæ–œæ ã€Œ\ã€å®šä¹‰æˆ‘ä»¬éœ€è¦åŒ¹é…çš„ç¬¦å·ã€Œ^.[$()|*+?{\ã€ï¼Œå› ä¸ºæˆ‘ä»¬å¯èƒ½è®¤ä¸ºè¿™äº›ç¬¦å·åœ¨åŽŸæ–‡æœ¬ä¸­æœ‰ç‰¹æ®Šçš„å«ä¹‰ã€‚ \$\d åŒ¹é…åœ¨å•ä¸ªæ•°å­—å‰æœ‰ç¬¦å·â€œ$â€çš„å­—ç¬¦ä¸² -&gt; Try it! (https://regex101.com/r/cO8lqs/9) æ³¨æ„æˆ‘ä»¬åŒæ ·èƒ½åŒ¹é… non-printable å­—ç¬¦ï¼Œä¾‹å¦‚ Tab ç¬¦ã€Œ\tã€ã€æ¢è¡Œç¬¦ã€Œ\nã€å’Œå›žè½¦ç¬¦ã€Œ\rã€ Flagsæˆ‘ä»¬å·²ç»äº†è§£å¦‚ä½•æž„å»ºæ­£åˆ™è¡¨è¾¾å¼ï¼Œä½†ä»ç„¶é—æ¼äº†ä¸€ä¸ªéžå¸¸åŸºç¡€çš„æ¦‚å¿µï¼šflagsã€‚ æ­£åˆ™è¡¨è¾¾å¼é€šå¸¸ä»¥/abc/è¿™ç§å½¢å¼å‡ºçŽ°ï¼Œå…¶ä¸­æœç´¢æ¨¡å¼ç”±ä¸¤ä¸ªåæ–œæ ã€Œ/ã€åˆ†ç¦»ã€‚è€Œåœ¨æ¨¡å¼çš„ç»“å°¾ï¼Œæˆ‘ä»¬é€šå¸¸å¯ä»¥æŒ‡å®šä»¥ä¸‹ flag é…ç½®æˆ–å®ƒä»¬çš„ç»„åˆï¼š gï¼ˆglobalï¼‰åœ¨ç¬¬ä¸€æ¬¡å®ŒæˆåŒ¹é…åŽå¹¶ä¸ä¼šè¿”å›žç»“æžœï¼Œå®ƒä¼šç»§ç»­æœç´¢å‰©ä¸‹çš„æ–‡æœ¬ã€‚ mï¼ˆmulti lineï¼‰å…è®¸ä½¿ç”¨^å’Œ$åŒ¹é…ä¸€è¡Œçš„å¼€å§‹å’Œç»“å°¾ï¼Œè€Œä¸æ˜¯æ•´ä¸ªåºåˆ—ã€‚ iï¼ˆinsensitiveï¼‰ä»¤æ•´ä¸ªè¡¨è¾¾å¼ä¸åŒºåˆ†å¤§å°å†™ï¼ˆä¾‹å¦‚/aBc/i å°†åŒ¹é… AbCï¼‰ã€‚ ä¸­çº§è¯­å¥åˆ†ç»„å’Œæ•èŽ·ï¼š()a(bc) åœ†æ‹¬å¼§ä¼šåˆ›å»ºä¸€ä¸ªæ•èŽ·æ€§åˆ†ç»„ï¼Œå®ƒä¼šæ•èŽ·åŒ¹é…é¡¹â€œbcâ€ a(?:bc)* ä½¿ç”¨ â€œ?:â€ ä¼šä½¿æ•èŽ·åˆ†ç»„å¤±æ•ˆï¼Œåªéœ€è¦åŒ¹é…å‰é¢çš„â€œaâ€ a(?&lt;foo&gt;bc) ä½¿ç”¨ â€œ?&lt;foo&gt;â€ ä¼šä¸ºåˆ†ç»„é…ç½®ä¸€ä¸ªåç§° æ•èŽ·æ€§åœ†æ‹¬å· () å’Œéžæ•èŽ·æ€§åœ†æ‹¬å¼§ (?:) å¯¹äºŽä»Žå­—ç¬¦ä¸²æˆ–æ•°æ®ä¸­æŠ½å–ä¿¡æ¯éžå¸¸é‡è¦ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Python ç­‰ä¸åŒçš„ç¼–ç¨‹è¯­è¨€å®žçŽ°è¿™ä¸€åŠŸèƒ½ã€‚ä»Žå¤šä¸ªåˆ†ç»„ä¸­æ•èŽ·çš„å¤šä¸ªåŒ¹é…é¡¹å°†ä»¥ç»å…¸çš„æ•°ç»„å½¢å¼å±•ç¤ºï¼šæˆ‘ä»¬å¯ä»¥ä½¿ç”¨åŒ¹é…ç»“æžœçš„ç´¢å¼•è®¿é—®å®ƒä»¬çš„å€¼ã€‚ å¦‚æžœéœ€è¦ä¸ºåˆ†ç»„æ·»åŠ åç§°ï¼ˆä½¿ç”¨ (?â€¦)ï¼‰ï¼Œæˆ‘ä»¬å°±èƒ½å¦‚å­—å…¸é‚£æ ·ä½¿ç”¨åŒ¹é…ç»“æžœæ£€ç´¢åˆ†ç»„çš„å€¼ï¼Œå…¶ä¸­å­—å…¸çš„é”®ä¸ºåˆ†ç»„çš„åç§°ã€‚ æ–¹æ‹¬å¼§è¡¨è¾¾å¼ï¼š[][abc] åŒ¹é…å¸¦æœ‰ä¸€ä¸ªâ€œaâ€ã€â€œabâ€æˆ–â€œacâ€çš„å­—ç¬¦ä¸² -&gt; ä¸Ž a|b|c ä¸€æ · [a-c] åŒ¹é…å¸¦æœ‰ä¸€ä¸ªâ€œaâ€ã€â€œabâ€æˆ–â€œacâ€çš„å­—ç¬¦ä¸² -&gt; ä¸Ž a|b|c ä¸€æ · [a-fA-F0-9] åŒ¹é…ä¸€ä¸ªä»£è¡¨16è¿›åˆ¶æ•°å­—çš„å­—ç¬¦ä¸²ï¼Œä¸åŒºåˆ†å¤§å°å†™ [0-9]% åŒ¹é…åœ¨%ç¬¦å·å‰é¢å¸¦æœ‰0åˆ°9è¿™å‡ ä¸ªå­—ç¬¦çš„å­—ç¬¦ä¸² [^a-zA-Z] åŒ¹é…ä¸å¸¦aåˆ°zæˆ–Aåˆ°Zçš„å­—ç¬¦ä¸²ï¼Œå…¶ä¸­^ä¸ºå¦å®šè¡¨è¾¾å¼ è®°ä½åœ¨æ–¹æ‹¬å¼§å†…ï¼Œæ‰€æœ‰ç‰¹æ®Šå­—ç¬¦ï¼ˆåŒ…æ‹¬åæ–œæ \ï¼‰éƒ½ä¼šå¤±åŽ»å®ƒä»¬åº”æœ‰çš„æ„ä¹‰ã€‚ ==Greedy å’Œ Lazy åŒ¹é…==æ•°é‡ç¬¦ï¼ˆ* + {}ï¼‰æ˜¯ä¸€ç§è´ªå¿ƒè¿ç®—ç¬¦ï¼Œæ‰€ä»¥å®ƒä»¬ä¼šéåŽ†ç»™å®šçš„æ–‡æœ¬ï¼Œå¹¶å°½å¯èƒ½åŒ¹é…ã€‚ä¾‹å¦‚ï¼Œ&lt;.+&gt; å¯ä»¥åŒ¹é…æ–‡æœ¬ã€ŒThis is a simple div testã€ä¸­çš„ã€Œsimple divã€ã€‚ä¸ºäº†ä»…æ•èŽ· div æ ‡ç­¾ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨ã€Œï¼Ÿã€ä»¤è´ªå¿ƒæœç´¢å˜å¾— Lazy ä¸€ç‚¹ï¼š &lt;.+?&gt; ä¸€æ¬¡æˆ–å¤šæ¬¡åŒ¹é… â€œ&lt;â€ å’Œ â€œ&gt;â€ é‡Œé¢çš„ä»»ä½•å­—ç¬¦ï¼Œå¯æŒ‰éœ€æ‰©å±• æ³¨æ„æ›´å¥½çš„è§£å†³æ–¹æ¡ˆåº”è¯¥éœ€è¦é¿å…ä½¿ç”¨ã€Œ.ã€ï¼Œè¿™æœ‰åˆ©äºŽå®žçŽ°æ›´ä¸¥æ ¼çš„æ­£åˆ™è¡¨è¾¾å¼ï¼š &lt;[^&lt;&gt;]+&gt; ä¸€æ¬¡æˆ–å¤šæ¬¡åŒ¹é… â€œ&lt;â€ å’Œ â€œ&gt;â€ é‡Œé¢çš„ä»»ä½•å­—ç¬¦ï¼Œé™¤åŽ» â€œ&lt;â€ æˆ– â€œ&gt;â€ å­—ç¬¦ é«˜çº§è¯­å¥ è¾¹ç•Œç¬¦ï¼š\b å’Œ \B \babc\b æ‰§è¡Œæ•´è¯åŒ¹é…æœç´¢ \b å¦‚æ’å…¥ç¬¦å·é‚£æ ·è¡¨ç¤ºä¸€ä¸ªé”šç‚¹ï¼ˆå®ƒä¸Ž$å’Œ^ç›¸åŒï¼‰æ¥åŒ¹é…ä½ç½®ï¼Œå…¶ä¸­ä¸€è¾¹æ˜¯ä¸€ä¸ªå•è¯ç¬¦å·ï¼ˆå¦‚\wï¼‰ï¼Œå¦ä¸€è¾¹ä¸æ˜¯å•è¯ç¬¦å·ï¼ˆä¾‹å¦‚å®ƒå¯èƒ½æ˜¯å­—ç¬¦ä¸²çš„èµ·å§‹ç‚¹æˆ–ç©ºæ ¼ç¬¦å·ï¼‰ã€‚ å®ƒåŒæ ·èƒ½è¡¨è¾¾ç›¸åçš„éžå•è¯è¾¹ç•Œã€Œ\Bã€ï¼Œå®ƒä¼šåŒ¹é…ã€Œ\bã€ä¸ä¼šåŒ¹é…çš„ä½ç½®ï¼Œå¦‚æžœæˆ‘ä»¬å¸Œæœ›æ‰¾åˆ°è¢«å•è¯å­—ç¬¦çŽ¯ç»•çš„æœç´¢æ¨¡å¼ï¼Œå°±å¯ä»¥ä½¿ç”¨å®ƒã€‚ \Babc\B åªè¦æ˜¯è¢«å•è¯å­—ç¬¦çŽ¯ç»•çš„æ¨¡å¼å°±ä¼šåŒ¹é… å‰å‘åŒ¹é…å’ŒåŽå‘åŒ¹é…ï¼š(?=) å’Œ (?&lt;=)d(?=r) åªæœ‰åœ¨åŽé¢è·Ÿç€â€œrâ€çš„æ—¶å€™æ‰åŒ¹é…â€œdâ€ï¼Œä½†æ˜¯â€œrâ€å¹¶ä¸ä¼šæˆä¸ºæ•´ä¸ªæ­£åˆ™è¡¨è¾¾å¼åŒ¹é…çš„ä¸€éƒ¨åˆ† (?&lt;=r)d åªæœ‰åœ¨å‰é¢è·Ÿç€â€œrâ€æ—¶æ‰åŒ¹é…â€œdâ€ï¼Œä½†æ˜¯â€œrâ€å¹¶ä¸ä¼šæˆä¸ºæ•´ä¸ªæ­£åˆ™è¡¨è¾¾å¼åŒ¹é…çš„ä¸€éƒ¨åˆ† æˆ‘ä»¬åŒæ ·èƒ½ä½¿ç”¨å¦å®šè¿ç®—å­ï¼š d(?!r) åªæœ‰åœ¨åŽé¢ä¸è·Ÿç€â€œrâ€çš„æ—¶å€™æ‰åŒ¹é…â€œdâ€ï¼Œä½†æ˜¯â€œrâ€å¹¶ä¸ä¼šæˆä¸ºæ•´ä¸ªæ­£åˆ™è¡¨è¾¾å¼åŒ¹é…çš„ä¸€éƒ¨åˆ† (?&lt;!r)d åªæœ‰åœ¨å‰é¢ä¸è·Ÿç€â€œrâ€æ—¶æ‰åŒ¹é…â€œdâ€ï¼Œä½†æ˜¯â€œrâ€å¹¶ä¸ä¼šæˆä¸ºæ•´ä¸ªæ­£åˆ™è¡¨è¾¾å¼åŒ¹é…çš„ä¸€éƒ¨åˆ† ç»“è¯­ æ­£å¦‚ä¸Šæ–‡æ‰€ç¤ºï¼Œæ­£åˆ™è¡¨è¾¾å¼çš„åº”ç”¨é¢†åŸŸéžå¸¸å¹¿ï¼Œå¾ˆå¯èƒ½å„ä½è¯»è€…åœ¨å¼€å‘çš„è¿‡ç¨‹ä¸­å·²ç»é‡åˆ°äº†å®ƒï¼Œä¸‹é¢æ˜¯æ­£åˆ™è¡¨è¾¾å¼å¸¸ç”¨çš„é¢†åŸŸï¼š æ•°æ®éªŒè¯ï¼Œä¾‹å¦‚æ£€æŸ¥æ—¶é—´å­—ç¬¦ä¸²æ˜¯å¦ç¬¦åˆæ ¼å¼ï¼› æ•°æ®æŠ“å–ï¼Œä»¥ç‰¹å®šé¡ºåºæŠ“å–åŒ…å«ç‰¹å®šæ–‡æœ¬æˆ–å†…å®¹çš„ç½‘é¡µï¼› æ•°æ®åŒ…è£…ï¼Œå°†æ•°æ®ä»ŽæŸç§åŽŸæ ¼å¼è½¬æ¢ä¸ºå¦å¤–ä¸€ç§æ ¼å¼ï¼› å­—ç¬¦ä¸²è§£æžï¼Œä¾‹å¦‚æ•èŽ·æ‰€æ‹¥æœ‰ URL çš„ GET å‚æ•°ï¼Œæˆ–æ•èŽ·ä¸€ç»„åœ†æ‹¬å¼§å†…çš„æ–‡æœ¬ï¼› å­—ç¬¦ä¸²æ›¿ä»£ï¼Œå°†å­—ç¬¦ä¸²ä¸­çš„æŸä¸ªå­—ç¬¦æ›¿æ¢ä¸ºå…¶å®ƒå­—ç¬¦ã€‚ å‚è€ƒé“¾æŽ¥https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650749657&amp;idx=4&amp;sn=da4852cb0c4919316d801fe19a64901d&amp;chksm=871afea7b06d77b1bda42ac134c5dddad5af24647f62a5a8e7bdcef4499f40fd4c97045a6f3d&amp;mpshare=1&amp;scene=23&amp;srcid=1009dbNFxJJahsQK6NGw4wS3%23rd pythonæ­£åˆ™è¡¨è¾¾å¼çš„ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼matchå’Œsearchçš„åŒºåˆ«]]></content>
      <categories>
        <category>çˆ¬è™«</category>
      </categories>
      <tags>
        <tag>æ­£åˆ™è¡¨è¾¾å¼</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019å¹´è®¡åˆ’]]></title>
    <url>%2Fpost%2F2019-plans%2F</url>
    <content type="text"><![CDATA[è¯»ä¸ªåšå‰ä»–å¯ä»¥å¼¹å”±å‡ é¦–æµè¡Œæ­Œæ›²å‡æœŸæŠ¥ä¸ªç­ï¼Œè¯·è€å¸ˆæŒ‡ç‚¹ä¸€ä¸‹ ç²¾è¯»50ç¯‡è®ºæ–‡å¹³å‡ä¸‹æ¥ï¼Œæ¯å‘¨éƒ½éœ€è¦è¯»ä¸€ç¯‡ï¼Œæ¯ä¸€ç¯‡éƒ½è¦å‹¾åˆ’ï¼Œå‘åšå®¢ã€‚ å‘2ç¯‡paperä¸ŠåŠå¹´ç§¯ç´¯çŸ¥è¯†ï¼Œåšå®žéªŒï¼Œæš‘å‡å¼€å§‹å†™ è€ƒæ‰˜ç¦è¿‡90ä¸ŠåŠå¹´èƒŒå•è¯ç­‰ ä¹°ä¸ªå¾®å•å­¦ä¸ªæ‹ç…§ä¸æ€¥ï¼ŒçŽ°åœ¨æ²¡é’±ï¼Œä¸‹åŠå¹´å…¥æ‰‹ä¸€ä¸ªå§ è‡ªå·±å‡ºé—¨æ—…è¡Œä¸€æ¬¡å—¯ï¼Œè¦æ˜¯è®ºæ–‡å†™å®Œå°±åŽ»å§ï¼ç›®å‰è®¡åˆ’åŽ»å±±ä¸œï¼Œé¡ºä¾¿çœ‹ä¸€ä¸‹æˆ‘äºŒå§‘ã€‚5å¤©å·¦å³çš„æ—…è¡Œå§ï¼ å­¦ä¹ æ»‘æ¿å¼€å§‹å¥èº«æ¯å‘¨åŽ»ä¸‰æ¬¡å¥èº«æˆ¿]]></content>
      <categories>
        <category>annual plans</category>
      </categories>
      <tags>
        <tag>plan</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ã€ŠAn Automatic Knowledge Graph Construction System for K-12 Educationã€‹é˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FAn%20Automatic%20Knowledge%20Graph%20Construction%20System%20for%20K-12%20Education%2F</url>
    <content type="text"><![CDATA[è®ºæ–‡åŽŸå€ã€‚æœ¬ç¯‡æ–‡ç« ä¸»è¦æå‡ºäº†ä¸€ä¸ªè‡ªåŠ¨åŒ–æž„å»ºæ•°å­¦é¢†åŸŸçŸ¥è¯†å›¾è°±çš„ç³»ç»Ÿï¼Œä¸»è¦åº”ç”¨çš„äº‹NERå’Œæ•°æ®æŒ–æŽ˜æŠ€æœ¯ï¼Œå…¶ä¸­NERä¸»è¦æ˜¯æŠ½å–æ•°å­¦æ¦‚å¿µï¼Œæ¦‚å¿µé—´çš„å…³ç³»æ˜¯ä½œè€…è‡ªå·±æž„å»ºçš„ï¼ˆä¾‹å¦‚å…ˆä¿®å…³ç³»ï¼‰ã€‚å¯¹äºŽæ•°æ®é›†ï¼Œä½œè€…ä¸»è¦ä»Žthe Chinese curriculum standards of mathematicsä¸Šæå–çš„æ¦‚å¿µå®žä½“ï¼Œä»Žè‡ªå·±çš„SLPå¹³å°ä¸Šï¼Œé€šè¿‡å¯¹å­¦ç”Ÿè¡¨çŽ°æ¥æå–å…³ç³»ï¼ˆæŠŠè¿™éƒ¨åˆ†ä½œä¸ºæ•°æ®æŒ–æŽ˜ï¼‰ã€‚æœ¬ç¯‡æ–‡ç« å®žé™…ä¸Šå¯ä»¥ä½œä¸ºæž„å»ºç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†å›¾è°±çš„ä¸€ä¸ªå‚è€ƒã€‚ challenges the desired educational concept entities are more abstract than real world entities like PERSON, ORGANIZATION, LOCATION the desired relations are more cognitive and implicit, so cannot be derived from the literal meanings of text like generic knowledge graphs contributions a novel but practical system entity recognition (NER) &amp; association rule mining algorithms demonstrate an exemplary case with constructing a knowledge graph for the subject of mathematics SYSTEM OVERVIEW Educational Concept Extraction Module: Implicit Relation Identification Module CONCEPT EXTRACTION çº¿æ€§é“¾å¼CRFæ¨¡åž‹ æ ‡ç­¾é¢„æµ‹ RELATION IDENTIFICATIONä¸¤ç§æ–¹æ³• support confidence From the perspective of prerequisite relation, if concept si is a prerequisite of concept sj, learners who do not master sivery likely do not master sj, and learners who master sjmost likely master si. EXEMPLARY CASE AND SYSTEM EVALUATIONConcept ExtractionDatasetthe Chinese curriculum standards of mathematics published by the ministry of education as the main data source Evaluation adopt precision, recall and F1- score The ground truth is manually labeled by two domain experts. Relation IdentificationDataset studentsâ€™ performance data collected by our SLP platform. EvaluationThe ground truth of the prerequisite relations between selected 9 concepts are annotated manually by two domain experts.]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>NER</tag>
        <tag>çŸ¥è¯†å›¾è°±</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ã€ŠRECURRENT NEURAL NETWORK REGULARIZATIONã€‹é˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FRECURRENT%20NEURAL%20NETWORK%20REGULARIZATION%2F</url>
    <content type="text"><![CDATA[è®ºæ–‡é“¾æŽ¥ã€‚è¿™ç¯‡è®ºæ–‡æå‡ºäº†LSTMçš„dropoutç­–ç•¥æ¥é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œå³åªåœ¨éžå¾ªçŽ¯é“¾æŽ¥å¤„é‡‡å–dropoutã€‚åœ¨BasicLSTMCellçš„æŽ¥å£å°±æ˜¯ä¾æ®è¿™ç¯‡è®ºæ–‡å®žçŽ°çš„ã€‚ æ–‡ç« æ•´ä½“æž¶æž„å’Œé‡ç‚¹ contribution present a simple regularization technique background dropout Srivastava(2013)ï¼Œå¯¹äºŽå‰å‘åé¦ˆç½‘ç»œæœ€æœ‰åŠ›çš„æ­£åˆ™åŒ–æ–¹æ³•å¹¶ä¸èƒ½å¾ˆå¥½çš„åº”ç”¨åœ¨RNNsä¸Šã€‚è¿™å¯¼è‡´RNNsè§„æ¨¡éƒ½å¾ˆå°ï¼Œå› ä¸ºå¤ªå¤§ä¼šè¿‡æ‹Ÿåˆã€‚ Bayer et al. (2013)æŒ‡å‡ºäº†å·ç§¯dropoutä¸èƒ½åœ¨RNNsä¸Šå¾ˆå¥½å·¥ä½œçš„åŽŸå› æ˜¯å¾ªçŽ¯ä¼šæ”¾å¤§å™ªéŸ³ã€‚ modelä»¿å°„å˜æ¢ï¼ˆaffine transformï¼‰å…³äºŽä»¿å°„å˜æ¢ï¼šçº¿æ€§å˜æ¢åŠ ä¸Šå¹³ç§»ï¼Œç›—ä¸ªçŸ¥ä¹Žä¸Šçš„å›¾ï¼ˆåŽŸæ–‡é“¾æŽ¥ï¼‰ æ¨¡åž‹ä¸»ä½“é‡‡ç”¨çš„æ˜¯Graves et al. (2013) dropoutç­–ç•¥ The main contribution of this paper is a recipe for applying dropout to LSTMs in a way that success- fully reduces overfitting. The main idea is to apply the dropout operator only to the non-recurrent connections. è§‚å¯Ÿå…¬å¼ï¼Œå®žé™…ä¸Šå°±æ˜¯é€šè¿‡åœ¨å±‚é—´ä¼ é€’ä¸­åº”ç”¨dropoutã€‚å¦‚ä¸‹å›¾ä¸­è™šçº¿æ‰€ç¤ºã€‚ ä»Žä¸Šå›¾ä¸­ä¹Ÿå¯ä»¥çœ‹åˆ°ï¼Œè¯¥dropoutçš„æ¬¡æ•°åªå’Œç½‘ç»œæ·±åº¦æœ‰å…³ï¼ˆæ•°å€¼ä¸ºç½‘ç»œæ·±åº¦+1ï¼‰ã€‚ experimentså®žéªŒéƒ¨åˆ†ä½œè€…åšäº†4éƒ¨åˆ†å®žéªŒæ¥è¯æ˜Žè‡ªå·±é‡‡ç”¨çš„æ–¹æ³•æœ‰æ•ˆï¼Œåˆ†åˆ«ä¸ºlanguage modeling, speech recognition, machine translation, image caption generationã€‚è¿™éƒ¨åˆ†æ²¡ä»€ä¹ˆéœ€è¦è§£é‡Šäº†ï¼Œæ„Ÿå…´è¶£å¯ä»¥è‡ªå·±çœ‹ä¸€ä¸‹å®žéªŒã€‚]]></content>
      <categories>
        <category>ç¥žç»ç½‘ç»œ</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Understanding LSTM Networks]]></title>
    <url>%2Fpost%2FUnderstanding%20LSTM%20Networks%2F</url>
    <content type="text"><![CDATA[åŽŸæ–‡é“¾æŽ¥ã€‚è¿™ç¯‡æ–‡ç« å¾ˆå¥½å¾ˆç»†çš„ä¸€æ­¥ä¸€æ­¥çš„åˆ†è§£è®²è§£äº†LSTMï¼Œä¹‹å‰çœ‹è¿‡ä¸€ç¯‡ç¿»è¯‘çš„åšå®¢ï¼ŒçŽ°åœ¨è‡ªå·±ç¿»è¯‘ä¸€éï¼Œæ„Ÿè§‰å¯¹LSTMçš„è®¤è¯†åŠ æ·±äº†è®¸å¤šï¼Œè™½ç„¶è¿˜æ˜¯å¯¹LSTMä¸­å­˜æœ‰ä¸€äº›é—®é¢˜ï¼Œæ¯”å¦‚ä¸ºä»€ä¹ˆç”¨tanhï¼Œsigmoidï¼Œä¸ºä»€ä¸é‡‡ç”¨å…¶ä»–çš„ï¼Ÿï¼Œä½†æ˜¯çœ‹è¿‡ä¹‹åŽè‡³å°‘å¯¹LSTMæ²¡æœ‰é‚£ä¹ˆç•æƒ§ï¼Œä¸è§‰å¾—è¿‡äºŽå¤æ‚äº†ã€‚ LSTMRecurrent Neural Networks å¾ªçŽ¯å…è®¸ä¿¡æ¯ä»Žä¸€ä¸ªç½‘ç»œä¼ å…¥ä¸‹ä¸€ä¸ªã€‚ ä¸€ä¸ªå¾ªçŽ¯ç½‘ç»œå¯ä»¥è¢«è®¤ä¸ºæ˜¯ç›¸åŒç½‘ç»œçš„å¤šä¸ªå¤åˆ¶ï¼Œæ¯ä¸€ä¸ªç½‘ç»œéƒ½å°†ä¿¡æ¯ä¼ é€’ç»™åŽç»§è€…ã€‚è¿™ç§ç±»ä¼¼é“¾çš„æ€§è´¨è¡¨æ˜Žï¼Œé€’å½’ç¥žç»ç½‘ç»œä¸Žåºåˆ—å’Œåˆ—è¡¨å¯†åˆ‡ç›¸å…³ã€‚ The Problem of Long-Term DependenciesRNNçš„ä¸€ä¸ªå¸å¼•åŠ›æ˜¯ä»–ä»¬å¯èƒ½èƒ½å¤Ÿå°†å…ˆå‰ä¿¡æ¯è¿žæŽ¥åˆ°å½“å‰ä»»åŠ¡ã€‚ æœ‰æ—¶ï¼Œæˆ‘ä»¬åªéœ€è¦æŸ¥çœ‹æœ€è¿‘çš„ä¿¡æ¯æ¥æ‰§è¡Œå½“å‰ä»»åŠ¡ã€‚ä¾‹å¦‚ï¼š If we are trying to predict the last word in â€œthe clouds are in the sky,â€ we donâ€™t need any further context â€“ itâ€™s pretty obvious the next word is going to be sky. åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¦‚æžœç›¸å…³ä¿¡æ¯ä¸Žå¾…é¢„æµ‹åœ°æ–¹ä¹‹é—´çš„å·®è·å¾ˆå°ï¼ŒRNNå¯ä»¥å­¦ä¹ ä½¿ç”¨è¿‡åŽ»çš„ä¿¡æ¯ã€‚ ä½†æ˜¯ï¼Œå¯¹äºŽä¸€äº›æƒ…å†µï¼Œæˆ‘ä»¬éœ€è¦æ›´å¤šçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ Consider trying to predict the last word in the text â€œI grew up in Franceâ€¦ I speak fluent French.â€ è¿™æ—¶ï¼Œç›¸å…³ä¿¡æ¯ä¸Žéœ€è¦å˜å¾—éžå¸¸å¤§çš„ç‚¹ä¹‹é—´çš„å·®è·å®Œå…¨æœ‰å¯èƒ½ã€‚ä¸å¹¸çš„æ˜¯ï¼Œéšç€å·®è·çš„æ‰©å¤§ï¼ŒRNNæ— æ³•å­¦ä¼šè¿žæŽ¥ä¿¡æ¯ã€‚ The problem was explored in depth by Hochreiter (1991) [German] and Bengio, et al. (1994), who found some pretty fundamental reasons why it might be difficult. LSTM Networks LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn! æ ‡å‡†RNNsçš„é‡å¤æ¨¡å—åªæœ‰ä¸€ä¸ªç®€å•çš„ç»“æž„ï¼Œæ¯”å¦‚tanhå±‚ã€‚ LSTMsä¹Ÿæœ‰åƒè¿™ç§é“¾å¼ç»“æž„ï¼Œä½†æ˜¯å®ƒçš„é‡å¤æ¨¡å—å…·æœ‰ä¸åŒçš„ç»“æž„ã€‚ æœ‰å››ä¸ªï¼Œè€Œä¸æ˜¯ä¸€ä¸ªç¥žç»ç½‘ç»œå±‚ï¼Œä»¥ä¸€ç§éžå¸¸ç‰¹æ®Šçš„æ–¹å¼è¿›è¡Œäº¤äº’ã€‚ åŸºæœ¬ç¬¦å·å¦‚ä¸‹ï¼š åœ¨ä¸Šå›¾ä¸­ï¼Œæ¯ä¸€è¡Œéƒ½æºå¸¦ä¸€ä¸ªå®Œæ•´çš„å‘é‡ï¼Œä»Žä¸€ä¸ªèŠ‚ç‚¹çš„è¾“å‡ºåˆ°å…¶ä»–èŠ‚ç‚¹çš„è¾“å…¥ã€‚ ç²‰è‰²åœ†åœˆè¡¨ç¤ºé€ç‚¹è¿ç®—ï¼Œå¦‚çŸ¢é‡åŠ æ³•ï¼Œè€Œé»„è‰²æ¡†è¡¨ç¤ºç¥žç»ç½‘ç»œå±‚ã€‚ è¡Œåˆå¹¶è¡¨ç¤ºè¿žæŽ¥ï¼Œè€Œè¡Œåˆ†å‰è¡¨ç¤ºå…¶å†…å®¹è¢«å¤åˆ¶ï¼Œå‰¯æœ¬å°†è½¬ç§»åˆ°ä¸åŒçš„ä½ç½®ã€‚ The Core Idea Behind LSTMsLSTMçš„å…³é”®æ˜¯å•å…ƒçŠ¶æ€ï¼Œæ°´å¹³çº¿è´¯ç©¿å›¾çš„é¡¶éƒ¨ã€‚ å•å…ƒçŠ¶æ€æœ‰ç‚¹åƒä¼ é€å¸¦ã€‚ å®ƒç›´æŽ¥æ²¿ç€æ•´ä¸ªé“¾è¿è¡Œï¼Œåªæœ‰ä¸€äº›å¾®å°çš„çº¿æ€§ç›¸äº’ä½œç”¨ã€‚ ä¿¡æ¯å¾ˆå®¹æ˜“æ²¿ç€å®ƒä¸å˜åœ°æµåŠ¨ã€‚ LSTMç¡®å®žèƒ½å¤Ÿç§»é™¤æˆ–æ·»åŠ ä¿¡æ¯åˆ°ç»†èƒžçŠ¶æ€ï¼Œç”±ç§°ä¸ºé—¨çš„ç»“æž„ç²¾å¿ƒè°ƒèŠ‚ã€‚ é—¨æ˜¯ä¸€ç§å¯é€‰æ‹©é€šè¿‡ä¿¡æ¯çš„æ–¹å¼ã€‚ å®ƒä»¬ç”±Sigmoidç¥žç»ç½‘ç»œå±‚å’Œé€ç‚¹ä¹˜æ³•è¿ç®—ç»„æˆã€‚ sigmoidå±‚è¾“å‡º0åˆ°1ä¹‹é—´çš„æ•°å­—ï¼Œæè¿°æ¯ä¸ªç»„ä»¶åº”è¯¥é€šè¿‡å¤šå°‘ã€‚ å€¼ä¸ºé›¶æ„å‘³ç€â€œä¸è®©ä»»ä½•ä¸œè¥¿é€šè¿‡â€ï¼Œè€Œå€¼ä¸º1åˆ™æ„å‘³ç€â€œè®©ä¸€åˆ‡éƒ½é€šè¿‡ï¼â€ LSTMå…·æœ‰ä¸‰ä¸ªè¿™æ ·çš„é—¨ï¼Œç”¨äºŽä¿æŠ¤å’ŒæŽ§åˆ¶å•å…ƒçŠ¶æ€ã€‚ Step-by-Step LSTM Walk Throughæˆ‘ä»¬çš„ç¬¬ä¸€æ­¥å°±æ˜¯ç¡®å®šæˆ‘ä»¬å°†ä»Žå•å…ƒçŠ¶æ€ä¸­ä¸¢å¼ƒçš„ä¿¡æ¯ã€‚è¿™ä¸ªå†³å®šæ˜¯ç”±ä¸€ä¸ªç§°ä¸ºâ€œé—å¿˜é—¨å±‚â€çš„sigmoidå±‚å†³å®šçš„ã€‚å®ƒæŸ¥çœ‹$h_{t-1}$å’Œ$x_t$ï¼Œå¹¶ä¸ºå•å…ƒçŠ¶æ€$C_{t-1}$ä¸­çš„æ¯ä¸€ä¸ªæ•°å­—è¾“å‡ºä¸€ä¸ªä»‹äºŽ0å’Œ1ä¹‹é—´çš„æ•°å­—ã€‚1ä»£è¡¨â€œå®Œå…¨ä¿ç•™è¿™ä¸ªâ€ï¼Œè€Œ0ä»£è¡¨â€œå®Œå…¨èˆå¼ƒè¿™ä¸ªâ€ã€‚ è®©æˆ‘ä»¬å›žåˆ°æˆ‘ä»¬çš„è¯­è¨€æ¨¡åž‹ç¤ºä¾‹ï¼Œè¯•å›¾æ ¹æ®ä»¥å‰çš„æ‰€æœ‰å•è¯é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ã€‚ åœ¨è¿™æ ·çš„é—®é¢˜ä¸­ï¼Œå•å…ƒçŠ¶æ€å¯èƒ½åŒ…æ‹¬å½“å‰å—è¯•è€…çš„æ€§åˆ«ï¼Œå› æ­¤å¯ä»¥ä½¿ç”¨æ­£ç¡®çš„ä»£è¯ã€‚ å½“æˆ‘ä»¬çœ‹åˆ°ä¸€ä¸ªæ–°ä¸»é¢˜æ—¶ï¼Œæˆ‘ä»¬æƒ³è¦å¿˜è®°æ—§ä¸»é¢˜çš„æ€§åˆ«ã€‚ ä¸‹ä¸€æ­¥æ˜¯ç¡®å®šæˆ‘ä»¬å°†åœ¨å•å…ƒçŠ¶æ€ä¸­å­˜å‚¨å“ªäº›æ–°ä¿¡æ¯ã€‚ è¿™æœ‰ä¸¤ä¸ªéƒ¨åˆ†ã€‚ é¦–å…ˆï¼Œç§°ä¸ºâ€œè¾“å…¥é—¨å±‚â€çš„sigmoidå±‚å†³å®šæˆ‘ä»¬å°†æ›´æ–°å“ªäº›å€¼ã€‚ æŽ¥ä¸‹æ¥ï¼Œtanhå±‚åˆ›å»ºå¯ä»¥æ·»åŠ åˆ°çŠ¶æ€çš„æ–°å€™é€‰å€¼$\tilde{C}_t$çš„å‘é‡ã€‚ åœ¨ä¸‹ä¸€æ­¥ä¸­ï¼Œæˆ‘ä»¬å°†ç»“åˆè¿™ä¸¤ä¸ªæ¥åˆ›å»ºçŠ¶æ€æ›´æ–°ã€‚ åœ¨æˆ‘ä»¬çš„è¯­è¨€æ¨¡åž‹çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬æƒ³è¦å°†æ–°ä¸»é¢˜çš„æ€§åˆ«æ·»åŠ åˆ°å•å…ƒæ ¼çŠ¶æ€ï¼Œä»¥æ›¿æ¢æˆ‘ä»¬å¿˜è®°çš„æ—§ä¸»é¢˜ã€‚ çŽ°åœ¨æ˜¯æ—¶å€™å°†æ—§çš„å•å…ƒçŠ¶æ€$C_{T-1}$æ›´æ–°ä¸ºæ–°çš„å•å…ƒçŠ¶æ€$C_t$ã€‚ å‰é¢çš„æ­¥éª¤å·²ç»å†³å®šè¦åšä»€ä¹ˆï¼Œæˆ‘ä»¬åªéœ€è¦å®žé™…åšåˆ°è¿™ä¸€ç‚¹ã€‚ æˆ‘ä»¬å°†æ—§çŠ¶æ€ä¹˜ä»¥$f_t$ï¼Œå¿˜è®°æˆ‘ä»¬ä¹‹å‰å†³å®šå¿˜è®°çš„äº‹æƒ…ã€‚ ç„¶åŽæˆ‘ä»¬æ·»åŠ $i_t * \tilde{C}_t$ã€‚ è¿™æ˜¯æ–°çš„å€™é€‰å€¼ï¼Œæ ¹æ®æˆ‘ä»¬å†³å®šæ›´æ–°æ¯ä¸ªçŠ¶æ€çš„å€¼æ¥ç¼©æ”¾ã€‚ åœ¨è¯­è¨€æ¨¡åž‹çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å®žé™…ä¸Šæ”¾å¼ƒäº†å…³äºŽæ—§ä¸»é¢˜çš„æ€§åˆ«çš„ä¿¡æ¯å¹¶æ·»åŠ æ–°ä¿¡æ¯ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨å‰é¢çš„æ­¥éª¤ä¸­æ‰€åšçš„é‚£æ ·ã€‚ æœ€åŽï¼Œæˆ‘ä»¬éœ€è¦å†³å®šæˆ‘ä»¬è¦è¾“å‡ºçš„å†…å®¹ã€‚ æ­¤è¾“å‡ºå°†åŸºäºŽæˆ‘ä»¬çš„å•å…ƒçŠ¶æ€ï¼Œä½†å°†æ˜¯è¿‡æ»¤ç‰ˆæœ¬ã€‚ é¦–å…ˆï¼Œæˆ‘ä»¬è¿è¡Œä¸€ä¸ªsigmoidå±‚ï¼Œå®ƒå†³å®šæˆ‘ä»¬è¦è¾“å‡ºçš„å•å…ƒçŠ¶æ€çš„å“ªäº›éƒ¨åˆ†ã€‚ ç„¶åŽï¼Œæˆ‘ä»¬å°†å•å…ƒæ ¼çŠ¶æ€è®¾ç½®ä¸ºtanhï¼ˆå°†å€¼æŽ¨åˆ°ä»‹äºŽ-1å’Œ1ä¹‹é—´ï¼‰å¹¶å°†å…¶ä¹˜ä»¥sigmoidé—¨çš„è¾“å‡ºï¼Œä»¥ä¾¿æˆ‘ä»¬åªè¾“å‡ºæˆ‘ä»¬å†³å®šçš„éƒ¨åˆ†ã€‚ å¯¹äºŽè¯­è¨€æ¨¡åž‹ç¤ºä¾‹ï¼Œç”±äºŽå®ƒåªæ˜¯çœ‹åˆ°ä¸€ä¸ªä¸»é¢˜ï¼Œå®ƒå¯èƒ½æƒ³è¦è¾“å‡ºä¸ŽåŠ¨è¯ç›¸å…³çš„ä¿¡æ¯ï¼Œä»¥é˜²æŽ¥ä¸‹æ¥ä¼šå‘ç”Ÿä»€ä¹ˆã€‚ ä¾‹å¦‚ï¼Œå®ƒå¯èƒ½è¾“å‡ºä¸»è¯­æ˜¯å•æ•°è¿˜æ˜¯å¤æ•°ï¼Œä»¥ä¾¿æˆ‘ä»¬çŸ¥é“åŠ¨è¯åº”è¯¥ä¸Žä»€ä¹ˆå½¢å¼å…±è½­ï¼Œå¦‚æžœæŽ¥ä¸‹æ¥çš„è¯ã€‚ Variants on Long Short Term Memoryåˆ°ç›®å‰ä¸ºæ­¢æˆ‘æ‰€æè¿°çš„æ˜¯ä¸€ä¸ªéžå¸¸æ­£å¸¸çš„LSTMã€‚ ä½†å¹¶éžæ‰€æœ‰LSTMéƒ½ä¸Žä¸Šè¿°ç›¸åŒã€‚ äº‹å®žä¸Šï¼Œä¼¼ä¹Žå‡ ä¹Žæ‰€æœ‰æ¶‰åŠLSTMçš„è®ºæ–‡éƒ½ä½¿ç”¨ç•¥æœ‰ä¸åŒçš„ç‰ˆæœ¬ã€‚ å·®å¼‚å¾ˆå°ï¼Œä½†å€¼å¾—ä¸€æçš„æ˜¯å…¶ä¸­ä¸€äº›ã€‚ One popular LSTM variant, introduced by Gers &amp; Schmidhuber (2000), is adding â€œpeephole connections.â€ This means that we let the gate layers look at the cell state. ä¸Šé¢çš„å›¾è¡¨ä¸ºæ‰€æœ‰é—¨å¢žåŠ äº†çª¥è§†å­”ï¼ˆpeepholeï¼‰ï¼Œä½†æ˜¯è®¸å¤šè®ºæ–‡ä¼šç»™ä¸€äº›çª¥è§†å­”è€Œä¸æ˜¯å…¶ä»–çš„ã€‚ å¦ä¸€ç§å˜åŒ–æ˜¯ä½¿ç”¨è€¦åˆçš„é—å¿˜å’Œè¾“å…¥é—¨ã€‚ æˆ‘ä»¬ä¸æ˜¯å•ç‹¬å†³å®šå¿˜è®°ä»€ä¹ˆä»¥åŠåº”è¯¥æ·»åŠ æ–°ä¿¡æ¯ï¼Œè€Œæ˜¯ä¸€èµ·åšå‡ºè¿™äº›å†³å®šã€‚æˆ‘ä»¬ä»…ä»…ä¼šå½“æˆ‘ä»¬åœ¨å½“å‰ä½ç½®å°†è¦è¾“å…¥æ—¶å¿˜è®°ã€‚æˆ‘ä»¬ä»…ä»…è¾“å…¥æ–°çš„å€¼åˆ°é‚£äº›æˆ‘ä»¬å·²ç»å¿˜è®°æ—§çš„ä¿¡æ¯çš„é‚£äº›çŠ¶æ€ ã€‚ å¦ä¸€ä¸ªæ”¹åŠ¨è¾ƒå¤§çš„å˜ä½“æ˜¯ Gated Recurrent Unit (GRU)ï¼Œè¿™æ˜¯ç”± Cho, et al. (2014) æå‡ºã€‚å®ƒå°†é—å¿˜å’Œè¾“å…¥é—¨ç»„åˆæˆä¸€ä¸ªâ€œæ›´æ–°é—¨â€ã€‚å®ƒè¿˜åˆå¹¶äº†å•å…ƒçŠ¶æ€å’Œéšè—çŠ¶æ€ï¼Œå¹¶è¿›è¡Œäº†ä¸€äº›å…¶ä»–æ›´æ”¹ã€‚ ç”±æ­¤äº§ç”Ÿçš„æ¨¡åž‹æ¯”æ ‡å‡†LSTMæ¨¡åž‹ç®€å•ï¼Œå¹¶ä¸”è¶Šæ¥è¶Šå—æ¬¢è¿Žã€‚ è¿™äº›åªæ˜¯æœ€ç€åçš„LSTMå˜ç§ä¸­çš„ä¸€å°éƒ¨åˆ†ã€‚ è¿˜æœ‰å¾ˆå¤šå…¶ä»–çš„ä¸œè¥¿ï¼Œå¦‚ Yao, et al. (2015) æå‡ºçš„ Depth Gated RNNã€‚ è¿˜æœ‰ä¸€äº›å®Œå…¨ä¸åŒçš„è§£å†³é•¿æœŸä¾èµ–å…³ç³»çš„æ–¹æ³•ï¼Œå¦‚ Koutnik, et al. (2014) æå‡ºçš„ Clockwork RNNã€‚ å“ªç§å˜ä½“æœ€å¥½ï¼Ÿ å·®å¼‚æ˜¯å¦é‡è¦ï¼Ÿ Greff, et al. (2015) å¯¹æµè¡Œå˜ä½“è¿›è¡Œäº†å¾ˆå¥½çš„æ¯”è¾ƒï¼Œå‘çŽ°å®ƒä»¬å‡ ä¹Žå®Œå…¨ç›¸åŒã€‚Jozefowicz, et al. (2015) æµ‹è¯•äº†è¶…è¿‡ä¸€ä¸‡ä¸ªRNNæž¶æž„ï¼Œæ‰¾åˆ°äº†ä¸€äº›åœ¨æŸäº›ä»»åŠ¡ä¸Šæ¯”LSTMæ›´å¥½çš„æž¶æž„ã€‚ Conclusionæ—©äº›æ—¶å€™ï¼Œæˆ‘æåˆ°äº†äººä»¬ç”¨RNNå–å¾—çš„æ˜¾ç€æˆæžœã€‚åŸºæœ¬ä¸Šæ‰€æœ‰è¿™äº›éƒ½æ˜¯ä½¿ç”¨LSTMå®žçŽ°çš„ã€‚å¯¹äºŽå¤§å¤šæ•°ä»»åŠ¡æ¥è¯´ï¼Œå®ƒä»¬ç¡®å®žå·¥ä½œå¾—æ›´å¥½ï¼ ä½œä¸ºä¸€ç»„æ–¹ç¨‹å†™ä¸‹æ¥ï¼ŒLSTMçœ‹èµ·æ¥éžå¸¸ä»¤äººç”Ÿç•ã€‚å¸Œæœ›ï¼Œåœ¨è¿™ç¯‡æ–‡ç« ä¸­é€æ­¥èµ°è¿‡å®ƒä»¬ä½¿ä»–ä»¬æ›´åŠ å¹³æ˜“è¿‘äººã€‚ LSTMæ˜¯æˆ‘ä»¬ç”¨RNNå®žçŽ°çš„é‡è¦ä¸€æ­¥ã€‚å¾ˆè‡ªç„¶åœ°æƒ³çŸ¥é“ï¼šè¿˜æœ‰å¦ä¸€ä¸ªé‡è¦çš„ä¸€æ­¥å—ï¼Ÿç ”ç©¶äººå‘˜çš„å…±åŒè§‚ç‚¹æ˜¯ï¼šâ€œæ˜¯çš„ï¼ä¸‹ä¸€æ­¥æ˜¯å®ƒçš„æ³¨æ„ï¼â€œæˆ‘ä»¬çš„æƒ³æ³•æ˜¯è®©RNNçš„æ¯ä¸€æ­¥éƒ½ä»Žä¸€äº›æ›´å¤§çš„ä¿¡æ¯é›†ä¸­é€‰æ‹©ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œå¦‚æžœæ‚¨ä½¿ç”¨RNNåˆ›å»ºæè¿°å›¾åƒçš„æ ‡é¢˜ï¼Œåˆ™å¯èƒ½ä¼šé€‰æ‹©å›¾åƒçš„ä¸€éƒ¨åˆ†æ¥æŸ¥çœ‹å…¶è¾“å‡ºçš„æ¯ä¸ªå•è¯ã€‚ äº‹å®žä¸Šï¼Œ Xu, et al.(2015) åšåˆ°è¿™ä¸€ç‚¹ - å¦‚æžœä½ æƒ³æŽ¢ç´¢æ³¨æ„åŠ›ï¼Œè¿™å¯èƒ½æ˜¯ä¸€ä¸ªæœ‰è¶£çš„èµ·ç‚¹ï¼ä½¿ç”¨æ³¨æ„åŠ›å·²ç»å–å¾—äº†è®¸å¤šéžå¸¸ä»¤äººå…´å¥‹çš„ç»“æžœï¼Œä¼¼ä¹Žè¿˜æœ‰æ›´å¤šçš„äº‹æƒ…å³å°†æ¥ä¸´â€¦â€¦ æ³¨æ„åŠ›ä¸æ˜¯RNNç ”ç©¶ä¸­å”¯ä¸€ä»¤äººå…´å¥‹çš„é—®é¢˜ã€‚ä¾‹å¦‚ï¼ŒKalchbrenner, et al. (2015) çš„Grid LSTMsä¼¼ä¹Žéžå¸¸æœ‰å¸Œæœ›ã€‚åœ¨ç”Ÿæˆæ¨¡åž‹ä¸­ä½¿ç”¨RNNå·¥ä½œ - ä¾‹å¦‚Gregor, et al. (2015), Chung, et al. (2015), æˆ–è€… Bayer &amp; Osendorfer (2015) ä¼¼ä¹Žä¹Ÿå¾ˆæœ‰è¶£ã€‚è¿‡åŽ»å‡ å¹´å¯¹äºŽåå¤å‡ºçŽ°çš„ç¥žç»ç½‘ç»œæ¥è¯´æ˜¯ä¸€ä¸ªæ¿€åŠ¨äººå¿ƒçš„æ—¶åˆ»ï¼Œå³å°†åˆ°æ¥çš„é‚£äº›æ‰¿è¯ºåªä¼šæ›´åŠ å¦‚æ­¤ï¼]]></content>
      <categories>
        <category>ç¥žç»ç½‘ç»œ</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ã€ŠLearning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translationã€‹é˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FLearning_Phrase_Representations_using_RNN_Encoder%E2%80%93Decoder_for_Statistical_Machine_Translation%2F</url>
    <content type="text"><![CDATA[åŽŸæ–‡é“¾æŽ¥ã€‚è¯¥è®ºæ–‡æ˜¯Sequence to Sequenceå­¦ä¹ çš„æœ€æ—©åŽŸåž‹ï¼Œè®ºæ–‡ä¸­æå‡ºä¸€ç§å´­æ–°çš„RNN(GRU) Encoder-Decoderç®—æ³•ï¼Œè™½ç„¶æ–‡ç« å±žäºŽæ¯”è¾ƒæ—§çš„æ–‡ç« ï¼Œä½†ä½œä¸ºseq2seqçš„åŸºç¡€åŽŸåž‹ï¼Œè¿˜æ˜¯éœ€è¦é˜…è¯»äº†è§£ä¸€ä¸‹çš„ã€‚æ–‡ç« å†™çš„æ¯”è¾ƒè¯¦ç»†ï¼Œå„éƒ¨åˆ†ç»†èŠ‚éƒ½æœ‰è®²è§£ã€‚ æ–‡ç« çš„ä¸»è¦ç»“æž„ contribution a novel RNN Encoderâ€“Decoder èƒ½å¤Ÿå¤„ç†å˜é•¿åºåˆ— a novel hidden unit reset gate update gate RNN Encoderâ€“Decoderæ¨¡åž‹ç»“æž„å›¾å¦‚ä¸‹ï¼š æ–‡ä¸­ä½œè€…å¯¹é½è¿›è¡Œæ€»ä½“æ¦‚è¿°ä¸ºï¼š From a probabilistic perspective, this new model is a general method to learn the conditional distribution over a variable-length sequence conditioned on yet another variable-length sequence ä»Žæ¦‚çŽ‡çš„è§’åº¦æ¥çœ‹ï¼Œè¿™ä¸ªæ–°æ¨¡åž‹æ˜¯å­¦ä¹ åœ¨å¦ä¸€ä¸ªå¯å˜é•¿åº¦åºåˆ—æ¡ä»¶ä¸‹çš„å¯å˜é•¿åº¦åºåˆ—ä¸Šçš„æ¡ä»¶åˆ†å¸ƒçš„ä¸€èˆ¬æ–¹æ³• Encoderè¿™éƒ¨åˆ†æ˜¯ä¸€ä¸ªRNNå•å…ƒã€‚æ¯ä¸ªæ—¶é—´æ­¥ï¼Œæˆ‘ä»¬å‘Encoderä¸­è¾“å…¥ä¸€ä¸ªå­—/è¯ï¼ˆä¸€èˆ¬ä¸ºå‘é‡å½¢å¼ï¼‰ï¼Œç›´åˆ°æˆ‘ä»¬è¾“å…¥è¿™ä¸ªå¥å­çš„æœ€åŽä¸€ä¸ªå­—/è¯$X_T$ï¼Œç„¶åŽè¾“å…¥æ•´ä¸ªå¥å­çš„è¯­ä¹‰å‘é‡cã€‚ç”±äºŽRNNçš„ç‰¹å¸¦ä½ å°±æ˜¯æŠŠå‰é¢æ¯ä¸€æ­¥çš„è¾“å…¥ä¿¡æ¯éƒ½è€ƒè™‘è¿›æ¥ï¼Œæ‰€ä»¥ç†è®ºä¸Šè¿™ä¸ªcå°±åŒ…å«äº†æ•´ä¸ªå¥å­çš„æ‰€æœ‰ä¿¡æ¯ã€‚æˆ‘ä»¬å¯ä»¥æŠŠå½“æˆè¿™ä¸ªå¥å­çš„ä¸€ä¸ªè¯­ä¹‰è¡¨ç¤ºã€‚ DecoderDecoderæ˜¯å¦ä¸€ä¸ªRNNï¼Œå…¶è¢«è®­ç»ƒå‡ºæ¥ä»¥é€šè¿‡é¢„æµ‹éšè—çŠ¶æ€$h_t$çš„ä¸‹ä¸€ä¸ªç¬¦å·$y_t$æ¥ç”Ÿæˆè¾“å‡ºåºåˆ—ã€‚è®¡ç®—å…¬å¼å¦‚ä¸‹ h_t = f(h_{t-1},y_{t-1},c)ä¸‹ä¸€ä¸ªåºåˆ—çš„è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š P(y_t|y_{t-1},y_{t-2},\dots,y_1,c)=g(h_t,y_{t-1},c)Hidden Unitè¯¥éƒ¨åˆ†æ˜¯å¯¹å„éƒ¨åˆ†å…·ä½“çš„å…¬å¼è®²è§£ï¼Œå®žé™…æ˜¯GRUçš„å…·ä½“å…¬å¼ç®—æ³•ï¼Œä¸å†æ­¤è¯¦ç»†å™è¿°äº†ã€‚ reset gate In this formulation, when the reset gate is close to 0, the hidden state is forced to ignore the pre- vious hidden state and reset with the current input only. This effectively allows the hidden state to drop any information that is found to be irrelevant later in the future, thus, allowing a more compact representation. è¿™æ®µåŽŸæ–‡ä¸»è¦è®²è§£äº†å¤ä½é—¨çš„ä½œç”¨ï¼šæœ‰æ•ˆåœ°å…è®¸éšè—çŠ¶æ€ä¸¢å¼ƒåœ¨å°†æ¥ç¨åŽå‘çŽ°ä¸ç›¸å…³çš„ä»»ä½•ä¿¡æ¯ï¼Œä»Žè€Œå…è®¸æ›´ç´§å‡‘çš„è¡¨ç¤ºã€‚ å½“æ•èŽ·çŸ­æœŸä¾èµ–æ—¶ï¼Œå¤ä½é—¨æ´»è·ƒ update gate the update gate controls how much information from the previous hidden state will carry over to the current hidden state. æ›´æ–°é—¨æŽ§åˆ¶æ¥è‡ªå…ˆå‰éšè—çŠ¶æ€çš„å¤šå°‘ä¿¡æ¯å°†è½¬ç§»åˆ°å½“å‰éšè—çŠ¶æ€ã€‚ å½“æ•èŽ·é•¿æœŸä¾èµ–æ—¶ï¼Œæ›´æ–°é—¨æ´»è·ƒ Statistical Machine Translation(SMT) Experimentsè¿™éƒ¨åˆ†ä½œè€…ä¸»è¦åšäº†é‡åŒ–åˆ†æžå’Œæ€§è´¨åˆ†æžï¼Œä¸»è¦å°±æ˜¯è¯´ä»–çš„æ¨¡åž‹æ€Žä¹ˆåŽ‰å®³ã€‚ã€‚ã€‚ï¼ˆæ²¡æœ‰å…·ä½“çš„æ•°å€¼æŒ‡æ ‡ï¼Œç¿»è¯‘çš„è¿˜ä¸æ˜¯ä¸­è‹±ç¿»è¯‘ï¼Œæƒ³çœ‹çš„è¯å¯ä»¥åŽ»çœ‹ä¸€ä¸‹ï¼Œå°±ä¸è´´å®žéªŒç»“æžœäº†ï¼‰ã€‚ futureè¿™é‡Œä½œè€…æå‡ºäº†å¯ä»¥ç”¨decoderç”Ÿæˆçš„ç›®æ ‡çŸ­è¯­æ¥æ›¿æ¢åŽŸå¥ä¸­çŸ­è¯­çš„æ€è·¯ï¼Œå¦‚æžœæ²¡è®°é”™çš„è¯ï¼Œè¿™ä¸ªæƒ³æ³•å¥½åƒå¯¹åŽé¢çš„æœºå™¨ç¿»è¯‘æœ‰å¾ˆå¤§çš„æŒ‡å¯¼ä½œç”¨ã€‚]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>seq2seq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ã€ŠDifferentiating Concepts and Instances for Knowledge Graph Embeddingã€‹é˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2Fread_Differentiating_Concepts_and_Instances_for_Knowledge_Graph_Embedding%2F</url>
    <content type="text"><![CDATA[è®ºæ–‡èŽ·å–åœ°å€ã€‚è¿™ç¯‡æ–‡ç« æœ€å¤§çš„äº®ç‚¹å°±æ˜¯æŠŠconceptæ˜ å°„ä¸ºä¸€ä¸ªçƒé¢ï¼Œç„¶åŽæŠŠinstanceæ˜ å°„ä¸ºä¸€ä¸ªå‘é‡ï¼Œé€šè¿‡è¿™ç§ç©ºé—´å…³ç³»æ¥è¿›è¡Œembeddingã€‚å¦‚æžœinstanceå’Œconceptæ»¡è¶³InstanceOfçš„å…³ç³»ï¼Œåˆ™instanceåº”è¯¥åœ¨çƒå†…ï¼›å¦‚æžœä¸¤ä¸ªconceptæ»¡è¶³SubClassOfçš„å…³ç³»ï¼Œåˆ™ä¸€ä¸ªçƒä¼šåœ¨å¦ä¸€ä¸ªçƒé¢å†…ã€‚ conceptA concept is a fundamental category of existence (Rosch, 1973) and can be reified by all of its actual or potential instances.Concepts, which represent a group of different instances sharing common properties, are essential information in knowledge representation. drawback of the previous method ignore to distinguish between concepts and instances will lead to two drawbacks: Insufficient concept representationï¼š cannot explicitly represent the difference between concepts and instances Lack transitivity of both isA relations: instanceOf and subClassOf (generally known as isA)isA relations exhibit transitivity contributions the first to propose and formalize the problem of knowledge graph embedding which differentiates between concepts and instances a novel knowledge embedding method named TransC state-of-the-art on link prediction and triple classification Translation-based ModelsTransE triple (h, r, t) should satisfy h + r â‰ˆ t loss function:$f_r(h,t) = ||h + r - t||^2_2$ suitable for 1-to-1 relations TransH It regards a relation vector r as a translation on a hyperplane with $w_r$ as the normal vector. loss function:$f_r(h,t) = ||h_{\bot} + r - t_{\bot}||^2_2$ï¼Œå…¶ä¸­$h_{\bot}=h-w^{\top}_r h w_r$ï¼Œ$t_{\bot}=t-w^{\top}_r t w_r$ suitable for 1-to-N, N-to-1, and N-to-N relations TransR/CTransR addresses the issue in TransE and TransH that some entities are similar in the entity space but comparably different in other specific aspects. loss function:$f_r(h,t) = ||M_rh +r -M_rt||^2_2$ï¼Œ$M_r$ for each relation r TransD considers the different types of entities and relations at the same time loss function:$f_r(h,t) = ||h_{\bot} + r - t_{\bot}||^2_2$ï¼Œ$h_{\bot} = M_{rh}h$å’Œ$t_{\bot} = M_{rt}t$ï¼Œ$M_{r,e}$ for each relation-entity pair (r, e) Bilinear ModelsRESCAL the first bilinear model It associates each entity with a vector to capture its latent semantics. Each relation is represented as a matrix which models pairwise interactions between latent factors. External Information Learning Models textual information entity descriptions Problem Formulationè¿™éƒ¨åˆ†ä¸­ä½œè€…è¯¦ç»†ä»‹ç»äº†çŸ¥è¯†å›¾è°±çš„ç»„æˆéƒ¨åˆ†ï¼šæ¦‚å¿µå’Œå®žä¾‹é›†ã€å…³ç³»é›†ï¼ˆåŒ…æ‹¬instanceOfã€subClassOfå’Œinstance relationï¼‰ï¼Œä¸‰å…ƒç»„é›†ï¼ˆæŒ‰ç…§å…³ç³»é›†åŒæ ·åˆ†ä¸ºä¸‰ä¸ªéƒ¨åˆ†ï¼‰ã€‚ä¸ºäº†èƒ½è¡¨è¾¾is Aå…³ç³»çš„ä¼ é€’æ€§ï¼Œä½œè€…å°†instanceOfå’ŒsubClassofä¸¤ç§å…³ç³»è¿›è¡Œäº†ç²¾å¿ƒçš„è®¾è®¡ï¼Œä¹Ÿæ˜¯è¯¥è®ºæ–‡çš„é‡ç‚¹ã€‚ For each concept c âˆˆ C, we learn a sphere s(p, m) with $p \in R^k$ and m denoting the sphere center and radius. TranCSpecifically, TransC encodes each concept in knowledge graph as a sphere and each instance as a vector in the same semantic space. InstanceOf Triple Representationloss functionï¼š$f_e(i,c) = ||i-p||_2 - m$ï¼Œå½“è¯¥å‡½æ•°å¤§äºŽ0æ—¶è¿›è¡Œä¼˜åŒ–ï¼Œä½¿å…¶å°äºŽé›¶ã€‚ SubClassOf Triple Representation å¦‚å›¾æ‰€ç¤ºï¼Œå…¶ä¸­å­å›¾ï¼ˆaï¼‰ä¸ºç›®æ ‡çŠ¶æ€ã€‚ä¸¤ä¸ªæ¦‚å¿µçš„çš„åœ†å¿ƒè·ç¦»ï¼š$d = ||p_i - p_j||_2$ã€‚éœ€è¦åšåˆ°çš„å°±æ˜¯$d-(m_j -m_i) \leq 0$å¹¶ä¸”$ (m_j &gt; m_i)$ã€‚ Relational Triple Representationè¿™éƒ¨åˆ†æŒ‰ç…§TranEçš„æ€è·¯è¿›è¡Œå¤„ç†ï¼Œ$||h+r-t||^2_3$ train modelmargin based lossè¯¦è§£unit and bern Regarding the strategy of constructing negative labels, we use â€œunifâ€ to denote the traditional way of replacing head or tail with equal probability, and use â€œbern.â€ to denote reducing false negative labels by replacing head or tail with different probabilities. the following research directions find a more expressive model instead of spheres to represent concepts A concept may have different meanings in different triples. use several typical vectors of instances as a conceptâ€™s centers to represent different meanings of a concept.]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>çŸ¥è¯†å›¾è°±</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Discourse-Level Named Entity Recognition and Relation Extraction Dataset for Chinese Literature Text é˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FREAD_A_Discourse-Level_Named_Entity_Recognition_and_Relation_Extraction_Dataset_for_Chinese_Literature_Text%2F</url>
    <content type="text"><![CDATA[è¯¥è®ºæ–‡æœ€ä¸»è¦çš„è´¡çŒ®å°±æ˜¯è¿™ä¸ªæ•°æ®ï¼Œæ•°æ®é›†åœ°å€ã€‚è®ºæ–‡ä¸­æåˆ°çš„æ ‡æ ‡ç­¾è¿‡ç¨‹ä¹Ÿæ˜¯ä¸€ä¸ªåˆ›æ–°ç‚¹ï¼Œè¿ç”¨äº†å¯å‘å¼å’Œæœºå™¨è¾…åŠ©æ ‡æ ‡ç­¾ï¼Œè¿™æ ·å¯ä»¥æé«˜å‡†ç¡®åº¦å¹¶å‡å°‘æ ‡æ³¨äººå‘˜å·¥ä½œã€‚ contribution provide a new dataset for joint learning of NER and RE for Chinese literature text the proposed dataset is based on the discourse level which provides additional context information introduce some widely used models to conduct experiments tagging processtwo methods:one is a heuristic tagging method and another is a machine auxiliary tagging method. Step 1: First Tagging Processfind a problem of data inconsistency. Step 2: Heuristic Tagging Based on Generic disambiguating Rules For example, remove all adjective words and only tag â€œentity headerâ€ . re-annotate all articles and correct all inconsistency entities and relations based on the heuristic rules. Step 3: Machine Auxiliary Tagging The core idea is to train a model to learn annotation guidelines on the subset of the corpus and produce predicted tags on the rest data. CRF tagging set Annotation FormatEntityEach entity is identified by T tag, which takes several attributes. Id: a unique number identifying the entity within the document. It starts at 0, and is incremented every time a new entity is identified within the same document. Type: one of the entity tags. Begin Index: the begin index of an entity. It starts at 0, and is incremented every character. End Index: the end index of an entity. It starts at 0, and is incremented every character. Value: words being referred to an identifiable object. RelationEach relation is identified by R tag, which can take several attributes: Id: a unique number identifying the relation within the document. It starts at 0, and is incremented every time a new relation is identified within the same document. Arg1 and Arg2: two entities associated with a relation. Type: one of the relation tags.]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>NER</tag>
        <tag>RE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandasçš„æ•°æ®ç±»åž‹æ“ä½œ]]></title>
    <url>%2Fpost%2FPandas_data_type_manipulation%2F</url>
    <content type="text"><![CDATA[åœ¨åŽŸæ–‡é“¾æŽ¥ä¸­æ‘˜æŠ„å‡ºéƒ¨åˆ†ä¿¡æ¯ä½œä¸ºè®°å½•å½¢æˆæœ¬æ–‡ã€‚ æ•°æ®ç±»åž‹ Pandas dtype Python ç±»åž‹ NumPy ç±»åž‹ ç”¨é€” object str string_, unicode_ æ–‡æœ¬ int64 int int_, int8, int16, int32, int64, uint8, uint16, uint32, uint64 æ•´æ•° float64 float float_, float16, float32, float64 æµ®ç‚¹æ•° bool bool bool_ å¸ƒå°”å€¼ datetime64 NA NA æ—¥æœŸæ—¶é—´ timedelta[ns] NA NA æ—¶é—´å·® category NA NA æœ‰é™é•¿åº¦çš„æ–‡æœ¬å€¼åˆ—è¡¨ æ•°æ®ç±»åž‹æ“ä½œ ä½¿ç”¨df.dtypeså¯ä»¥æ˜¾ç¤ºæ•°æ®æ‰€æœ‰åˆ—çš„ç±»åž‹ df.infoï¼ˆï¼‰ å‡½æ•°å¯ä»¥æ˜¾ç¤ºæ›´æœ‰ç”¨çš„ä¿¡æ¯ ä½¿ç”¨ astype() å‡½æ•°ä½¿ç”¨æ¡ä»¶ æ•°æ®æ˜¯å¹²å‡€çš„ï¼Œå¯ä»¥ç®€å•åœ°è§£é‡Šä¸ºä¸€ä¸ªæ•°å­— ä½ æƒ³è¦å°†ä¸€ä¸ªæ•°å€¼è½¬æ¢ä¸ºä¸€ä¸ªå­—ç¬¦ä¸²å¯¹è±¡ å¦‚æžœæ•°æ®å…·æœ‰éžæ•°å­—å­—ç¬¦æˆ–å®ƒä»¬é—´ä¸åŒè´¨ï¼ˆhomogeneousï¼‰ï¼Œé‚£ä¹ˆ astype() å¹¶ä¸æ˜¯ç±»åž‹è½¬æ¢çš„å¥½é€‰æ‹©ã€‚ä½ éœ€è¦è¿›è¡Œé¢å¤–çš„å˜æ¢æ‰èƒ½å®Œæˆæ­£ç¡®çš„ç±»åž‹è½¬æ¢ã€‚ ä½¿ç”¨æ–¹å¼ä¸ºäº†çœŸæ­£ä¿®æ”¹åŽŸå§‹ dataframe ä¸­æ•°æ®ç±»åž‹ï¼Œè®°å¾—æŠŠ astype() å‡½æ•°çš„è¿”å›žå€¼é‡æ–°èµ‹å€¼ç»™ dataframeï¼Œå› ä¸º astype() ä»…è¿”å›žæ•°æ®çš„å‰¯æœ¬è€Œä¸åŽŸåœ°ä¿®æ”¹ã€‚ å‚è€ƒé“¾æŽ¥ https://juejin.im/post/5acc36e66fb9a028d043c2a5]]></content>
      <categories>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>pandas</tag>
        <tag>python</tag>
        <tag>æ•°æ®åˆ†æž</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pythonçˆ¬å–ä¸­æ–‡ç½‘é¡µæ—¶ä¸­æ–‡å­—ç¬¦å˜è‹±æ–‡çš„è§£å†³æ–¹æ³•]]></title>
    <url>%2Fpost%2Fsolution_of_python_for_Chinese_characters_to_become_English_when_crawling_Chinese_web_pages%2F</url>
    <content type="text"><![CDATA[ä½¿ç”¨pythonçš„scrapyçˆ¬å–ç½‘é¡µæ—¶ï¼Œæºä»£ç ä¸­çš„ä¸­æ–‡å­—ç¬¦åœ¨çˆ¬å–ä¸‹æ¥åŽå˜æˆäº†è‹±æ–‡å­—ç¬¦ã€‚ é—®é¢˜ä¸¾ä¾‹ä¾‹å¦‚ï¼ŒåŽŸç½‘é¡µä¸ºï¼š çˆ¬å–ç»“æžœä¸ºï¼š è§£å†³æ–¹æ³•ä¿®æ”¹è¯·æ±‚å¤´ï¼šåœ¨settings.pyæ–‡ä»¶ä¸­æ‰¾åˆ°ä¸‹å±žä»£ç ï¼š # Override the default request headers: #DEFAULT_REQUEST_HEADERS = { # 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', # 'Accept-Language': 'en', #} æ”¹ä¸ºï¼š # Override the default request headers: DEFAULT_REQUEST_HEADERS = { 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Language': 'zh-CN', } ä¿®æ”¹ç»“æžœå±•ç¤ºï¼š å‚è€ƒé“¾æŽ¥ https://blog.csdn.net/wuqili_1025/article/details/79690103]]></content>
      <categories>
        <category>çˆ¬è™«</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>çˆ¬è™«</tag>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pythonåŠ¨æ€ç½‘é¡µçˆ¬å–ä¹‹å®‰è£…dockerå’Œsplash]]></title>
    <url>%2Fpost%2FPython_dynamic_web_crawler_installed_docker_and_splash%2F</url>
    <content type="text"><![CDATA[åˆ©ç”¨pythonè¿›è¡ŒåŠ¨æ€ç½‘é¡µçˆ¬å–æ—¶ï¼Œåœ¨å®‰è£…dockerå’Œsplashæ—¶è¸©è¿‡çš„å‘ï¼Œè®°å½•äº†ä¸€ä¸‹è‡ªå·±çš„å®‰è£…è¿‡ç¨‹ã€‚ç”¨çš„ç³»ç»Ÿæ˜¯mac osã€‚ å®‰è£…scrapy-splash åˆ©ç”¨pipå®‰è£…scrapy-splashåº“ï¼š$ pip install scrapy-splash å®‰è£…Docker==ä¸‹é¢ðŸ‘‡è¿™æ ·å®‰ä¸ä¸‹åŽ»äº†== å¦‚æžœæ˜¯Macçš„è¯éœ€è¦ä½¿ç”¨brewå®‰è£…ï¼Œå¦‚ä¸‹ï¼šbrew install docker æŠ¥é”™ï¼š Error: Failure while executing; `git config --local --replace-all homebrew.private true` exited with 1. è§£å†³æ–¹æ³•ï¼š xcode-select --install ç„¶åŽåœ¨æ‰§è¡Œï¼š brew install docker å†ç»§ç»­ï¼š service docker start æŠ¥é”™ï¼š -bash: service: command not foundä¸Šç½‘ä¸ŠæŸ¥ä¸€å †ä¹±ä¸ƒå…«ç³Ÿçš„è§£å†³æ–¹å¼ï¼Œè¯¥è·¯å¾„å•¥çš„ï¼ŒçœŸçš„ä¸æƒ³æ”¹è·¯å¾„ï¼Œæ€•æŠŠå…¶ä»–çš„æ”¹å´©äº†ã€‚æœ€åŽæ”¾å¼ƒè¿™ç§æ–¹å¼ï¼Œå¦‚æžœæœ‰å…´è¶£ä¹Ÿå¯ä»¥å°è¯•è§£å†³ã€‚ ==å°è¯•å¦‚ä¸‹å®‰è£…DOCKERæ–¹æ³•== åŽ»å®˜ç½‘ä¸‹è½½è¿™ç§æ–¹æ³•ä¸‹è½½dockerå®¢æˆ·ç«¯éœ€è¦ä»ŽæœåŠ¡å™¨ä¸‹è½½ï¼Œè‡ªå·±ç”µè„‘ä¸‹è½½12k/sï¼Œç®€ç›´æ…¢æ­»äº†ã€‚ æ‹‰å–é•œåƒ(pull the image)ï¼šdocker pull scrapinghub/splash ç”¨dockerè¿è¡Œscrapinghub/splashï¼š docker run -p 8050:8050 scrapinghub/splash åœ¨æµè§ˆå™¨ä¸­è¾“å…¥localhost:8050 ==å®‰è£…æˆåŠŸ== å‚è€ƒé“¾æŽ¥ http://www.morecoder.com/article/1001249.html https://www.jianshu.com/p/e54a407c8a0a]]></content>
      <categories>
        <category>çˆ¬è™«</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>çˆ¬è™«</tag>
        <tag>splash</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ç”¨python3è¯»å†™csvæ–‡æ¡£]]></title>
    <url>%2Fpost%2Fread%20the%20CSV%20document%20using%20python3%2F</url>
    <content type="text"><![CDATA[å¯¹äºŽå¤§å¤šæ•°çš„CSVæ ¼å¼çš„æ•°æ®è¯»å†™é—®é¢˜ï¼Œéƒ½å¯ä»¥ä½¿ç”¨ csv åº“ã€‚ ä¾‹å¦‚ï¼šå‡è®¾ä½ åœ¨ä¸€ä¸ªåå«stocks.csvæ–‡ä»¶ä¸­æœ‰ä¸€äº›è‚¡ç¥¨å¸‚åœºæ•°æ®ï¼Œå°±åƒè¿™æ ·ï¼š Symbol,Price,Date,Time,Change,Volume "AA",39.48,"6/11/2007","9:36am",-0.18,181800 "AIG",71.38,"6/11/2007","9:36am",-0.15,195500 "AXP",62.58,"6/11/2007","9:36am",-0.46,935000 "BA",98.31,"6/11/2007","9:36am",+0.12,104800 "C",53.08,"6/11/2007","9:36am",-0.25,360900 "CAT",78.29,"6/11/2007","9:36am",-0.23,225400 csvæ–‡æ¡£çš„è¯»å–1. å¸¸è§„è¯»å–ä¸‹é¢å‘ä½ å±•ç¤ºå¦‚ä½•å°†è¿™äº›æ•°æ®è¯»å–ä¸ºä¸€ä¸ªå…ƒç»„çš„åºåˆ—ï¼š import csv with open('stocks.csv') as f: f_csv = csv.reader(f) headers = next(f_csv) for row in f_csv: # Process row ... åœ¨ä¸Šé¢çš„ä»£ç ä¸­ï¼Œ row ä¼šæ˜¯ä¸€ä¸ªåˆ—è¡¨ã€‚å› æ­¤ï¼Œä¸ºäº†è®¿é—®æŸä¸ªå­—æ®µï¼Œä½ éœ€è¦ä½¿ç”¨ä¸‹æ ‡ï¼Œå¦‚ row[0]è®¿é—®Symbolï¼Œ row[4] è®¿é—®Changeã€‚==è¿™æ ·å¯ä»¥é€šè¿‡å¤–å»ºå­—å…¸æ¥å­˜å‚¨è¯»å‡ºçš„csvæ•°æ®ã€‚== 2. å‘½åå…ƒç»„ç”±äºŽè¿™ç§ä¸‹æ ‡è®¿é—®é€šå¸¸ä¼šå¼•èµ·æ··æ·†ï¼Œä½ å¯ä»¥è€ƒè™‘ä½¿ç”¨==å‘½åå…ƒç»„==ã€‚ä¾‹å¦‚ï¼š from collections import namedtuple with open('stock.csv') as f: f_csv = csv.reader(f) headings = next(f_csv) Row = namedtuple('Row', headings) for r in f_csv: row = Row(*r) # Process row ... å®ƒå…è®¸ä½ ä½¿ç”¨åˆ—åå¦‚ row.Symbol å’Œ row.Change ä»£æ›¿ä¸‹æ ‡è®¿é—®ã€‚ éœ€è¦æ³¨æ„çš„æ˜¯è¿™ä¸ªåªæœ‰åœ¨åˆ—åæ˜¯åˆæ³•çš„Pythonæ ‡è¯†ç¬¦çš„æ—¶å€™æ‰ç”Ÿæ•ˆã€‚å¦‚æžœä¸æ˜¯çš„è¯ï¼Œ ä½ å¯èƒ½éœ€è¦ä¿®æ”¹ä¸‹åŽŸå§‹çš„åˆ—å(å¦‚å°†éžæ ‡è¯†ç¬¦å­—ç¬¦æ›¿æ¢æˆä¸‹åˆ’çº¿ä¹‹ç±»çš„)ã€‚ 3. å­—å…¸å¦å¤–ä¸€ä¸ªé€‰æ‹©å°±æ˜¯å°†æ•°æ®è¯»å–åˆ°ä¸€ä¸ªå­—å…¸åºåˆ—ä¸­åŽ»ã€‚å¯ä»¥è¿™æ ·åšï¼š import csv with open('stocks.csv') as f: f_csv = csv.DictReader(f) for row in f_csv: # process row ... åœ¨è¿™ä¸ªç‰ˆæœ¬ä¸­ï¼Œä½ å¯ä»¥ä½¿ç”¨åˆ—ååŽ»è®¿é—®æ¯ä¸€è¡Œçš„æ•°æ®äº†ã€‚æ¯”å¦‚ï¼Œrow[&#39;Symbol&#39;] æˆ–è€… row[&#39;Change&#39;]ã€‚ fieldnames æ˜¯dict_readerçš„ä¸€ä¸ªå±žæ€§ï¼Œè¡¨ç¤ºCSVæ–‡æ¡£çš„æ•°æ®åç§°ã€‚å¯ä»¥é€šè¿‡f_csv.fieldnamesæ¥è®¿é—®æ•°æ®åç§°é‚£ä¸€è¡Œã€‚ CSVæ–‡ä»¶å†™å…¥ä¸ºäº†å†™å…¥CSVæ•°æ®ï¼Œä½ ä»ç„¶å¯ä»¥ä½¿ç”¨csvæ¨¡å—ï¼Œä¸è¿‡è¿™æ—¶å€™å…ˆåˆ›å»ºä¸€ä¸ª writer å¯¹è±¡ã€‚ä¾‹å¦‚: headers = ['Symbol','Price','Date','Time','Change','Volume'] rows = [('AA', 39.48, '6/11/2007', '9:36am', -0.18, 181800), ('AIG', 71.38, '6/11/2007', '9:36am', -0.15, 195500), ('AXP', 62.58, '6/11/2007', '9:36am', -0.46, 935000), ] with open('stocks.csv','w') as f: f_csv = csv.writer(f) f_csv.writerow(headers) f_csv.writerows(rows) å¦‚æžœä½ æœ‰ä¸€ä¸ªå­—å…¸åºåˆ—çš„æ•°æ®ï¼Œå¯ä»¥åƒè¿™æ ·åšï¼š headers = ['Symbol', 'Price', 'Date', 'Time', 'Change', 'Volume'] rows = [{'Symbol':'AA', 'Price':39.48, 'Date':'6/11/2007', 'Time':'9:36am', 'Change':-0.18, 'Volume':181800}, {'Symbol':'AIG', 'Price': 71.38, 'Date':'6/11/2007', 'Time':'9:36am', 'Change':-0.15, 'Volume': 195500}, {'Symbol':'AXP', 'Price': 62.58, 'Date':'6/11/2007', 'Time':'9:36am', 'Change':-0.46, 'Volume': 935000}, ] with open('stocks.csv','w') as f: f_csv = csv.DictWriter(f, headers) f_csv.writeheader() f_csv.writerows(rows) å…¶ä¸­f_csv.writeheader()ä¹Ÿå¯ä»¥æ›¿æ¢æˆf_csv.writerow(dict(zip(headers, headers))) å‚è€ƒé“¾æŽ¥ https://python3-cookbook.readthedocs.io/zh_CN/latest/c06/p01_read_write_csv_data.html https://blog.csdn.net/guoziqing506/article/details/52014506]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>æ–‡ä»¶è¯»å–</tag>
        <tag>csv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neo4jåˆå§‹åŒ–èŠ‚ç‚¹æ˜¾ç¤ºè®¾ç½®]]></title>
    <url>%2Fpost%2FNeo4j_initializes_the_node_display_Settings%2F</url>
    <content type="text"><![CDATA[é—®é¢˜æè¿°ï¼šneo4jä¸­æœ‰é»˜è®¤çš„åˆå§‹åŒ–èŠ‚ç‚¹æ˜¾ç¤ºè®¾ç½®ä¸º300ä¸ªèŠ‚ç‚¹ï¼Œå¦‚æžœæƒ³è¦æ˜¾ç¤ºçš„èŠ‚ç‚¹å¤šäºŽ300ä¸ªï¼Œåˆ™ä¼šåªæ˜¾ç¤º300ä¸ªï¼Œå¹¶ç»™äºˆä»¥ä¸‹æç¤ºè¯­å¥ï¼š Not all return nodes are being displayed due to Initial Node Display setting. Only 300 of 300 nodes are being displayed. è§£å†³æ–¹æ³•ï¼šåœ¨å¦‚å›¾æ‰€ç¤ºinitial Node Displayå¤„å¯ä»¥ä¿®æ”¹ï¼Œåœ¨æ­¤å¤„ä¿®æ”¹ä¸º300000.]]></content>
      <categories>
        <category>Neo4j</category>
      </categories>
      <tags>
        <tag>Neo4j</tag>
        <tag>æ•°æ®åº“</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ã€ŠBidirectional LSTM-CRF Models for Sequence Taggingã€‹é˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2Fread_Bidirectional_LSTM-CRF_Models_for_Sequence_Tagging%2F</url>
    <content type="text"><![CDATA[è¿™ç¯‡è®ºæ–‡å¯ä»¥ä½œä¸ºä¸€ä¸ªRNNå’ŒLSTMå­¦ä¹ çš„ä¸€ä¸ªä¾‹å­æ¥çœ‹ï¼Œæœ‰åˆ©äºŽæ–°æ‰‹å¯¹LSTMçš„ç†è§£ã€‚å¯¹äºŽNERçš„å¤„ç†ä¸»è¦æ˜¯ä½œä¸ºä¸€ä¸ªåºåˆ—æ ‡æ³¨é—®é¢˜ã€‚ä½†æ˜¯ä½œä¸ºç»å…¸æ–‡ç« è¿˜æ˜¯å¯ä»¥è¯»ä¸€è¯»äº†è§£ä¸€ä¸‹çš„ã€‚ åœ¨æœ¬ç¯‡è®ºæ–‡ä¸­ï¼Œä½œè€…æå‡ºäº†4ç§æ¨¡åž‹ï¼šLSTMã€BI-LSTMã€LSTM-CRFå’ŒBI-LSTM-CRFã€‚ contribution(è´¡çŒ®) ä½œè€…åœ¨NLPæ ‡æ³¨æ•°æ®é›†ä¸Šç³»ç»Ÿçš„å¯¹æ¯”äº†ä»¥ä¸Šå››ä¸ªæ¨¡åž‹ï¼› ä½œè€…æ˜¯é¦–å…ˆæå‡ºæŠŠBI-LSTM-CRFæ¨¡åž‹ç”¨äºŽNLPåºåˆ—æ ‡æ³¨ï¼Œå¹¶ä¸”è¾¾åˆ°äº†state-of-the-artçš„æ°´å¹³ï¼› ä½œè€…å±•ç¤ºäº†BI-LSTM-CRFæ˜¯robustï¼Œå¹¶ä¸”æžå°‘ä¾èµ–äºŽè¯å‘é‡ã€‚ model(æ¨¡åž‹)LSTMé¦–å…ˆï¼Œä½œè€…å…ˆä»‹ç»äº†RNNçš„ç»“æž„å’Œå·¥ä½œåŽŸç†ï¼Œå¦‚å›¾ï¼š å…¶ä¸­è¾“å…¥ä¸ºå¥å­ï¼šEU rejects German call to boycott British lambã€‚è¾“å‡ºä¸ºæ ‡ç­¾ï¼šB-ORG O B-MISC O O O B-MISC O Oï¼Œå…¶ä¸­B-ï¼ŒI-è¡¨ç¤ºå®žä½“å¼€å§‹å’Œä¸­é—´ä½ç½®ã€‚æ ‡ç­¾ç§ç±»ä¸ºï¼šother (O)å’Œå››ç§å®žä½“æ ‡ç­¾ï¼šPerson (PER), Location (LOC), Organization (ORG), and Miscellaneous (MISC). è¾“å…¥å±‚è¡¨ç¤ºåœ¨æ—¶é—´æ­¥ t çš„ç‰¹å¾ã€‚å®ƒä»¬å¯ä»¥æ˜¯ one-hot-encoding çš„è¯ç‰¹å¾ï¼Œç¨ å¯†æˆ–è€…ç¨€ç–çš„å‘é‡ç‰¹å¾ã€‚è¾“å…¥å±‚ä¸Žç‰¹å¾æœ‰ç›¸åŒå¤§å°çš„ç»´åº¦ã€‚è¾“å‡ºå±‚è¡¨ç¤ºåœ¨æ—¶é—´æ­¥ t çš„æ ‡ç­¾ä¸Šçš„æ¦‚çŽ‡åˆ†å¸ƒï¼Œç»´åº¦ä¸Žæ ‡æ³¨æ•°é‡ç›¸åŒã€‚ç›¸æ¯”å‰é¦ˆç¥žç»ç½‘ç»œï¼ŒRNN å¼•å…¥å‰ä¸€ä¸ªéšè—çŠ¶æ€å’Œå½“å‰éšè—çŠ¶æ€çš„ç»“åˆï¼Œå› æ­¤å¯ä»¥å‚¨å­˜åŽ†å²ä¿¡æ¯ã€‚ æ¶‰åŠå…¬å¼ä¸ºï¼š å…¶ä¸­ï¼ŒUï¼ŒWï¼ŒVéƒ½æ˜¯æƒé‡ï¼Œå‡½æ•°fï¼Œgåˆ†åˆ«ä¸ºsigmoidå’Œsoftmaxå‡½æ•°ã€‚ æŽ¥ä¸‹æ¥ï¼Œä½œè€…å±•ç¤ºäº†LSTMçš„ç»“æž„å’ŒåŽŸç†ï¼Œå¦‚å›¾ï¼š å…¬å¼ï¼š å…¶ä¸­ï¼ŒÏƒæ˜¯é€»è¾‘sigmoidå‡½æ•°ï¼Œi, f, o å’Œ cåˆ†åˆ«æ˜¯è¾“å…¥é—¨ï¼Œå¿˜è®°é—¨ï¼Œè¾“å‡ºé—¨å’Œç»†èƒžå‘é‡ï¼Œæ‰€æœ‰çš„å¤§å°éƒ½å’Œå‘é‡hä¸€æ ·ã€‚wæƒé‡çš„å«ä¹‰å¦‚å…¶ä¸‹è¡¨æ‰€ç¤ºã€‚ LSTMåºåˆ—æ ‡æ³¨æ¨¡åž‹å¦‚å›¾æ‰€ç¤ºï¼š å…¶ä¸­ï¼Œä¸­é—´çš„ç”»æ–œçº¿çš„æ ¼å­å³ä¸ºå›¾2ä¸­æ‰€ç¤ºéƒ¨åˆ†ã€‚ Bidirectional LSTM(åŒå‘LSTM)ä½œè€…å±•ç¤ºäº†åŒå‘LSTMçš„ç»“æž„ï¼Œå¦‚å›¾æ‰€ç¤ºï¼š åŒå‘LSTMç½‘ç»œå¯ä»¥æœ‰æ•ˆåˆ©ç”¨è¿‡åŽ»ç‰¹å¾å’Œæœªæ¥ç‰¹å¾ã€‚åœ¨ä½œè€…çš„å®žçŽ°ä¸­ï¼Œå¯¹äºŽæ•´ä¸ªå¥å­çš„å‰å‘å’ŒåŽå‘æ“ä½œï¼Œä½œè€…åªéœ€è¦åœ¨æ¯ä¸ªå¥å­å¼€å§‹æ—¶å°†éšè—çŠ¶æ€é‡ç½®ä¸º0ã€‚ä½œè€…é‡‡ç”¨æ‰¹å¤„ç†ï¼Œä½¿å¾—å¯ä»¥åŒæ—¶å¤„ç†å¤šä¸ªå¥å­ã€‚ CRFä½¿ç”¨é‚»å±…æ ‡è®°ä¿¡æ¯é¢„æµ‹å½“å‰æ ‡è®°æœ‰ä¸¤ç§ä¸åŒçš„æ–¹æ³•ï¼š é¢„æµ‹æ¯ä¸ªæ—¶é—´æ­¥é•¿çš„æ ‡ç­¾åˆ†å¸ƒï¼Œç„¶åŽä½¿ç”¨æ³¢æŸå¼è§£ç æ¥æ‰¾åˆ°æœ€ä¼˜çš„æ ‡ç­¾åºåˆ—ï¼Œä»£è¡¨æ–¹æ³•ï¼šMEMMs æ³¨é‡å¥å­å±‚æ¬¡è€Œä¸æ˜¯ä¸ªä½“ä½ç½®ï¼Œä»£è¡¨æ–¹æ³•ï¼šCRFï¼Œè¾“å…¥å’Œè¾“å‡ºæ˜¯ç›´æŽ¥ç›¸è¿žçš„ï¼›å¦‚å›¾ï¼š ç ”ç©¶è¡¨æ˜Žï¼ŒCRFsä¸€èˆ¬èƒ½å¤Ÿäº§ç”Ÿæ›´é«˜çš„æ ‡ç­¾ç²¾åº¦ã€‚ LSTM-CRFä½œè€…å±•ç¤ºäº†LSTM-CRFçš„ç»“æž„ï¼Œå¦‚å›¾ï¼š è¿™ç½‘ç»œå¯ä»¥æœ‰æ•ˆåœ°é€šè¿‡ LSTM åˆ©ç”¨è¿‡åŽ»çš„è¾“å…¥ç‰¹å¾å’Œé€šè¿‡ CRF åˆ©ç”¨å¥å­çº§çš„æ ‡æ³¨ä¿¡æ¯ã€‚å›¾ä¸­CRFå±‚ç”±è¿žæŽ¥è¿žç»­è¾“å‡ºå±‚çš„çº¿è¡¨ç¤ºã€‚CRFå±‚æœ‰ä¸€ä¸ªçŠ¶æ€è½¬ç§»çŸ©é˜µä½œä¸ºå‚æ•°ã€‚ å…¬å¼ä¸ºï¼š å‡½æ•°fä¸ºç½‘ç»œçš„è¾“å‡ºåˆ†æ•°ï¼Œ[x]ä¸ºè¾“å…¥ï¼Œ [fÎ¸]i,t ä¸ºå¸¦æœ‰å‚æ•°Î¸ï¼ˆå¥å­xï¼Œç¬¬i ä¸ªæ ‡ç­¾ï¼Œç¬¬tä¸ªå•è¯ï¼‰çš„ç½‘ç»œè¾“å‡ºï¼› [A]i,jä¸ºè½¬ç§»åˆ†æ•°ï¼Œä»Žè¿žç»­çš„æ—¶é—´æ­¥içŠ¶æ€åˆ°jçŠ¶æ€çš„è½¬ç§»åˆ†æ•°ã€‚æ³¨æ„ï¼Œè¯¥è½¬æ¢çŸ©é˜µæ˜¯ä½ç½®æ— å…³çš„ã€‚ BI-LSTM-CRFä½œè€…å±•ç¤ºäº†BI-LSTM-CRFçš„ç»“æž„ï¼Œå¦‚å›¾æ‰€ç¤ºï¼š ä½œè€…åœ¨å®žéªŒä¸­å±•ç¤ºäº†é¢å¤–çš„æœªæ¥ç‰¹å¾å¯ä»¥æé«˜æ ‡ç­¾çš„å‡†ç¡®çŽ‡ã€‚ è®­ç»ƒè¿‡ç¨‹æœ¬æ–‡ä½¿ç”¨çš„æ‰€æœ‰æ¨¡åž‹éƒ½å…±äº«ä¸€ä¸ªé€šç”¨SGDå‰å‘å’ŒåŽå‘è®­ç»ƒè¿‡ç¨‹ã€‚ä½œè€…å±•ç¤ºäº†BI-LSTM-CRFçš„ç®—æ³•ï¼Œå¦‚å›¾ ä½œè€…è®¾ç½®äº†æ‰¹æ¬¡å¤§å°ä¸º100ã€‚ å®žéªŒdataä½œè€…åœ¨ä»¥ä¸‹ä¸‰ä¸ªæ•°æ®é›†ä¸Šæµ‹è¯•è‡ªå·±çš„æ¨¡åž‹ï¼šPenn TreeBank (PTB) POS tagging, CoNLL 2000 chunking, and CoNLL 2003 named entity tagging.æ•°æ®é›†ä¿¡æ¯å±•ç¤ºå¦‚ä¸‹ï¼š Featuresä½œè€…ä»Žä¸‰ä¸ªæ•°æ®é›†ä¸­æå–å‡ºå…¶å…¬å…±ç‰¹å¾ã€‚ç‰¹å¾å¯ä»¥åˆ†ä¸ºæ‹¼å†™ç‰¹å¾å’Œä¸Šä¸‹æ–‡ç‰¹å¾ã€‚æœ€ç»ˆï¼Œä½œè€…å¯¹äºŽPOSï¼ˆè¯æ€§æ ‡æ³¨ï¼‰ã€chunkingï¼ˆç»„å—ï¼‰å’ŒNERï¼ˆå‘½åå®žä½“è¯†åˆ«ï¼‰åˆ†åˆ«æå–401Kï¼Œ76Kå’Œ341Kä¸ªç‰¹å¾ã€‚ spelling featuresï¼ˆæ‹¼å†™ç‰¹å¾ï¼‰é™¤äº†å°å†™å­—æ¯ç‰¹å¾ä¹‹å¤–ï¼Œæˆ‘ä»¬æå–ç»™å®šå•è¯çš„ä»¥ä¸‹ç‰¹å¾ã€‚ context featursï¼ˆä¸Šä¸‹æ–‡ç‰¹å¾ï¼‰å¯¹äºŽå•è¯ç‰¹å¾ï¼Œä½œè€…ä½¿ç”¨unigramå’Œbi-gramsç‰¹å¾ã€‚å¯¹äºŽåœ¨CoNLL2000æ•°æ®é›†ä¸­çš„POSç‰¹å¾å’Œåœ¨CoNLL2003æ•°æ®é›†ä¸­çš„ POS &amp; CHUNKç‰¹å¾ï¼Œä½œè€…ä½¿ç”¨äº†unigramï¼Œbi-gramå’Œtri-gramç‰¹å¾ã€‚ è¯å‘é‡è¯å‘é‡åœ¨æ”¹è¿›åºåˆ—æ ‡æ³¨ä»»åŠ¡çš„è¡¨çŽ°æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œæˆ‘ä»¬ä½¿ç”¨ 130K è¯æ±‡å¹¶ä¸”æ¯ä¸ªè¯æ±‡çš„è¯å‘é‡ç»´åº¦æ˜¯ 50 ç»´ã€‚æˆ‘ä»¬å°† one-hot-encodingè¯è¡¨ç¤ºæ›¿æ¢æ¯ä¸ªè¯å¯¹åº”çš„è¯å‘é‡ã€‚ Features connection tricksæˆ‘ä»¬å¯ä»¥å°†æ‹¼å†™å’Œä¸Šä¸‹æ–‡ç‰¹å¾ä¸Žå•è¯ç‰¹å¾ä¸€æ ·å¯¹å¾…ã€‚è¿™æ ·ç½‘ç»œçš„è¾“å…¥åŒ…æ‹¬å•è¯ï¼Œå•è¯çš„æ‹¼å†™å’Œä¸Šä¸‹æ–‡ç‰¹å¾ã€‚ç„¶è€Œï¼Œ==æˆ‘ä»¬å‘çŽ°å°†æ‹¼å†™å’Œä¸Šä¸‹æ–‡ç‰¹å¾ä¸Žè¾“å‡ºç›´æŽ¥è¿žæŽ¥å¯ä»¥åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ï¼ŒåŒæ—¶ä¹Ÿèƒ½ä¿æŒæ ‡æ³¨çš„å‡†ç¡®çŽ‡ï¼Œ==å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š æˆ‘ä»¬æ³¨æ„åˆ°ï¼Œè¿™ç§ç‰¹å¾çš„ä½¿ç”¨ä¸Žä½¿ç”¨çš„æœ€å¤§ç†µç‰¹å¾ç±»ä¼¼ã€‚åŒºåˆ«åœ¨äºŽé‡‡ç”¨ç‰¹å¾ä¸‰åˆ—æŠ€æœ¯å¯èƒ½ä¼šå‘ç”Ÿç‰¹å¾å†²çªã€‚ç”±äºŽåºåˆ—æ ‡æ³¨æ•°æ®é›†ä¸­çš„è¾“å‡ºæ ‡ç­¾å°äºŽè¯­è¨€æ¨¡åž‹ï¼ˆé€šå¸¸ä¸ºæ•°åä¸‡ï¼‰ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥åœ¨ç‰¹å¾å’Œè¾“å‡ºä¹‹é—´å»ºç«‹å®Œæ•´çš„è¿žæŽ¥ï¼Œä»¥é¿å…æ½œåœ¨çš„ç‰¹å¾å†²çªã€‚ ç»“æžœåœ¨ç›¸åŒçš„æ•°æ®é›†ä¸Šåˆ†åˆ«è®­ç»ƒLSTMï¼ŒBI-LSTMï¼ŒCRFï¼ŒLSTM-CRFå’ŒBI-LSTM-CRFæ¨¡åž‹ï¼Œå¹¶ä¸”é‡‡ç”¨ä¸¤ç§æ–¹å¼åˆå§‹åŒ–word embeddingï¼šéšæœºå’ŒSennaæ–¹å¼ã€‚æ¨¡åž‹çš„è®­ç»ƒé€ŸçŽ‡ä¸º0.1ï¼Œéšè—å±‚æ•°é‡ä¸º300.ä¸åŒæ¨¡åž‹åœ¨ä¸åŒword embeddingä¸‹çš„ç»“æžœå¦‚è¡¨2æ‰€ç¤ºï¼ŒåŒæ—¶åˆ—å‡ºäº†ä¹‹å‰æœ€å¥½æ¨¡åž‹Cov-CRFã€‚ ä¸ŽCov-CRFæ¯”è¾ƒ å®žéªŒä¸­è®¾ç½®äº†3ä¸ªåŸºå‡†æ¨¡åž‹ï¼Œåˆ†åˆ«ä¸ºLSTMã€BI-LSTMå’ŒCRFï¼Œç»“æžœä¸­LSTMåœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸­æ•ˆæžœæœ€å·®ï¼ŒBI-LSTMè·ŸCRFåœ¨POSå’Œchunkingä¸­æ•ˆæžœæŽ¥è¿‘ï¼Œä½†æ˜¯åœ¨NERä¸­åŽè€…è¦ä¼˜äºŽå‰è€…ã€‚æœ‰è¶£çš„æ˜¯è¡¨çŽ°æœ€å¥½çš„æ¨¡åž‹BI-LSTM-CRFç›¸å¯¹äºŽCov-CRFæ¥è¯´å¯¹Senna embeddingçš„ä¾èµ–ç¨‹åº¦æ›´å°ã€‚ (robustness)æ¨¡åž‹é²æ£’æ€§ ä¸ºéªŒè¯æ¨¡åž‹çš„é²æ£’æ€§ï¼Œå¯¹ä¸åŒæ¨¡åž‹åªé‡‡ç”¨word featureç‰¹å¾è¿›è¡Œè®­ç»ƒï¼Œè®­ç»ƒç»“æžœå¦‚è¡¨3ï¼Œæ‹¬å·ä¸­æ•°å­—è¡¨ç¤ºç›¸æ¯”äºŽå…¨éƒ¨ç‰¹å¾ï¼Œæ¨¡åž‹çš„ç»“æžœä¸‹é™æ•°å€¼ã€‚ ä¸Žå…¶ä»–ç³»ç»Ÿçš„æ¯”è¾ƒ è¿™é‡Œå°±ä¸è´´å›¾äº†ï¼Œæ€»ä¹‹å°±æ˜¯é˜è¿°ä½œè€…è‡ªå·±æ¨¡åž‹å¥½ã€‚ ç»“è®ºæ€»ä¹‹ä½œè€…çš„æ¨¡åž‹æ˜¯åŸºäºŽä¹‹å‰æ¨¡åž‹çš„ä¸€äº›æ”¹è¿›ï¼Œä¸»è¦è¿ç”¨äº†IBI-LSTMå’ŒCRFçš„ç»“åˆã€‚ è®ºæ–‡ä¸‹è½½åœ°å€]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>NER</tag>
        <tag>LSTM</tag>
        <tag>BI-LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CN-DBpedia A Never-Ending Chinese Knowledge Extraction SystemCN-DBpedia System]]></title>
    <url>%2Fpost%2Fessay%2F</url>
    <content type="text"><![CDATA[å‰è¨€ æœ¬ç¯‡è®ºæ–‡ä¸º2016å¹´çš„ä¸€ç¯‡è®ºæ–‡ï¼Œä¸»è¦ä»‹ç»äº†ä½œè€…æž„å»ºä¸­æ–‡çŸ¥è¯†å›¾è°±æ‰€é‡åˆ°çš„ä¸€äº›é—®é¢˜å’Œè§£å†³æ–¹æ³•ã€‚ challenge å¦‚ä½•é™ä½ŽäººåŠ›æˆæœ¬ï¼Ÿ å¦‚ä½•ä¿æŒçŸ¥è¯†åº“çš„æ–°é²œåº¦ï¼Ÿ è´¡çŒ® åœ¨æž„å»ºä¸­æ–‡çŸ¥è¯†åº“ä¸­é™ä½Žäº†äººåŠ›æˆæœ¬ï¼š é‡å¤åˆ©ç”¨å·²ç»å­˜åœ¨çš„æœ¬ä½“è®º æå‡ºäº†ä¸€ä¸ªä¸ç”¨äººå·¥ç›‘ç£çš„ç«¯åˆ°ç«¯çš„æ·±åº¦å­¦ä¹ æ¨¡åž‹ æå‡ºäº†ä¸€ä¸ªæ™ºèƒ½ä¸»åŠ¨æ›´æ–°ç­–ç•¥ ç³»ç»Ÿç»“æž„ æé«˜çŸ¥è¯†åº“è´¨é‡ï¼š Normalizationï¼š normalize the attributes and values Enrichmentï¼šreuse the ontology Correctionï¼štwo steps error detection: rule-based detection based on user feedbacks error correction crowd-sourcing é™ä½ŽäººåŠ›æˆæœ¬è¿™éƒ¨åˆ†ä½œè€…é‡‡ç”¨äº†ä¸¤ç§æ–¹æ³•ï¼š é‡å¤åˆ©ç”¨å·²ç»å­˜åœ¨åœ¨çŸ¥è¯†åº“çš„æœ¬ä½“è®ºå’Œç±»åž‹åŒ–çš„ä¸­æ–‡å®žä½“ æž„å»ºä¸€ä¸ªç«¯åˆ°ç«¯æå–å™¨ Cross-Lingual Entity Typingï¼ˆè·¨è¯­è¨€çš„å®žä½“ç±»åž‹ï¼‰ ç¬¬ä¸€æ­¥æ˜¯é€šè¿‡ç”¨è‹±æ–‡DBpediaç±»åž‹æ¥ç±»åž‹åŒ–ä¸­æ–‡å®žä½“ã€‚ä¸ºäº†è¾¾åˆ°è¿™ä¸ªç›®çš„ï¼Œä½œè€…æå‡ºäº†å¦‚ä¸‹ç³»ç»Ÿï¼šç³»ç»Ÿå»ºç«‹äº†ç›‘ç£å±‚æ¬¡åˆ†ç±»æ¨¡åž‹ï¼Œç³»ç»Ÿè¾“å…¥ä¸ºæ²¡æœ‰æ ‡è®°ç±»åž‹çš„ä¸­æ–‡å®žä½“ï¼Œè¾“å‡ºä¸ºåœ¨DBä¸­æ‰€æœ‰æœ‰æ•ˆçš„è‹±æ–‡ç±»åž‹ã€‚ä½œè€…å°†ä¸­æ–‡å®žä½“ä¸Žå…±äº«ç›¸åŒä¸­æ–‡æ ‡ç­¾åç§°çš„è‹±è¯­å®žä½“é…å¯¹ï¼Œè¿™æ ·ä¸­æ–‡å®žä½“ä»¥åŠé…å¯¹è‹±è¯­å®žä½“çš„ç±»åž‹è‡ªç„¶æ˜¯æ ‡è®°æ ·æœ¬ã€‚ ç”¨ä¸Šè¿°æ–¹æ³•å¾—åˆ°çš„è®­ç»ƒé›†å¯èƒ½å‡ºçŽ°ä¸‹é¢ä¸€äº›é—®é¢˜ï¼š è‹±æ–‡DBpediaå®žä½“ç±»åž‹åœ¨è®¸å¤šæƒ…å†µä¸‹å¯èƒ½ä¸å®Œå…¨ï¼› è‹±æ–‡DBpediaå®žä½“ç±»åž‹åœ¨è®¸å¤šæƒ…å†µä¸‹å¯èƒ½æ˜¯é”™è¯¯çš„ï¼› ä¸­è‹±æ–‡é“¾æŽ¥å¯èƒ½å‡ºé”™ï¼› ä¸­æ–‡å®žä½“çš„ç‰¹å¾é€šå¸¸ä¸å®Œæ•´ã€‚ ä¸ºäº†è§£å†³ä»¥ä¸Šé—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸¤ç§æ–¹æ³•ï¼š å®Œå–„è‹±æ–‡DBpediaå®žä½“ç±»åž‹ï¼› è®¾è®¡ä¸€ä¸ªè¿‡æ»¤æ­¥éª¤æ¥å‰”é™¤é”™è¯¯æ ·æœ¬ã€‚ infobox completion Infobox completion is a task to extract object for a given pair of entity and predicate from encyclopedia articles. ä½œè€…å»ºæ¨¡äº†ä¸€ä¸ªseq2seqæ¨¡åž‹ï¼Œè¾“å…¥ä¸ºåŒ…å«tokensçš„è‡ªç„¶è¯­è¨€å¥å­ï¼Œè¾“å‡ºä¸ºæ¯ä¸ªtokençš„æ ‡ç­¾ã€‚å¯¹äºŽæ ‡ç­¾ä¸º0æˆ–1ã€‚ å¯¹äºŽå»ºç«‹ä¸€ä¸ªæœ‰æ•ˆçš„æå–å™¨æœ‰ä»¥ä¸‹ä¸¤ä¸ªå…³é”®ï¼š å¦‚ä½•æž„å»ºè®­ç»ƒé›†ï¼šä½œè€…é‡‡ç”¨è¿œç¨‹ç›‘ç£æ–¹æ³•ï¼ˆåˆ©ç”¨Wikipediaï¼‰ å¦‚ä½•é€‰å–æœŸæœ›çš„æå–æ¨¡åž‹ï¼šLSTM-RNNï¼Œå¦‚å›¾æ‰€ç¤º çŸ¥è¯†åº“æ›´æ–°ä½œè€…é‡‡ç”¨åŠ¨æ€æ›´æ–°ï¼šè¯†åˆ«æ–°å®žä½“æˆ–å¯èƒ½åŒ…å«æ–°äº‹å®žçš„æ—§å®žä½“ ä½œè€…æ ¹æ®ä»¥ä¸‹ä¸¤æ–¹é¢æ¥è¾¨åˆ«è¿™äº›å®žä½“ï¼š è¿‘æœŸçƒ­ç‚¹æ–°é—»ä¸­æåŠçš„å®žä½“ åœ¨æœç´¢å¼•æ“Žçš„æµè¡Œæœç´¢å…³é”®å­—æˆ–å…¶ä»–æµè¡Œç½‘é¡µä¸­æåˆ°çš„å®žä½“ å¯¹äºŽå¦‚ä½•ä»Žæ–°é—»æ ‡é¢˜å’Œæœç´ æŒ‡ä»¤ä¸­æå–å®žä½“åå­—ï¼Œä½œè€…é‡‡ç”¨ç®€å•çš„è¯åˆ†å‰²æ–¹æ³•ï¼Œä»Žç™¾ç§‘å…¨ä¹¦ä¸­åˆ¤æ–­å…¶æ˜¯å¦ä¸ºå®žä½“ï¼Œå¹¶æå‡ºIDFå€¼ä½Žçš„åˆ†å‰²å­ä¸²ã€‚ ç»Ÿè®¡æ•°æ® è®ºæ–‡ä¸‹è½½é“¾æŽ¥]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>çŸ¥è¯†å›¾è°±</tag>
        <tag>æœºå™¨å­¦ä¹ </tag>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ontology reasoning with deep neural networks]]></title>
    <url>%2Fpost%2Fessay%2F</url>
    <content type="text"><![CDATA[Ontology reasoning with deep neural networksï¼ˆåŸºäºŽæ·±åº¦ç¥žç»ç½‘ç»œçš„æœ¬ä½“æŽ¨ç†ï¼‰å‰è¨€ æœ¬è®ºæ–‡å±žäºŽçŸ¥è¯†å›¾è°±çš„ä¸‹æ¸¸ä»»åŠ¡ï¼Œä¹Ÿå°±æ˜¯çŸ¥è¯†å›¾è°±çš„åº”ç”¨çš„ä¸€ä¸ªä¾‹å­ã€‚è¿™ç¯‡è®ºæ–‡çš„æ–¹æ³•æ ¹æ®ä½œè€…æè¿°RRNæ˜¯ç¬¬ä¸€ä¸ªåŸºäºŽæ·±åº¦å­¦ä¹ çš„å…¨é¢æœ¬ä½“æŽ¨ç†æ–¹æ³•ã€‚ ç›®æ ‡èŽ·å¾—ä¸€ä¸ªå¯ä»¥åœ¨ä¸åŒçš„åœºæ™¯è¿›è¡Œæœ‰æ•ˆæŽ¨ç†çš„æ¨¡ é—®é¢˜æè¿°åŸºäºŽæœºå™¨å­¦ä¹ çš„æŽ¨ç†æ–‡ç« é€šå¸¸å‡è®¾äº†ä¸€ä¸ªç‰¹å®šçš„åº”ç”¨æ¡ˆä¾‹ï¼šè‡ªç„¶è¯­è¨€æˆ–è§†è§‰è¾“å…¥çš„æŽ¨ç†ã€‚ä½œè€…é‡‡ç”¨ä¸€ä¸ªä¸åŒçš„æ–¹æ³•ï¼šå°†æ­£å¼çš„æŽ¨ç†é—®é¢˜ä½œä¸ºèµ·ç‚¹ã€‚å¯¹äºŽç‰¹å®šçš„é—®é¢˜é€‰æ‹©ï¼šé€‰æ‹©ä¸€ç§åœ¨è¡¨çŽ°åŠ›ä¸Žå¦ä¸€æ–¹é¢å¤æ‚æ€§ä¹‹é—´å–å¾—é€‚å½“å¹³è¡¡çš„æ–¹æ³•é€šå¸¸æ˜¯æ˜Žæ™ºçš„ã€‚OWL RLæœ¬ä½“æŽ¨ç†æ˜¯æŒ‡ä¸€ç§å¸¸è§çš„åœºæ™¯ï¼Œåœ¨è¿™ç§åœºæ™¯ä¸­ï¼Œç”¨äºŽæŽ¨ç†çš„æŽ¨ç†è§„åˆ™ï¼ˆåœ¨æ­¤ä¸Šä¸‹æ–‡ä¸­ç§°ä¸ºæœ¬ä½“ï¼‰ä¸Žæˆ‘ä»¬å¯»æ±‚æŽ¨ç†çš„äº‹å®žä¿¡æ¯ä¸€èµ·æŒ‡å®šã€‚ æœ¬ä½“æŽ¨ç†æ˜¯ä¸€ç§éžå¸¸çµæ´»çš„å·¥å…·ï¼Œå®ƒå…è®¸å¯¹å¤§é‡ä¸åŒçš„åœºæ™¯è¿›è¡Œå»ºæ¨¡ï¼Œå› æ­¤æ»¡è¶³äº†æˆ‘ä»¬å¯¹é€‚ç”¨äºŽå„ç§åº”ç”¨çš„ç³»ç»Ÿçš„éœ€æ±‚ã€‚==é¦–å…ˆå¼•å‡ºäº†ä»€ä¹ˆæ˜¯æœ¬è´¨æŽ¨ç†ï¼Œç„¶åŽè¿›ä¸€æ­¥é˜è¿°ä¸ºä»€ä¹ˆè¦ç”¨æœºå™¨å­¦ä¹ == ä»Šå¤©ç”¨äºŽæŽ¨ç†çš„å¤§å¤šæ•°KRRå½¢å¼éƒ½æ¤æ ¹äºŽç¬¦å·é€»è¾‘,è¿™äº›æ–¹æ³•åœ¨å®žè·µä¸­ä¼šé‡åˆ°è®¸å¤šé—®é¢˜ï¼šä¾‹å¦‚å¤„ç†ä¸å®Œæ•´ï¼Œå†²çªæˆ–ä¸ç¡®å®šæ•°æ®çš„å›°éš¾æœºå™¨å­¦ä¹ æ¨¡åž‹é€šå¸¸å…·æœ‰é«˜åº¦å¯æ‰©å±•æ€§ï¼Œæ›´èƒ½æŠµæŠ—æ•°æ®ä¸­çš„å¹²æ‰°ï¼Œå¹¶ä¸”å³ä½¿æ‰€æä¾›çš„å½¢å¼æ˜¯é”™è¯¯çš„ä¹Ÿèƒ½å¤Ÿæä¾›é¢„æµ‹ã€‚ ä½œè€…çš„ç›®æ ‡æ˜¯é€šè¿‡é‡‡ç”¨å°–ç«¯çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œç›®æ ‡æ˜¯åœ¨è¿‘ä¼¼äºŽå½¢å¼æ–¹æ³•çš„é«˜åº¦æœŸæœ›ï¼ˆç†è®ºï¼‰å±žæ€§å’Œå¦ä¸€æ–¹é¢åˆ©ç”¨æœºå™¨å­¦ä¹ çš„ç¨³å¥æ€§ä¹‹é—´ç®¡ç†å¹³è¡¡è¡Œä¸ºã€‚ å¯¹äºŽç”¨äºŽæŽ¨ç†çš„çŸ¥è¯†å›¾è°±ï¼šä½œè€…é‡‡ç”¨çš„æ˜¯ç”±ä¸ªä½“ã€ç±»å’ŒäºŒå…ƒå…³ç³»ç»„æˆçš„ä¿¡æ¯æž„æˆï¼Œå…¶ä¸­ä¸ªä½“å¯¹åº”äºŽé¡¶ç‚¹ï¼Œå…³ç³»å¯¹åº”äºŽè¢«æ ‡è®°çš„æœ‰å‘è¾¹ç¼˜ï¼Œç±»å¯¹åº”äºŽäºŒè¿›åˆ¶é¡¶ç‚¹æ ‡ç­¾ã€‚å…³ç³»æ˜¯ä¸»ä½“å’Œå®¢ä½“ä¹‹é—´çš„å…³ç³»æˆ–è€…ä¸ªäººå’Œç±»ä¹‹é—´çš„å…³ç³»ã€‚è¿™ä¸Žå…³ç³»å­¦ä¹ ä¸åŒï¼šåœ¨å…³ç³»å­¦ä¹ çš„èƒŒæ™¯ä¸‹ï¼ŒçŸ¥è¯†å›¾é€šå¸¸é€šè¿‡å°†ç±»è§†ä¸ºä¸ªäººä»¥åŠå°†æˆå‘˜è§†ä¸ºæ™®é€šå…³ç³»æ¥ç®€åŒ–ã€‚ç„¶è€Œï¼Œå°±ä½œè€…çš„ç›®çš„è€Œè¨€ï¼Œæ˜Žç¡®åŒºåˆ†ç±»å’Œå…³ç³»æ˜¯å¾ˆé‡è¦çš„ï¼Œå› ä¸ºåœ¨ç”¨äºŽæŽ¨ç†çš„çŸ¥è¯†å›¾è°±ä¸­ç±»å’Œå…³ç³»å¯èƒ½ä¸åŒã€‚ æ¨¡åž‹æ€»è§ˆæ•´ä¸ªæ¨¡åž‹æ˜¯ä»¥RRNä¸ºåŸºç¡€è¿›è¡Œæž„å»ºçš„ï¼Œæ¯ä¸ªRRNéƒ½é’ˆå¯¹ç‰¹å®šçš„æœ¬ä½“è¿›è¡Œè®­ç»ƒã€‚å½“è®­ç»ƒæ¨¡åž‹åº”ç”¨äºŽä¸€ç»„ç‰¹å®šçš„äº‹å®žæ—¶ï¼Œå®ƒåˆ†ä¸ºå¦‚ä¸‹ä¸¤ä¸ªæ­¥éª¤ï¼š å®ƒä¸ºæ‰€æœ‰çš„æ­¥éª¤ç”ŸæˆçŸ¢é‡è¡¨ç¤ºï¼Œä¹Ÿå°±æ˜¯åµŒå…¥åœ¨æ‰€è€ƒè™‘æ•°æ®ä¸­å‡ºçŽ°çš„ä¸ªä½“ã€‚ å®ƒä»…åŸºäºŽè¿™äº›ç”Ÿæˆå‘é‡è®¡ç®—æŸ¥è¯¢é¢„æµ‹ åœ¨å›¾ä¸­ï¼Œ aä¸­å®ƒè€ƒè™‘ä¸€ä¸ªäº‹å®žä¸‰å…ƒç»„ï¼Œå¹¶æ ¹æ®æ•°æ®é›†é‡å¤å¤šæ¬¡ã€‚ bä¸­å®ƒæ¯è¯»å–ä¸€ä¸ªäº‹å®žå°±èŽ·å–ä¸‰å…ƒç»„ä¸­çš„ä¸ªä½“æ½œå…¥ï¼Œå¹¶å°†ä»–ä»¬çš„åé¦ˆé€å…¥æ›´æ–°å±‚ï¼Œè¯¥å±‚äº§ç”Ÿå·²æä¾›çš„åµŒå…¥çš„æ›´æ–°ç‰ˆæœ¬ï¼Œç„¶åŽå°†å…¶å­˜å‚¨åœ¨å‰ä¸€ä¸ªç‰ˆæœ¬çš„ä½ç½®ã€‚ cä¸­ä»Žéšæœºç”Ÿæˆçš„å‘é‡å¼€å§‹ï¼Œé€æ­¥æ›´æ–°åµŒå…¥ï¼Œä»¥ä¾¿å¯¹å…³äºŽå®ƒä»¬æ‰€ä»£è¡¨çš„ä¸ªä½“çš„äº‹å®žå’ŒæŽ¨è®ºè¿›è¡Œç¼–ç ã€‚ è¯„ä¼°ä½œè€…åœ¨å››ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šè®­ç»ƒå’Œè¯„ä¼°äº†RRNï¼Œå…¶ä¸­ä¸¤ä¸ªæ˜¯äººå·¥ç”Ÿæˆçš„çŽ©å…·æ•°æ®é›†ï¼Œä¸¤ä¸ªæ˜¯ä»ŽçŽ°å®žä¸–ç•Œçš„æ•°æ®åº“ä¸­æå–çš„ã€‚è¿™æ ·åšçš„åŽŸå› ï¼š çŽ©å…·é—®é¢˜å…·æœ‰å¾ˆå¤§çš„ä¼˜åŠ¿ï¼Œå³å¾ˆæ˜Žæ˜¾æŸäº›æŽ¨è®ºæ˜¯å¤šä¹ˆå›°éš¾ï¼Œä»Žè€Œä¸ºæˆ‘ä»¬æä¾›äº†å¯¹æ¨¡åž‹èƒ½åŠ›çš„ç›¸å½“å¥½çš„å°è±¡ã€‚ åœ¨çŽ°å®žçŽ¯å¢ƒä¸­è¯„ä¼°æ–¹æ³•å½“ç„¶æ˜¯æ€§èƒ½ä¸å¯æˆ–ç¼ºçš„è¡¡é‡æ ‡å‡† ä½œè€…ä¸ºäº†è¯„ä¼°çœŸå®žä¸–ç•Œæ•°æ®çš„RRNæ¨¡åž‹ï¼Œè¿˜ä»Žä»Žä¸¤ä¸ªè‘—åçš„çŸ¥è¯†åº“DBpediaå’ŒClarosä¸­æå–äº†æ•°æ®é›†ã€‚ ç»“æžœ RRNèƒ½å¤Ÿæœ‰æ•ˆåœ°ç¼–ç æä¾›çš„å…³äºŽç±»å’Œå…³ç³»çš„äº‹å®ž å¯¹äºŽå…³ç³»çš„æŽ¨ç†ï¼Œå¯ä»¥çœ‹åˆ°DBpediaçš„å‡†ç¡®åº¦ç•¥ä½ŽäºŽ98.9ï¼…ï¼Œè€Œå…¶ä»–æ•°æ®é›†ä¸­çš„å¯å¯¼å‡ºå…³ç³»åœ¨æ‰€æœ‰æƒ…å†µä¸­è‡³å°‘99.6ï¼…è¢«æ­£ç¡®é¢„æµ‹ã€‚ å¯ä»¥é¢„æµ‹è¯¥æ¨¡åž‹åœ¨é¢„æµ‹å¯æŽ¨æ–­ç±»åˆ«æ–¹é¢æ¯”åœ¨å…³ç³»æ–¹é¢è¡¨çŽ°æ›´å¥½ï¼Œå› ä¸ºå¤§å¤šæ•°è¿™äº›éƒ½æ˜¯ä»…ä¾èµ–äºŽå•ä¸ªä¸‰å…ƒç»„çš„æŽ¨è®ºã€‚ ä¸ºäº†è¯„ä¼°ä½œè€…æå‡ºçš„KRRæ–¹æ³•å¸¸å¸¸é‡åˆ°çš„é—®é¢˜ï¼Œä½œè€…è¿›è¡Œäº†å¦‚ä¸‹å®žéªŒï¼š å¯¹äºŽç¼ºå°‘ä¿¡æ¯çš„é—®é¢˜ï¼Œä½œè€…éšæœºåˆ é™¤äº†ä¸€ä¸ªæ— æ³•é€šè¿‡æ¯ä¸ªæ ·æœ¬çš„ç¬¦å·æŽ¨ç†æŽ¨æ–­å‡ºçš„äº‹å®žï¼Œå¹¶æ£€æŸ¥æ¨¡åž‹æ˜¯å¦èƒ½å¤Ÿæ­£ç¡®åœ°é‡å»ºå®ƒã€‚ç»“æžœï¼šå¯¹äºŽDBpediaæ¥è¯´ï¼Œ33.8ï¼…çš„å¤±è¸ªä¸‰å…ƒç»„å°±æ˜¯è¿™ç§æƒ…å†µï¼Œè€Œå¯¹äºŽClarosæ¥è¯´ï¼Œ38.4ï¼…è¢«æ­£ç¡®é¢„æµ‹ å¯¹äºŽå†²çªçš„é—®é¢˜ï¼Œä½œè€…é€šè¿‡åœ¨æ¯ä¸ªæµ‹è¯•æ ·æœ¬ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªäº‹å®žæ¥æµ‹è¯•æ¨¡åž‹è§£å†³å†²çªçš„èƒ½åŠ›ï¼Œå¹¶æ·»åŠ ç›¸åŒçš„å¦å®šç‰ˆæœ¬ä½œä¸ºå¦ä¸€ä¸ªäº‹å®žã€‚å¯¹äºŽDBpediaï¼ŒRRNæ­£ç¡®è§£å†³äº†88.4ï¼…çš„å¼•å…¥å†²çªï¼Œè€Œå¯¹äºŽClarosï¼Œå®ƒç”šè‡³è¾¾åˆ°äº†96.2ï¼…ã€‚ç„¶è€Œï¼Œæœ€é‡è¦çš„æ˜¯ï¼Œå¯¹äºŽä»»ä½•ä¸€ä¸ªæŸåçš„æ•°æ®é›†ï¼Œä¹‹å‰æŠ¥å‘Šçš„æ€»ç²¾åº¦éƒ½æ²¡æœ‰ä¸‹é™è¶…è¿‡0.9ã€‚ æ‰€æœ‰RRNçš„æŸ¥è¯¢é¢„æµ‹éƒ½å®Œå…¨åŸºäºŽå®ƒä¸ºå„ä¸ªæ•°æ®é›†ä¸­çš„ä¸ªä½“ç”Ÿæˆçš„åµŒå…¥ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆä»”ç»†ç ”ç©¶è¿™æ ·ä¸€ç»„åµŒå…¥å‘é‡æ˜¯æœ‰ç›Šçš„ã€‚ æ€è€ƒæœ¬è®ºæ–‡å±žäºŽçŸ¥è¯†å›¾è°±çš„ä¸‹æ¸¸ä»»åŠ¡ï¼Œä¹Ÿå°±æ˜¯çŸ¥è¯†å›¾è°±çš„åº”ç”¨çš„ä¸€ä¸ªä¾‹å­ã€‚è¿™ç¯‡è®ºæ–‡çš„æ–¹æ³•æ ¹æ®ä½œè€…æè¿°RRNæ˜¯ç¬¬ä¸€ä¸ªåŸºäºŽæ·±åº¦å­¦ä¹ çš„å…¨é¢æœ¬ä½“æŽ¨ç†æ–¹æ³•ã€‚ä½†æ˜¯å…·ä½“çš„æ“ä½œæ–¹æ³•è®ºæ–‡ä¸­å†™çš„æ¯”è¾ƒæ¸…æ™°ï¼Œæ„Ÿè§‰è‡ªå·±æ˜¯ç†è§£äº†ã€‚é‡ç‚¹å°±æ˜¯å¯¹äºŽä¸ªä½“çš„åµŒå…¥è¡¨ç¤ºï¼Œå¦‚æžœç±»æ¯”çš„è¯å°±æ˜¯è¯å‘é‡ï¼Œä½œè€…é€šè¿‡ä¸æ–­çš„å¤„ç†æ›´æ–°è¿™ä¸ªè¯å‘é‡ï¼Œæœ€åŽé€šè¿‡æ‰€èŽ·çš„è¯å‘é‡è¿›è¡ŒæŽ¨ç†ã€‚å¹¶ä¸”ä»Žè¿™ç¯‡æ–‡ç« ä¸­å¯ä»¥çœ‹åˆ°ä½œè€…ä½¿ç”¨çš„çŸ¥è¯†å›¾è°±å’Œæˆ‘ä¹‹å‰åœ¨å¼„çš„å…³ç³»ä¸‰å…ƒç»„æœ‰æ‰€åŒºåˆ«ã€‚ è®ºæ–‡ä¸‹è½½é“¾æŽ¥]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>çŸ¥è¯†å›¾è°±</tag>
        <tag>æ·±åº¦å­¦ä¹ </tag>
        <tag>Ontology</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[åˆæ¬¡è§é¢ï¼Œä½ å¥½NYSDYï¼]]></title>
    <url>%2Fpost%2Fessay%2F</url>
    <content type="text"></content>
  </entry>
</search>
