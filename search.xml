<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[《Interaction Embeddings for Prediction and Explanation》阅读笔记]]></title>
    <url>%2Fpost%2FInteraction%20Embeddings%20for%20Prediction%20and%20Explanation%2F</url>
    <content type="text"><![CDATA[论文下载地址，此论文主要提出了实体和关系的交互作用对于知识图谱嵌入的影响，和提出了新的嵌入评估方案 - 搜索预测解释。 论文贡献 提出了CrossE，一种通过学习一个交互矩阵来给实体和关系的交互建模的新型知识图谱嵌入。 我们使用三个基准数据集评估CrossE与链接预测任务上的各种其他KGE的比较，并显示CrossE在具有适度参数大小的复杂且更具挑战性的数据集上实现最先进的结果。 我们提出了一种新的嵌入评估方案 - 搜索预测解释，并表明CrossE能够生成比其他方法更可靠的解释。 这表明交互嵌入更能在不同的三元组环境中捕捉实体和关系之间的相似性。 介绍给定知识图谱和一个要预测的三元组的头实体和关系，在预测尾实体的过程中，头实体和关系之间是有交叉交互的crossover interaction, 即关系决定了在预测的过程中哪些头实体的信息是有用的，而对预测有用的头实体的信息又决定了采用什么逻辑去推理出尾实体，文中通过一个模拟的知识图谱进行了说明如下图所示： 参考链接 http://blog.openkg.cn/%E8%AE%BA%E6%96%87%E6%B5%85%E5%B0%9D-interaction-embeddings-for-prediction-and-explanation/ 相关工作论文中在这部分对KGE（Knowledge graph embedding）进行了分类总结： KGEs with general embeddings KGEs with multiple embeddings. KGEs that utilize extra information. 这部分总结中对大量的方法进行描述，可以作为背景知识进行阅读。 CrossE模型基于对头实体和关系之间交叉交互的观察，本文提出了一个新的知识图谱表示学习模型CrossE. CrossE除了学习实体和关系的向量表示，同时还学习了一个交互矩阵C，C与关系相关，并且用于生成实体和关系经过交互之后的向量表示，所以在CrossE中实体和关系不仅仅有通用向量表示，同时还有很多交互向量表示。CrossE核心想法如下图： 目标函数粉四步生成： Interaction Embedding for Entities：根据头实体向量和交互矩阵（以关系确定的）来确定头实体的交互表示。 Interaction Embedding for Relations：根据头实体的交互表示和关系作用生成关系的交互表示 Combination Operator：将头实体的交互表示和关系的交互表示相结合，并进行非线性处理（tanh） Similarity Operator：计算结合后表示和尾实体表示之间的相似度。 最后分数函数： 损失函数：（这里就是一个交叉熵函数，但是写的有问题f(x)项应该在括号外） 对于预测的解释这部分作者描述了如何生成预测三元组的解释，并介绍了基于嵌入的路径搜索算法，主要步骤如下： Search for similar relations：修剪掉不合理路径 Search for paths between h and t：作者定义了6种路径（班汉一个或两个关系） Search similar entities：捕获实体之间的相似性方面越有能力，就越有可能存在（hs，r，ts） : Search for similar structures as supports：我们只将知识图中至少有一个支持的路径视为解释。 实验数据集 链接预测 从实验结果中我们可以看出，CrossE实现了较好的链接预测结果。我们去除CrossE中的头实体和关系的交叉交互，构造了模型 CrossES，CrossE 和 CrossES 的比较说明了交叉交互的有效性。 生成解释我们提出了一种基于相似结构通过知识图谱的表示学习结果生成预测结果解释的方法，并提出了两种衡量解释结果的指标，AvgSupport和Recall。Recall是指模型能给出解释的预测结果的占比，其介于0和1之间且值越大越好；AvgSupport是模型能给出解释的预测结果的平均support个数，AvgSupport是一个大于0的数且越大越好。可解释的评估结果如下： 链接预测和可解释的实验从两个不同的方面评估了知识图谱表示学习的效果，同时也说明了链接预测的准确性和可解释性没有必然联系，链接预测效果好的模型并不一定能够更好地提供解释，反之亦然。]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
        <tag>知识图谱嵌入</tag>
        <tag>知识图谱推理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Differentiable Learning of Logical Rules for Knowledge Base Reasoning》阅读笔记]]></title>
    <url>%2Fpost%2FDifferentiable%20Learning%20of%20Logical%20Rules%20for%20Knowledge%20Base%20Reasoning%2F</url>
    <content type="text"><![CDATA[论文下载地址，本文研究用于于知识图谱推理的学习概率一阶逻辑规则的问题，提出了Neural Logic Programming（Neural-LP）框架，它结合了端到端可微分模型中一阶逻辑规则的参数和结构学习。为了在可微分的框架中同时学习参数和结构，作者设计了一个具有注意机制和记忆的神经控制器系统，以学习顺序组成TensorLog使用的原始可微操作。作者采用的注意机制是作为逻辑规则的置信度并且有寓意含义的。 下图展示了一个使用逻辑规则进行知识图谱推理的例子 使用概率逻辑的优点是通过为逻辑规则配备概率，可以更好地模拟统计复杂和噪声数据。 statistical relational learning（统计关系学习）：学习关系规则的集合 ==inductive logic programming（归纳逻辑规划）：==当学习涉及提出新的逻辑规则时。（这应该和我正在做的方向是相关的，都是带有归纳性质的，有新的东西产生）。 FrameworkKnowledge base reasoning为了推理知识库，对于每个查询我们都有兴趣学习以下形式的加权链式逻辑规则，类似于==随机逻辑程序==： 其中$\alpha$是和规则有关的置信度，R是知识库中的关系，query(Y,X) 表示一个三元组，query 表示一个关系。 TensorLog for KB reasoning将实体转换成one-hot变量；并用一个矩阵$M_R$表示关系，该矩阵只在（i，j）处为1，i、j为第i、j个实体。 结合两个操作，逻辑规则推理$R(Y,X) \gets P(Y,X) \bigwedge Q(Z,X)$可以被表示为：$M_P \cdot M_P \cdot v_x \doteq s$，向量s中为1的位置就是Y的答案。 对于一条查询，所有的逻辑规则的右边部分被表示为以下形式： 其中，l表示所有的可能规则的个数，$\alpha_l$是规则l的置信度，$\beta_l$是某特定关系里的有序关系列表，所以在inference时，给定实体$v_x​$，实体y的score等于向量s中的对应y的位置的值。对于推理，给定实体x，实体y的score等于向量s中的对应y的位置的值。 所以总结本文关心的优化问题如下： Learning the logical rules在上式的优化问题中，算法需要学习的部分分为两个：一个是规则的结构，即一个规则是由哪些条件组合而成的；另一个是规则的置信度。由于每一条规则的置信度都是依赖于具体的规则形式，而规则结构的组成也是一个离散化的过程，因此上式整体是不可微的。因此作者对前面的式子做了以下更改： 对比与式（2）：主要交换了连乘和累加的计算顺序，对预一个关系的相关的规则，为每个关系在每个步骤都学习了一个权重，即上式的 $a_t^k$。 由于上式固定了每个规则的长度都为 T，这显然是不合适的。为了能够学习到变长的规则，Neural LP中设计了记忆向量 $u_t$,表示每个步骤输出的答案–每个实体作为答案的概率分布，还设计了两个注意力向量：一个为记忆注意力向量 $b_t$ ——表示在步骤 t 时对于之前每个步骤的注意力；一个为算子注意力向量 $a_t$ ——表示在步骤 t 时对于每个关系算子的注意力。每个步骤的输出由下面三个式子生成： 其中$b_t$和$a_t$由以下公式通过RNN获得： 推理机的整体框架是： 其中memory存的就是每步的推理结果（实体），最后的输出（例如$u_{T+1}$，目标就是最大化 $logv_y^Tu$，加log是因为非线性能让效果变好。 整个算法如下： 实验（1） 两个标准数据集上的统计关系学习相关的实验 Unified Medical Language System (UMLS)：The entities are biomedical concepts (e.g. disease, antibiotic) and relations are like treats and diagnoses. Kinship：contains kinship relationships among members of the Alyawarra tribe from Central Australia [ （2） 在$16*16$的网格上的路径寻找的实验 （3） 知识库补全实验实验所用数据集信息： FB15KSelected：这是通过从FB15K中去除近似重复和反向关系而构造的 实验结果： 为了证明Neural LP的归纳推理的能力，本文还特别设计了一个实验，在训练数据集中去掉所有涉及测试集中包含的实体的三元组，然后训练并预测，得到结果如下： （4） 知识库问答的实验 总结本文提出了一个可微的规则学习模型，并强调了知识库中的规则应该是实体无关的，对于我目前在做的方向，本体论也是与实体无关的，这种规则学习有一定的借鉴性，但是好像所区别。这个规则推理也可以看成某些关系之间的包含关系3.1中举的HasOfficeInCity(New York,Uber) and CityInCountry(USA,New York)的例子，可以看作是2对于1有包含关系。并且可以看到本篇论文中，作者设计了丰富的实验。 参考链接 https://toutiao.io/posts/wrxf4z/preview https://zhuanlan.zhihu.com/p/46024825]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
        <tag>知识图谱推理</tag>
        <tag>规则学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《OK Google, What Is Your Ontology? Or/ Exploring Freebase Classification to Understand Google’s Knowledge Graph？》阅读笔记]]></title>
    <url>%2Fpost%2FOK%20Google%2C%20What%20Is%20Your%20Ontology%3F%20Or%2F%20Exploring%20Freebase%20Classification%20to%20Understand%20Google%E2%80%99s%20Knowledge%20Graph%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[本论文详细阐述Freebase中的数据格式，并进行了重构。通过考虑整体架构的三个部分：Freebase类型系统及其缺乏继承和依赖于不兼容性，允许表示值的不确定性的实现，以及合并和拆分对象的实现。来对本体进行阐述。论文下载地址 这篇论文重构了Freebase数据转储来理解谷歌语义搜索特征背后的本体。论文将会探索Freebase本体如何由许多力量塑造的，这些力量也通过深入研究本体论和一个小的相关性研究来形成分类系统。这些发现将会提供知识图谱专有黑盒的一瞥。 The structures found in the Freebase/Knowledge Graph ontology will be analyzed in light of the findings on classification systems in a key text by Bowker and Star (2000) [5]. 术语定义 ObjectFreebase对象是一个全局唯一的标识符，它是Freebase中世界上某种东西的表示。 TypeFreebase类型用来表达类的概念。 PropertyFreebase属性是描述对象如何链接到其他值或对象的关系。 Property Detail属性详细信息指的是可以通过属性链接的对象或值的约束。 RDF triple资源描述格式（RDF）是用于“三元组”（或N = 3元组）格式的数据表示的规范[17]。 Ontology对于本文，Freebase本体是类型，属性和属性详细信息的正式结构和描述，用于指定对象如何相互关联。 Architecture在本文中，架构指的是可以在本体中找到的一般模式和关系。 ==本体是否允许类（或Freebase用语中的类型）之间的继承？ 是否有与属性相关的默认值？ 如何处理“零”或空值？ 这些类型的问题不一定关注本体（飞机，火车或汽车）中具体表达的内容，而是关于本体表达方式的更多问题应该通过检查架构来解决。== Methodology作者把数据进行切分：按照RDF中三元组的谓语进行分类，例如： Freebase Ontology and Classification As Bowker and Star note, “Information infrastructure is a tricky thing to analyze…the easier they are to use, the harder they are to see.” [5]. What does the system make sense of? What is left out? What is privileged and by extension what is ignored by Google? 虽然Freebase本体可能不会立即看起来像一个分类系统，但类型（类）和属性的结构是一个基于对各种事物进行分类的系统。 作为对世界事物表征进行排序和分类的系统，将根据Bowker和Star的分类结果讨论Freebase本体。他们将对亚里士多德和原型分类（Aristotelian and prototype classification）进行了区分。 亚里士多德的分类“按照一组二元特征进行操作，被分类的物体呈现或不呈现”，而原型分类则认为“在我们心目中对于椅子是什么的广泛描述; 我们用隐喻和类比来扩展这张图片“ 5.1. Freebase’s Type System不兼容性的概念出现在Freebase系统中，用于表示对象如何具有某些类型，而这些类型必须将其排除在其他类型之外。 没有继承（not implement inheritance）：上述不兼容性在确保数据不表达可能在Google KP中提供的令人尴尬，有害或不正确的陈述方面发挥了足够强大的作用。 缺乏继承也可能是一种允许实体具有更大灵活性的特征。这里作者举了一个狗为电影演员的例子。 5.2. Has Value or Has No Value?三元组如何表达估计值，不确定值或空值？实际处理时用“Has Value” (HV) and “Has No Value” (HNV)来分别表达不确定值和空值。 以这种方式表达未知数和空值的有趣实现可能表明Freebase / KG最初并不是为了支持这种不确定性而建立的。Google的数据编码某些不确定性的概念并未向最终用户公开，尽管它肯定以这种独特的方式实现。 5.3. Dealing with Doppelgangers and Chimeras涉及Freebase如何处理“合并”重复对象（doppelgangers）和“拆分”混合对象（嵌合体）。 the property “/dataworld/gardening hint/replaced by” is used to implement merges be- tween various objects (e.g. by saying “/m/xyz123 - Replaced By - /m/abc123”). A Small Correlational Study主要探索这个问题：域的本体的复杂性（人物，电影等领域的类型，属性等）与表达与本体相关的事实（“知识库”）的三元组数量之间是否存在关联？ 对于本研究，通过考虑与域相关的属性详细信息量（多少描述，约束等）来实现“复杂性”和“成熟度”。 对于89个域中的每一个，获得了关于每个域的本体的以下统计： ==域中的类型和属性数== ==每种类型和属性的描述数== ==每种类型和属性的属性详细信息数== 通过获取域中每种类型和属性的平均描述数和属性详细信息来计算简单的复杂性分数。 所有域的RDF三元组计数与此复杂性得分之间的Pearson相关系数与0.2824呈正相关，简单线性回归的斜率为78,424.08（见图6）。 当排除异常音乐切片时，相关性和斜率分别变为0.6680和33,899.53。 虽然需要进一步的工作来探索这个研究问题，但这个小的相关性研究为进一步的实验提供了一些有希望的初步结果 discussion考虑整体架构的三个部分：Freebase类型系统及其缺乏继承和依赖于不兼容性，允许表示值的不确定性的实现，以及合并和拆分对象的实现。此外，还进行了一项小型相关研究，以检验基于Bowker和Star推动的预感的假设。在很大程度上，分类系统中的许多特征也可以在Freebase的本体和体系结构中找到。 本文具体而言，探讨了支持整个交付流程的基础结构（本体和体系结构），而不是Freebase / KG中表示的特定事实。 conclusion 应通过探索Freebase本体和体系结构的其他方面以及对Freebase进行更全面的实验分析来进行进一步的研究。]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>Ontology</tag>
        <tag>知识图谱</tag>
        <tag>freebase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式]]></title>
    <url>%2Fpost%2FRegular%20expression%2F</url>
    <content type="text"><![CDATA[本文参考了一些链接，记录了一些常用正则表达式的详细使用方法。 在自然语言处理中，很多时候我们都需要从文本或字符串中抽取出想要的信息，并进一步做语义理解或其它处理。在本文中，作者由基础到高级介绍了很多正则表达式，这些表达式或规则在很多编程语言中都是通用的。 书写正则表达式网站 正则表达式（regex 或 regexp）对于从文本中抽取信息极其有用，它一般会搜索匹配特定模式的语句，而这种模式及具体的 ASCII 序列或 Unicode 字符。从解析/替代字符串、预处理数据到网页爬取，正则表达式的应用范围非常广。 其中一个比较有意思的地方是，只要我们学会了正则表达式的语句，我们几乎可以将其应用于多有的编程语言，包括 JavaScript、Python、Ruby 和 Java 等。只不过对于各编程语言所支持的最高级特征与语法有细微的区别。 下面我们可以具体讨论一些案例与解释。 基本语句锚点：^ 和 $1234^The 匹配任何以“The”开头的字符串 end$ 匹配以“end”为结尾的字符串^The end$ 抽取匹配从“The”开始到“end”结束的字符串roar 匹配任何带有文本“roar”的字符串 数量符：*、+、？和 {}**12345678abc* 匹配在“ab”后面跟着零个或多个“c”的字符串 abc+ 匹配在“ab”后面跟着一个或多个“c”的字符串abc? 匹配在“ab”后面跟着零个或一个“c”的字符串abc&#123;2&#125; 匹配在“ab”后面跟着两个“c”的字符串abc&#123;2,&#125; 匹配在“ab”后面跟着两个或更多“c”的字符串abc&#123;2,5&#125; 匹配在“ab”后面跟着2到5个“c”的字符串a(bc)* 匹配在“a”后面跟着零个或更多“bc”序列的字符串a(bc)&#123;2,5&#125; 匹配在“a”后面跟着2到5个“bc”序列的字符串 或运算符：| 、 []12a(b|c) 匹配在“a”后面跟着“b”或“c”的字符串 a[bc] 匹配在“a”后面跟着“b”或“c”的字符串 字符类：\d、\w、\s 和 .**1234\d 匹配数字型的单个字符 \w 匹配单个词字（字母加下划线） \s 匹配单个空格字符（包括制表符和换行符） . 匹配任意字符 使用「.」运算符需要非常小心，因为常见类或排除型字符类都要更快与精确。\d、\w 和\s 同样有它们各自的排除型字符类，即\D、\W 和\S。例如\D 将执行与\d 完全相反的匹配方法： 1\D 匹配单个非数字型的字符 为了正确地匹配，我们必须使用转义符反斜杠「\」定义我们需要匹配的符号「^.[$()|*+?{\」，因为我们可能认为这些符号在原文本中有特殊的含义。 1\$\d 匹配在单个数字前有符号“$”的字符串 -&gt; Try it! (https://regex101.com/r/cO8lqs/9) 注意我们同样能匹配 non-printable 字符，例如 Tab 符「\t」、换行符「\n」和回车符「\r」 Flags我们已经了解如何构建正则表达式，但仍然遗漏了一个非常基础的概念：flags。 正则表达式通常以/abc/这种形式出现，其中搜索模式由两个反斜杠「/」分离。而在模式的结尾，我们通常可以指定以下 flag 配置或它们的组合： g（global）在第一次完成匹配后并不会返回结果，它会继续搜索剩下的文本。 m（multi line）允许使用^和$匹配一行的开始和结尾，而不是整个序列。 i（insensitive）令整个表达式不区分大小写（例如/aBc/i 将匹配 AbC）。 中级语句分组和捕获：()123a(bc) 圆括弧会创建一个捕获性分组，它会捕获匹配项“bc” a(?:bc)* 使用 “?:” 会使捕获分组失效，只需要匹配前面的“a” a(?&lt;foo&gt;bc) 使用 “?&lt;foo&gt;” 会为分组配置一个名称 捕获性圆括号 () 和非捕获性圆括弧 (?:) 对于从字符串或数据中抽取信息非常重要，我们可以使用 Python 等不同的编程语言实现这一功能。从多个分组中捕获的多个匹配项将以经典的数组形式展示：我们可以使用匹配结果的索引访问它们的值。 如果需要为分组添加名称（使用 (?…)），我们就能如字典那样使用匹配结果检索分组的值，其中字典的键为分组的名称。 方括弧表达式：[]12345[abc] 匹配带有一个“a”、“ab”或“ac”的字符串 -&gt; 与 a|b|c 一样 [a-c] 匹配带有一个“a”、“ab”或“ac”的字符串 -&gt; 与 a|b|c 一样[a-fA-F0-9] 匹配一个代表16进制数字的字符串，不区分大小写 [0-9]% 匹配在%符号前面带有0到9这几个字符的字符串[^a-zA-Z] 匹配不带a到z或A到Z的字符串，其中^为否定表达式 记住在方括弧内，所有特殊字符（包括反斜杠\）都会失去它们应有的意义。 ==Greedy 和 Lazy 匹配==数量符（* + {}）是一种贪心运算符，所以它们会遍历给定的文本，并尽可能匹配。例如，&lt;.+&gt; 可以匹配文本「This is a simple div test」中的「simple div」。为了仅捕获 div 标签，我们需要使用「？」令贪心搜索变得 Lazy 一点： 1&lt;.+?&gt; 一次或多次匹配 “&lt;” 和 “&gt;” 里面的任何字符，可按需扩展 注意更好的解决方案应该需要避免使用「.」，这有利于实现更严格的正则表达式： 1&lt;[^&lt;&gt;]+&gt; 一次或多次匹配 “&lt;” 和 “&gt;” 里面的任何字符，除去 “&lt;” 或 “&gt;” 字符 高级语句 边界符：\b 和 \B 1\babc\b 执行整词匹配搜索 \b 如插入符号那样表示一个锚点（它与$和^相同）来匹配位置，其中一边是一个单词符号（如\w），另一边不是单词符号（例如它可能是字符串的起始点或空格符号）。 它同样能表达相反的非单词边界「\B」，它会匹配「\b」不会匹配的位置，如果我们希望找到被单词字符环绕的搜索模式，就可以使用它。 1\Babc\B 只要是被单词字符环绕的模式就会匹配 前向匹配和后向匹配：(?=) 和 (?&lt;=)12d(?=r) 只有在后面跟着“r”的时候才匹配“d”，但是“r”并不会成为整个正则表达式匹配的一部分 (?&lt;=r)d 只有在前面跟着“r”时才匹配“d”，但是“r”并不会成为整个正则表达式匹配的一部分 我们同样能使用否定运算子： 12d(?!r) 只有在后面不跟着“r”的时候才匹配“d”，但是“r”并不会成为整个正则表达式匹配的一部分 (?&lt;!r)d 只有在前面不跟着“r”时才匹配“d”，但是“r”并不会成为整个正则表达式匹配的一部分 结语 正如上文所示，正则表达式的应用领域非常广，很可能各位读者在开发的过程中已经遇到了它，下面是正则表达式常用的领域： 数据验证，例如检查时间字符串是否符合格式； 数据抓取，以特定顺序抓取包含特定文本或内容的网页； 数据包装，将数据从某种原格式转换为另外一种格式； 字符串解析，例如捕获所拥有 URL 的 GET 参数，或捕获一组圆括弧内的文本； 字符串替代，将字符串中的某个字符替换为其它字符。 参考链接https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650749657&amp;idx=4&amp;sn=da4852cb0c4919316d801fe19a64901d&amp;chksm=871afea7b06d77b1bda42ac134c5dddad5af24647f62a5a8e7bdcef4499f40fd4c97045a6f3d&amp;mpshare=1&amp;scene=23&amp;srcid=1009dbNFxJJahsQK6NGw4wS3%23rd python正则表达式的使用正则表达式match和search的区别]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019年计划]]></title>
    <url>%2Fpost%2F2019-plans%2F</url>
    <content type="text"><![CDATA[读个博吉他可以弹唱几首流行歌曲假期报个班，请老师指点一下 精读50篇论文平均下来，每周都需要读一篇，每一篇都要勾划，发博客。 发2篇paper上半年积累知识，做实验，暑假开始写 考托福过90上半年背单词等 买个微单学个拍照不急，现在没钱，下半年入手一个吧 自己出门旅行一次嗯，要是论文写完就去吧！目前计划去山东，顺便看一下我二姑。5天左右的旅行吧！ 学习滑板开始健身每周去三次健身房]]></content>
      <categories>
        <category>annual plans</category>
      </categories>
      <tags>
        <tag>plan</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《An Automatic Knowledge Graph Construction System for K-12 Education》阅读笔记]]></title>
    <url>%2Fpost%2FAn%20Automatic%20Knowledge%20Graph%20Construction%20System%20for%20K-12%20Education%2F</url>
    <content type="text"><![CDATA[论文原址。本篇文章主要提出了一个自动化构建数学领域知识图谱的系统，主要应用的事NER和数据挖掘技术，其中NER主要是抽取数学概念，概念间的关系是作者自己构建的（例如先修关系）。对于数据集，作者主要从the Chinese curriculum standards of mathematics上提取的概念实体，从自己的SLP平台上，通过对学生表现来提取关系（把这部分作为数据挖掘）。本篇文章实际上可以作为构建特定领域的知识图谱的一个参考。 challenges the desired educational concept entities are more abstract than real world entities like PERSON, ORGANIZATION, LOCATION the desired relations are more cognitive and implicit, so cannot be derived from the literal meanings of text like generic knowledge graphs contributions a novel but practical system entity recognition (NER) &amp; association rule mining algorithms demonstrate an exemplary case with constructing a knowledge graph for the subject of mathematics SYSTEM OVERVIEW Educational Concept Extraction Module: Implicit Relation Identification Module CONCEPT EXTRACTION 线性链式CRF模型 标签预测 RELATION IDENTIFICATION两种方法 support confidence From the perspective of prerequisite relation, if concept si is a prerequisite of concept sj, learners who do not master sivery likely do not master sj, and learners who master sjmost likely master si. EXEMPLARY CASE AND SYSTEM EVALUATIONConcept ExtractionDatasetthe Chinese curriculum standards of mathematics published by the ministry of education as the main data source Evaluation adopt precision, recall and F1- score The ground truth is manually labeled by two domain experts. Relation IdentificationDataset students’ performance data collected by our SLP platform. EvaluationThe ground truth of the prerequisite relations between selected 9 concepts are annotated manually by two domain experts.]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>NER</tag>
        <tag>知识图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《RECURRENT NEURAL NETWORK REGULARIZATION》阅读笔记]]></title>
    <url>%2Fpost%2FRECURRENT%20NEURAL%20NETWORK%20REGULARIZATION%2F</url>
    <content type="text"><![CDATA[论文链接。这篇论文提出了LSTM的dropout策略来防止过拟合，即只在非循环链接处采取dropout。在BasicLSTMCell的接口就是依据这篇论文实现的。 文章整体架构和重点 contribution present a simple regularization technique background dropout Srivastava(2013)，对于前向反馈网络最有力的正则化方法并不能很好的应用在RNNs上。这导致RNNs规模都很小，因为太大会过拟合。 Bayer et al. (2013)指出了卷积dropout不能在RNNs上很好工作的原因是循环会放大噪音。 model仿射变换（affine transform）关于仿射变换：线性变换加上平移，盗个知乎上的图（原文链接） 模型主体采用的是Graves et al. (2013) dropout策略 The main contribution of this paper is a recipe for applying dropout to LSTMs in a way that success- fully reduces overfitting. The main idea is to apply the dropout operator only to the non-recurrent connections. 观察公式，实际上就是通过在层间传递中应用dropout。如下图中虚线所示。 从上图中也可以看到，该dropout的次数只和网络深度有关（数值为网络深度+1）。 experiments实验部分作者做了4部分实验来证明自己采用的方法有效，分别为language modeling, speech recognition, machine translation, image caption generation。这部分没什么需要解释了，感兴趣可以自己看一下实验。]]></content>
      <categories>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Understanding LSTM Networks]]></title>
    <url>%2Fpost%2FUnderstanding%20LSTM%20Networks%2F</url>
    <content type="text"><![CDATA[原文链接。这篇文章很好很细的一步一步的分解讲解了LSTM，之前看过一篇翻译的博客，现在自己翻译一遍，感觉对LSTM的认识加深了许多，虽然还是对LSTM中存有一些问题，比如为什么用tanh，sigmoid，为什不采用其他的？，但是看过之后至少对LSTM没有那么畏惧，不觉得过于复杂了。 LSTMRecurrent Neural Networks 循环允许信息从一个网络传入下一个。 一个循环网络可以被认为是相同网络的多个复制，每一个网络都将信息传递给后继者。这种类似链的性质表明，递归神经网络与序列和列表密切相关。 The Problem of Long-Term DependenciesRNN的一个吸引力是他们可能能够将先前信息连接到当前任务。 有时，我们只需要查看最近的信息来执行当前任务。例如： If we are trying to predict the last word in “the clouds are in the sky,” we don’t need any further context – it’s pretty obvious the next word is going to be sky. 在这种情况下，如果相关信息与待预测地方之间的差距很小，RNN可以学习使用过去的信息。 但是，对于一些情况，我们需要更多的上下文信息。 Consider trying to predict the last word in the text “I grew up in France… I speak fluent French.” 这时，相关信息与需要变得非常大的点之间的差距完全有可能。不幸的是，随着差距的扩大，RNN无法学会连接信息。 The problem was explored in depth by Hochreiter (1991) [German] and Bengio, et al. (1994), who found some pretty fundamental reasons why it might be difficult. LSTM Networks LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn! 标准RNNs的重复模块只有一个简单的结构，比如tanh层。 LSTMs也有像这种链式结构，但是它的重复模块具有不同的结构。 有四个，而不是一个神经网络层，以一种非常特殊的方式进行交互。 基本符号如下： 在上图中，每一行都携带一个完整的向量，从一个节点的输出到其他节点的输入。 粉色圆圈表示逐点运算，如矢量加法，而黄色框表示神经网络层。 行合并表示连接，而行分叉表示其内容被复制，副本将转移到不同的位置。 The Core Idea Behind LSTMsLSTM的关键是单元状态，水平线贯穿图的顶部。 单元状态有点像传送带。 它直接沿着整个链运行，只有一些微小的线性相互作用。 信息很容易沿着它不变地流动。 LSTM确实能够移除或添加信息到细胞状态，由称为门的结构精心调节。 门是一种可选择通过信息的方式。 它们由Sigmoid神经网络层和逐点乘法运算组成。 sigmoid层输出0到1之间的数字，描述每个组件应该通过多少。 值为零意味着“不让任何东西通过”，而值为1则意味着“让一切都通过！” LSTM具有三个这样的门，用于保护和控制单元状态。 Step-by-Step LSTM Walk Through我们的第一步就是确定我们将从单元状态中丢弃的信息。这个决定是由一个称为“遗忘门层”的sigmoid层决定的。它查看$h_{t-1}$和$x_t$，并为单元状态$C_{t-1}$中的每一个数字输出一个介于0和1之间的数字。1代表“完全保留这个”，而0代表“完全舍弃这个”。 让我们回到我们的语言模型示例，试图根据以前的所有单词预测下一个单词。 在这样的问题中，单元状态可能包括当前受试者的性别，因此可以使用正确的代词。 当我们看到一个新主题时，我们想要忘记旧主题的性别。 下一步是确定我们将在单元状态中存储哪些新信息。 这有两个部分。 首先，称为“输入门层”的sigmoid层决定我们将更新哪些值。 接下来，tanh层创建可以添加到状态的新候选值$\tilde{C}_t$的向量。 在下一步中，我们将结合这两个来创建状态更新。 在我们的语言模型的例子中，我们想要将新主题的性别添加到单元格状态，以替换我们忘记的旧主题。 现在是时候将旧的单元状态$C_{T-1}$更新为新的单元状态$C_t$。 前面的步骤已经决定要做什么，我们只需要实际做到这一点。 我们将旧状态乘以$f_t$，忘记我们之前决定忘记的事情。 然后我们添加$i_t * \tilde{C}_t$。 这是新的候选值，根据我们决定更新每个状态的值来缩放。 在语言模型的情况下，我们实际上放弃了关于旧主题的性别的信息并添加新信息，正如我们在前面的步骤中所做的那样。 最后，我们需要决定我们要输出的内容。 此输出将基于我们的单元状态，但将是过滤版本。 首先，我们运行一个sigmoid层，它决定我们要输出的单元状态的哪些部分。 然后，我们将单元格状态设置为tanh（将值推到介于-1和1之间）并将其乘以sigmoid门的输出，以便我们只输出我们决定的部分。 对于语言模型示例，由于它只是看到一个主题，它可能想要输出与动词相关的信息，以防接下来会发生什么。 例如，它可能输出主语是单数还是复数，以便我们知道动词应该与什么形式共轭，如果接下来的话。 Variants on Long Short Term Memory到目前为止我所描述的是一个非常正常的LSTM。 但并非所有LSTM都与上述相同。 事实上，似乎几乎所有涉及LSTM的论文都使用略有不同的版本。 差异很小，但值得一提的是其中一些。 One popular LSTM variant, introduced by Gers &amp; Schmidhuber (2000), is adding “peephole connections.” This means that we let the gate layers look at the cell state. 上面的图表为所有门增加了窥视孔（peephole），但是许多论文会给一些窥视孔而不是其他的。 另一种变化是使用耦合的遗忘和输入门。 我们不是单独决定忘记什么以及应该添加新信息，而是一起做出这些决定。我们仅仅会当我们在当前位置将要输入时忘记。我们仅仅输入新的值到那些我们已经忘记旧的信息的那些状态 。 另一个改动较大的变体是 Gated Recurrent Unit (GRU)，这是由 Cho, et al. (2014) 提出。它将遗忘和输入门组合成一个“更新门”。它还合并了单元状态和隐藏状态，并进行了一些其他更改。 由此产生的模型比标准LSTM模型简单，并且越来越受欢迎。 这些只是最着名的LSTM变种中的一小部分。 还有很多其他的东西，如 Yao, et al. (2015) 提出的 Depth Gated RNN。 还有一些完全不同的解决长期依赖关系的方法，如 Koutnik, et al. (2014) 提出的 Clockwork RNN。 哪种变体最好？ 差异是否重要？ Greff, et al. (2015) 对流行变体进行了很好的比较，发现它们几乎完全相同。Jozefowicz, et al. (2015) 测试了超过一万个RNN架构，找到了一些在某些任务上比LSTM更好的架构。 Conclusion早些时候，我提到了人们用RNN取得的显着成果。基本上所有这些都是使用LSTM实现的。对于大多数任务来说，它们确实工作得更好！ 作为一组方程写下来，LSTM看起来非常令人生畏。希望，在这篇文章中逐步走过它们使他们更加平易近人。 LSTM是我们用RNN实现的重要一步。很自然地想知道：还有另一个重要的一步吗？研究人员的共同观点是：“是的！下一步是它的注意！“我们的想法是让RNN的每一步都从一些更大的信息集中选择信息。例如，如果您使用RNN创建描述图像的标题，则可能会选择图像的一部分来查看其输出的每个单词。 事实上， Xu, et al.(2015) 做到这一点 - 如果你想探索注意力，这可能是一个有趣的起点！使用注意力已经取得了许多非常令人兴奋的结果，似乎还有更多的事情即将来临…… 注意力不是RNN研究中唯一令人兴奋的问题。例如，Kalchbrenner, et al. (2015) 的Grid LSTMs似乎非常有希望。在生成模型中使用RNN工作 - 例如Gregor, et al. (2015), Chung, et al. (2015), 或者 Bayer &amp; Osendorfer (2015) 似乎也很有趣。过去几年对于反复出现的神经网络来说是一个激动人心的时刻，即将到来的那些承诺只会更加如此！]]></content>
      <categories>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation》阅读笔记]]></title>
    <url>%2Fpost%2FLearning_Phrase_Representations_using_RNN_Encoder%E2%80%93Decoder_for_Statistical_Machine_Translation%2F</url>
    <content type="text"><![CDATA[原文链接。该论文是Sequence to Sequence学习的最早原型，论文中提出一种崭新的RNN(GRU) Encoder-Decoder算法，虽然文章属于比较旧的文章，但作为seq2seq的基础原型，还是需要阅读了解一下的。文章写的比较详细，各部分细节都有讲解。 文章的主要结构 contribution a novel RNN Encoder–Decoder 能够处理变长序列 a novel hidden unit reset gate update gate RNN Encoder–Decoder模型结构图如下： 文中作者对齐进行总体概述为： From a probabilistic perspective, this new model is a general method to learn the conditional distribution over a variable-length sequence conditioned on yet another variable-length sequence 从概率的角度来看，这个新模型是学习在另一个可变长度序列条件下的可变长度序列上的条件分布的一般方法 Encoder这部分是一个RNN单元。每个时间步，我们向Encoder中输入一个字/词（一般为向量形式），直到我们输入这个句子的最后一个字/词$X_T$，然后输入整个句子的语义向量c。由于RNN的特带你就是把前面每一步的输入信息都考虑进来，所以理论上这个c就包含了整个句子的所有信息。我们可以把当成这个句子的一个语义表示。 DecoderDecoder是另一个RNN，其被训练出来以通过预测隐藏状态$h_t$的下一个符号$y_t$来生成输出序列。计算公式如下 $$h_t = f(h_{t-1},y_{t-1},c)$$ 下一个序列的计算公式如下： $$P(y_t|y_{t-1},y_{t-2},\dots,y_1,c)=g(h_t,y_{t-1},c)$$ Hidden Unit该部分是对各部分具体的公式讲解，实际是GRU的具体公式算法，不再此详细叙述了。 reset gate In this formulation, when the reset gate is close to 0, the hidden state is forced to ignore the pre- vious hidden state and reset with the current input only. This effectively allows the hidden state to drop any information that is found to be irrelevant later in the future, thus, allowing a more compact representation. 这段原文主要讲解了复位门的作用：有效地允许隐藏状态丢弃在将来稍后发现不相关的任何信息，从而允许更紧凑的表示。 当捕获短期依赖时，复位门活跃 update gate the update gate controls how much information from the previous hidden state will carry over to the current hidden state. 更新门控制来自先前隐藏状态的多少信息将转移到当前隐藏状态。 当捕获长期依赖时，更新门活跃 Statistical Machine Translation(SMT) Experiments这部分作者主要做了量化分析和性质分析，主要就是说他的模型怎么厉害。。。（没有具体的数值指标，翻译的还不是中英翻译，想看的话可以去看一下，就不贴实验结果了）。 future这里作者提出了可以用decoder生成的目标短语来替换原句中短语的思路，如果没记错的话，这个想法好像对后面的机器翻译有很大的指导作用。]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>seq2seq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Differentiating Concepts and Instances for Knowledge Graph Embedding》阅读笔记]]></title>
    <url>%2Fpost%2Fread_Differentiating_Concepts_and_Instances_for_Knowledge_Graph_Embedding%2F</url>
    <content type="text"><![CDATA[论文获取地址。这篇文章最大的亮点就是把concept映射为一个球面，然后把instance映射为一个向量，通过这种空间关系来进行embedding。如果instance和concept满足InstanceOf的关系，则instance应该在球内；如果两个concept满足SubClassOf的关系，则一个球会在另一个球面内。 conceptA concept is a fundamental category of existence (Rosch, 1973) and can be reified by all of its actual or potential instances.Concepts, which represent a group of different instances sharing common properties, are essential information in knowledge representation. drawback of the previous method ignore to distinguish between concepts and instances will lead to two drawbacks: Insufficient concept representation： cannot explicitly represent the difference between concepts and instances Lack transitivity of both isA relations: instanceOf and subClassOf (generally known as isA)isA relations exhibit transitivity contributions the first to propose and formalize the problem of knowledge graph embedding which differentiates between concepts and instances a novel knowledge embedding method named TransC state-of-the-art on link prediction and triple classification Translation-based ModelsTransE triple (h, r, t) should satisfy h + r ≈ t loss function:$f_r(h,t) = ||h + r - t||^2_2$ suitable for 1-to-1 relations TransH It regards a relation vector r as a translation on a hyperplane with $w_r$ as the normal vector. loss function:$f_r(h,t) = ||h_{\bot} + r - t_{\bot}||^2_2$，其中$h_{\bot}=h-w^{\top}_r h w_r$，$t_{\bot}=t-w^{\top}_r t w_r$ suitable for 1-to-N, N-to-1, and N-to-N relations TransR/CTransR addresses the issue in TransE and TransH that some entities are similar in the entity space but comparably different in other specific aspects. loss function:$f_r(h,t) = ||M_rh +r -M_rt||^2_2$，$M_r$ for each relation r TransD considers the different types of entities and relations at the same time loss function:$f_r(h,t) = ||h_{\bot} + r - t_{\bot}||^2_2$，$h_{\bot} = M_{rh}h$和$t_{\bot} = M_{rt}t$，$M_{r,e}$ for each relation-entity pair (r, e) Bilinear ModelsRESCAL the first bilinear model It associates each entity with a vector to capture its latent semantics. Each relation is represented as a matrix which models pairwise interactions between latent factors. External Information Learning Models textual information entity descriptions Problem Formulation这部分中作者详细介绍了知识图谱的组成部分：概念和实例集、关系集（包括instanceOf、subClassOf和instance relation），三元组集（按照关系集同样分为三个部分）。为了能表达is A关系的传递性，作者将instanceOf和subClassof两种关系进行了精心的设计，也是该论文的重点。 For each concept c ∈ C, we learn a sphere s(p, m) with $p \in R^k$ and m denoting the sphere center and radius. TranCSpecifically, TransC encodes each concept in knowledge graph as a sphere and each instance as a vector in the same semantic space. InstanceOf Triple Representationloss function：$f_e(i,c) = ||i-p||_2 - m$，当该函数大于0时进行优化，使其小于零。 SubClassOf Triple Representation 如图所示，其中子图（a）为目标状态。两个概念的的圆心距离：$d = ||p_i - p_j||_2$。需要做到的就是$d-(m_j -m_i) \leq 0$并且$ (m_j &gt; m_i)$。 Relational Triple Representation这部分按照TranE的思路进行处理，$||h+r-t||^2_3$ train modelmargin based loss详解unit and bern Regarding the strategy of constructing negative labels, we use “unif” to denote the traditional way of replacing head or tail with equal probability, and use “bern.” to denote reducing false negative labels by replacing head or tail with different probabilities. the following research directions find a more expressive model instead of spheres to represent concepts A concept may have different meanings in different triples. use several typical vectors of instances as a concept’s centers to represent different meanings of a concept.]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Discourse-Level Named Entity Recognition and Relation Extraction Dataset for Chinese Literature Text 阅读笔记]]></title>
    <url>%2Fpost%2FREAD_A_Discourse-Level_Named_Entity_Recognition_and_Relation_Extraction_Dataset_for_Chinese_Literature_Text%2F</url>
    <content type="text"><![CDATA[该论文最主要的贡献就是这个数据，数据集地址。论文中提到的标标签过程也是一个创新点，运用了启发式和机器辅助标标签，这样可以提高准确度并减少标注人员工作。 contribution provide a new dataset for joint learning of NER and RE for Chinese literature text the proposed dataset is based on the discourse level which provides additional context information introduce some widely used models to conduct experiments tagging processtwo methods:one is a heuristic tagging method and another is a machine auxiliary tagging method. Step 1: First Tagging Processfind a problem of data inconsistency. Step 2: Heuristic Tagging Based on Generic disambiguating Rules For example, remove all adjective words and only tag “entity header” . re-annotate all articles and correct all inconsistency entities and relations based on the heuristic rules. Step 3: Machine Auxiliary Tagging The core idea is to train a model to learn annotation guidelines on the subset of the corpus and produce predicted tags on the rest data. CRF tagging set Annotation FormatEntityEach entity is identified by T tag, which takes several attributes. Id: a unique number identifying the entity within the document. It starts at 0, and is incremented every time a new entity is identified within the same document. Type: one of the entity tags. Begin Index: the begin index of an entity. It starts at 0, and is incremented every character. End Index: the end index of an entity. It starts at 0, and is incremented every character. Value: words being referred to an identifiable object. RelationEach relation is identified by R tag, which can take several attributes: Id: a unique number identifying the relation within the document. It starts at 0, and is incremented every time a new relation is identified within the same document. Arg1 and Arg2: two entities associated with a relation. Type: one of the relation tags.]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>NER</tag>
        <tag>RE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas的数据类型操作]]></title>
    <url>%2Fpost%2FPandas_data_type_manipulation%2F</url>
    <content type="text"><![CDATA[在原文链接中摘抄出部分信息作为记录形成本文。 数据类型 Pandas dtype Python 类型 NumPy 类型 用途 object str string_, unicode_ 文本 int64 int int_, int8, int16, int32, int64, uint8, uint16, uint32, uint64 整数 float64 float float_, float16, float32, float64 浮点数 bool bool bool_ 布尔值 datetime64 NA NA 日期时间 timedelta[ns] NA NA 时间差 category NA NA 有限长度的文本值列表 数据类型操作 使用df.dtypes可以显示数据所有列的类型 df.info（） 函数可以显示更有用的信息 使用 astype() 函数使用条件 数据是干净的，可以简单地解释为一个数字 你想要将一个数值转换为一个字符串对象 如果数据具有非数字字符或它们间不同质（homogeneous），那么 astype() 并不是类型转换的好选择。你需要进行额外的变换才能完成正确的类型转换。 使用方式为了真正修改原始 dataframe 中数据类型，记得把 astype() 函数的返回值重新赋值给 dataframe，因为 astype() 仅返回数据的副本而不原地修改。 参考链接 https://juejin.im/post/5acc36e66fb9a028d043c2a5]]></content>
      <categories>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>pandas</tag>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬取中文网页时中文字符变英文的解决方法]]></title>
    <url>%2Fpost%2Fsolution_of_python_for_Chinese_characters_to_become_English_when_crawling_Chinese_web_pages%2F</url>
    <content type="text"><![CDATA[使用python的scrapy爬取网页时，源代码中的中文字符在爬取下来后变成了英文字符。 问题举例例如，原网页为： 爬取结果为： 解决方法修改请求头：在settings.py文件中找到下属代码： 12345# Override the default request headers:#DEFAULT_REQUEST_HEADERS = &#123;# 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',# 'Accept-Language': 'en',#&#125; 改为： 12345# Override the default request headers:DEFAULT_REQUEST_HEADERS = &#123; 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Language': 'zh-CN',&#125; 修改结果展示： 参考链接 https://blog.csdn.net/wuqili_1025/article/details/79690103]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python动态网页爬取之安装docker和splash]]></title>
    <url>%2Fpost%2FPython_dynamic_web_crawler_installed_docker_and_splash%2F</url>
    <content type="text"><![CDATA[利用python进行动态网页爬取时，在安装docker和splash时踩过的坑，记录了一下自己的安装过程。用的系统是mac os。 安装scrapy-splash 利用pip安装scrapy-splash库：$ pip install scrapy-splash 安装Docker==下面👇这样安不下去了== 如果是Mac的话需要使用brew安装，如下：brew install docker 报错： 1Error: Failure while executing; `git config --local --replace-all homebrew.private true` exited with 1. 解决方法： 1xcode-select --install 然后在执行： 1brew install docker 再继续： service docker start 报错： -bash: service: command not found上网上查一堆乱七八糟的解决方式，该路径啥的，真的不想改路径，怕把其他的改崩了。最后放弃这种方式，如果有兴趣也可以尝试解决。 ==尝试如下安装DOCKER方法== 去官网下载这种方法下载docker客户端需要从服务器下载，自己电脑下载12k/s，简直慢死了。 拉取镜像(pull the image)：docker pull scrapinghub/splash 用docker运行scrapinghub/splash： docker run -p 8050:8050 scrapinghub/splash 在浏览器中输入localhost:8050 ==安装成功== 参考链接 http://www.morecoder.com/article/1001249.html https://www.jianshu.com/p/e54a407c8a0a]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>splash</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用python3读写csv文档]]></title>
    <url>%2Fpost%2Fread%20the%20CSV%20document%20using%20python3%2F</url>
    <content type="text"><![CDATA[对于大多数的CSV格式的数据读写问题，都可以使用 csv 库。 例如：假设你在一个名叫stocks.csv文件中有一些股票市场数据，就像这样： 1234567Symbol,Price,Date,Time,Change,Volume&quot;AA&quot;,39.48,&quot;6/11/2007&quot;,&quot;9:36am&quot;,-0.18,181800&quot;AIG&quot;,71.38,&quot;6/11/2007&quot;,&quot;9:36am&quot;,-0.15,195500&quot;AXP&quot;,62.58,&quot;6/11/2007&quot;,&quot;9:36am&quot;,-0.46,935000&quot;BA&quot;,98.31,&quot;6/11/2007&quot;,&quot;9:36am&quot;,+0.12,104800&quot;C&quot;,53.08,&quot;6/11/2007&quot;,&quot;9:36am&quot;,-0.25,360900&quot;CAT&quot;,78.29,&quot;6/11/2007&quot;,&quot;9:36am&quot;,-0.23,225400 csv文档的读取1. 常规读取下面向你展示如何将这些数据读取为一个元组的序列： 1234567import csvwith open('stocks.csv') as f: f_csv = csv.reader(f) headers = next(f_csv) for row in f_csv: # Process row ... 在上面的代码中， row 会是一个列表。因此，为了访问某个字段，你需要使用下标，如 row[0]访问Symbol， row[4] 访问Change。==这样可以通过外建字典来存储读出的csv数据。== 2. 命名元组由于这种下标访问通常会引起混淆，你可以考虑使用==命名元组==。例如： 123456789from collections import namedtuplewith open('stock.csv') as f: f_csv = csv.reader(f) headings = next(f_csv) Row = namedtuple('Row', headings) for r in f_csv: row = Row(*r) # Process row ... 它允许你使用列名如 row.Symbol 和 row.Change 代替下标访问。 需要注意的是这个只有在列名是合法的Python标识符的时候才生效。如果不是的话， 你可能需要修改下原始的列名(如将非标识符字符替换成下划线之类的)。 3. 字典另外一个选择就是将数据读取到一个字典序列中去。可以这样做： 123456import csvwith open('stocks.csv') as f: f_csv = csv.DictReader(f) for row in f_csv: # process row ... 在这个版本中，你可以使用列名去访问每一行的数据了。比如，row[&#39;Symbol&#39;] 或者 row[&#39;Change&#39;]。 fieldnames 是dict_reader的一个属性，表示CSV文档的数据名称。可以通过f_csv.fieldnames来访问数据名称那一行。 CSV文件写入为了写入CSV数据，你仍然可以使用csv模块，不过这时候先创建一个 writer 对象。例如: 12345678910headers = ['Symbol','Price','Date','Time','Change','Volume']rows = [('AA', 39.48, '6/11/2007', '9:36am', -0.18, 181800), ('AIG', 71.38, '6/11/2007', '9:36am', -0.15, 195500), ('AXP', 62.58, '6/11/2007', '9:36am', -0.46, 935000), ]with open('stocks.csv','w') as f: f_csv = csv.writer(f) f_csv.writerow(headers) f_csv.writerows(rows) 如果你有一个字典序列的数据，可以像这样做： 12345678910111213headers = ['Symbol', 'Price', 'Date', 'Time', 'Change', 'Volume']rows = [&#123;'Symbol':'AA', 'Price':39.48, 'Date':'6/11/2007', 'Time':'9:36am', 'Change':-0.18, 'Volume':181800&#125;, &#123;'Symbol':'AIG', 'Price': 71.38, 'Date':'6/11/2007', 'Time':'9:36am', 'Change':-0.15, 'Volume': 195500&#125;, &#123;'Symbol':'AXP', 'Price': 62.58, 'Date':'6/11/2007', 'Time':'9:36am', 'Change':-0.46, 'Volume': 935000&#125;, ]with open('stocks.csv','w') as f: f_csv = csv.DictWriter(f, headers) f_csv.writeheader() f_csv.writerows(rows) 其中f_csv.writeheader()也可以替换成f_csv.writerow(dict(zip(headers, headers))) 参考链接 https://python3-cookbook.readthedocs.io/zh_CN/latest/c06/p01_read_write_csv_data.html https://blog.csdn.net/guoziqing506/article/details/52014506]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>文件读取</tag>
        <tag>csv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neo4j初始化节点显示设置]]></title>
    <url>%2Fpost%2FNeo4j_initializes_the_node_display_Settings%2F</url>
    <content type="text"><![CDATA[问题描述：neo4j中有默认的初始化节点显示设置为300个节点，如果想要显示的节点多于300个，则会只显示300个，并给予以下提示语句： Not all return nodes are being displayed due to Initial Node Display setting. Only 300 of 300 nodes are being displayed. 解决方法：在如图所示initial Node Display处可以修改，在此处修改为300000.]]></content>
      <categories>
        <category>Neo4j</category>
      </categories>
      <tags>
        <tag>Neo4j</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Bidirectional LSTM-CRF Models for Sequence Tagging》阅读笔记]]></title>
    <url>%2Fpost%2Fread_Bidirectional_LSTM-CRF_Models_for_Sequence_Tagging%2F</url>
    <content type="text"><![CDATA[这篇论文可以作为一个RNN和LSTM学习的一个例子来看，有利于新手对LSTM的理解。对于NER的处理主要是作为一个序列标注问题。但是作为经典文章还是可以读一读了解一下的。 在本篇论文中，作者提出了4种模型：LSTM、BI-LSTM、LSTM-CRF和BI-LSTM-CRF。 contribution(贡献) 作者在NLP标注数据集上系统的对比了以上四个模型； 作者是首先提出把BI-LSTM-CRF模型用于NLP序列标注，并且达到了state-of-the-art的水平； 作者展示了BI-LSTM-CRF是robust，并且极少依赖于词向量。 model(模型)LSTM首先，作者先介绍了RNN的结构和工作原理，如图： 其中输入为句子：EU rejects German call to boycott British lamb。输出为标签：B-ORG O B-MISC O O O B-MISC O O，其中B-，I-表示实体开始和中间位置。标签种类为：other (O)和四种实体标签：Person (PER), Location (LOC), Organization (ORG), and Miscellaneous (MISC). 输入层表示在时间步 t 的特征。它们可以是 one-hot-encoding 的词特征，稠密或者稀疏的向量特征。输入层与特征有相同大小的维度。输出层表示在时间步 t 的标签上的概率分布，维度与标注数量相同。相比前馈神经网络，RNN 引入前一个隐藏状态和当前隐藏状态的结合，因此可以储存历史信息。 涉及公式为： 其中，U，W，V都是权重，函数f，g分别为sigmoid和softmax函数。 接下来，作者展示了LSTM的结构和原理，如图： 公式： 其中，σ是逻辑sigmoid函数，i, f, o 和 c分别是输入门，忘记门，输出门和细胞向量，所有的大小都和向量h一样。w权重的含义如其下表所示。 LSTM序列标注模型如图所示： 其中，中间的画斜线的格子即为图2中所示部分。 Bidirectional LSTM(双向LSTM)作者展示了双向LSTM的结构，如图所示： 双向LSTM网络可以有效利用过去特征和未来特征。在作者的实现中，对于整个句子的前向和后向操作，作者只需要在每个句子开始时将隐藏状态重置为0。作者采用批处理，使得可以同时处理多个句子。 CRF使用邻居标记信息预测当前标记有两种不同的方法： 预测每个时间步长的标签分布，然后使用波束式解码来找到最优的标签序列，代表方法：MEMMs 注重句子层次而不是个体位置，代表方法：CRF，输入和输出是直接相连的；如图： 研究表明，CRFs一般能够产生更高的标签精度。 LSTM-CRF作者展示了LSTM-CRF的结构，如图： 这网络可以有效地通过 LSTM 利用过去的输入特征和通过 CRF 利用句子级的标注信息。图中CRF层由连接连续输出层的线表示。CRF层有一个状态转移矩阵作为参数。 公式为： 函数f为网络的输出分数，[x]为输入， [fθ]i,t 为带有参数θ（句子x，第i 个标签，第t个单词）的网络输出； [A]i,j为转移分数，从连续的时间步i状态到j状态的转移分数。注意，该转换矩阵是位置无关的。 BI-LSTM-CRF作者展示了BI-LSTM-CRF的结构，如图所示： 作者在实验中展示了额外的未来特征可以提高标签的准确率。 训练过程本文使用的所有模型都共享一个通用SGD前向和后向训练过程。作者展示了BI-LSTM-CRF的算法，如图 作者设置了批次大小为100。 实验data作者在以下三个数据集上测试自己的模型：Penn TreeBank (PTB) POS tagging, CoNLL 2000 chunking, and CoNLL 2003 named entity tagging.数据集信息展示如下： Features作者从三个数据集中提取出其公共特征。特征可以分为拼写特征和上下文特征。最终，作者对于POS（词性标注）、chunking（组块）和NER（命名实体识别）分别提取401K，76K和341K个特征。 spelling features（拼写特征）除了小写字母特征之外，我们提取给定单词的以下特征。 context featurs（上下文特征）对于单词特征，作者使用unigram和bi-grams特征。对于在CoNLL2000数据集中的POS特征和在CoNLL2003数据集中的 POS &amp; CHUNK特征，作者使用了unigram，bi-gram和tri-gram特征。 词向量词向量在改进序列标注任务的表现方面起着至关重要的作用，我们使用 130K 词汇并且每个词汇的词向量维度是 50 维。我们将 one-hot-encoding词表示替换每个词对应的词向量。 Features connection tricks我们可以将拼写和上下文特征与单词特征一样对待。这样网络的输入包括单词，单词的拼写和上下文特征。然而，==我们发现将拼写和上下文特征与输出直接连接可以加速训练过程，同时也能保持标注的准确率，==如下图所示： 我们注意到，这种特征的使用与使用的最大熵特征类似。区别在于采用特征三列技术可能会发生特征冲突。由于序列标注数据集中的输出标签小于语言模型（通常为数十万），所以我们可以在特征和输出之间建立完整的连接，以避免潜在的特征冲突。 结果在相同的数据集上分别训练LSTM，BI-LSTM，CRF，LSTM-CRF和BI-LSTM-CRF模型，并且采用两种方式初始化word embedding：随机和Senna方式。模型的训练速率为0.1，隐藏层数量为300.不同模型在不同word embedding下的结果如表2所示，同时列出了之前最好模型Cov-CRF。 与Cov-CRF比较 实验中设置了3个基准模型，分别为LSTM、BI-LSTM和CRF，结果中LSTM在三个数据集中效果最差，BI-LSTM跟CRF在POS和chunking中效果接近，但是在NER中后者要优于前者。有趣的是表现最好的模型BI-LSTM-CRF相对于Cov-CRF来说对Senna embedding的依赖程度更小。 (robustness)模型鲁棒性 为验证模型的鲁棒性，对不同模型只采用word feature特征进行训练，训练结果如表3，括号中数字表示相比于全部特征，模型的结果下降数值。 与其他系统的比较 这里就不贴图了，总之就是阐述作者自己模型好。 结论总之作者的模型是基于之前模型的一些改进，主要运用了IBI-LSTM和CRF的结合。 论文下载地址]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>NER</tag>
        <tag>LSTM</tag>
        <tag>BI-LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CN-DBpedia A Never-Ending Chinese Knowledge Extraction SystemCN-DBpedia System]]></title>
    <url>%2Fpost%2Fessay%2F</url>
    <content type="text"><![CDATA[前言 本篇论文为2016年的一篇论文，主要介绍了作者构建中文知识图谱所遇到的一些问题和解决方法。 challenge 如何降低人力成本？ 如何保持知识库的新鲜度？ 贡献 在构建中文知识库中降低了人力成本： 重复利用已经存在的本体论 提出了一个不用人工监督的端到端的深度学习模型 提出了一个智能主动更新策略 系统结构 提高知识库质量： Normalization： normalize the attributes and values Enrichment：reuse the ontology Correction：two steps error detection: rule-based detection based on user feedbacks error correction crowd-sourcing 降低人力成本这部分作者采用了两种方法： 重复利用已经存在在知识库的本体论和类型化的中文实体 构建一个端到端提取器 Cross-Lingual Entity Typing（跨语言的实体类型） 第一步是通过用英文DBpedia类型来类型化中文实体。为了达到这个目的，作者提出了如下系统：系统建立了监督层次分类模型，系统输入为没有标记类型的中文实体，输出为在DB中所有有效的英文类型。作者将中文实体与共享相同中文标签名称的英语实体配对，这样中文实体以及配对英语实体的类型自然是标记样本。 用上述方法得到的训练集可能出现下面一些问题： 英文DBpedia实体类型在许多情况下可能不完全； 英文DBpedia实体类型在许多情况下可能是错误的； 中英文链接可能出错； 中文实体的特征通常不完整。 为了解决以上问题，作者提出了两种方法： 完善英文DBpedia实体类型； 设计一个过滤步骤来剔除错误样本。 infobox completion Infobox completion is a task to extract object for a given pair of entity and predicate from encyclopedia articles. 作者建模了一个seq2seq模型，输入为包含tokens的自然语言句子，输出为每个token的标签。对于标签为0或1。 对于建立一个有效的提取器有以下两个关键： 如何构建训练集：作者采用远程监督方法（利用Wikipedia） 如何选取期望的提取模型：LSTM-RNN，如图所示 知识库更新作者采用动态更新：识别新实体或可能包含新事实的旧实体 作者根据以下两方面来辨别这些实体： 近期热点新闻中提及的实体 在搜索引擎的流行搜索关键字或其他流行网页中提到的实体 对于如何从新闻标题和搜素指令中提取实体名字，作者采用简单的词分割方法，从百科全书中判断其是否为实体，并提出IDF值低的分割子串。 统计数据 论文下载链接]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
        <tag>机器学习</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ontology reasoning with deep neural networks]]></title>
    <url>%2Fpost%2Fessay%2F</url>
    <content type="text"><![CDATA[Ontology reasoning with deep neural networks（基于深度神经网络的本体推理）前言 本论文属于知识图谱的下游任务，也就是知识图谱的应用的一个例子。这篇论文的方法根据作者描述RRN是第一个基于深度学习的全面本体推理方法。 目标获得一个可以在不同的场景进行有效推理的模 问题描述基于机器学习的推理文章通常假设了一个特定的应用案例：自然语言或视觉输入的推理。作者采用一个不同的方法：将正式的推理问题作为起点。对于特定的问题选择：选择一种在表现力与另一方面复杂性之间取得适当平衡的方法通常是明智的。OWL RL本体推理是指一种常见的场景，在这种场景中，用于推理的推理规则（在此上下文中称为本体）与我们寻求推理的事实信息一起指定。 本体推理是一种非常灵活的工具，它允许对大量不同的场景进行建模，因此满足了我们对适用于各种应用的系统的需求。==首先引出了什么是本质推理，然后进一步阐述为什么要用机器学习== 今天用于推理的大多数KRR形式都植根于符号逻辑,这些方法在实践中会遇到许多问题：例如处理不完整，冲突或不确定数据的困难机器学习模型通常具有高度可扩展性，更能抵抗数据中的干扰，并且即使所提供的形式是错误的也能够提供预测。 作者的目标是通过采用尖端的深度学习技术，目标是在近似于形式方法的高度期望（理论）属性和另一方面利用机器学习的稳健性之间管理平衡行为。 对于用于推理的知识图谱：作者采用的是由个体、类和二元关系组成的信息构成，其中个体对应于顶点，关系对应于被标记的有向边缘，类对应于二进制顶点标签。关系是主体和客体之间的关系或者个人和类之间的关系。这与关系学习不同：在关系学习的背景下，知识图通常通过将类视为个人以及将成员视为普通关系来简化。然而，就作者的目的而言，明确区分类和关系是很重要的，因为在用于推理的知识图谱中类和关系可能不同。 模型总览整个模型是以RRN为基础进行构建的，每个RRN都针对特定的本体进行训练。当训练模型应用于一组特定的事实时，它分为如下两个步骤： 它为所有的步骤生成矢量表示，也就是嵌入在所考虑数据中出现的个体。 它仅基于这些生成向量计算查询预测 在图中， a中它考虑一个事实三元组，并根据数据集重复多次。 b中它每读取一个事实就获取三元组中的个体潜入，并将他们的反馈送入更新层，该层产生已提供的嵌入的更新版本，然后将其存储在前一个版本的位置。 c中从随机生成的向量开始，逐步更新嵌入，以便对关于它们所代表的个体的事实和推论进行编码。 评估作者在四个不同的数据集上训练和评估了RRN，其中两个是人工生成的玩具数据集，两个是从现实世界的数据库中提取的。这样做的原因： 玩具问题具有很大的优势，即很明显某些推论是多么困难，从而为我们提供了对模型能力的相当好的印象。 在现实环境中评估方法当然是性能不可或缺的衡量标准 作者为了评估真实世界数据的RRN模型，还从从两个著名的知识库DBpedia和Claros中提取了数据集。 结果 RRN能够有效地编码提供的关于类和关系的事实 对于关系的推理，可以看到DBpedia的准确度略低于98.9％，而其他数据集中的可导出关系在所有情况中至少99.6％被正确预测。 可以预测该模型在预测可推断类别方面比在关系方面表现更好，因为大多数这些都是仅依赖于单个三元组的推论。 为了评估作者提出的KRR方法常常遇到的问题，作者进行了如下实验： 对于缺少信息的问题，作者随机删除了一个无法通过每个样本的符号推理推断出的事实，并检查模型是否能够正确地重建它。结果：对于DBpedia来说，33.8％的失踪三元组就是这种情况，而对于Claros来说，38.4％被正确预测 对于冲突的问题，作者通过在每个测试样本中随机选择一个事实来测试模型解决冲突的能力，并添加相同的否定版本作为另一个事实。对于DBpedia，RRN正确解决了88.4％的引入冲突，而对于Claros，它甚至达到了96.2％。然而，最重要的是，对于任何一个损坏的数据集，之前报告的总精度都没有下降超过0.9。 所有RRN的查询预测都完全基于它为各个数据集中的个体生成的嵌入，这就是为什么仔细研究这样一组嵌入向量是有益的。 思考本论文属于知识图谱的下游任务，也就是知识图谱的应用的一个例子。这篇论文的方法根据作者描述RRN是第一个基于深度学习的全面本体推理方法。但是具体的操作方法论文中写的比较清晰，感觉自己是理解了。重点就是对于个体的嵌入表示，如果类比的话就是词向量，作者通过不断的处理更新这个词向量，最后通过所获的词向量进行推理。并且从这篇文章中可以看到作者使用的知识图谱和我之前在弄的关系三元组有所区别。 论文下载链接]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>Ontology</tag>
        <tag>知识图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初次见面，你好NYSDY！]]></title>
    <url>%2Fpost%2Fessay%2F</url>
    <content type="text"></content>
  </entry>
</search>
