<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Modeling Relational Data with Graph Convolutional Networks é˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FModeling%20Relational%20Data%20with%20Graph%20Convolutional%20Networks%2F</url>
    <content type="text"><![CDATA[1. æ€»è§ˆè¦è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿï¼ˆwhatï¼Ÿï¼‰é“¾æ¥é¢„æµ‹å’Œå®ä½“åˆ†ç±» ä¸ºä»€ä¹ˆè¦è§£å†³è¿™ä¸ªé—®é¢˜ï¼Ÿï¼ˆwhyï¼Ÿï¼‰å› ä¸ºè™½ç„¶çŸ¥è¯†å›¾è°±ç”¨é€”å¾ˆå¤šï¼Œè€Œç°æœ‰çš„çŸ¥è¯†å›¾è°±éƒ½å­˜åœ¨ä¸å®Œæ•´çš„é—®é¢˜ã€‚ ç”¨ä»€ä¹ˆæ–¹æ³•è§£å†³ï¼Ÿï¼ˆhowï¼Ÿï¼‰ ç”¨å›¾å·ç§¯ç½‘ç»œå’Œå› å¼åˆ†è§£ç›¸ç»“åˆæ¥è§£å†³é“¾æ¥é¢„æµ‹é—®é¢˜ï¼› ç”¨å›¾å·ç§¯ç½‘ç»œå•ç‹¬è§£å†³å®ä½“åˆ†ç±»é—®é¢˜ã€‚ æ–‡ç« æœ‰ä»€ä¹ˆåˆ›æ–°ï¼Ÿ é¦–æ¬¡æŠŠGCNå¼•å…¥å…³ç³»æ•°æ®å»ºæ¨¡ï¼› æå‡ºäº†ä¸€ç§å‚æ•°å…±äº«å’Œå¢å¼ºç¨€ç–é™åˆ¶çš„æ–¹æ³•â€”â€”æƒé‡çŸ©é˜µçš„åŸºæœ¬åˆ†è§£å’Œå—åˆ†è§£ ç”¨GCNä¸å› å¼åˆ†è§£ç»„æˆauto-encoderçš„æ–¹æ³•ï¼Œå¯ä»¥æé«˜å› å¼åˆ†è§£æ¨¡å‹åœ¨é“¾æ¥é¢„æµ‹ä¸Šçš„æ•ˆæœã€‚ æ•ˆæœå¦‚ä½•ï¼Ÿåœ¨FB15k-237ä¸Šé«˜å‡ºbaseline 29.8% è¿˜å­˜åœ¨ä»€ä¹ˆé—®é¢˜ï¼Ÿ2 æ¨¡å‹R-GCN 2.1 Relational graph convolutional networksä½œè€…åŸºäºæ¶ˆæ¯ä¼ é€’æ¡†æ¶ï¼š h_{i}^{(l+1)}=\sigma\left(\sum_{m \in \mathcal{M}_{i}} g_{m}\left(h_{i}^{(l)}, h_{j}^{(l)}\right)\right) å…¶ä¸­$h_{i}^{(l)} \in \mathbb{R}^{d^{(l)}}$æ˜¯èŠ‚ç‚¹$v_i$åœ¨ç¬¬$l$å±‚ç¥ç»ç½‘ç»œçš„éšè—çŠ¶æ€ï¼›$d^{(l)}$æ˜¯å½“å‰å±‚è¡¨ç¤ºçš„ç»´åº¦ï¼›$g_m$æ˜¯ä¼ å…¥æ¶ˆæ¯çš„ç´¯ç§¯å½¢å¼ã€‚ è®¾è®¡äº†ä¸€ä¸ªåœ¨å…³ç³»å¤šå›¾ä¸­çš„ä¼ æ’­ç­–ç•¥ï¼š h_{i}^{(l+1)}=\sigma\left(\sum_{r \in \mathcal{R}} \sum_{j \in \mathcal{N}_{i}^{r}} \frac{1}{c_{i, r}} W_{r}^{(l)} h_{j}^{(l)}+W_{0}^{(l)} h_{i}^{(l)}\right) å…¶ä¸­$\mathcal{N}_{i}^{r}$ä»£è¡¨åœ¨å…³ç³»$r \in \mathcal{R}$ä¸‹èŠ‚ç‚¹$i$çš„é‚»å±…ç´¢å¼•é›†åˆã€‚å…¶ä¸­$c_{i, r}$æ˜¯ä¸€ä¸ªç‰¹å®šäºé—®é¢˜çš„å½’ä¸€åŒ–å¸¸æ•°ï¼Œå¯ä»¥é¢„å…ˆå­¦ä¹ æˆ–é€‰æ‹©$c_{i, r}=\left|\mathcal{N}_{i}^{r}\right|$ã€‚$W_{0}^{(l)} h_{i}^{(l)}$æ˜¯ä½œè€…æ·»åŠ çš„å¯¹äºæ¯ä¸ªèŠ‚ç‚¹çš„ä¸€ä¸ªç‰¹å®šå…³ç³»çš„è‡ªè¿æ¥ã€‚ æ­¤å¤„é‡‡å–ç®€å•çš„çº¿æ€§æ¶ˆæ¯è½¬æ¢ï¼Œå…¶å®å¯ä»¥é€‰æ‹©æ›´çµæ´»çš„å‡½æ•°ï¼Œå¦‚å¤šå±‚ç¥ç»ç½‘ç»œ(ä»¥ç‰ºç‰²è®¡ç®—æ•ˆç‡ä¸ºä»£ä»·)ã€‚ 2.2 Regularization | æ­£åˆ™åŒ–æ ¸å¿ƒé—®é¢˜ï¼šåœ¨å¤„ç†å¤šå…ƒå…³ç³»æ•°æ®æ—¶ï¼Œå›¾ä¸­å‚æ•°æ•°é‡å’Œå…³ç³»æ•°é‡å¿«é€Ÿå¢é•¿å¯èƒ½ä¼šå¯¼è‡´å¯¹ç¨€æœ‰å…³ç³»çš„è¿‡æ‹Ÿåˆå’Œæ¨¡å‹è§„æ¨¡è¿‡å¤§ã€‚ ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ä½œè€…æå‡ºäº†ä¸¤ç§è°ƒæ•´R-GCNå±‚çš„æƒé‡çš„æ–¹å¼ï¼š basis-decomposition: W_{r}^{(l)}=\sum_{b=1}^{B} a_{r b}^{(l)} V_{b}^{(l)} å…¶ä¸­$V_{b}^{(l)} \in \mathbb{R}^{d^{(l+1)} \times d^{(l)}}$æ˜¯ä¸€ä¸ªä½œä¸ºåŸºç¡€å˜æ¢çš„çº¿æ€§ç»„åˆï¼Œ$a_{r b}^{(l)}$æ˜¯ä¸€ä¸ªåªä¾èµ–äº$r$çš„ç³»æ•°ã€‚ åŸºå‡½æ•°åˆ†è§£å¯ä»¥çœ‹ä½œæ˜¯ä¸åŒå…³ç³»ç±»å‹ä¹‹é—´æœ‰æ•ˆæƒé‡å…±äº«çš„ä¸€ç§å½¢å¼ block-diagonal-decomposition: W_{r}^{(l)}=\bigoplus_{b=1}^{B} Q_{b r}^{(l)} å…¶ä¸­$Q_{b, r}^{(l)} \in \mathbb{R}^{\frac{d(l+1)}{B} \times \frac{d^{(l)}}{B}}$,$W_{r}^{(l)}$æ˜¯å¯¹è§’å—çŸ©é˜µï¼š$\operatorname{diag}\left(Q_{1 r}^{(l)}, \ldots, Q_{B r}^{(l)}\right)$ã€‚ è€Œå—åˆ†è§£å¯ä»¥çœ‹åšæ˜¯æ¯ç§å…³ç³»ç±»å‹å¯¹æƒé‡çŸ©é˜µçš„ç³»æ•°çº¦æŸã€‚ å—åˆ†è§£ç»“æ„ç¼–ç ä¸€ç§ç›´è§‰ï¼Œå³å¯ä»¥å°†æ½œåœ¨ç‰¹å¾åˆ†ç»„ä¸ºå˜é‡é›†ï¼Œè¿™äº›å˜é‡é›†åœ¨ç»„å†…æ¯”ç»„é—´æ›´åŠ ç´§å¯†åœ°è”ç³»ã€‚ åŒæ—¶ï¼Œä½œè€…æœŸæœ›åŸºæœ¬å‚æ•°åŒ–å¯ä»¥å‡è½»ç¨€ç–å…³ç³»çš„è¿‡åº¦æ‹Ÿåˆï¼Œå› ä¸ºç¨€ç–å…³ç³»å’Œæ›´é¢‘ç¹å…³ç³»ä¹‹é—´å…±äº«å‚æ•°æ›´æ–°ã€‚ å¯¹æ‰€æœ‰çš„R-GCNæ¨¡å‹éƒ½é‡‡å–ä»¥ä¸‹å½¢å¼ï¼š æŒ‰ç…§ä½œè€…æå‡ºçš„ä¼ æ’­æ¨¡å‹è¿›è¡Œå †å $L$å±‚ å¦‚æœä¸å­˜åœ¨å…¶ä»–ç‰¹å¾ï¼Œåˆ™å¯ä»¥å°†ç¬¬ä¸€å±‚çš„è¾“å…¥é€‰æ‹©ä¸ºå›¾ä¸­æ¯ä¸ªèŠ‚ç‚¹çš„å”¯ä¸€one-hotå‘é‡ï¼› å¯¹äºå—è¡¨ç¤ºï¼Œä½œè€…å°†one-hotå‘é‡é€šè¿‡ä¸€ä¸ªå•ä¸€çš„çº¿æ€§è½¬æ¢ä¸ºä¸€ä¸ªdenseè¡¨ç¤ºï¼› ä½œè€…åªè€ƒè™‘ç”¨ä¸€ä¸ªfeaturelessçš„æ–¹æ³•ï¼Œä¸åŒäºGCNæ¨¡å‹ã€‚ 3 å®ä½“åˆ†ç±» ä½œè€…é€šè¿‡å †å R-GCNçš„ä¼ æ’­å‡½æ•°ï¼Œæœ€åä¸€å±‚é€šè¿‡softmaxå‡½æ•°è¾“å…¥ï¼Œåœ¨æ‰€æœ‰èŠ‚ç‚¹ä¸Šé‡‡ç”¨äº¤å‰ç†µæŸå¤±ï¼ˆå¿½ç•¥æ— æ ‡ç­¾èŠ‚ç‚¹ï¼‰ï¼š \mathcal{L}=-\sum_{i \in \mathcal{Y}} \sum_{k=1}^{K} t_{i k} \ln h_{i k}^{(L)} å…¶ä¸­ $\mathcal{Y}$ä»£è¡¨æœ‰æ ‡ç­¾çš„èŠ‚ç‚¹ç´¢å¼•é›†ï¼Œ$h_{i k}^{(L)}$ä»£è¡¨ç¬¬$i$ä¸ªæ ‡è®°èŠ‚ç‚¹çš„ç½‘ç»œè¾“å‡ºçš„ç¬¬$k$ä¸ªæ¡ç›®ï¼Œ$t_{ik}$è¡¨ç¤ºçœŸå®æ ‡ç­¾ã€‚ 4 é“¾æ¥é¢„æµ‹ ä½œè€…å¼•å…¥ä¸€ä¸ªauto-encoderæ¨¡å‹ï¼š encoderï¼šå°†æ¯ä¸ªå®ä½“$v_{i} \in \mathcal{V}$ æ˜ å°„åˆ°ä¸€ä¸ªæ˜¯æŒ‡å‘é‡ $\vec{e}_{i} \in \mathbb{R}^{d}$ ï¼› decoderï¼šæ ¹æ®é¡¶ç‚¹è¡¨ç¤ºé‡å»ºå›¾çš„è¾¹ï¼šé€šè¿‡ä¸€ä¸ªscorceå‡½æ•° $s: \mathbb{R}^{d} \times \mathcal{R} \times \mathbb{R}^{d} \rightarrow \mathbb{R}$ æ¥å°†ä¸‰å…ƒç»„(subject, relation, object)æ˜ å°„ä¸ºä¸€ä¸ªå®æ•°åˆ†æ•°ã€‚ åœ¨æœ¬æ–‡ä¸­ï¼Œä½œè€…é‡‡ç”¨çš„æ˜¯DistMultå‡½æ•°ä½œä¸ºè§£ç å™¨ï¼Œæ¯ä¸€ä¸ªå…³ç³»$r$è¢«æ˜ å°„ä¸ºä¸€ä¸ªå¯¹è§’çŸ©é˜µ $R_{r} \in \mathbb{R}^{d \times d}$ï¼Œä¸€ä¸ªä¸‰å…ƒç»„(s,r,o)çš„åˆ†æ•°ä¸ºï¼š f(s, r, o)=e_{s}^{T} R_{r} e_{o}æœ€åä½œè€…å¾—å‡ºè‡ªå·±çš„æŸå¤±å‡½æ•°ï¼š \mathcal{L}=-\frac{1}{(1+w)|\hat{\mathcal{E}}|} \sum_{(s, r, o, y) \in \mathcal{T}} y \log l(f(s, r, o))+(1-y) \log (1-l(f(s, r, o))) å…¶ä¸­ $\mathcal{T}$ æ˜¯çœŸå®ä¸‰å…ƒç»„å’Œç ´åå¾—åˆ°çš„è´Ÿæ ·ä¾‹ä¸‰å…ƒç»„çš„æ€»æ ·æœ¬ã€‚ 5 å®éªŒ 5.1 å®ä½“åˆ†ç±»å®éªŒ æ•°æ®é›†ï¼šAIFBã€MUTAGã€BGSã€AM baselineï¼šFeatã€WLã€RDF2Vec è¯„ä»·å‡†åˆ™ï¼šå‡†ç¡®ç‡ ç»“æœï¼š 5.2 é“¾æ¥é¢„æµ‹ å…³ç³»æŠ½å–å®éªŒï¼š æ•°æ®é›†ï¼šWordNet(WN18)ï¼ŒFreebase(FB15K) baselineï¼šLinkFeat,DistMult,CP,TransE,HolE,ComplEx è¯„ä»·å‡†åˆ™ï¼šMRR(mean reciprocal rank)ï¼ˆRawï¼ŒFilteredï¼‰ï¼ŒHits @ï¼ˆ1ï¼Œ3ï¼Œ10ï¼‰ å®éªŒç»“æœï¼š å…¶ä¸­ $f(s, r, t)_{\mathrm{R}-\mathrm{GCN+}} = \alpha f(s, r, t)_{\mathrm{R}-\mathrm{GCN}}+(1-\alpha) f(s, r, t)_{\text {DistMult }}$,R-GCN å’Œ DistMultéƒ½æ˜¯å„è‡ªè®­ç»ƒå¥½çš„ã€‚ å‚è€ƒé“¾æ¥ https://blog.csdn.net/yyl424525/article/details/102764903 https://blog.csdn.net/weixin_35505731/article/details/104581783 è®ºæ–‡ä¸‹è½½åœ°å€ï¼šModeling Relational Data with Graph Convolutional Networks]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>ESWC</tag>
        <tag>2018</tag>
        <tag>GCN</tag>
        <tag>Link Prediction</tag>
        <tag>Entity Classification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubantuç³»ç»Ÿå®‰è£…pytorch GPUç‰ˆæœ¬]]></title>
    <url>%2Fpost%2Fubantu%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85pytorch-GPU%E7%89%88%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[0 å‡†å¤‡å·¥ä½œç”¨condaå®‰è£…Pytorchè¿‡ç¨‹ä¸­ä¼šè¿æ¥å¤±è´¥ï¼Œè¿™æ˜¯å› ä¸ºAnaconda.orgçš„æœåŠ¡å™¨åœ¨å›½å¤–ï¼Œéœ€è¦åˆ‡æ¢åˆ°å›½å†…é•œåƒæºï¼š conda config --add channels &lt;https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/> conda config --add channels &lt;https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/> conda config --add channels &lt;https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/> è®¾ç½®æœç´¢æ—¶æ˜¾ç¤ºé€šé“åœ°å€ conda config --set show_channel_urls yes åˆ›å»ºè™šæ‹Ÿç¯å¢ƒå¹¶æ¿€æ´»ä¹‹ï¼Œåœ¨è¯¥ç¯å¢ƒä¸‹å®‰è£…ä¸‹é¢çš„åŒ…ï¼š conda create -n pytorch_gpu python=3.6 source activate pytorch_gpu 1 å®‰è£…æ˜¾å¡é©±åŠ¨1.1. æŸ¥çœ‹æ˜¾å¡ç¡¬ä»¶å‹å·åœ¨ç»ˆç«¯è¾“å…¥ï¼šubuntu-drivers devicesï¼Œå¯ä»¥çœ‹åˆ°å¦‚ä¸‹ç•Œé¢ï¼š ä»ä¸Šå›¾å¯ä»¥çœ‹å‡ºï¼Œæˆ‘çš„æ˜¾å¡æ˜¯ï¼š[GeForce GTX 1080 Ti]ï¼Œæ‰€ä»¥æ¨èå®‰è£…çš„ç‰ˆæœ¬å·æ˜¯nvidia-driver-435 - distro non-free recommendedã€‚ 1.2. å¼€å§‹å®‰è£… å¦‚æœåŒæ„å®‰è£…æ¨èç‰ˆæœ¬ï¼Œé‚£æˆ‘ä»¬åªéœ€è¦ç»ˆç«¯è¾“å…¥ï¼šsudo ubuntu-drivers autoinstall å°±å¯ä»¥è‡ªåŠ¨å®‰è£…äº†ã€‚ å½“ç„¶æˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨ apt å‘½ä»¤å®‰è£…è‡ªå·±æƒ³è¦å®‰è£…çš„ç‰ˆæœ¬ï¼Œæ¯”å¦‚æˆ‘æƒ³å®‰è£… 340 è¿™ä¸ªç‰ˆæœ¬å·çš„ç‰ˆæœ¬ï¼Œç»ˆç«¯è¾“å…¥ï¼šsudo apt install nvidia-340 å°±è‡ªåŠ¨å®‰è£…äº†ã€‚ å®‰è£…è¿‡ç¨‹ä¸­æŒ‰ç…§æç¤ºæ“ä½œï¼Œé™¤éä½ çŸ¥é“æ¯ä¸ªæç¤ºçš„çœŸå®å«ä¹‰ï¼Œå¦åˆ™æ‰€æœ‰çš„æç¤ºéƒ½é€‰æ‹©é»˜è®¤å°±å¯ä»¥äº†ï¼Œå®‰è£…å®Œæˆåé‡å¯ç³»ç»Ÿï¼ŒNVIDIA æ˜¾å¡å°±å¯ä»¥æ­£å¸¸å·¥ä½œäº†ã€‚å®‰è£…å®Œæˆåä½ å¯ä»¥å‚ç…§ https://linuxconfig.org/benchmark-your-graphics-card-on-linux ä¸Šçš„ä»‹ç»æµ‹è¯•ä½ çš„æ˜¾å¡ã€‚ æœ€årebooté‡å¯å°±å¯ä»¥äº† 1.3. æŸ¥çœ‹NVIDIAé©±åŠ¨ç‰ˆæœ¬è¾“å…¥nvidia-smiï¼šæ˜¾ç¤ºå¦‚ä¸‹ï¼š 2 å®‰è£…CUDA0 CUDAå¯¹åº”çš„NVIDIAé©±åŠ¨ç‰ˆæœ¬å¯¹ç…§è¡¨ä¸€èˆ¬è€Œè¨€ï¼Œä¸åŒç‰ˆæœ¬çš„CUDAè¦æ±‚ä¸åŒçš„NVIDIAé©±åŠ¨ç‰ˆæœ¬,åŒæ—¶æ˜¾å¡é©±åŠ¨ç‰ˆæœ¬è¦ä¸ä½äºCUDAçš„å®‰è£…ç‰ˆæœ¬ï¼Œå…·ä½“çš„å¯¹ç…§å…³ç³»å¦‚ä¸‹ï¼šå®˜ç½‘åœ°å€ 1 å®‰è£…CUDAæŒ‰ç…§ä¸Šè¿°å¯¹åº”è¡¨ï¼Œæ‰¾åˆ°è¦æŒ‰ç…§çš„CUDAç‰ˆæœ¬ï¼Œæ¯”å¦‚æŒ‰ç…§ä¸Šå›¾æ¥è¯´åº”è¯¥å®‰è£…9.2ç‰ˆæœ¬ conda install cudatoolkit=9.2 -n pytorch_gpu -c &lt;https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/linux-64/> 2 å®‰è£…cuDNNæ‰€å®‰è£…çš„cuDNNç‰ˆæœ¬æ³¨æ„å’ŒCUDAå¯¹åº”ï¼Œå¯ä»¥åœ¨CUDAå®˜ç½‘æ‰¾åˆ°ç‰ˆæœ¬å¯¹åº”å…³ç³»ï¼š conda install cudnn=7.6.5 -n pytorch_gpu -c &lt;https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/linux-64/> 3 æ‰¾åˆ°å¯¹åº”pytorchç‰ˆæœ¬å…ˆåˆ°å®˜ç½‘æ‰¾åˆ°åœ¨ä½ çš„æ“ä½œç³»ç»Ÿã€åŒ…ã€CUDAç‰ˆæœ¬ã€è¯­è¨€ç‰ˆæœ¬ä¸‹å¯¹åº”çš„å®‰è£…è„šæœ¬ï¼Œå®˜ç½‘åœ°å€ï¼Œç›´æ¥æ ¹æ®ä½ çš„å®é™…æƒ…å†µé€‰æ‹©Pytorchå®‰è£…åŒ…ç‰ˆæœ¬ï¼Œç„¶åå¤åˆ¶é¡µé¢è‡ªåŠ¨ç”Ÿæˆçš„è„šæœ¬è¿›è¡Œå®‰è£…ã€‚ è¿è¡Œï¼š conda install pytorch torchvision cudatoolkit=9.2 -c pytorch # 3 æµ‹è¯•import torch flag = torch.cuda.is_available() print(flag) ngpu= 1 # Decide which device we want to run on device = torch.device("cuda:0" if (torch.cuda.is_available() and ngpu > 0) else "cpu") print(device) print(torch.cuda.get_device_name(0)) print(torch.rand(3,3).cuda()) ç»“æœï¼š True cuda:0 GeForce GTX 1080 tensor([[0.9530, 0.4746, 0.9819], [0.7192, 0.9427, 0.6768], [0.8594, 0.9490, 0.6551]], device='cuda:0') å‚è€ƒé“¾æ¥ Anacondaç¯å¢ƒå®‰è£…GPUç‰ˆæœ¬Pytorchanaconda pytorch gpu CUDAå¯¹åº”çš„NVIDIAé©±åŠ¨ç‰ˆæœ¬å¯¹ç…§è¡¨_zhw864680355çš„åšå®¢-CSDNåšå®¢_cudaå¯¹åº”çš„é©±åŠ¨ç‰ˆæœ¬ pytorchï¼šæµ‹è¯•GPUæ˜¯å¦å¯ç”¨_æ˜æœˆå‡ æ—¶æœ‰ï¼ŒæŠŠé…’é—®é’å¤©-CSDNåšå®¢_torch gpu]]></content>
      <categories>
        <category>æŠ€æœ¯</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Diachronic Embedding for Temporal Knowledge Graph Completion]]></title>
    <url>%2Fpost%2FDiachronic%20Embedding%20for%20Temporal%20Knowledge%20Graph%20Completion%2F</url>
    <content type="text"><![CDATA[è®ºæ–‡ä¸‹è½½åœ°å€ Introductionwhat your paper is about?temporal knowledge graph completion what problem it solves?æå‡ºäº†ä¸€ä¸ªDiachronic Embedding èƒ½å’Œå·²æœ‰çš„staticæ¨¡å‹ç»“åˆæ¥è§£å†³temporal knowledge graph completionï¼Œå¹¶ä¸”èƒ½ç”Ÿæˆä¸€ä¸ªunseen timestampsã€‚ why the problem is interesting?Developing temporal KG embedding models is an increasingly important problem. what is really new (and what isnâ€™t)? novel models for temporal KG completion through equipping static models with a diachronic entity embedding function which provides the characteristics of entities at any point in time. This is in contrast to the existing temporal KG embedding approaches where only static entity features are provided. The proposed embedding function is model-agnostic and can be potentially combined with any static model. combining it with SimplE results in a fully expressive model for temporal KG completion.ï¼ˆä½œè€…è¿˜æä¾›äº†è¯æ˜ï¼Œä¸è¿‡æˆ‘æ²¡çœ‹ï¼‰ Diachronic Embedding Î³ç”¨æ¥æ§åˆ¶temporalçš„æ¯”ä¾‹ã€‚ experiments Activation Function Adding Diachronic Embedding for Relations: å®éªŒå±•ç¤ºäº†å…³ç³»éšæ—¶é—´å˜åŒ–å½±å“å°ï¼Œå³ä½¿åŠ å…¥å…³ç³»å’Œæ—¶é—´çš„è”ç³»å¯¹ç»“æœæå‡ä¹Ÿä¸å¤§ Generalizing to Unseen Timestamps we created a variant of the ICEWS14 dataset by including every fact except those on the 5th, 15th, and 25th day of each month in the train set. We split the excluded facts randomly into validation and test sets (removing the ones including entities not observed in the train set). This ensures that none of the timestamps in the validation or test sets has been observed by the model in the train set.]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>KGE</tag>
        <tag>KG</tag>
        <tag>temporal KG</tag>
        <tag>2019</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ›´æ–°åšå®¢ä¸»é¢˜]]></title>
    <url>%2Fpost%2FUpdate-blog-theme%2F</url>
    <content type="text"><![CDATA[æ­£å¥½èµ¶ä¸Šæœ€è¿‘æœ‰ç©ºï¼Œä¿®æ”¹å‡çº§ä¸€ä¸‹è‡ªå·±çš„åšå®¢ï¼Œä¹‹å‰ç”¨çš„æ˜¯nextä¸»é¢˜ï¼Œå¶ç„¶é—´å‘ç°äº†materyä¸»é¢˜ï¼Œæ„Ÿè§‰æ›´åŠ æ¼‚äº®ä¸€äº›ï¼Œå°±å†³å®šæ›´æ¢ä¸€ä¸‹ï¼Œé¡ºä¾¿è®°å½•è‡ªå·±çš„ä¸€äº›æ›´æ¢æ“ä½œã€‚ é…ç½®åˆ‡æ¢ä¸»é¢˜ ä¸‹è½½ç›¸åº”ä¸»é¢˜æ”¾å…¥\themeè·¯å¾„ä¸‹å³å¯ ä¿®æ”¹ Hexo æ ¹ç›®å½•ä¸‹çš„ _config.yml çš„ theme çš„å€¼ï¼štheme: hexo-theme-matery _config.yml æ–‡ä»¶çš„å…¶å®ƒä¿®æ”¹å»ºè®®: è¯·ä¿®æ”¹ _config.yml çš„ url çš„å€¼ä¸ºä½ çš„ç½‘ç«™ä¸» URLï¼ˆå¦‚ï¼šhttp://xxx.github.ioï¼‰ã€‚ å»ºè®®ä¿®æ”¹ä¸¤ä¸ª per_page çš„åˆ†é¡µæ¡æ•°å€¼ä¸º 6 çš„å€æ•°ï¼Œå¦‚ï¼š12ã€18 ç­‰ï¼Œè¿™æ ·æ–‡ç« åˆ—è¡¨åœ¨å„ä¸ªå±å¹•ä¸‹éƒ½èƒ½è¾ƒå¥½çš„æ˜¾ç¤ºã€‚ å¦‚æœä½ æ˜¯ä¸­æ–‡ç”¨æˆ·ï¼Œåˆ™å»ºè®®ä¿®æ”¹ language çš„å€¼ä¸º zh-CNã€‚ æ–°å»ºåˆ†ç±» categories é¡µcategories é¡µæ˜¯ç”¨æ¥å±•ç¤ºæ‰€æœ‰åˆ†ç±»çš„é¡µé¢ï¼Œå¦‚æœåœ¨ä½ çš„åšå®¢ source ç›®å½•ä¸‹è¿˜æ²¡æœ‰ categories/index.md æ–‡ä»¶ï¼Œé‚£ä¹ˆä½ å°±éœ€è¦æ–°å»ºä¸€ä¸ªï¼Œå‘½ä»¤å¦‚ä¸‹ï¼š hexo new page "categories" ç¼–è¾‘ä½ åˆšåˆšæ–°å»ºçš„é¡µé¢æ–‡ä»¶ /source/categories/index.mdï¼Œè‡³å°‘éœ€è¦ä»¥ä¸‹å†…å®¹ï¼š --- title: åˆ†ç±» date: 2018-09-24 12:03:14 type: "categories" layout: "categories" --- ç›¸æ¯”nextè€Œè¨€ï¼Œé¡µé¢æ–‡ä»¶ä¸­å¤šåŠ äº†layoutä¸€é¡¹ï¼Œè¯¥é¡¹ä¸­æ˜¯å¦åŠ å…¥åŒå¼•å·å‡å¯ã€‚ æ–°å»ºæ ‡ç­¾ tags é¡µé¢ï¼Œå¦‚æœåœ¨ä½ çš„åšå®¢ source ç›®å½•ä¸‹è¿˜æ²¡æœ‰ tags/index.md æ–‡ä»¶ï¼Œé‚£ä¹ˆä½ å°±éœ€è¦æ–°å»ºä¸€ä¸ªï¼Œå‘½ä»¤å¦‚ä¸‹ï¼š hexo new page "tags" ç¼–è¾‘ä½ åˆšåˆšæ–°å»ºçš„é¡µé¢æ–‡ä»¶ /source/tags/index.mdï¼Œè‡³å°‘éœ€è¦ä»¥ä¸‹å†…å®¹ï¼š --- title: æ ‡ç­¾ type: tags date: 2018-09-24 12:06:54 layout: tags --- æ–°å»ºå…³äºæˆ‘ about é¡µabout é¡µæ˜¯ç”¨æ¥å±•ç¤ºå…³äºæˆ‘å’Œæˆ‘çš„åšå®¢ä¿¡æ¯çš„é¡µé¢ï¼Œå¦‚æœåœ¨ä½ çš„åšå®¢ source ç›®å½•ä¸‹è¿˜æ²¡æœ‰ about/index.md æ–‡ä»¶ï¼Œé‚£ä¹ˆä½ å°±éœ€è¦æ–°å»ºä¸€ä¸ªï¼Œå‘½ä»¤å¦‚ä¸‹ï¼š hexo new page "about" ç¼–è¾‘ä½ åˆšåˆšæ–°å»ºçš„é¡µé¢æ–‡ä»¶ /source/about/index.mdï¼Œè‡³å°‘éœ€è¦ä»¥ä¸‹å†…å®¹ï¼š --- title: about date: 2018-09-30 17:25:30 type: "about" layout: "about" --- æ–°å»ºå‹æƒ…è¿æ¥ friends é¡µï¼ˆå¯é€‰çš„ï¼‰friends é¡µæ˜¯ç”¨æ¥å±•ç¤ºå‹æƒ…è¿æ¥ä¿¡æ¯çš„é¡µé¢ï¼Œå¦‚æœåœ¨ä½ çš„åšå®¢ source ç›®å½•ä¸‹è¿˜æ²¡æœ‰ friends/index.md æ–‡ä»¶ï¼Œé‚£ä¹ˆä½ å°±éœ€è¦æ–°å»ºä¸€ä¸ªï¼Œå‘½ä»¤å¦‚ä¸‹ï¼š hexo new page "friends" ç¼–è¾‘ä½ åˆšåˆšæ–°å»ºçš„é¡µé¢æ–‡ä»¶ /source/friends/index.mdï¼Œè‡³å°‘éœ€è¦ä»¥ä¸‹å†…å®¹ï¼š --- title: friends date: 2020-03-25 00:35:38 type: "friends" layout: "friends" --- åŒæ—¶ï¼Œåœ¨ä½ çš„åšå®¢ source ç›®å½•ä¸‹æ–°å»º _data ç›®å½•ï¼Œåœ¨ _data ç›®å½•ä¸­æ–°å»º friends.json æ–‡ä»¶ï¼Œæ–‡ä»¶å†…å®¹å¦‚ä¸‹æ‰€ç¤ºï¼š [{ "avatar": "http://image.luokangyuan.com/1_qq_27922023.jpg", "name": "ç é…±", "introduction": "æˆ‘ä¸æ˜¯å¤§ä½¬ï¼Œåªæ˜¯åœ¨è¿½å¯»å¤§ä½¬çš„è„šæ­¥", "url": "http://luokangyuan.com/", "title": "å‰å»å­¦ä¹ " }, { "avatar": "http://image.luokangyuan.com/4027734.jpeg", "name": "é—ªçƒä¹‹ç‹", "introduction": "ç¼–ç¨‹ç•Œå¤§ä½¬ï¼ŒæŠ€æœ¯ç‰›ï¼Œäººè¿˜ç‰¹åˆ«å¥½ï¼Œä¸æ‡‚çš„éƒ½å¯ä»¥è¯·æ•™å¤§ä½¬", "url": "https://blinkfox.github.io/", "title": "å‰å»å­¦ä¹ " }, { "avatar": "http://image.luokangyuan.com/avatar.jpg", "name": "ja_rome", "introduction": "å¹³å‡¡çš„è„šæ­¥ä¹Ÿå¯ä»¥èµ°å‡ºä¼Ÿå¤§çš„è¡Œç¨‹", "url": "ttps://me.csdn.net/jlh912008548", "title": "å‰å»å­¦ä¹ " }] ä»£ç é«˜äº®ç”±äº Hexo è‡ªå¸¦çš„ä»£ç é«˜äº®ä¸»é¢˜æ˜¾ç¤ºä¸å¥½çœ‹ï¼Œæ‰€ä»¥ä¸»é¢˜ä¸­ä½¿ç”¨åˆ°äº† hexo-prism-plugin çš„ Hexo æ’ä»¶æ¥åšä»£ç é«˜äº®ï¼Œå®‰è£…å‘½ä»¤å¦‚ä¸‹ï¼š npm i -S hexo-prism-plugin ç„¶åï¼Œä¿®æ”¹ Hexo æ ¹ç›®å½•ä¸‹ _config.yml æ–‡ä»¶ä¸­ highlight.enable çš„å€¼ä¸º falseï¼Œå¹¶æ–°å¢ prism æ’ä»¶ç›¸å…³çš„é…ç½®ï¼Œä¸»è¦é…ç½®å¦‚ä¸‹ï¼š highlight: enable: false prism_plugin: mode: 'preprocess' # realtime/preprocess theme: 'tomorrow' line_number: false # default false custom_css: æœç´¢æœ¬ä¸»é¢˜ä¸­è¿˜ä½¿ç”¨åˆ°äº† hexo-generator-search çš„ Hexo æ’ä»¶æ¥åšå†…å®¹æœç´¢ï¼Œå®‰è£…å‘½ä»¤å¦‚ä¸‹ï¼š npm install hexo-generator-search --save åœ¨ Hexo æ ¹ç›®å½•ä¸‹çš„ _config.yml æ–‡ä»¶ä¸­ï¼Œæ–°å¢ä»¥ä¸‹çš„é…ç½®é¡¹ï¼š search: path: search.xml field: post format: html limit: 10000 ä¸­æ–‡é“¾æ¥è½¬æ‹¼éŸ³ï¼ˆå¯é€‰çš„ï¼‰(æœªä½¿ç”¨)å¦‚æœä½ çš„æ–‡ç« åç§°æ˜¯ä¸­æ–‡çš„ï¼Œé‚£ä¹ˆ Hexo é»˜è®¤ç”Ÿæˆçš„æ°¸ä¹…é“¾æ¥ä¹Ÿä¼šæœ‰ä¸­æ–‡ï¼Œè¿™æ ·ä¸åˆ©äº SEOï¼Œä¸” gitment è¯„è®ºå¯¹ä¸­æ–‡é“¾æ¥ä¹Ÿä¸æ”¯æŒã€‚æˆ‘ä»¬å¯ä»¥ç”¨ hexo-permalink-pinyin Hexo æ’ä»¶ä½¿åœ¨ç”Ÿæˆæ–‡ç« æ—¶ç”Ÿæˆä¸­æ–‡æ‹¼éŸ³çš„æ°¸ä¹…é“¾æ¥ã€‚ å®‰è£…å‘½ä»¤å¦‚ä¸‹ï¼š npm i hexo-permalink-pinyin --save åœ¨ Hexo æ ¹ç›®å½•ä¸‹çš„ _config.yml æ–‡ä»¶ä¸­ï¼Œæ–°å¢ä»¥ä¸‹çš„é…ç½®é¡¹ï¼š permalink_pinyin: enable: true separator: '-' # default: '-' æ³¨ï¼šé™¤äº†æ­¤æ’ä»¶å¤–ï¼Œhexo-abbrlink æ’ä»¶ä¹Ÿå¯ä»¥ç”Ÿæˆéä¸­æ–‡çš„é“¾æ¥ã€‚ æ–‡ç« å­—æ•°ç»Ÿè®¡æ’ä»¶ï¼ˆå¯é€‰çš„ï¼‰å¦‚æœä½ æƒ³è¦åœ¨æ–‡ç« ä¸­æ˜¾ç¤ºæ–‡ç« å­—æ•°ã€é˜…è¯»æ—¶é•¿ä¿¡æ¯ï¼Œå¯ä»¥å®‰è£… hexo-wordcountæ’ä»¶ã€‚ å®‰è£…å‘½ä»¤å¦‚ä¸‹ï¼š npm i --save hexo-wordcount ç„¶ååªéœ€åœ¨æœ¬ä¸»é¢˜ä¸‹çš„ _config.yml æ–‡ä»¶ä¸­ï¼Œæ¿€æ´»ä»¥ä¸‹é…ç½®é¡¹å³å¯ï¼š wordCount: enable: false # å°†è¿™ä¸ªå€¼è®¾ç½®ä¸º true å³å¯. postWordCount: true min2read: true totalCount: true æ·»åŠ  RSS è®¢é˜…æ”¯æŒï¼ˆå¯é€‰çš„ï¼‰æœ¬ä¸»é¢˜ä¸­è¿˜ä½¿ç”¨åˆ°äº† hexo-generator-feed çš„ Hexo æ’ä»¶æ¥åš RSSï¼Œå®‰è£…å‘½ä»¤å¦‚ä¸‹ï¼š npm install hexo-generator-feed --save åœ¨ Hexo æ ¹ç›®å½•ä¸‹çš„ _config.yml æ–‡ä»¶ä¸­ï¼Œæ–°å¢ä»¥ä¸‹çš„é…ç½®é¡¹ï¼š feed: type: atom path: atom.xml limit: 20 hub: content: content_limit: 140 content_limit_delim: ' ' order_by: -date æ‰§è¡Œ hexo clean &amp;&amp; hexo g é‡æ–°ç”Ÿæˆåšå®¢æ–‡ä»¶ï¼Œç„¶ååœ¨ public æ–‡ä»¶å¤¹ä¸­å³å¯çœ‹åˆ° atom.xml æ–‡ä»¶ï¼Œè¯´æ˜ä½ å·²ç»å®‰è£…æˆåŠŸäº†ã€‚ ä¿®æ”¹é¡µè„šé¡µè„šä¿¡æ¯å¯èƒ½éœ€è¦åšå®šåˆ¶åŒ–ä¿®æ”¹ï¼Œè€Œä¸”å®ƒä¸ä¾¿äºåšæˆé…ç½®ä¿¡æ¯ï¼Œæ‰€ä»¥å¯èƒ½éœ€è¦ä½ è‡ªå·±å»å†ä¿®æ”¹å’ŒåŠ å·¥ã€‚ä¿®æ”¹çš„åœ°æ–¹åœ¨ä¸»é¢˜æ–‡ä»¶çš„ /layout/_partial/footer.ejs æ–‡ä»¶ä¸­ï¼ŒåŒ…æ‹¬ç«™ç‚¹ã€ä½¿ç”¨çš„ä¸»é¢˜ã€è®¿é—®é‡ç­‰ã€‚ ä¿®æ”¹ç¤¾äº¤é“¾æ¥åœ¨ä¸»é¢˜çš„ _config.yml æ–‡ä»¶ä¸­ï¼Œé»˜è®¤æ”¯æŒ QQã€GitHub å’Œé‚®ç®±çš„é…ç½®ï¼Œä½ å¯ä»¥åœ¨ä¸»é¢˜æ–‡ä»¶çš„ /layout/_partial/social-link.ejs æ–‡ä»¶ä¸­ï¼Œæ–°å¢ã€ä¿®æ”¹ä½ éœ€è¦çš„ç¤¾äº¤é“¾æ¥åœ°å€ï¼Œå¢åŠ é“¾æ¥å¯å‚è€ƒå¦‚ä¸‹ä»£ç ï¼š &lt;a href="https://github.com/blinkfox" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50"> &lt;i class="fa fa-github">&lt;/i> &lt;/a> å…¶ä¸­ï¼Œç¤¾äº¤å›¾æ ‡ï¼ˆå¦‚ï¼šfa-githubï¼‰ä½ å¯ä»¥åœ¨ Font Awesome ä¸­æœç´¢æ‰¾åˆ°ã€‚ä»¥ä¸‹æ˜¯å¸¸ç”¨ç¤¾äº¤å›¾æ ‡çš„æ ‡è¯†ï¼Œä¾›ä½ å‚è€ƒï¼š Facebook: fa-facebook Twitter: fa-twitter Google-plus: fa-google-plus Linkedin: fa-linkedin Tumblr: fa-tumblr Medium: fa-medium Slack: fa-slack æ–°æµªå¾®åš: fa-weibo å¾®ä¿¡: fa-wechat QQ: fa-qq æ³¨æ„: æœ¬ä¸»é¢˜ä¸­ä½¿ç”¨çš„ Font Awesome ç‰ˆæœ¬ä¸º 4.7.0ã€‚ ä¿®æ”¹æ‰“èµçš„äºŒç»´ç å›¾ç‰‡åœ¨ä¸»é¢˜æ–‡ä»¶çš„ source/medias/reward æ–‡ä»¶ä¸­ï¼Œä½ å¯ä»¥æ›¿æ¢æˆä½ çš„çš„å¾®ä¿¡å’Œæ”¯ä»˜å®çš„æ‰“èµäºŒç»´ç å›¾ç‰‡ã€‚ é…ç½®éŸ³ä¹æ’­æ”¾å™¨ï¼ˆå¯é€‰çš„ï¼‰(æœªä½¿ç”¨)è¦æ”¯æŒéŸ³ä¹æ’­æ”¾ï¼Œå°±å¿…é¡»å¼€å¯éŸ³ä¹çš„æ’­æ”¾é…ç½®å’ŒéŸ³ä¹æ•°æ®çš„æ–‡ä»¶ã€‚ é¦–å…ˆï¼Œåœ¨ä½ çš„åšå®¢ source ç›®å½•ä¸‹çš„ _data ç›®å½•ï¼ˆæ²¡æœ‰çš„è¯å°±æ–°å»ºä¸€ä¸ªï¼‰ä¸­æ–°å»º musics.json æ–‡ä»¶ï¼Œæ–‡ä»¶å†…å®¹å¦‚ä¸‹æ‰€ç¤ºï¼š [{ "name": "äº”æœˆé›¨å˜å¥ç”µéŸ³", "artist": "AnimeVibe", "url": "http://xxx.com/music1.mp3", "cover": "http://xxx.com/music-cover1.png" }, { "name": "Take me hand", "artist": "DAISHI DANCE,Cecile Corbel", "url": "/medias/music/music2.mp3", "cover": "/medias/music/cover2.png" }, { "name": "Shape of You", "artist": "J.Fla", "url": "http://xxx.com/music3.mp3", "cover": "http://xxx.com/music-cover3.png" }] æ³¨ï¼šä»¥ä¸Š JSON ä¸­çš„å±æ€§ï¼šnameã€artistã€urlã€cover åˆ†åˆ«è¡¨ç¤ºéŸ³ä¹çš„åç§°ã€ä½œè€…ã€éŸ³ä¹æ–‡ä»¶åœ°å€ã€éŸ³ä¹å°é¢ã€‚ ç„¶åï¼Œåœ¨ä¸»é¢˜çš„ _config.yml é…ç½®æ–‡ä»¶ä¸­æ¿€æ´»é…ç½®å³å¯ï¼š # æ˜¯å¦åœ¨é¦–é¡µæ˜¾ç¤ºéŸ³ä¹. music: enable: true showTitle: false title: å¬å¬éŸ³ä¹ fixed: false # æ˜¯å¦å¼€å¯å¸åº•æ¨¡å¼ autoplay: false # æ˜¯å¦è‡ªåŠ¨æ’­æ”¾ theme: '#42b983' loop: 'all' # éŸ³é¢‘å¾ªç¯æ’­æ”¾, å¯é€‰å€¼: 'all', 'one', 'none' order: 'list' # éŸ³é¢‘å¾ªç¯é¡ºåº, å¯é€‰å€¼: 'list', 'random' preload: 'auto' # é¢„åŠ è½½ï¼Œå¯é€‰å€¼: 'none', 'metadata', 'auto' volume: 0.7 # é»˜è®¤éŸ³é‡ï¼Œè¯·æ³¨æ„æ’­æ”¾å™¨ä¼šè®°å¿†ç”¨æˆ·è®¾ç½®ï¼Œç”¨æˆ·æ‰‹åŠ¨è®¾ç½®éŸ³é‡åé»˜è®¤éŸ³é‡å³å¤±æ•ˆ listFolded: false # åˆ—è¡¨é»˜è®¤æŠ˜å  listMaxHeight: # åˆ—è¡¨æœ€å¤§é«˜åº¦ Front-matter é€‰é¡¹è¯¦è§£Front-matter é€‰é¡¹ä¸­çš„æ‰€æœ‰å†…å®¹å‡ä¸ºéå¿…å¡«çš„ã€‚ä½†æˆ‘ä»ç„¶å»ºè®®è‡³å°‘å¡«å†™ title å’Œ date çš„å€¼ã€‚ é…ç½®é€‰é¡¹ é»˜è®¤å€¼ æè¿° title Markdown çš„æ–‡ä»¶æ ‡é¢˜ æ–‡ç« æ ‡é¢˜ï¼Œå¼ºçƒˆå»ºè®®å¡«å†™æ­¤é€‰é¡¹ date æ–‡ä»¶åˆ›å»ºæ—¶çš„æ—¥æœŸæ—¶é—´ å‘å¸ƒæ—¶é—´ï¼Œå¼ºçƒˆå»ºè®®å¡«å†™æ­¤é€‰é¡¹ï¼Œä¸”æœ€å¥½ä¿è¯å…¨å±€å”¯ä¸€ author æ ¹ _config.yml ä¸­çš„ author æ–‡ç« ä½œè€… img featureImages ä¸­çš„æŸä¸ªå€¼ æ–‡ç« ç‰¹å¾å›¾ï¼Œæ¨èä½¿ç”¨å›¾åºŠ(è…¾è®¯äº‘ã€ä¸ƒç‰›äº‘ã€åˆæ‹äº‘ç­‰)æ¥åšå›¾ç‰‡çš„è·¯å¾„.å¦‚: http://xxx.com/xxx.jpg top true æ¨èæ–‡ç« ï¼ˆæ–‡ç« æ˜¯å¦ç½®é¡¶ï¼‰ï¼Œå¦‚æœ top å€¼ä¸º trueï¼Œåˆ™ä¼šä½œä¸ºé¦–é¡µæ¨èæ–‡ç«  cover false v1.0.2ç‰ˆæœ¬æ–°å¢ï¼Œè¡¨ç¤ºè¯¥æ–‡ç« æ˜¯å¦éœ€è¦åŠ å…¥åˆ°é¦–é¡µè½®æ’­å°é¢ä¸­ coverImg æ—  v1.0.2ç‰ˆæœ¬æ–°å¢ï¼Œè¡¨ç¤ºè¯¥æ–‡ç« åœ¨é¦–é¡µè½®æ’­å°é¢éœ€è¦æ˜¾ç¤ºçš„å›¾ç‰‡è·¯å¾„ï¼Œå¦‚æœæ²¡æœ‰ï¼Œåˆ™é»˜è®¤ä½¿ç”¨æ–‡ç« çš„ç‰¹è‰²å›¾ç‰‡ password æ—  æ–‡ç« é˜…è¯»å¯†ç ï¼Œå¦‚æœè¦å¯¹æ–‡ç« è®¾ç½®é˜…è¯»éªŒè¯å¯†ç çš„è¯ï¼Œå°±å¯ä»¥è®¾ç½® password çš„å€¼ï¼Œè¯¥å€¼å¿…é¡»æ˜¯ç”¨ SHA256 åŠ å¯†åçš„å¯†ç ï¼Œé˜²æ­¢è¢«ä»–äººè¯†ç ´ã€‚å‰ææ˜¯åœ¨ä¸»é¢˜çš„ config.yml ä¸­æ¿€æ´»äº† verifyPassword é€‰é¡¹ toc true æ˜¯å¦å¼€å¯ TOCï¼Œå¯ä»¥é’ˆå¯¹æŸç¯‡æ–‡ç« å•ç‹¬å…³é—­ TOC çš„åŠŸèƒ½ã€‚å‰ææ˜¯åœ¨ä¸»é¢˜çš„ config.yml ä¸­æ¿€æ´»äº† toc é€‰é¡¹ mathjax false æ˜¯å¦å¼€å¯æ•°å­¦å…¬å¼æ”¯æŒ ï¼Œæœ¬æ–‡ç« æ˜¯å¦å¼€å¯ mathjaxï¼Œä¸”éœ€è¦åœ¨ä¸»é¢˜çš„ _config.yml æ–‡ä»¶ä¸­ä¹Ÿéœ€è¦å¼€å¯æ‰è¡Œ summary æ—  æ–‡ç« æ‘˜è¦ï¼Œè‡ªå®šä¹‰çš„æ–‡ç« æ‘˜è¦å†…å®¹ï¼Œå¦‚æœè¿™ä¸ªå±æ€§æœ‰å€¼ï¼Œæ–‡ç« å¡ç‰‡æ‘˜è¦å°±æ˜¾ç¤ºè¿™æ®µæ–‡å­—ï¼Œå¦åˆ™ç¨‹åºä¼šè‡ªåŠ¨æˆªå–æ–‡ç« çš„éƒ¨åˆ†å†…å®¹ä½œä¸ºæ‘˜è¦ categories æ—  æ–‡ç« åˆ†ç±»ï¼Œæœ¬ä¸»é¢˜çš„åˆ†ç±»è¡¨ç¤ºå®è§‚ä¸Šå¤§çš„åˆ†ç±»ï¼Œåªå»ºè®®ä¸€ç¯‡æ–‡ç« ä¸€ä¸ªåˆ†ç±» tags æ—  æ–‡ç« æ ‡ç­¾ï¼Œä¸€ç¯‡æ–‡ç« å¯ä»¥å¤šä¸ªæ ‡ç­¾ æ³¨æ„: å¦‚æœ img å±æ€§ä¸å¡«å†™çš„è¯ï¼Œæ–‡ç« ç‰¹è‰²å›¾ä¼šæ ¹æ®æ–‡ç« æ ‡é¢˜çš„ hashcode çš„å€¼å–ä½™ï¼Œç„¶åé€‰å–ä¸»é¢˜ä¸­å¯¹åº”çš„ç‰¹è‰²å›¾ç‰‡ï¼Œä»è€Œè¾¾åˆ°è®©æ‰€æœ‰æ–‡ç« éƒ½çš„ç‰¹è‰²å›¾å„æœ‰ç‰¹è‰²ã€‚ date çš„å€¼å°½é‡ä¿è¯æ¯ç¯‡æ–‡ç« æ˜¯å”¯ä¸€çš„ï¼Œå› ä¸ºæœ¬ä¸»é¢˜ä¸­ Gitalk å’Œ Gitment è¯†åˆ« id æ˜¯é€šè¿‡ date çš„å€¼æ¥ä½œä¸ºå”¯ä¸€æ ‡è¯†çš„ã€‚ å¦‚æœè¦å¯¹æ–‡ç« è®¾ç½®é˜…è¯»éªŒè¯å¯†ç çš„åŠŸèƒ½ï¼Œä¸ä»…è¦åœ¨ Front-matter ä¸­è®¾ç½®é‡‡ç”¨äº† SHA256 åŠ å¯†çš„ password çš„å€¼ï¼Œè¿˜éœ€è¦åœ¨ä¸»é¢˜çš„ _config.yml ä¸­æ¿€æ´»äº†é…ç½®ã€‚æœ‰äº›åœ¨çº¿çš„ SHA256 åŠ å¯†çš„åœ°å€ï¼Œå¯ä¾›ä½ ä½¿ç”¨ï¼šå¼€æºä¸­å›½åœ¨çº¿å·¥å…·ã€chahuoã€ç«™é•¿å·¥å…·ã€‚ ä»¥ä¸‹ä¸ºæ–‡ç« çš„ Front-matter ç¤ºä¾‹ã€‚ æœ€ç®€ç¤ºä¾‹--- title: typora-vue-themeä¸»é¢˜ä»‹ç» date: 2018-09-07 09:25:00 --- æœ€å…¨ç¤ºä¾‹--- title: typora-vue-themeä¸»é¢˜ä»‹ç» date: 2018-09-07 09:25:00 author: èµµå¥‡ img: /source/images/xxx.jpg top: true cover: true coverImg: /images/1.jpg password: 8d969eef6ecad3c29a3a629280e686cf0c3f5d5a86aff3ca12020c923adc6c92 toc: false mathjax: false summary: è¿™æ˜¯ä½ è‡ªå®šä¹‰çš„æ–‡ç« æ‘˜è¦å†…å®¹ï¼Œå¦‚æœè¿™ä¸ªå±æ€§æœ‰å€¼ï¼Œæ–‡ç« å¡ç‰‡æ‘˜è¦å°±æ˜¾ç¤ºè¿™æ®µæ–‡å­—ï¼Œå¦åˆ™ç¨‹åºä¼šè‡ªåŠ¨æˆªå–æ–‡ç« çš„éƒ¨åˆ†å†…å®¹ä½œä¸ºæ‘˜è¦ categories: Markdown tags: - Typora - Markdown --- ä¿®æ”¹ä¸»é¢˜é¢œè‰²åœ¨ä¸»é¢˜æ–‡ä»¶çš„ /source/css/matery.css æ–‡ä»¶ä¸­ï¼Œæœç´¢ .bg-color æ¥ä¿®æ”¹èƒŒæ™¯é¢œè‰²ï¼š /* æ•´ä½“èƒŒæ™¯é¢œè‰²ï¼ŒåŒ…æ‹¬å¯¼èˆªã€ç§»åŠ¨ç«¯çš„å¯¼èˆªã€é¡µå°¾ã€æ ‡ç­¾é¡µç­‰çš„èƒŒæ™¯é¢œè‰². */ .bg-color { background-image: linear-gradient(to right, #4cbf30 0%, #0f9d58 100%); } @-webkit-keyframes rainbow { /* åŠ¨æ€åˆ‡æ¢èƒŒæ™¯é¢œè‰². */ } @keyframes rainbow { /* åŠ¨æ€åˆ‡æ¢èƒŒæ™¯é¢œè‰². */ } ä¿®æ”¹ banner å›¾å’Œæ–‡ç« ç‰¹è‰²å›¾ä½ å¯ä»¥ç›´æ¥åœ¨ /source/medias/banner æ–‡ä»¶å¤¹ä¸­æ›´æ¢ä½ å–œæ¬¢çš„ banner å›¾ç‰‡ï¼Œä¸»é¢˜ä»£ç ä¸­æ˜¯æ¯å¤©åŠ¨æ€åˆ‡æ¢ä¸€å¼ ï¼Œåªéœ€ 7 å¼ å³å¯ã€‚å¦‚æœä½ ä¼š JavaScript ä»£ç ï¼Œå¯ä»¥ä¿®æ”¹æˆä½ è‡ªå·±å–œæ¬¢åˆ‡æ¢é€»è¾‘ï¼Œå¦‚ï¼šéšæœºåˆ‡æ¢ç­‰ï¼Œbanner åˆ‡æ¢çš„ä»£ç ä½ç½®åœ¨ /layout/_partial/bg-cover-content.ejs æ–‡ä»¶çš„ `` ä»£ç ä¸­ï¼š $('.bg-cover').css('background-image', 'url(/medias/banner/' + new Date().getDay() + '.jpg)'); åœ¨ /source/medias/featureimages æ–‡ä»¶å¤¹ä¸­é»˜è®¤æœ‰ 24 å¼ ç‰¹è‰²å›¾ç‰‡ï¼Œä½ å¯ä»¥å†å¢åŠ æˆ–è€…å‡å°‘ï¼Œå¹¶éœ€è¦åœ¨ _config.yml åšåŒæ­¥ä¿®æ”¹ã€‚ åŠ¨æ€æ ‡ç­¾æ åœ¨theme/matery/layout/layout.ejsä¸‹æ·»åŠ å¦‚ä¸‹ä»£ç ï¼š &lt;script type="text/javascript"> var OriginTitile = document.title, st; document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£å–”å“Ÿï¼Œå´©æºƒå•¦ï¼", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) &lt;/script> æ·»åŠ æ•°å­¦å…¬å¼æ˜¾ç¤ºå®‰è£…ä¸é…ç½®$ npm install hexo-math --save åœ¨ç«™ç‚¹é…ç½®æ–‡ä»¶ _config.yml ä¸­æ·»åŠ ï¼š math: engine: &#39;mathjax&#39; # or &#39;katex&#39; mathjax: # src: custom_mathjax_source config: # MathJax config åœ¨ next ä¸»é¢˜é…ç½®æ–‡ä»¶ä¸­ themes/next-theme/_config.yml ä¸­å°† mathJax è®¾ä¸º true: # MathJax Support mathjax: enable: true per_page: true cdn: //cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML ä¹Ÿå¯ä»¥åœ¨æ–‡ç« çš„å¼€å§‹é›†æˆæ’ä»¶æ”¯æŒï¼Œä½†ä¸å»ºè®®è¿™ä¹ˆåšï¼š &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt; &lt;/script&gt; ä½¿ç”¨å…¬å¼æ’å…¥æ ¼å¼ï¼š $æ•°å­¦å…¬å¼$ è¡Œå†… ä¸ç‹¬å ä¸€è¡Œ $$æ•°å­¦å…¬å¼$$ è¡Œé—´ ç‹¬å ä¸€è¡Œ ä¾‹å¦‚ï¼š $f(x)=ax+b$ å¦‚æœæ˜¯è¡Œé—´åˆ™ä½¿ç”¨ï¼š $$f(x)=ax+b$$ æ›´æ¢ Hexo çš„ markdown æ¸²æŸ“å¼•æ“ï¼Œhexo-renderer-kramed å¼•æ“æ˜¯åœ¨é»˜è®¤çš„æ¸²æŸ“å¼•æ“ hexo-renderer-marked çš„åŸºç¡€ä¸Šä¿®æ”¹äº†ä¸€äº› bug ï¼Œä¸¤è€…æ¯”è¾ƒæ¥è¿‘ï¼Œä¹Ÿæ¯”è¾ƒè½»é‡çº§ã€‚ $ npm uninstall hexo-renderer-marked --save $ npm install hexo-renderer-kramed --save æ‰§è¡Œä¸Šé¢çš„å‘½ä»¤å³å¯ï¼Œå…ˆå¸è½½åŸæ¥çš„æ¸²æŸ“å¼•æ“ï¼Œå†å®‰è£…æ–°çš„ã€‚ç„¶åï¼Œè·Ÿæ¢å¼•æ“åè¡Œé—´å…¬å¼å¯ä»¥æ­£ç¡®æ¸²æŸ“äº†ï¼Œä½†æ˜¯è¿™æ ·è¿˜æ²¡æœ‰å®Œå…¨è§£å†³é—®é¢˜ï¼Œè¡Œå†…å…¬å¼çš„æ¸²æŸ“è¿˜æ˜¯æœ‰é—®é¢˜ï¼Œå› ä¸º hexo-renderer-kramed å¼•æ“ä¹Ÿæœ‰è¯­ä¹‰å†²çªçš„é—®é¢˜ã€‚æ¥ä¸‹æ¥åˆ°åšå®¢æ ¹ç›®å½•ä¸‹ï¼Œæ‰¾åˆ°node_modules\kramed\lib\rules\inline.jsï¼ŒæŠŠç¬¬11è¡Œçš„ escape å˜é‡çš„å€¼åšç›¸åº”çš„ä¿®æ”¹ï¼š //escape: /^\\([\\`*{}\[\]()#$+\-.!_>])/, escape: /^\\([`*\[\]()#$+\-.!_>])/, è¿™ä¸€æ­¥æ˜¯åœ¨åŸåŸºç¡€ä¸Šå–æ¶ˆäº†å¯¹\,{,}çš„è½¬ä¹‰(escape)ã€‚åŒæ—¶æŠŠç¬¬20è¡Œçš„emå˜é‡ä¹Ÿè¦åšç›¸åº”çš„ä¿®æ”¹ã€‚ //em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, é‡æ–°å¯åŠ¨hexoï¼ˆå…ˆcleanå†generateï¼‰,é—®é¢˜å®Œç¾è§£å†³ã€‚å“¦ï¼Œå¦‚æœä¸å¹¸è¿˜æ²¡è§£å†³çš„è¯ï¼Œçœ‹çœ‹æ˜¯ä¸æ˜¯è¿˜éœ€è¦åœ¨ä½¿ç”¨çš„ä¸»é¢˜ä¸­é…ç½®mathjaxå¼€å…³ã€‚ è§£å†³MathJaxå’Œprismæ’ä»¶å†²çªæ–‡ç« å¼€å¤´åˆ†æçš„ç»“æœï¼Œåªè¦æˆ‘ä»¬æŠŠè¯­æ³•å“äº®çš„preçš„classå±æ€§ç”±åŸçš„class=&quot;language-lang-*&quot;å˜æˆäº†class=&quot;language-*&quot;å³å¯å¯¹è¯­æ³•è¿›è¡Œè¯†åˆ«äº†ï¼Œäºæ˜¯æˆ‘å¯¹kramedçš„æ–‡ä»¶è¿›è¡Œäº†åˆ†æï¼Œæ ¹æ®ä»¥å¾€åœ¨nextä¸»é¢˜ä¸‹çš„ç»éªŒï¼Œä¿®æ”¹ä¸¤å¤„äº†langPrefix:çš„å‚æ•°ã€‚ ä¸€å¤„åœ¨/node_modules/kramed/lib/kramed.jsä¸­çš„ç¬¬134è¡Œï¼š // Renderer options - langPrefix: 'lang-', + langPrefix: '', smartypants: false, headerPrefix: '', headerAutoId: true, xhtml: false, è¿˜æœ‰ä¸€å¤„åœ¨`/node_modules/kramed/lib/renderer.jsä¸­çš„ç¬¬10è¡Œï¼š var defaultOptions = { - langPrefix: 'lang-', + langPrefix: '', smartypants: false, headerPrefix: '', headerAutoId: true, xhtml: false, }; ç„¶ååœ¨é‡æ–°è¿›è¡Œæ¸²æŸ“ï¼ŒåŸæ¥çš„class=&quot;language-lang-*&quot;å˜æˆäº†class=&quot;language-*&quot;ï¼Œè¯­æ³•å“äº®ä¹Ÿå°±æ­£å¸¸äº†ã€‚ å¢åŠ å»ºç«™æ—¶é—´ä¿®æ”¹/themes/matery/layout/_partial/footer.ejsæ–‡ä»¶ï¼Œåœ¨æœ€ååŠ ä¸Š &lt;script language=javascript> function siteTime() { window.setTimeout("siteTime()", 1000); var seconds = 1000; var minutes = seconds * 60; var hours = minutes * 60; var days = hours * 24; var years = days * 365; var today = new Date(); var todayYear = today.getFullYear(); var todayMonth = today.getMonth() + 1; var todayDate = today.getDate(); var todayHour = today.getHours(); var todayMinute = today.getMinutes(); var todaySecond = today.getSeconds(); /* Date.UTC() -- è¿”å›dateå¯¹è±¡è·ä¸–ç•Œæ ‡å‡†æ—¶é—´(UTC)1970å¹´1æœˆ1æ—¥åˆå¤œä¹‹é—´çš„æ¯«ç§’æ•°(æ—¶é—´æˆ³) year - ä½œä¸ºdateå¯¹è±¡çš„å¹´ä»½ï¼Œä¸º4ä½å¹´ä»½å€¼ month - 0-11ä¹‹é—´çš„æ•´æ•°ï¼Œåšä¸ºdateå¯¹è±¡çš„æœˆä»½ day - 1-31ä¹‹é—´çš„æ•´æ•°ï¼Œåšä¸ºdateå¯¹è±¡çš„å¤©æ•° hours - 0(åˆå¤œ24ç‚¹)-23ä¹‹é—´çš„æ•´æ•°ï¼Œåšä¸ºdateå¯¹è±¡çš„å°æ—¶æ•° minutes - 0-59ä¹‹é—´çš„æ•´æ•°ï¼Œåšä¸ºdateå¯¹è±¡çš„åˆ†é’Ÿæ•° seconds - 0-59ä¹‹é—´çš„æ•´æ•°ï¼Œåšä¸ºdateå¯¹è±¡çš„ç§’æ•° microseconds - 0-999ä¹‹é—´çš„æ•´æ•°ï¼Œåšä¸ºdateå¯¹è±¡çš„æ¯«ç§’æ•° */ var t1 = Date.UTC(2017, 09, 11, 00, 00, 00); //åŒ—äº¬æ—¶é—´2018-2-13 00:00:00 var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond); var diff = t2 - t1; var diffYears = Math.floor(diff / years); var diffDays = Math.floor((diff / days) - diffYears * 365); var diffHours = Math.floor((diff - (diffYears * 365 + diffDays) * days) / hours); var diffMinutes = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours) / minutes); var diffSeconds = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours - diffMinutes * minutes) / seconds); document.getElementById("sitetime").innerHTML = "æœ¬ç«™å·²è¿è¡Œ " +diffYears+" å¹´ "+diffDays + " å¤© " + diffHours + " å°æ—¶ " + diffMinutes + " åˆ†é’Ÿ " + diffSeconds + " ç§’"; }/*å› ä¸ºå»ºç«™æ—¶é—´è¿˜æ²¡æœ‰ä¸€å¹´ï¼Œå°±å°†ä¹‹æ³¨é‡Šæ‰äº†ã€‚éœ€è¦çš„å¯ä»¥å–æ¶ˆ*/ siteTime(); &lt;/script> ç„¶ååœ¨åˆé€‚çš„åœ°æ–¹ï¼ˆæ¯”å¦‚copyrightå£°æ˜åé¢ï¼‰åŠ ä¸Šä¸‹é¢çš„ä»£ç å°±è¡Œäº†ï¼š &lt;span id="sitetime">&lt;/span> åœ¨ hexo new ä¹‹åç«‹å³æ‰“å¼€æ–°å»ºçš„ Markdown æ–‡ç¨¿ Tommy æŒ‡å‡ºï¼Œå¯ä»¥åœ¨ Hexo ç›®å½•ä¸‹çš„ scripts ç›®å½•ï¼ˆè‹¥æ²¡æœ‰ï¼Œåˆ™æ–°å»ºä¸€ä¸ªï¼‰ä¸­åˆ›å»ºä¸€ä¸ª JavaScript è„šæœ¬ï¼ˆè¯¥è„šæœ¬åç§°éšæ„æŒ‡å®šï¼‰ï¼Œç›‘å¬ hexo new è¿™ä¸ªåŠ¨ä½œã€‚å¹¶åœ¨æ£€æµ‹åˆ° hexo new ä¹‹åï¼Œæ‰§è¡Œç¼–è¾‘å™¨æ‰“å¼€çš„å‘½ä»¤ã€‚ var exec = require('child_process').exec; // Hexo 3 hexo.on('new', function(data){ exec('open -a "/Applications/Typora.app" ' + data.path); }); å®‰è£…shelljsæ¨¡å—ï¼Œå®ç°è‡ªåŠ¨éƒ¨ç½²åŠ è½½jsè„šæœ¬ï¼Œé”®å…¥ä»¥ä¸‹å‘½ä»¤ï¼š npm install --save shelljs ä¸çŸ¥é“è¯¥æ“ä½œæ˜¯å¦å¿…è¦ï¼Œä½†æ˜¯è¿™ä¸¤æ­¥éƒ½åšå®Œåæ˜¯å¯ä»¥å®Œæˆè‡ªåŠ¨æ‰“å¼€åŠŸèƒ½äº†ã€‚ ä¿®æ”¹é¡µé¢ä¸Šæ–¹æ¨ªæ¡é¢œè‰²åœ¨/themes/matery/source/css/matery.cssä¸­å¯¹ä»¥ä¸‹éƒ¨åˆ†ä½œå‡ºä¿®æ”¹ï¼š .bg-color { background-image: linear-gradient(to right, #a524be 0%, #0f9d58 100%); } å¢åŠ é¦–é¡µå°é¢è½®æ’­# Index cover carousel configuration. # é¦–é¡µå°é¢è½®æ’­å›¾çš„ç›¸å…³é…ç½®. cover: showPrevNext: true # æ˜¯å¦æ˜¾ç¤ºå·¦å³åˆ‡æ¢æŒ‰é’®. Whether to display the left and right toggle buttons. showIndicators: true # æ˜¯å¦æ˜¾ç¤ºæŒ‡ç¤ºå™¨. # Whether to display the indicators autoLoop: true # æ˜¯å¦è‡ªåŠ¨è½®æ’­. Whether it is automatically rotated. duration: 120 # åˆ‡æ¢å»¶è¿Ÿæ—¶é—´. Switching delay time. intervalTime: 5000 # è‡ªåŠ¨åˆ‡æ¢ä¸‹ä¸€å¼ çš„é—´éš”æ—¶é—´. Automatically switch the interval of the next one. ç›®å‰é—ç•™é—®é¢˜ï¼šä»£ç é«˜äº®é—®é¢˜ å‚è€ƒé“¾æ¥ï¼š é—ªçƒä¹‹ç‹ Hexoåšå®¢ä¸»é¢˜ä¹‹hexo-theme-materyçš„ä»‹ç» éŸ¦é˜³çš„åšå®¢ https://shengbinyu.top/mathjaxprism.html]]></content>
      <categories>
        <category>åšå®¢</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2016 TransG : A Generative Model for Knowledge Graph Embeddingé˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FTransG_%3A_A_Generative_Model_for_Knowledge_Graph_Embedding%2F</url>
    <content type="text"><![CDATA[è®ºæ–‡ä¸‹è½½åœ°å€ è§£å†³é—®é¢˜multiple relation semanticsï¼ˆå¤šé‡å…³ç³»è¯­ä¹‰ï¼‰ï¼šä¸€ä¸ªå…³ç³»å¯èƒ½å…·æœ‰ä¸å¯¹åº”çš„ä¸‰å…ƒç»„å…³è”çš„å®ä½“å¯¹æ­ç¤ºçš„å¤šç§å«ä¹‰ã€‚ è¿™é‡Œä»¥TransEçš„å¯è§†åŒ–ä¸ºä¾‹ï¼Œè¡¨æ˜ï¼šç‰¹å®šå…³ç³»æœ‰ä¸åŒçš„èšç±»ï¼Œå¹¶ä¸”ä¸åŒçš„èšç±»è¡¨ç¤ºä¸åŒçš„æ½œåœ¨è¯­ä¹‰ï¼Œè¯å®äº†è¯¥é—®é¢˜çš„å­˜åœ¨æ€§ã€‚ è¯¥ç°è±¡äº§ç”ŸåŸå›  äººä¸ºç®€åŒ– çŸ¥è¯†åº“ç­–å±•äººä¸èƒ½æ¶‰åŠå¤ªå¤šç›¸ä¼¼å…³ç³»ï¼Œå› æ­¤å°†å¤šä¸ªç›¸ä¼¼å…³ç³»æŠ½è±¡ä¸ºä¸€ä¸ªç‰¹å®šå…³ç³»æ˜¯ä¸€ç§å¸¸è§çš„æŠ€å·§ çŸ¥è¯†æ€§è´¨ è¯­è¨€å’ŒçŸ¥è¯†è¡¨ç¤ºå½¢å¼å¸¸å¸¸æ¶‰åŠä¸æ˜ç¡®çš„ä¿¡æ¯ã€‚ çŸ¥è¯†çš„æ¨¡ç³Šæ€§æ„å‘³ç€è¯­ä¹‰ä¸Šçš„æ··åˆ TransGåˆ©ç”¨è´å¶æ–¯éå‚æ•°æ— é™æ··åˆæ¨¡å‹é€šè¿‡ä¸ºå…³ç³»ç”Ÿæˆå¤šä¸ªç¿»è¯‘ç»„ä»¶æ¥å¤„ç†å¤šä¸ªå…³ç³»è¯­ä¹‰]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>2016</tag>
        <tag>ACL</tag>
        <tag>KGE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2016 Knowledge Graph Completion with Adaptive Sparse Transfer Matrixé˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2F2016_Knowledge_Graph_Completion_with_Adaptive_Sparse_Transfer_Matrix%2F</url>
    <content type="text"><![CDATA[ä¸ºè§£å†³heterogeneityå’Œimbalanceé—®é¢˜ï¼Œä½œè€…é’ˆå¯¹åŒä¸€å…³ç³»é“¾æ¥å®ä½“å¯¹çš„æ•°é‡å’ŒåŒä¸€å…³ç³»ä¸åŒå¤´å°¾å®ä½“æ•°é‡ï¼Œåˆ†åˆ«è®¾è®¡äº†ä¸åŒç¨€ç–ç¨‹åº¦çš„è½¬ç§»çŸ©é˜µã€‚å­˜åœ¨ç¼ºç‚¹ï¼šå¹¶æ²¡æœ‰åŒæ—¶è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ã€‚ è®ºæ–‡ä¸‹è½½åœ°å€ Problem Statement heterogeneityï¼ˆå¼‚è´¨æ€§ï¼‰ï¼šä¸€äº›å…³ç³»é“¾æ¥äº†å¾ˆå¤šå®ä½“å¯¹ï¼Œå¦ä¸€äº›æ²¡æœ‰ imbalanceï¼ˆä¸å¹³è¡¡ï¼‰ï¼š åœ¨ä¸€ç§å…³ç³»ä¸­ï¼Œå¤´å®ä½“å’Œå°¾å®ä½“çš„æ•°é‡ä¸åŒ Contribution ä½œè€…æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•è€ƒè™‘äº†å…ˆå‰æ¨¡å‹ä¸­æœªä½¿ç”¨çš„å¼‚è´¨æ€§å’Œä¸å¹³è¡¡æ€§ï¼Œä»¥åµŒå…¥çŸ¥è¯†å›¾æ¥å®Œæˆå®ƒä»¬ï¼› ä½œè€…çš„æ–¹æ³•é«˜æ•ˆä¸”å‚æ•°è¾ƒå°‘ï¼Œå› æ­¤å¾ˆå®¹æ˜“æ‰©å±•åˆ°å¤§è§„æ¨¡çŸ¥è¯†å›¾ï¼› ä½œè€…ä¸ºè½¬ç§»çŸ©é˜µæä¾›äº†ä¸¤ç§ç¨€ç–æ¨¡å¼ï¼Œå¹¶åˆ†æäº†å®ƒä»¬çš„ä¼˜ç¼ºç‚¹ï¼› åœ¨ä¸‰å…ƒç»„åˆ†ç±»å’Œé“¾æ¥é¢„æµ‹ä»»åŠ¡ä¸­ï¼Œä½œè€…çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ Sparse Matrixå®šä¹‰ç¨€ç–çŸ©é˜µæ˜¯æŒ‡å¤§å¤šæ•°æ¡ç›®ï¼ˆentriesï¼‰ä¸ºé›¶çš„çŸ©é˜µã€‚ é›¶å…ƒç´ å çŸ©é˜µå…ƒç´ æ€»æ•°çš„æ¯”ä¾‹ç§°ä¸ºç¨€ç–åº¦ï¼ˆsparse degreeï¼‰ã€‚ ç±»åˆ« ç»“æ„åŒ–çš„ éç»“æ„åŒ–çš„ ä¸¤è€…é‡è¦åŒºåˆ« ç»“æ„åŒ–æ¨¡å¼æœ‰åˆ©äºçŸ©é˜µå‘é‡ä¹˜ç§¯è¿ç®—ã€‚ éç»“æ„åŒ–å¾€å¾€å¯ä»¥å¸¦æ¥æ›´å¥½çš„å®éªŒç»“æœï¼šç”±äºæ›´åŠ çµæ´»åœ°è¿œèŒƒå›´çº¿æ€§ç»„åˆã€‚ Sparse Matrix vs Low-Rank Matrixç‰¹ç‚¹ low-rankçŸ©é˜µå¼ºåˆ¶ä¸€äº›å˜é‡è¦æ»¡è¶³ç‰¹å®šçš„çº¦æŸï¼Œå› æ­¤ï¼ŒçŸ©é˜µMæ— æ³•è‡ªç”±åœ°è¿›è¡Œèµ‹å€¼ã€‚ sparseçŸ©é˜µæ˜¯ä½œè€…ä»¤å…¶ä¸­çš„éƒ¨åˆ†å…ƒç´ å€¼ä¸º0ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸æ”¹å˜å®ƒçš„å€¼ï¼Œå…¶ä»–çš„éé›¶å€¼è¿›è¡Œè®­ç»ƒã€‚ å¯¹æ¯” ç¨€ç–çŸ©é˜µæ¯”ä½ç§©çŸ©é˜µæ›´çµæ´»ï¼Œå¯ä»¥æœ‰æ›´å¤§çš„è‡ªç”±åº¦ï¼šä½¿ç”¨ä½ç§©çŸ©é˜µï¼Œé‚£ä¹ˆçŸ©é˜µçš„è‡ªç”±åº¦ä¼šå—åˆ°ä¸¥æ ¼çš„ç§©é™åˆ¶ã€‚ç„¶è€Œï¼Œsparse matrixçš„ç¨€ç–æ€§åªæ˜¯æ§åˆ¶çŸ©é˜µå…ƒç´ ä¸­é›¶å…ƒç´ çš„ä¸ªæ•°ã€‚ ç¨€ç–çŸ©é˜µæ¯”ä½ç§©çŸ©é˜µæ›´æœ‰æ•ˆç‡ï¼šåªæœ‰éé›¶æ¡ç›®å‚ä¸è®¡ç®—ï¼Œæå¤§åœ°å‡å°‘äº†è®¡ç®—é‡ ModelTranSpare(share) è§£å†³heterogeneityé—®é¢˜ ç‰¹ç‚¹ è½¬ç§»çŸ©é˜µçš„ç¨€ç–åº¦ç”±å…³ç³»é“¾æ¥çš„å®ä½“å¯¹çš„æ•°é‡ç¡®å®š å¹¶ä¸”å…³ç³»çš„ä¸¤ä¾§å…±äº«ç›¸åŒçš„è½¬ç§»çŸ©é˜µ å¯¹äºå¤æ‚å…³ç³»çš„è½¬ç§»çŸ©é˜µæ›´åŠ ç¨€ç– ä¸çŸ¥é“ä¸ºä»€ä¹ˆå¤æ‚è½¬ç§»çŸ©é˜µä¼šæ›´åŠ ç¨€ç–ï¼Ÿ è½¬ç§»çŸ©é˜µçš„ç¨€ç–ç¨‹åº¦ \theta_{r}=1-\left(1-\theta_{\min }\right) N_{r} / N_{r^{*}}å…¶ä¸­ï¼Œ$N_r$ä»£è¡¨é“¾æ¥å…³ç³»$r$çš„å®ä½“å¯¹çš„æ•°é‡ï¼Œ$r^$ä»£è¡¨é“¾æ¥æœ€å¤šå®ä½“å¯¹çš„å…³ç³»ï¼Œ$\theta_{\min }\left(0 \leq \theta_{\min } \leq 1\right)$æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œä»£è¡¨çŸ©é˜µ$M_{r^{}}$çš„æœ€å°ç³»æ•°ç¨‹åº¦ã€‚ ä¸ç†è§£è¿™é‡Œä¸ºä»€ä¹ˆæŠŠç¨€ç–ç¨‹åº¦å®šä¹‰æˆè¿™ä¹ˆéº»çƒ¦ï¼Œä¸ºä»€ä¸ç›´æ¥å®šä¹‰æˆ$\theta_{\min} N_{r} / N_{r^{*}}$ æ˜ å°„å‘é‡ \mathbf{h}_{p}=\mathbf{M}_{r}\left(\theta_{r}\right) \mathbf{h}, \quad \mathbf{t}_{p}=\mathbf{M}_{r}\left(\theta_{r}\right) \mathbf{t}TranSpare(separate) è§£å†³imbalanceé—®é¢˜ ç‰¹ç‚¹ æ¯ä¸ªå…³ç³»å…·æœ‰ä¸¤ä¸ªå•ç‹¬çš„ç¨€ç–ä¼ é€’çŸ©é˜µï¼Œä¸€ä¸ªç”¨äºå¤´å®ä½“ï¼Œå¦ä¸€ä¸ªç”¨äºå°¾å®ä½“ ç¨€ç–åº¦å–å†³äºé€šè¿‡å…³ç³»é“¾æ¥çš„å¤´ï¼ˆå°¾ï¼‰å®ä½“çš„æ•°é‡ è½¬ç§»çŸ©é˜µçš„ç¨€ç–ç¨‹åº¦ \theta_{r}^{l}=1-\left(1-\theta_{\min }\right) N_{r}^{l} / N_{r^{*}}^{l^{*}} \quad(l=h, t)å’Œshareç±»ä¼¼ï¼Œåªæ˜¯å¤´å°¾å®ä½“ä¸ç›¸åŒï¼Œå¢åŠ læ¥ä»£è¡¨å¤´å°¾å®ä½“æ•°é‡ã€‚ æ˜ å°„å‘é‡ \theta_{r}^{l}=1-\left(1-\theta_{\min }\right) N_{r}^{l} / N_{r^{*}}^{l^{*}} \quad(l=h, t)åˆ†æ•°å‡½æ•°ä¸¤è€…çš„åˆ†æ•°å‡½æ•°ç›¸åŒå‡ä¸ºï¼š f_{r}(\mathbf{h}, \mathbf{t})=\left\|\mathbf{h}_{p}+\mathbf{r}-\mathbf{t}_{p}\right\|_{\ell_{1 / 2}}^{2}æ€»lossé‡‡ç”¨margin-based ranking loss $L=\sum_{(h, r, t) \in \Delta\left(h^{\prime}, r, t\right) \in \Delta^{\prime}}\left[\gamma+f_{r}(\mathbf{h}, \mathbf{t})-f_{r}\left(\mathbf{h}^{\prime}, \mathbf{t}^{\prime}\right)\right]_{+}$è®­ç»ƒè¿‡ç¨‹ä¸ºäº†åŠ é€Ÿè®­ç»ƒæ—¶æ”¶æ•›ä»¥åŠé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œä½œè€…ä½¿ç”¨TransEç®—æ³•çš„ç»“æœè¿›è¡Œåˆå§‹åŒ–å®ä½“å’Œå…³ç³»çš„embeddingå‘é‡ï¼Œå¯¹äºè½¬åŒ–çŸ©é˜µï¼Œä½œè€…ä½¿ç”¨å•ä½çŸ©é˜µè¿›è¡Œåˆå§‹åŒ–ã€‚ä½†æ˜¯è¿™ä¸æ˜¯éå¿…é¡»çš„ å¯¹äºè½¬åŒ–çŸ©é˜µ(å‡è®¾ä¸ºå•ä½çŸ©é˜µ)ï¼Œéé›¶å‘é‡çš„ä¸ªæ•°$n z=\lfloor\theta \times n \times n\rfloor$ï¼Œç”±äºä½œè€…ä½¿ç”¨å•ä½å‘é‡åˆå§‹åŒ–ï¼Œæ‰€ä»¥é™¤äº†å¯¹è§’çº¿ä¸Šçš„éé›¶å…ƒç´ ä¹‹å¤–ï¼Œå…¶ä»–éé›¶å…ƒç´ çš„ä¸ªæ•°ä¸º$n z^{\prime}=n z-n$ï¼Œå¦‚æœ$n z \leq n$ï¼Œé‚£ä¹ˆä½œè€…è®¾ç½®$n z^{\prime}=0$ã€‚ åœ¨æ„å»ºç»“æ„åŒ–çš„è½¬åŒ–çŸ©é˜µ$\mathbf{M}(\theta)$çš„æ—¶å€™ï¼Œä½œè€…è¦è®©$n z^{\prime}$éé›¶å…ƒç´ å¯¹ç§°åˆ†å¸ƒåœ¨å¯¹è§’çº¿çš„ä¸¤è¾¹ï¼Œå¦‚æœ$n z^{\prime}$ä¸èƒ½æ»¡è¶³è¦æ±‚ï¼Œé‚£ä¹ˆä½œè€…é€‰æ‹©å¦å¤–ä¸€ä¸ªæ•´æ•°ã€‚ åœ¨æ„å»ºéç»“æ„åŒ–çš„è½¬åŒ–çŸ©é˜µ$\mathbf{M}(\theta)$çš„æ—¶å€™ï¼Œä½œè€…åªéšæœºæ•£å¸ƒ$\mathbf{M}(\theta)$ä¸­çš„$n z^{\prime}$éé›¶å…ƒç´ ï¼ˆä½†ä¸åœ¨å¯¹è§’çº¿ä¸Šï¼‰ã€‚ åœ¨è®­ç»ƒå‰ï¼Œä½œè€…é¦–å…ˆè®¾ç½®è¶…å‚æ•°$\theta_{\min }$ï¼Œç„¶åè®¡ç®—æ¯ä¸ªè½¬åŒ–çŸ©é˜µçš„ç¨€ç–ç¨‹åº¦ï¼Œç„¶åï¼Œä½œè€…ä½¿ç”¨ç»“æ„åŒ–æˆ–éç»“æ„åŒ–æ¨¡å¼æ„å»ºç¨€ç–è½¬åŒ–çŸ©é˜µã€‚ å®éªŒä¸å¸¸è§„ä¸åŒçš„å°±æ˜¯åŠ äº†ä¸€ä¸ªå®éªŒ]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>2016</tag>
        <tag>KGE</tag>
        <tag>AAAI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2015 TransA An Adaptive Approach for Knowledge Graph Embeddiné˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2F2015_TransA_An_Adaptive_Approach_for_Knowledge_Graph_Embeddin%2F</url>
    <content type="text"><![CDATA[è¿™ç¯‡æ–‡ç« å…·ä½“çš„å…¬å¼æ“ä½œæ¯”è¾ƒéš¾ä»¥ç†è§£ï¼Œä½†æ˜¯å¯¹äºæˆ‘æ¥è¯´ï¼Œå®ƒçš„æ¯ä¸€ç»´åº¦åŠ æƒå’Œæœ€ååˆ¤æ–­æ—¶ä¹Ÿåº”è¯¥è€ƒè™‘ä¸åŒç»´åº¦å·®å¼‚å’Œæˆ‘çš„æ€è·¯æ¯”è¾ƒåƒã€‚ è®ºæ–‡ä¸‹è½½åœ°å€ Problem Statementä¹‹å‰çš„æœºé‡ç¿»è¯‘çš„æ–¹æ³•è¿‡äºç®€åŒ–æŸå¤±åº¦é‡ï¼Œä½¿å¾—æ¨¡å‹æ²¡æœ‰è¶³å¤Ÿçš„èƒ½åŠ›æ¥æ¨¡æ‹ŸçŸ¥è¯†åº“ä¸­çš„å„ç§å¤æ‚å®ä½“/å…³ç³»ã€‚ Introduction ç”±äºæŸå¤±åº¦é‡çš„ä¸çµæ´»æ€§ï¼Œå½“å‰åŸºäºç¿»è¯‘çš„æ–¹æ³•åº”ç”¨å…·æœ‰ä¸åŒåˆç†æ€§çš„çƒé¢ç­‰åŠ¿è¶…è¡¨é¢ï¼Œå…¶ä¸­ä¸‰å…ƒç»„æ›´é è¿‘ä¸­å¿ƒï¼Œä¸‰å…ƒç»„æ›´åˆç†ã€‚å¦‚å›¾1ï¼ˆaï¼‰æ‰€ç¤ºï¼Œçƒé¢ç­‰åŠ¿è¶…è¡¨é¢ä¸å¤Ÿçµæ´»ï¼Œä¸è¶³ä»¥è¡¨å¾æ‹“æ‰‘ç»“æ„ã€‚ å…¶æ¬¡ï¼Œç”±äºè¿‡åº¦ç®€åŒ–çš„æŸå¤±åº¦é‡ï¼Œå½“å‰åŸºäºç¿»è¯‘çš„æ–¹æ³•ç”¨åŒæ€§çš„æ¬§æ°è·ç¦»æ— æ³•ä½“ç°å‡ºå„ä¸ªç»´åº¦çš„é‡è¦æ€§ï¼Œè€Œæ˜¯å°†å„ä¸ªç»´åº¦çš„ç‰¹å¾éƒ½ç­‰åŒçœ‹å¾…ã€‚å¯¼è‡´å›¾2ä¸­å‡ºç°çš„é—®é¢˜ï¼š ç”±äºå¯¹æ¯ä¸€ç»´åº¦å¤„ç†ç›¸åŒï¼Œä¸æ­£ç¡®çš„å®ä½“å°†è¢«åŒ¹é…ã€‚ä½†æ˜¯é€šè¿‡å¯¹ä¸åŒç»´åº¦è¿›è¡Œä¸åŒåŠ æƒå°†ä¼šé¿å…è¿™ç§ç”±äºè·ç¦»ç›¸åŒè¢«åŒ¹é…çš„é”™è¯¯ã€‚ Model ä½œè€…æå‡ºTransAï¼Œä¸€ç§åˆ©ç”¨è‡ªé€‚åº”å’Œçµæ´»åº¦é‡çš„åµŒå…¥æ–¹æ³•ï¼š TransAé‡‡ç”¨æ¤­åœ†å½¢è¡¨é¢è€Œä¸æ˜¯çƒå½¢è¡¨é¢ï¼šé€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¤æ‚å…³ç³»å¼•èµ·çš„å¤æ‚åµŒå…¥æ‹“æ‰‘ç»“æ„å¯ä»¥å¾—åˆ°æ›´å¥½çš„è¡¨ç°ã€‚ å¦‚â€œè‡ªé€‚åº”åº¦é‡æ–¹æ³•â€ä¸­æ‰€åˆ†æçš„é‚£æ ·ï¼ŒTransAå¯ä»¥è¢«è§†ä¸ºåŠ æƒå˜æ¢ç‰¹å¾ç»´åº¦ã€‚ å› æ­¤ï¼Œæ¥è‡ªä¸ç›¸å…³å°ºå¯¸çš„å™ªå£°è¢«æŠ‘åˆ¶ã€‚ è‡ªé€‚åº”åº¦é‡åˆ†æ•°å‡½æ•°TransAé‡‡ç”¨è‡ªé€‚åº”Mahalanobisç»å¯¹æŸå¤±è·ç¦»ï¼š f_{r}(h, t)=(|\mathbf{h}+\mathbf{r}-\mathbf{t}|)^{\top} \mathbf{W}_{\mathbf{r}}(|\mathbf{h}+\mathbf{r}-\mathbf{t}|)å…¶ä¸­ |\mathbf{h}+\mathbf{r}-\mathbf{t}| \doteq\left(\left|h_{1}+r_{1}-t_{1}\right|,\left|h_{2}+r_{2}-t_{2}\right|, \ldots, | h_{n}+\right.\left.\left.r_{n}-t_{n}\right\rfloor\right)ï¼Œ$W_r$æ˜¯å¯¹åº”äºç‰¹å®šå…³ç³»çš„å¯¹ç§°éè´Ÿæƒé‡çŸ©é˜µï¼Œä¹Ÿæ˜¯è‡ªé€‚åº”æƒé‡çŸ©é˜µã€‚ä¸ä¼ ç»Ÿçš„å¾—åˆ†å‡½æ•°ä¸åŒï¼Œä½œè€…å–ç»å¯¹å€¼ï¼Œå› ä¸ºæƒ³è¦æµ‹é‡ï¼ˆh + rï¼‰å’Œtä¹‹é—´çš„ç»å¯¹æŸå¤±ã€‚åŸå› æœ‰ä»¥ä¸‹ä¸¤ç‚¹ï¼š ä½œè€…å°†çš„åˆ†æ•°å‡½æ•°æ‰©å±•ä¸ºè¯±å¯¼è§„èŒƒï¼š N_{r}(\mathbf{e})=\sqrt{f_{r}(h, t)}è¿™æ ·å¯ä»¥ç”¨ä»¥ä¸‹æ–¹å¼æ¥ç®€åŒ–è¿ç®—ï¼š \begin{array}{l}{\text { inequality } N_{r}\left(\mathbf{e}_{1}+\mathbf{e}_{2}\right)=\sqrt{\left|\mathbf{e}_{1}+\mathbf{e}_{2}\right|^{\top} \mathbf{W}_{\mathbf{r}}\left|\mathbf{e}_{\mathbf{1}}+\mathbf{e}_{\mathbf{2}}\right|} \leq} {\sqrt{\left|\mathbf{e}_{\mathbf{1}}\right|^{\top} \mathbf{W}_{\mathbf{r}}\left|\mathbf{e}_{\mathbf{1}}\right|}+\sqrt{\left|\mathbf{e}_{\mathbf{2}}\right|^{\top} \mathbf{W}_{\mathbf{r}}\left|\mathbf{e}_{\mathbf{2}}\right|}=N_{r}\left(\mathbf{e}_{\mathbf{1}}\right)+N_{r}\left(\mathbf{e}_{\mathbf{2}}\right)}\end{array} åœ¨å‡ ä½•ä¸­ï¼Œè´Ÿå€¼æˆ–æ­£å€¼è¡¨ç¤ºå‘ä¸‹æˆ–å‘ä¸Šçš„æ–¹å‘ã€‚è€Œåœ¨ä½œè€…çš„æ–¹æ³•ä¸­ï¼Œä½œè€…ä¸è€ƒè™‘è¿™ä¸ªå› ç´ ã€‚ è®©ä½œè€…çœ‹ä¸€ä¸‹å¦‚å›¾2æ‰€ç¤ºçš„å®ä¾‹ã€‚ å¯¹äºå®ä½“Goniffï¼Œå…¶æŸè€—å‘é‡çš„xè½´åˆ†é‡æ˜¯è´Ÿçš„ï¼Œå› æ­¤æ‰©å¤§è¯¥åˆ†é‡å°†ä½¿æ•´ä½“æŸå¤±æ›´å°ï¼Œè€Œè¿™ç§æƒ…å†µåº”è¯¥ä½¿æ•´ä½“æŸå¤±æ›´å¤§ã€‚ å› æ­¤ï¼Œç»å¯¹ç®—å­å¯¹ä½œè€…çš„æ–¹æ³•è‡³å…³é‡è¦ã€‚ å¯¹äºæ²¡æœ‰ç»å¯¹ç®—å­çš„æ•°å€¼ä¾‹å­ï¼Œå½“åµŒå…¥ç»´æ•°ä¸º2æ—¶ï¼Œæƒé‡çŸ©é˜µä¸º[0 1; 1 0]å’ŒæŸå¤±çŸ¢é‡ï¼ˆh + r - tï¼‰=ï¼ˆe1ï¼Œe2ï¼‰ï¼Œæ€»æŸå¤±ä¸º2e1e2ã€‚ å¦‚æœe1â‰¥0ä¸”e2â‰¤0ï¼Œåˆ™ç»å¯¹æ›´å¤§çš„e2å°†å‡å°‘æ€»æŸè€—ï¼Œè¿™æ˜¯ä¸å¸Œæœ›çš„ ä»ç­‰åŠ¿é¢çš„è§’åº¦å¯¹äºå…¶ä»–åŸºäºç¿»è¯‘çš„æ–¹æ³•ï¼Œç­‰åŠ¿è¶…æ›²é¢æ˜¯æ¬§å‡ é‡Œå¾·è·ç¦»å®šä¹‰çš„çƒä½“ï¼š \|(\mathbf{t}-\mathbf{h})-\mathbf{r}\|_{2}^{2}=\mathcal{C}å…¶ä¸­ï¼Œ$\mathcal{C}$è¡¨ç¤ºé˜ˆå€¼æˆ–ç­‰åŠ¿å€¼ã€‚ è€Œå¯¹äºTransAæ¥è¯´ï¼Œç­‰åŠ¿é¢æ˜¯æ¤­åœ† |(\mathbf{t}-\mathbf{h})-\mathbf{r}|^{\top} \mathbf{W}_{\mathbf{r}}|(\mathbf{t}-\mathbf{h})-\mathbf{r}|=\mathcal{C}â€‹ ç”±äºçŸ¥è¯†åº“æ˜¯å¤§è§„æ¨¡ä¸”éå¸¸å¤æ‚çš„å®é™…æƒ…å†µï¼ŒåµŒå…¥çš„æ‹“æ‰‘ç»“æ„ä¸èƒ½åƒçƒä½“é‚£æ ·å‡åŒ€åˆ†å¸ƒï¼Œå¦‚å›¾1æ‰€ç¤ºã€‚ å› æ­¤ï¼Œç”¨æ¤­åœ†å½¢æ›¿æ¢çƒé¢ç­‰åŠ¿è¶…æ›²é¢å°†å¢å¼ºåµŒå…¥ ä»ç‰¹å¾æƒé‡è§’åº¦TransAå¯ä»¥çœ‹åšæ˜¯å¸¦æœ‰æƒé‡çš„ç‰¹å¾å˜æ¢ï¼Œå‡è®¾æƒé‡çŸ©é˜µ$W_r$æ˜¯å¯¹ç§°çŸ©é˜µï¼Œé‚£ä¹ˆå¯ä»¥é€šè¿‡LDLåˆ†è§£å°†æƒé‡åˆ†è§£ä¸º \begin{array}{c}{\mathbf{W}_{\mathbf{r}}=\mathbf{L}_{\mathbf{r}}^{\top} \mathbf{D}_{\mathbf{r}} \mathbf{L}_{\mathbf{r}}} \\ {f_{r}=\left(\mathbf{L}_{\mathbf{r}}|\mathbf{h}+\mathbf{r}-\mathbf{t}|\right)^{\top} \mathbf{D}_{\mathbf{r}}\left(\mathbf{L}_{\mathbf{r}}|\mathbf{h}+\mathbf{r}-\mathbf{t}|\right)}\end{array}ç›¸å½“äºå¯¹losså‘é‡é€šè¿‡$L_r$è¿›è¡Œç‰¹å¾å˜æ¢ï¼Œå…¶ä¸­ï¼Œ$\mathbf{D}_{r}=\operatorname{diag}\left(w_{1}, w_{2}, \ldots\right)$æ˜¯ä¸€ä¸ªå¯¹è§’çŸ©é˜µï¼Œå¯¹è§’å…ƒç´ çš„å€¼å°±æ˜¯ä¸åŒåµŒå…¥ç»´åº¦çš„æƒå€¼ã€‚ å¯¹æ¯”ä¹‹å‰æ–¹æ³•ä¸å…³äºæ—‹è½¬å’Œç¼©æ”¾åµŒå…¥ç©ºé—´çš„TransRï¼ŒTransAå…·æœ‰ä¸¤ä¸ªä¼˜åŠ¿ã€‚ é¦–å…ˆï¼Œä½œè€…å¯¹ç‰¹å¾å°ºå¯¸è¿›è¡ŒåŠ æƒä»¥é¿å…å™ªéŸ³ã€‚ å…¶æ¬¡ï¼Œä½œè€…æ”¾æ¾äº†PSDæ¡ä»¶ä»¥è·å¾—çµæ´»çš„è¡¨ç¤ºã€‚ ä¸ä½¿ç”¨é¢„å…ˆè®¡ç®—çš„ç³»æ•°é‡æ–°æ„é€ å¯¹ç‰¹å¾å°ºå¯¸è¿›è¡ŒåŠ æƒçš„TransMï¼ŒTransAå…·æœ‰ä¸¤ä¸ªä¼˜åŠ¿ã€‚ é¦–å…ˆï¼Œä½œè€…ä»æ•°æ®ä¸­å­¦ä¹ æƒé‡ï¼Œè¿™ä½¿å¾—åˆ†æ•°å‡½æ•°æ›´å…·é€‚åº”æ€§ã€‚ å…¶æ¬¡ï¼Œä½œè€…åº”ç”¨ç‰¹å¾è½¬æ¢ï¼Œä½¿åµŒå…¥æ›´æœ‰æ•ˆ ç®—æ³•è®­ç»ƒlosså‡½æ•°ä¸ºï¼š \begin{array}{c}{\mathcal{L}=\sum_{(h, l, t) \in \Delta\left(h^{\prime}, l^{\prime}, t^{\prime}\right) \in \Delta^{\prime}}\left[\gamma+f_{r}(\mathrm{h}, \mathrm{t})-f_{r^{\prime}}\left(\mathrm{h}^{\prime}, \mathrm{t}^{\prime}\right)\right]_{+}+\lambda\left(\sum_{r \in R}\|\mathbf{W} r\|_{F}^{2}\right)+C\left(\sum_{e \in E}\|\mathbf{e}\|_{2}^{2}+\sum_{r \in R}\left\|\mathbf{r}_{2}^{2}\right\|\right)} \\ {\text { s.t. }\left[\mathbf{W}_{r}\right]_{i j} \geq 0}\end{array}ä¿è¯éè´Ÿæ€§ï¼Œä½œè€…å°†æ‰€æœ‰å¦å®šæ¡ç›®æƒé‡çš„å€¼èµ‹ä¸º0 \mathbf{W}_{r}=-\sum_{(h, r, t) \in \Delta}\left(|\mathbf{h}+\mathbf{r}-\mathbf{t} \| \mathbf{h}+\mathbf{r}-\mathbf{t}|^{\top}\right)+\sum_{\left(h^{\prime}, r^{\prime}, t^{\prime}\right) \in \Delta^{\prime}}\left(\left|\mathbf{h}^{\prime}+\mathbf{r}^{\prime}-\mathbf{t}^{\prime}\right|\left|\mathbf{h}^{\prime}+\mathbf{r}^{\prime}-\mathbf{t}^{\prime}\right|^{\top}\right)æ¨¡å‹å¤æ‚åº¦è‡³äºä½œè€…æ¨¡å‹çš„å¤æ‚æ€§ï¼Œæƒé‡çŸ©é˜µå®Œå…¨ç”±ç°æœ‰çš„åµŒå…¥å‘é‡è®¡ç®—ï¼Œè¿™æ„å‘³ç€TransAå‡ ä¹å…·æœ‰ä¸TransEç›¸åŒçš„è‡ªç”±å‚æ•°æ•°ã€‚ è‡³äºä½œè€…æ¨¡å‹çš„æ•ˆç‡ï¼Œæƒé‡çŸ©é˜µæœ‰ä¸€ä¸ªå°é—­çš„è§£å†³æ–¹æ¡ˆï¼Œè¿™åœ¨å¾ˆå¤§ç¨‹åº¦ä¸ŠåŠ å¿«äº†åŸ¹è®­è¿‡ç¨‹ å®éªŒä½œè€…ç»Ÿè®¡äº†ä¸€ä¸ªATPEå€¼ï¼ˆAveraged Triple number Per Entity.ï¼‰è¯¥æ•°é‡è¡¡é‡æ•°æ®é›†çš„å¤šæ ·æ€§å’Œå¤æ‚æ€§ ä½œè€…å¯¹äºTransAåœ¨WN18ä¸Šè¡¨ç°ä¸å¥½è¿›è¡Œäº†åˆ†æè§£é‡Šã€‚ TransAåœ¨WN18æ•°æ®é›†ä¸Šçš„å¹³å‡æ’åè¡¨ç°ä¸ä½³ã€‚ æ·±å…¥ç ”ç©¶è¯¦ç»†æƒ…å†µï¼Œæˆ‘ä»¬å‘ç°æœ‰27ä¸ªæµ‹è¯•ä¸‰å…ƒç»„ï¼ˆæµ‹è¯•é›†çš„0.54ï¼…ï¼‰ï¼Œå…¶æ’åè¶…è¿‡30,000ï¼Œè¿™å‡ ä¸ªæ¡ˆä¾‹å°†å¯¼è‡´çº¦162ä¸ªå¹³å‡ç­‰çº§æŸå¤±ã€‚ æ‰€æœ‰è¿™äº›ä¸‰å…ƒç»„çš„å°¾éƒ¨æˆ–å¤´éƒ¨å®ä½“ä»æœªä¸è®­ç»ƒé›†ä¸­çš„ç›¸åº”å…³ç³»å…±å­˜ã€‚ è®­ç»ƒæ•°æ®ä¸è¶³å¯¼è‡´æƒé‡çŸ©é˜µè¿‡åº¦æ‰­æ›²ï¼Œæƒé‡çŸ©é˜µè¿‡åº¦æ‰­æ›²å¯¼è‡´å¹³å‡æ’åä¸è‰¯ ä½œè€…è¿˜åšäº†ä¸€ä¸ªå®éªŒ ä½œè€…è§£é‡Šï¼š ç²¾åº¦éšé‡é‡å·®å¼‚è€Œå˜åŒ–ï¼Œè¿™æ„å‘³ç€ç‰¹å¾åŠ æƒæœ‰åˆ©äºç²¾ç¡®åº¦ã€‚ è¿™è¯æ˜äº†TransAçš„ç†è®ºåˆ†æå’Œæœ‰æ•ˆæ€§ ä½†æ˜¯æˆ‘å¹¶ä¸è®¤ä¸ºè¿™ä¸ªå¯ä»¥è¯´æ˜ä»€ä¹ˆï¼Œé‚£æ˜¯ä¸æ˜¯è°ƒé«˜å…¶ä»–å…³ç³»çš„æƒé‡ä¼šå¸¦æ¥è¯¥å…³ç³»æ•ˆæœçš„æå‡ï¼Ÿ]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
        <category>KG</category>
      </categories>
      <tags>
        <tag>KGE</tag>
        <tag>TransA</tag>
        <tag>2015</tag>
        <tag>arxiv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Transition-based Knowledge Graph Embedding with Relational Mapping Propertiesé˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FTransition-based_Knowledge_Graph_Embedding_with_Relational_Mapping_Properties%2F</url>
    <content type="text"><![CDATA[è®ºæ–‡ä¸‹è½½åœ°å€ Related WorkæŠŠçŸ¥è¯†å›¾è°±æ˜ å°„åˆ°ä½ç»´å‘é‡çš„åŸå› ï¼šç¬¦å·å’Œç¦»æ•£çš„å­˜å‚¨ç»“æ„ä½¿æˆ‘ä»¬å¾ˆéš¾åˆ©ç”¨è¿™äº›çŸ¥è¯†æ¥å¢å¼ºå…¶ä»–æ™ºèƒ½è·å–çš„åº”ç”¨ç¨‹åºï¼ˆä¾‹å¦‚é—®ç­”ç³»ç»Ÿï¼‰ï¼Œå› ä¸ºè®¸å¤šä¸AIç›¸å…³çš„ç®—æ³•æ›´å€¾å‘äºè¿›è¡Œå…³äºè¿ç»­æ•°æ®è®¡ç®—ã€‚ æ–‡ä¸­ç”¨ONE-TO-ONE (husband-to-wife), MANY-TO-ONE (children-to-father), ONE-TO- MANY (mother-to-children), MANY-TO-MANY (parents-to-children) æ¥è¿›è¡Œéå•ä¸€å…³ç³»çš„ä¾‹å­è§‰å¾—å¾ˆå¥½ã€‚ ä»¥å‰ç®—æ³•çš„ç›®æ ‡å‡½æ•°å’Œå‚æ•°å¤æ‚åº¦ï¼š Modelç†è§£ä¸äº†æ˜¯å¦‚ä½•åŒºåˆ†1å¯¹å¤šå…³ç³»çš„ï¼Ÿï¼Ÿï¼Ÿï¼Ÿ ä½œè€…å°†æœ€ä¼˜å‡½æ•°å°†é€šè¿‡å¯¹åº”äºè¯¥å…³ç³»çš„é¢„å…ˆè®¡ç®—çš„æƒé‡ç»™å‡ºæ¯ä¸ªè®­ç»ƒä¸‰å…ƒç»„çš„ä¸åŒæ–¹é¢ã€‚ å®é™…ä¸Šï¼Œå¤§çº¦åªæœ‰26.2ï¼…çš„ONE-TO-ONEä¸‰å…ƒç»„é€‚åˆç”±TransEå»ºæ¨¡ã€‚ å¦ä¸€æ–¹é¢ï¼Œå…¶ä½™ä¸‰å…ƒç»„ï¼ˆ73.8ï¼…ï¼‰å—åˆ°å½±å“ï¼Œå¦‚å›¾1å·¦ä¾§æ‰€ç¤ºã€‚ åŠ¨æœºæ ¹æ®ä½œè€…çš„è§‚å¯Ÿï¼Œä¸‰å…ƒç»„çš„æ˜ å°„å±æ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºå®ƒçš„å…³ç³»ã€‚ ç›®æ ‡å‡½æ•°æƒé‡æ˜¯å…³ç³»ç‰¹å®šçš„ï¼Œä½œè€…ä¸ºä¸‰å…ƒç»„ï¼ˆhï¼Œrï¼Œtï¼‰æå‡ºçš„æ–°è¯„åˆ†å‡½æ•°æ˜¯ï¼š f_{r}(h, t)=w_{\mathbf{r}}\|\mathbf{h}+\mathbf{r}-\mathbf{t}\|_{L_{1} / L_{2}} w_{r}=\frac{1}{\log \left(h_{r} p t_{r}+t_{r} p h_{r}\right)} æµ‹é‡å…³ç³»çš„æ˜ å°„å±æ€§ç¨‹åº¦çš„ä¸€ç§ç®€å•æ–¹æ³•æ˜¯è®¡ç®—æ¯ä¸ªä¸åŒå¤´éƒ¨å®ä½“çš„å°¾éƒ¨å®ä½“çš„å¹³å‡æ•°é‡ï¼Œåä¹‹äº¦ç„¶ã€‚ è¿™é‡Œè¿˜æ˜¯ä¸ç†è§£è¿™ä¸ªæƒé‡çš„æ„ä¹‰åœ¨å“ªé‡Œï¼ŸğŸ§ æŸå¤±å‡½æ•° \begin{array}{l}{\mathcal{L}=\min \sum_{(h, r, t) \in \Delta\left(h^{\prime}, r, t^{\prime}\right) \in \Delta_{(h, r, t)}^{\prime}}\left[\gamma+f_{r}(h, t)-f_{r}\left(h^{\prime}, t^{\prime}\right)\right]_{+}} \\ {\text {s.t. } \quad \forall e \in E,\|e\|_{2}=1}\end{array}å¯¹äº$|e|_{2}=1$çš„è§£é‡Šï¼šçº¦æŸä½äºå•ä½çƒä¸Šçš„æ¯ä¸ªå®ä½“çš„åŸå› æ˜¯ä¸ºäº†ä¿è¯å®ƒä»¬å¯ä»¥ä»¥ç›¸åŒçš„æ¯”ä¾‹æ›´æ–°ï¼Œè€Œä¸æ˜¯å¤ªå¤§æˆ–å¤ªå°è€Œä¸èƒ½æ»¡è¶³æœ€ä½³ç›®æ ‡ã€‚ Experimentsè§è®ºæ–‡ï¼Œæ²¡ä»€ä¹ˆå¥½é˜è¿°çš„ã€‚ All the codes for the related models can be downloaded from https://github.com/glorotxa/SME]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
        <category>KG</category>
      </categories>
      <tags>
        <tag>KGE</tag>
        <tag>2014</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Embedding Edge-attributed Relational Hierarchiesé˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FEmbedding_Edge-attributed_Relational_Hierarchies%2F</url>
    <content type="text"><![CDATA[è®ºæ–‡ä¸‹è½½åœ°å€]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Knowledge Graph Embedding via Dynamic Mapping Matrixé˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FKnowledge_Graph_Embedding_via_Dynamic_Mapping_Matrix%2F</url>
    <content type="text"><![CDATA[è®ºæ–‡ä¸‹è½½åœ°å€ Problem Statement å¯¹äºç‰¹å®šçš„å…³ç³»$r$ï¼Œæ‰€æœ‰çš„å®ä½“éƒ½å…±äº«ç›¸åŒçš„æ˜ å°„çŸ©é˜µ$M_r$ã€‚ç„¶è€Œï¼Œç”±å…³ç³»é“¾æ¥çš„å®ä½“æ€»æ˜¯åŒ…å«å„ç§ç±»å‹å’Œå±æ€§ã€‚ æŠ•å½±æ˜¯å®ä½“å’Œå…³ç³»ä¹‹é—´çš„äº¤äº’è¿‡ç¨‹ï¼Œæ˜ å°„çŸ©é˜µåªèƒ½ç”±å…³ç³»å†³å®šæ˜¯ä¸åˆç†çš„ã€‚ çŸ©é˜µå‘é‡ä¹˜æ³•ä½¿å…¶å…·æœ‰å¤§é‡è®¡ç®—ï¼Œå¹¶ä¸”å½“å…³ç³»æ•°å¤§æ—¶ï¼Œå®ƒè¿˜å…·æœ‰æ¯”TransEå’ŒTransHæ›´å¤šçš„å‚æ•°ã€‚ ç”±äºå¤æ‚æ€§ï¼ŒTransR / CTransRéš¾ä»¥åº”ç”¨äºå¤§è§„æ¨¡çŸ¥è¯†å›¾ Contribution ä½œè€…æ„å»ºäº†ä¸€ä¸ªæ–°é¢–çš„æ¨¡å‹TransDï¼Œé€šè¿‡åŒæ—¶è€ƒè™‘å®ä½“å’Œå…³ç³»çš„å¤šæ ·æ€§ï¼Œä¸ºæ¯ä¸€ä¸ªå®ä½“-å…³ç³»æ„å»ºåŠ¨æ€æ˜ å°„çŸ©é˜µã€‚å®ƒä¸ºå®ä½“è¡¨ç¤ºæ˜ å°„åˆ°å…³ç³»å‘é‡ç©ºé—´æä¾›çµæ´»çš„æ ·å¼ã€‚ ä¸TransR / CTransRç›¸æ¯”ï¼ŒTransDå…·æœ‰æ›´å°‘çš„å‚æ•°å¹¶ä¸”æ²¡æœ‰çŸ©é˜µå‘é‡ä¹˜æ³• åœ¨å®éªŒä¸­ï¼Œä½œè€…çš„æ–¹æ³•ä¼˜äºä¹‹å‰çš„æ¨¡å‹ã€‚ Modelæ¨¡å‹åœ¨TransDä¸­ï¼Œæ¯ä¸ªå‘½åçš„ç¬¦å·å¯¹è±¡ï¼ˆå®ä½“å’Œå…³ç³»ï¼‰ç”±ä¸¤ä¸ªå‘é‡è¡¨ç¤ºã€‚ ç¬¬ä¸€ä¸ªæ•è·å®ä½“ï¼ˆå…³ç³»ï¼‰çš„å«ä¹‰ï¼Œå¦ä¸€ä¸ªç”¨äºæ„é€ æ˜ å°„çŸ©é˜µã€‚ å¯¹äºä¸€ä¸ªä¸‰å…ƒç»„$(h,r,t)$,å‘é‡ä¸€å…±æœ‰$h, h_p, r, r_p, t, t_p$,å…¶ä¸­å¸¦$p$çš„ä¸ºæ˜ å°„å‘é‡ï¼Œåˆ™æœ‰ \begin{aligned} \mathbf{M}_{r h} &=\mathbf{r}_{p} \mathbf{h}_{p}^{\top}+\mathbf{I}^{m \times n} \\ \mathbf{M}_{r t} &=\mathbf{r}_{p} \mathbf{t}_{p}^{\top}+\mathbf{I}^{m \times n} \end{aligned}æ•… \mathbf{h}_{\perp}=\mathbf{M}_{r h} \mathbf{h}, \quad \mathbf{t}_{\perp}=\mathbf{M}_{r t} \mathbf{t}å¯ä»¥ç»¼åˆä¸ºï¼š \begin{aligned} \mathbf{h}_{\perp} &=\mathbf{M}_{r h} \mathbf{h}=\mathbf{h}+\mathbf{h}_{p}^{\top} \mathbf{h} \mathbf{r}_{p} \\ \mathbf{t}_{\perp} &=\mathbf{M}_{r t} \mathbf{t}=\mathbf{t}+\mathbf{t}_{p}^{\top} \mathbf{t} \mathbf{r}_{p} \end{aligned}è¿™æ ·å°±æ²¡æœ‰çŸ©é˜µå’Œå‘é‡é—´çš„ä¹˜æ³•è¿ç®—ï¼Œå˜æˆå‘é‡é—´è¿ç®—ï¼Œæå‡è®¡ç®—é€Ÿåº¦ã€‚ Experiments and Results Analysiså¸¸è§„å®éªŒï¼štriplets classification and link predictionä¸å†èµ˜è¿°ã€‚ ä½œè€…åœ¨å®éªŒè¿‡ç¨‹ä¸­å…³æ³¨äº†ä¸€äº›å…·æœ‰æ›´ä½accuracyçš„å…³ç³»ã€‚ åˆ†æï¼š 1. å¯¹äº$similar_to$å…³ç³»ä¸»è¦å› ä¸ºè®­ç»ƒæ•°æ®ä¸å……è¶³ï¼Œåªå äº†1.5%ã€‚ 2. å¯¹äºæœ€å³ä¾§çš„å›¾è¯´æ˜äº†bernæ–¹æ³•çš„æ•ˆæœè¦å¥½äºunif Properties of Projection Vectorsä½œè€…è¿˜åšäº†case studyï¼Œé€šè¿‡ä¸åŒç±»å‹å®ä½“å’Œå…³ç³»çš„æŠ•å½±å‘é‡çš„ç›¸ä¼¼æ€§è¡¨æ˜äº†ä½œè€…æ–¹æ³•çš„åˆç†æ€§]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>KGE</tag>
        <tag>TransD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[From Knowledge Graph Embedding to Ontology Embedding An Analysis of the Compatibility between Vector Space Representations and Rulesé˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FFrom_Knowledge_Graph_Embedding_to_Ontology_Embedding_An_Analysis_of_the_Compatibility_between_Vector_Space_Representations_and_Rules%2F</url>
    <content type="text"><![CDATA[è®ºæ–‡ä¸‹è½½åœ°å€]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Knowledge graph embedding with conceptsé˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FKnowledge_graph_embedding_with_concepts%2F</url>
    <content type="text"><![CDATA[è¿™ç¯‡è®ºæ–‡ï¼Œè¿ç”¨skip-gramæ–¹æ³•ï¼Œå°†å®ä½“å¯¹åº”ç›¸å…³æ¦‚å¿µå¼•å…¥å®ä½“å‘é‡è¡¨ç¤ºï¼Œä»¥å¢å¼ºè¡¨ç¤ºæ•ˆæœã€‚å®ä½“å’Œæ¦‚å¿µåœ¨åŒä¸€ç©ºé—´ä¸­ï¼Œä½†æ˜¯æ¦‚å¿µæ˜¯ç©ºé—´ä¸­çš„ä¸€ä¸ªè¶…å¹³é¢ï¼ˆç±»ä¼¼äºtransHï¼‰ã€‚æ–‡ä¸­ä¸¾ä¾‹å¾ˆå¤šä¾‹å­æ¥è¾…åŠ©è¯´æ˜ï¼Œä½¿å¾—æ–‡ç« å¯è¯»æ€§å¤§å¹…æå‡ã€‚æ–‡ä¸­å®éªŒæœ€åä¿©ä¸ªæ¯”è¾ƒæœ‰æ„æ€ã€‚æœ¬æ–‡å€¼å¾—æ€è€ƒå€Ÿé‰´çš„ä¸œè¥¿ä¸å°‘ï¼Œå€¼å¾—å†å¥½å¥½å›é¡¾ã€‚ è®ºæ–‡ä¸‹è½½åœ°å€ problem statement å·²ç»å­˜åœ¨çš„KGEæ¨¡å‹ä¸»è¦é›†ä¸­äºå®ä½“-å…³ç³»-å®ä½“ä¸‰å…ƒç»„æˆ–è€…æ–‡æœ¬è¯­æ–™äº¤äº’ã€‚ ä¸‰å…ƒç»„æ˜¯ç¼ºå°‘ä¿¡æ¯çš„ï¼Œå¹¶ä¸”åŸŸå†…æ–‡æœ¬ä¸æ€»æ˜¯å¯ä»¥è·å¾—çš„â€”â€”å¯¼è‡´åµŒå…¥ç»“æœåç¦»å®é™… å¸¸è¯†æ¦‚å¿µçŸ¥è¯†å‘æŒ¥å¾ˆé‡è¦çš„ä½œç”¨ã€‚ background For example, for two triplets (Apple, Developer, IPhone) and (Apple, Developer, Samsung Mobile), it is quite difficult to distinguish which is the true triplet that contains fact triplets only, because â€˜â€˜IPhoneâ€™â€™ and â€˜â€˜Samsung Mobileâ€™â€™ both belong to mobile phones. However, in the concept graph, â€˜â€˜IPhoneâ€™â€™ has a concept â€˜â€˜apple deviceâ€™â€™, but â€˜â€˜Samsung Mobileâ€™â€™ does not. Thus, it is easy to infer the correct triplet by mapping â€˜â€˜IPhoneâ€™â€™ to the â€˜â€˜apple deviceâ€™â€™concept å¾ˆå¥½çš„ä¸€ä¸ªä¸¾ä¾‹å…³äºå¦‚ä½•è¿ç”¨conceptæ¥è¾…åŠ©å…³ç³»è¯†åˆ« Specifically, when a corpus about technology is provided, embedding methods with technical textual descriptions could easily infer the fact (Apple, Developer, IPhone), because the keywords â€˜â€˜hardware productsâ€™â€™ and â€˜â€˜iPhone smartphoneâ€™â€™ occur frequently in the textual description of â€˜â€˜Appleâ€™â€™. However, it is difficult to infer the fact (Apple, Taste, Sweet), which is irrelevant to textual descriptions of â€˜â€˜Appleâ€™â€™ about the specific topic of â€˜â€˜technology company è¿™é‡Œä½œè€…ä¸¾ä¾‹è¯´æ˜ï¼šä¸å…·æœ‰æ–‡æœ¬ä¿¡æ¯çš„åµŒå…¥æ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰æ¦‚å¿µä¿¡æ¯çš„åµŒå…¥æ–¹æ³•åœ¨å…¶ä»»åŠ¡ä¸­æ›´åŠ é€šç”¨ï¼Œå¹¶ä¸”å®ƒä¸ä¾èµ–äºè¯­æ–™åº“çš„ä¸»é¢˜ã€‚ ä½œè€…æŠŠKGEåˆ†æˆäº†ä¸‰ç±»ï¼Œå¦‚ä¸‹ï¼š Embedding with symbolic tripletsï¼štransç³»åˆ—éƒ½æ”¾åˆ°äº†è¿™éƒ¨åˆ†ä¸­ Embedding with textual information Embedding with category information Methodologyconcept graph embeddingä½œè€…é‡‡ç”¨skip-gramæ¥å­¦ä¹ å¯ä»¥æ•è·å…¶è¯­ä¹‰ç›¸å…³æ€§çš„æ¦‚å¿µå’Œå®ä½“çš„è¡¨ç¤ºã€‚ å…¶ä¸­ï¼Œæ¯ä¸ªå®ä½“å¯¹åº”å¤šä¸ªæ¦‚å¿µï¼Œæ¯ä¸ªæ¦‚å¿µåˆåŒ…å«å¤šä¸ªå®ä½“ï¼ˆè¿™äº›å®ä½“ä½œä¸ºå®ä½“çš„ä¸Šä¸‹æ–‡ï¼‰ã€‚ åˆ™ï¼Œskip-gramå‡½æ•°å¯ä»¥å†™ä¸ºï¼š \begin{array}{l}{P\left(e_{c} | e_{t}\right)=\frac{\exp \left(e_{c} \cdot e_{t}\right)}{\sum_{e \in E} \exp \left(e \cdot e_{t}\right)}} \\ {P\left(e_{c} | c_{i}\right)=\frac{\exp \left(e_{c} \cdot c_{i}\right)}{\sum_{e \in E} \exp \left(e \cdot c_{i}\right)}}\end{array}æ•…æŸå¤±å‡½æ•°ä¸ºï¼š L=\frac{1}{|D|} \sum_{\left(e_{c}, e_{t}\right) \in D}\left[\log P\left(e_{c} | e_{t}\right)+\sum_{c_{i} \in C\left(e_{t}\right)} \log P\left(e_{c} | c_{i}\right)\right]å­¦ä¹ ç‡è®¾ä¸ºï¼š Î± = starting_alphaÃ—(1âˆ’count_actual/(real)(iter Ã— total_size+1)) è¿™é‡Œä½œè€…è¯´ä¸ºäº†é¿å…è¿‡æ‹Ÿåˆï¼Œå¯¹ä¼˜åŒ–ç›®æ ‡é‡‡ç”¨â€œè´ŸæŠ½æ ·â€æ–¹æ³•ã€‚&quot;è´ŸæŠ½æ ·&quot;æ–¹æ³•è¿˜å¯ä»¥é¿å…è¿‡æ‹Ÿåˆï¼Ÿ knowledge graph embeddingå°†ç‰¹å®šä¸‰å…ƒç»„åµŒå…¥åˆ°æ¦‚å¿µå­ç©ºé—´ä¸­ï¼Œé¦–å…ˆæ„å»ºä¸€ä¸ªè¶…å¹³é¢ï¼Œå…¶ä¸­æ³•å‘é‡$c$ä¸ºæ¦‚å¿µå­ç©ºé—´ï¼š c=C\left(e_{h}, e_{t}\right)=\frac{e_{h}-e_{t}}{\left\|e_{h}-e_{t}\right\|_{2}^{2}}æ ¹æ®TransEä¸‰å…ƒç»„çš„åµŒå…¥æŸå¤±ä¸ºï¼š l=h+r-tæ‰€ä»¥ï¼Œå¯ä»¥è®¡ç®—å‡ºæ³•å‘é‡æ–¹å‘ä¸Šçš„æŸå¤±åˆ†é‡æ˜¯ï¼š \left(c^{T} l c\right)ç„¶åï¼ŒæŠ•å½±åˆ°è¶…å¹³é¢ä¸Šçš„å¦ä¸€ä¸ªæ­£äº¤åˆ†é‡æ˜¯ï¼š \left(l-c^{T} l c\right) å®šä¹‰æ€»æŸå¤±å‡½æ•°ï¼š f_{r}(h, t)=-\lambda\left\|l-c^{T} l c\right\|_{2}^{2}+\|l\|_{2}^{2}Model interpretation å¯ä»¥é€šè¿‡æ¦‚å¿µæ¥è¾…åŠ©ä¸‰å…ƒç»„è¯†åˆ«ï¼Œæ–‡ä¸­ä»¥(Christopher Plummer, /people/person/nationality, Canada)ä¸¾ä¾‹ å¯ä»¥è§£å†³åœ¨å½“ä¸¤ä¸ªå€™é€‰å®ä½“åœ¨KGEï¼Œä¸­è®¡ç®—lossç›¸ç­‰æ—¶è¾¨åˆ«è¿™ä¸¤ä¸ªå“ªä¸ªæ˜¯çœŸå®çš„ã€‚æ–‡ä¸­ä»¥â€œwhich the director made the film â€˜â€˜WALL-Eâ€™â€™â€ä¸ºä¾‹æ¥è¿›è¡Œè¯´æ˜ éƒ½æ˜¯é€šè¿‡æŸ¥è¯¢å®ä½“å¯¹åº”æ¦‚å¿µæ¥è¿›è¡Œè¾…åŠ© Objectives and trainingmargin-based loss functionï¼š L=\sum_{(h, r, t) \in S} \sum_{\left(h^{\prime}, r, t^{\prime}\right) \in S_{(h, r, t)}^{\prime}}\left[\gamma+f_{r}(h, t)-f_{r^{\prime}}\left(h^{\prime}, t^{\prime}\right)\right]_{+}train å…ˆé¢„è®­ç»ƒæ¦‚å¿µå›¾æ¨¡å‹åµŒå…¥ï¼Œè·å¾—åœ¨æ¦‚å¿µç©ºé—´ä¸­çš„å®ä½“å‘é‡ åˆ©ç”¨1ä¸­è·å¾—çš„å®ä½“å‘é‡è¿›è¡Œæ›´æ–°ã€‚ datasets WN18 and FB15K Microsoft Concept Graph å…¶ä¸­ï¼Œrelationsè¡¨ç¤ºé¢‘ç‡ çœŸçš„æœ‰ç»Ÿè®¡é¢‘ç‡çš„è¿™ç§ ExperimentsKnowledge graph completionEntity classificationConcept relevance analysis è¿™ä¸ªå®éªŒæ¯”è¾ƒæœ‰æ„æ€ï¼šæ¯ä¸ªå•å…ƒæ ¼ä¸­çš„æ•°å­—è¡¨ç¤ºåœ¨TransEä¸­æ’åå¤§äºmä¸”åœ¨æˆ‘ä»¬çš„æ¨¡å‹ä¸­å°äºnçš„ä¸‰å…ƒç»„çš„æ•°é‡ã€‚ Precise semantic expression analysisæˆ‘ä»¬åœ¨é“¾æ¥é¢„æµ‹ï¼ˆæ¢å¥è¯è¯´ï¼Œè¿™äº›æ˜¯TransEçš„éš¾ä»¥è¯æ˜çš„ä¾‹å­ï¼‰ä¸­æ”¶é›†é‚£äº›å¾—åˆ†ç•¥é«˜äºçœŸå®ä¸‰å…ƒç»„ä½œä¸ºè´Ÿä¸‰å…ƒç»„ ç„¶ååœ¨KECä¸­å¯¹æ¯”ä¸¤è€…çš„åˆ†æ•°å·®å€¼ã€‚ å³è¾¹æ¡è¡¨ç¤ºKECåœ¨TransEå¤±è´¥æ—¶ä½œå‡ºæ­£ç¡®å†³å®šï¼Œå·¦è¾¹æ¡è¡¨ç¤ºKECå’ŒTransEéƒ½å¤±è´¥ã€‚]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
        <category>KG</category>
      </categories>
      <tags>
        <tag>KGE</tag>
        <tag>ontology</tag>
        <tag>concept</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Universal Representation Learning of Knowledge Bases by Jointly Embedding Instances and Ontological Conceptsé˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FUniversal_Representation_Learning_of_Knowledge_Bases_by_Jointly_Embedding_Instances_and_Ontological_Concepts%2F</url>
    <content type="text"><![CDATA[ è®ºæ–‡ä¸‹è½½åœ°å€ Problem StatementExisting KG embedding models merely focus on representing of an ontology view for abstract and commonsense concepts or an instance view for special entities that are instantiated from ontological concepts. Challenge mappings difficult :the semantic mappings from entities to concepts and from relations to meta-relations are complicated and difficult to be precisely captured by any current embedding models inadequate cross-view links: the known cross-view links inadequately cover a vast number of entities, which leads to insufficient information to align both views of the KB, and curtails discovering new cross-view links the scales and topological structures are different in ontological views and instance views Introduction ä»ä¸¤ç§è§†å›¾æ¥å­¦ä¹ è¡¨ç¤ºæœ‰ä»¥ä¸‹ä¸¤ç‚¹å¥½å¤„ï¼š instance embeddings provide detailed and rich information for their corresponding ontological concepts. a concept embedding provides a high-level summary of its instances, which is extremely helpful when an instance is rarely observed. contribution a novel KG embedding model named JOIE, which jointly encodes both the ontology and instance views of a KB cross-view association model : a novel KG embedding model named JOIE, which jointly encodes both the ontology and instance views of a KB cross-view grouping technique : assumes that the two views can be forced into the same embedding space cross-view transformation technique : enables non-linear transformations from the instance embedding space to the ontology embedding space intra-view embedding model : characterizes the relational facts of ontology and instance views in two separate embedding spaces three state-of-the-art translational or similarity-based relational embedding techniques hierarchy-aware embedding: based on intra-view non- linear transformations to preserve ontologies hierarchical substructures. implement two experiments: the triple completion task : confirm the effectiveness of JOIE for populating knowledge in both ontology and instance-view KGs, which has significantly outperformed various baseline models. the entity typing task : show that JOIE is competent in discovering cross-view links to align the ontology-view and the instance-view KGs. Modeling Cross-view Association Model Cross-view Grouping (CG)è¯¥æ¨¡å‹å¯ä»¥è§†ä¸ºgrouping-based regularizationï¼Œ å‡è®¾æœ¬ä½“è§†å›¾KGå’Œå®ä¾‹è§†å›¾KGå¯ä»¥è¢«åµŒå…¥åˆ°åŒä¸€ç©ºé—´ä¸­ï¼Œå¹¶å¼ºåˆ¶ä½¿å®ä¾‹å‘é‡é è¿‘ä¸å®ƒç›¸å…³è”çš„æ¦‚å¿µå‘é‡ï¼Œå¦‚å›¾3(a)æ‰€ç¤º å®šä¹‰æŸå¤±å‡½æ•°å¦‚ä¸‹ï¼š J_{\text { Cross }}^{\mathrm{CG}}=\frac{1}{|\mathcal{S}|} \sum_{(e, c) \in \mathcal{S}}\left[\|\mathbf{c}-\mathbf{e}\|_{2}-\gamma^{\mathrm{CG}}\right]_{+}Cross-view Transformation (CT)è¯•å›¾åœ¨å®ä½“åµŒå…¥ç©ºé—´å’Œæ¦‚å¿µç©ºé—´ä¹‹é—´è½¬æ¢ä¿¡æ¯ï¼Œå¦‚å›¾3(b)æ‰€ç¤º å®šä¹‰æ˜ å°„å‡½æ•°ï¼Œå°†å®ä¾‹æ˜ å°„åˆ°æœ¬ä½“è§†å›¾ç©ºé—´ï¼Œè¯¥æ˜ å°„åå‘é‡åº”è¯¥é è¿‘å®ƒçš„ç›¸å…³è”æ¦‚å¿µï¼š \mathbf{c} \leftarrow f_{\mathrm{CT}}(\mathbf{e}), \forall(e, c) \in \mathcal{S}å…¶ä¸­ï¼Œ f_{\mathrm{CT}}(\mathbf{e})=\sigma\left(\mathbf{W}_{\mathrm{ct}} \cdot \mathbf{e}+\mathbf{b}_{\mathrm{ct}}\right)æ•´ä¸ªCTçš„æŸå¤±å‡½æ•°ä¸ºï¼š J_{\text { Cross }}^{\mathrm{CT}}=\frac{1}{|\mathcal{S}|} \sum_{(e, c) \in \mathcal{S} \atop \wedge\left(e, c^{\prime}\right) \in \mathcal{S}}\left[\gamma^{\mathrm{CT}}+\left\|\mathbf{c}-f_{\mathrm{CT}}(\mathbf{e})\right\|_{2}-\left\|\mathbf{c}^{\prime}-f_{\mathrm{CT}}(\mathbf{e})\right\|_{2}\right]_{+}Intra-view Modelè¯¥æ¨¡å‹çš„ç›®çš„ï¼šåœ¨ä¸¤ä¸ªåµŒå…¥ç©ºé—´ä¸­åˆ†åˆ«ä¿ç•™KBçš„æ¯ä¸ªè§†å›¾ä¸­çš„åŸå§‹ç»“æ„ä¿¡æ¯ã€‚ Default Intra-view Modelä½œè€…é‡‡ç”¨ä¸‰ç§æ–¹å¼ï¼š \begin{aligned} f_{\text { TransE }}(\mathbf{h}, \mathbf{r}, \mathbf{t}) &=-\|\mathbf{h}+\mathbf{r}-\mathbf{t}\|_{2} \\ f_{\text { Mult }}(\mathbf{h}, \mathbf{r}, \mathbf{t}) &=(\mathbf{h} \circ \mathbf{t}) \cdot \mathbf{r} \\ f_{\text { HolE }}(\mathbf{h}, \mathbf{r}, \mathbf{t}) &=(\mathbf{h} \star \mathbf{t}) \cdot \mathbf{r} \end{aligned}æŸå¤±å‡½æ•°ï¼š J_{\text { Intra }}^{G}=\frac{1}{|\mathcal{G}|} \sum_{(h, r, t) \in \mathcal{G}}\left[\gamma^{\mathcal{G}}+f\left(\mathbf{h}^{\prime}, \mathbf{r}, \mathbf{t}^{\prime}\right)-f(\mathbf{h}, \mathbf{r}, \mathbf{t})\right]_{+}intraæŸå¤±å‡½æ•°ï¼š J_{\text { Intra }}=J_{\text { Intra }}^{\mathcal{G}_{I}}+\alpha_{1} \cdot J_{\text { Intra }}^{\mathcal{G}_{O}}Hierarchy-Aware Intra-view Model for the Ontologyè¿›ä¸€æ­¥åŒºåˆ†äº†æ„æˆæœ¬ä½“å±‚æ¬¡ç»“æ„çš„å…ƒå…³ç³»å’Œè§†å›¾å†…æ¨¡å‹ä¸­çš„è§„åˆ™è¯­ä¹‰å…³ç³»(å¦‚â€œrelated_toâ€)ã€‚ ç»™å®šæ¦‚å¿µå¯¹ï¼ˆclï¼Œchï¼‰ï¼Œæˆ‘ä»¬å°†è¿™ç§å±‚æ¬¡ç»“æ„å»ºæ¨¡ä¸ºç²—ç•¥æ¦‚å¿µå’Œç›¸å…³æ›´ç²¾ç»†æ¦‚å¿µä¹‹é—´çš„éçº¿æ€§å˜æ¢: g_{\mathrm{HA}}\left(\mathbf{c}_{h}\right)=\sigma\left(\mathbf{W}_{\mathrm{HA}} \cdot \mathbf{c}_{l}+\mathbf{b}_{\mathrm{HA}}\right)æŸå¤±å‡½æ•°ä¸ºï¼š J_{\text { Intra }}^{\mathrm{HA}}=\frac{1}{|\mathcal{T}|} \sum_{\left(c_{l}, c_{h}\right) \in \mathcal{T}}\left[\gamma^{\mathrm{HA}}+\left\|\mathbf{c}_{h}-g\left(\mathbf{c}_{l}\right)\right\|_{2}-\left\|\mathbf{c}_{\mathrm{h}}^{\prime}-g\left(\mathbf{c}_{1}\right)\right\|_{2}\right]_{+}æ•…ï¼Œè¯¥éƒ¨åˆ†æŸå¤±å‡½æ•°ä¸ºï¼š J_{\text { Intra }}=J_{\text { Intra }}^{G_{I}}+\alpha_{1} \cdot J_{\text { Intra }}^{\mathcal{G} o \backslash \mathcal{T}}+\alpha_{2} \cdot J_{\text { Intra }}^{\mathrm{HA}} $J_{\text { Intra }}^{\mathcal{G} o} \backslash \mathcal{T}$: é»˜è®¤çš„è§†å›¾å†…æ¨¡å‹çš„ä¸¢å¤±ï¼Œè¯¥æ¨¡å‹ä»…åœ¨å…·æœ‰è§„åˆ™è¯­ä¹‰å…³ç³»çš„ä¸‰å…ƒç»„ä¸Šè®­ç»ƒ $J_{\text { Intra }}^{\mathrm{HA}}$æ˜ç¡®è®­ç»ƒä¸‰å…ƒç»„ä¸å½¢æˆæœ¬ä½“å±‚æ¬¡ç»“æ„çš„å…ƒå…³ç³» æ„Ÿè§‰è¿™éƒ¨åˆ†å°±æ˜¯ä¼ é€’å…³ç³»ï¼Œç±»ä¼¼æ¨ç†æ€§è´¨çš„ã€‚ æ²¡æ˜ç™½ä¸¤ç§ontologyå…³ç³»çš„åŒºåˆ†ç‚¹ Joint Training on Two-View KBsè”åˆæŸå¤±å‡½æ•°ï¼š J=J_{\text { Intra }}+\omega \cdot J_{\text { Cross }}ä½œè€…å¹¶ä¸ç›´æ¥æ›´æ–°$J$ï¼Œè€Œæ˜¯äº¤æ›¿æ›´æ–°$J_{\text { Intra }}^{\mathcal{G}_{I}}, J_{\text { Intra }}^{\mathcal{G} O} \text { and } J_{\text { Cross }}$. EXPERIMENTSå…·ä½“ç»†èŠ‚ç›´æ¥è§è®ºæ–‡ datasetæ•°æ®é›†æ˜¯ä½œè€…è‡ªå·±æ„å»ºçš„ï¼Œä¿¡æ¯å¦‚ä¸Šå›¾æ‰€ç¤ºã€‚ Case StudyOntology Populationä½œè€…æƒ³é¢„æµ‹åœ¨å…ƒå…³ç³»è¯è¡¨ä¸­å¹¶ä¸å­˜åœ¨çš„å…ƒå…³ç³»ï¼Œä¾‹å¦‚ï¼šé¢„æµ‹(â€œOffice Holderâ€, ?r, â€œCountryâ€) è¿™é‡Œï¼Œä½œè€…é‡‡å–çš„æ–¹å¼æ˜¯å°†æ¦‚å¿µé€šè¿‡ä¹‹å‰æåˆ°çš„å®ä½“ç©ºé—´åˆ°æ¦‚å¿µç©ºé—´çš„æ˜ å°„æ¥è¿›è¡Œåæ˜ å°„ã€‚ç„¶åæŒ‰ç…§$f_{\mathrm{CT}}^{\mathrm{inv}}\left(\mathbf{c}_{\text { country }}\right)-f_{\mathrm{CT}}^{\mathrm{inv}}\left(\mathbf{c}_{\text { office }}\right)$æ¥åœ¨å®ä½“åµŒå…¥ç©ºé—´è¿›è¡Œæœç´¢ä¸ä¹‹ç›¸è¿‘çš„å®ä½“é—´å…³ç³»ã€‚ Long-tail entity typing In KGs, the frequency of entities and relations often follow a long-tail distribution (Zipfâ€™s law) ä½œè€…æŠ½å–äº†ä½é¢‘æ¬¡å®ä½“è¿›è¡Œäº†è®­ç»ƒï¼Œå‘ç°JOIEæ¨¡å‹çš„æ•ˆæœè™½ç„¶æœ‰ä¸‹é™ï¼Œä½†å°šåœ¨å¯ä»¥æ¥å—çš„ç¨‹åº¦å†…ã€‚ FUTURE WORK Particularly, instead of optimizing structure loss with triples (first-order neighborhood) locally, we plan to adopt more complex embedding models which leverage information from higher order neighborhood, logic paths or even global knowledge graph structures. We also plan to explore the alignment on relations and meta-relations like entity-concept. exploring different triple encoding techniques Note that we are also aware of the fact that there are more comprehensive properties of relations and meta-relations in the two views such as logical rules of relations and entity types. Incorporating such properties into the learning process is left as future work. æ€è€ƒè¿™ç¯‡è®ºæ–‡å’Œä¹‹å‰è·Ÿå¼ è€å¸ˆå®šçš„æˆ‘çš„è®ºæ–‡çš„æ€è·¯åŸºæœ¬ä¸€è‡´ï¼Œé¢ï¼Œæœ‰ç‚¹æ„Ÿè§‰æœ‰ç‚¹å—æ‰“å‡»ã€‚è¿™ç¯‡æ–‡ç« ä¹Ÿæ˜¯è¯¥ä½œè€…åšå£«æ¯•ä¸šè®ºæ–‡ä¸­çš„ä¸€éƒ¨åˆ†ï¼Œæ‰€ä»¥åº”è¯¥æ˜¯è¿™ä¸ªä½œè€…æ—©å°±æœ‰è¿™ä¸ªæ€è·¯äº†ï¼Œæ‰€ä»¥ä¹Ÿæ²¡ä»€ä¹ˆå¥½çº ç»“çš„ã€‚è¿™ç¯‡æ–‡ç« ä¹Ÿæ˜¯èµ°çš„transçš„è·¯çº¿ï¼Œå’Œåˆ˜çš„è®ºæ–‡åˆä¸ä¸€æ ·çš„æ€è·¯ï¼Œä½†æ˜¯éƒ½æ˜¯æ¦‚å¿µæœ¬ä½“è¿™ç±»çš„ã€‚å…¶ä¸­æœ‰ä¸€ç‚¹ä¸ä¸€æ ·ï¼Œå°±æ˜¯is_aå…³ç³»å¯èƒ½ä¸¤ç¯‡è®ºæ–‡ç”¨çš„ä¸ä¸€æ ·ã€‚è¿™ç¯‡è®ºæ–‡ä¸­æåˆ°äº†æ•°æ®é›†å¼€æºï¼Œå¯æ˜¯githubçš„é“¾æ¥ä¸­å¹¶æ²¡æœ‰æ•°æ®é›†ã€‚è™½ç„¶è½®æ–‡ä¸­è¯´ä»–ç»“åˆäº†æ¦‚å¿µå’Œå®ä¾‹çš„è§†å›¾ï¼Œä½†æ˜¯å…¶å®åƒåˆ˜çš„è®ºæ–‡å°±å·²ç»æå‡ºç»“åˆäº†æ¦‚å¿µå’Œå®ä¾‹çš„è§’åº¦äº†ã€‚]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
        <category>KG</category>
      </categories>
      <tags>
        <tag>KGE</tag>
        <tag>ontology</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DocRED A Large-Scale Document-Level Relation Extraction Dataseté˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FDocRED_A_Large-Scale_Document-Level_Relation_Extraction_Dataset%2F</url>
    <content type="text"><![CDATA[è¿™æ˜¯ä¸€ä¸ªä»‹ç»æ•°æ®é›†çš„è®ºæ–‡ï¼Œä¸»è¦æ˜¯æ–‡æ¡£çº§åˆ«çš„å…³ç³»æŠ½å–æ•°æ®é›†ã€‚ è®ºæ–‡ä¸‹è½½åœ°å€ Problem Statementexisting datasets for document-level RE either only have a small number of manually-annotated relations and entities, or exhibit noisy annotations from distant supervision, or serve specific domains or approaches. Contribution (DocRED) constructed from Wikipedia and Wikidata DocRED contains 132, 375 entities and 56, 354 relational facts annotated on 5, 053 Wikipedia documents As at least 40.7% of the relational facts in DocRED can only be extracted from multiple sentences also provide large-scale distantly supervised data to support weakly supervised RE research indicate the existing methods deal with the taks document level RE is more difficult sentence-level RE. data]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>RE</tag>
        <tag>dataset</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Learning Entity and Relation Embeddings for Knowledge Graph Completioné˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FLearning_Entity_and_Relation_Embeddings_for_Knowledge_Graph_Completion%2F</url>
    <content type="text"><![CDATA[TransR embeds entities and relations in distinct entity space and relation space, and learns embeddings via translation between projected entities.CTransR models internal complicated correlations within each relation type. è®ºæ–‡ä¸‹è½½åœ°å€ Problem StatementIn fact, an entity may have multiple aspects and various relaitons may focus on different aspects of entities, which makes a common space insurficient for modeling. Contribution propose a TransR model which models entities and relations in distinct spaces CTransR models internal complicated correlations within each relation type. experiment on benchmark datasets of WordNet and Freebase and gain consistent improvements compared to state-of-the-art models Future work Existing models including TransR consider each relational fact separately. relation transitive explore a unified embedding model of both text side and knowledge graph modeling internal correlations within each relation type TransR for each triple$(h, r, t)$, entities embeddings are set as $\mathbf{h}, \mathbf{t} \in \mathbb{R}^{k}$ and relation embedding is set as $\mathbf{r} \in \mathbb{R}^{d}$, $k \neq d$ for each relation $r$, set a projection matrix $\mathbf{M}_{r} \in\mathbb{R}^{k \times d}$ projects entities from entity space to relation space projected vectors of entities as \mathbf{h}_{r}=\mathbf{h} \mathbf{M}_{r}, \quad \mathbf{t}_{r}=\mathbf{t} \mathbf{M}_{r} score function: f_{r}(h, t)=\left\|\mathbf{h}_{r}+\mathbf{r}-\mathbf{t}_{r}\right\|_{2}^{2} Cluster-based TransR (CTransR)why propose CTransRTransE, TransH and TransR, learn a unique vector for each relation, which may be under-representative to fit all entity pairs under this relation, because these relations are usually rather diverse. basic idea incorporate the idea of piecewise linear regression Ritzema and others 1994 segment input instances into several groups process for a specific relation r, all entity pairs (h, t) in the training data are clustered into multiple groups, and entity pairs in each group are expected to exhibit similar r relation. All entity pairs (h, t) are represented with their vector offsets (h âˆ’ t) for clustering, where h and t are obtained with TransE. learn a separate relation vector $r_c$for each cluster and matrix $M_r$ for each relation, respectively projected vectors of entities as $\mathbf{h}_{r, c}=\mathbf{h} \mathbf{M}_{r} \text { and } \mathbf{t}_{r, c}=\mathbf{t} \mathbf{M}_{r}$ sorce fuction f_{r}(h, t)=\left\|\mathbf{h}_{r, c}+\mathbf{r}_{c}-\mathbf{t}_{r, c}\right\|_{2}^{2}+\alpha\left\|\mathbf{r}_{c}-\mathbf{r}\right\|_{2}^{2}the later item aims to ensure cluster-specific relation vector rcnot too far away from the original relation vector r dataseté‡‡ç”¨å’Œå‰äººæ‰€ç”¨ä¸€æ ·çš„æ•°æ®é›† Dataset #Rel #Ent #Train #Valid # Test WN18 18 40,943 141,442 5,000 5,000 FB15K 1,345 14,951 483,142 50,000 59,071 WN11 11 38,696 112,581 2,609 10,544 FB13 13 75,043 316,232 5,908 23,733 FB40K 1,336 39528 370,648 67,946 96,678 Experimentä½œè€…é‡‡å–å¸¸è§„å®éªŒ Link Predictionè¿™é‡Œä½œè€…å¯¹å…³ç³»ä¸­èšç±»è¿›è¡Œäº†å±•ç¤ºï¼š æˆ‘è§‰å¾—è¿™ç§æ–¹å¼æ˜¯å€¼å¾—å°è¯•çš„ã€‚ Triple classificationMoreover, the â€œbernâ€ sampling technique improves the performance of TransE, TransH and TransR on all three data sets. berné‡‡æ ·æ–¹æ³•éœ€è¦æŒæ¡ã€‚ Relation Extraction from Text]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>KGE</tag>
        <tag>KGR</tag>
        <tag>TransR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neural Relation Extraction with Selective Attention over Instancesé˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FNeural_Relation_Extraction_with_Selective_Attention_over_Instances%2F</url>
    <content type="text"><![CDATA[è¿™ç¯‡æ–‡ç« ä¹‹å‰çœ‹è¿‡ğŸ˜‚ã€‚ è®ºä¸‹è½½åœ°å€ Problem Statementâ€‹ Distant supervision inevitably accompanies with the wrong labelling problem, and thse noisy data will substantially hurt the performance of relation extraction. Contribution As compared to existing neural relation extraction model, our model can make full use of all informative sentences of each entity pair. To address the wrong labelling problem in distant supervision, we propose selective attention to de-emphasize those noisy instances. In the experiments, we show that selective attention is beneficial to two kinds of CNN models in the task of relation extraction. Methodologyæ¨¡å‹æ•´ä½“æ¶æ„å¦‚ä¸‹æ‰€ç¤ºï¼š]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>RE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Learning as the Unsupervised Alignment of Conceptual Systemsé˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FLearning_as_the_Unsupervised_Alignment_of_Conceptual_Systems%2F</url>
    <content type="text"><![CDATA[è¿™ç¯‡æ–‡ç« æ²¡æ€ä¹ˆçœ‹æ‡‚ï¼Œä¸»è¦æ€æƒ³åº”è¯¥æ˜¯ä»£è¡¨åŒæ—¶æ¦‚å¿µçš„ä¸åŒå½¢å¼ï¼ˆæ–‡æœ¬ï¼Œå›¾åƒï¼Œè¯­éŸ³ç­‰ï¼‰åº”è¯¥å…·æœ‰ç›¸ä¼¼çš„åˆ†å¸ƒï¼Œä»¥æ­¤æ¥è¿›è¡Œæ— ç›‘ç£çš„æ¦‚å¿µå¯¹é½ã€‚è¿™ç§æ€è·¯æŒºä¸é”™çš„ï¼Œä¸è¿‡è¿˜æ²¡æœ‰æ·±å…¥çš„æƒ³æ³•ï¼Œç®—æ˜¯æ‹“å±•è§†é‡å§ï¼ KEYThe key insight is that each concept has a unique signature within one conceptual system (e.g., images) that is recapitulated in other systems (e.g., text or audio) Problem Statement For supervised approaches, as the number of concepts grows, so does the number of required training examples V. W. Quine argued, even supervised instruction contains a substantial amount of ambiguity (Quine, 1960).Quine suggested that meaning may derive from somethingâ€™s place within a conceptual system. ModelIn order to solve Quinneâ€™s problem, we align a system of word labels and a system of visual semantics that both refer to the same underlying reality and therefore have related structure that can be discovered by unsupervised means (Figure 1ï¼‰]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ERNIE Enhanced Language Representation with Informative Entitiesé˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FERNIE_Enhanced_Language_Representation_with_Informative_Entities%2F</url>
    <content type="text"><![CDATA[è¯¥ç¯‡è®ºæ–‡å€Ÿé‰´BERTï¼Œè¯•å›¾å°†å®ä½“ä¿¡æ¯ï¼ˆTransEï¼‰èå…¥token(singal word)ä¸­ï¼Œé€šè¿‡ç±»ä¼¼å®ä½“å¯¹é½çš„æ–¹æ³•å°†å®ä½“ä¸tokenå¯¹é½ï¼ˆå¹¶é‡‡å–maskæ–¹å¼è¿›è¡Œé¢„è®­ç»ƒï¼‰ï¼Œé€šè¿‡infromation fusion å°†tokenä¸å®ä½“èåˆæ˜ å°„å…¥ç›¸å…³è”çš„ä¸¤ä¸ªå‘é‡ç©ºé—´ã€‚ è®ºæ–‡ä¸‹è½½åœ°å€ Problem Statementthe existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. ChallengeFor incorporating external knowledge into language representation models Structured Knowledge Encoding regarding to the given text, how to effectively extract and encode its related informative facts in KGs for language representation models Heterogeneous Information Fusion how to design a special pre-training objective to fuse the lexical, syntactic, and knowledge information is another challenge. Methodology Model ArchitectureERNIE the underlying textual encoder (T-Encoder)è´Ÿè´£ä»æ–‡æœ¬ä¸­æ•è·åŸºæœ¬çš„è¯æ³•å’Œè¯­æ³•ä¿¡æ¯ \left\{\boldsymbol{w}_{1}, \ldots, \boldsymbol{w}_{n}\right\}=\mathrm{T}-\operatorname{Encoder}\left(\left\{w_{1}, \ldots, w_{n}\right\}\right)T-Encoder(Â·) is a multi-layer bidirectional Transformer encoder the upper knowledgeable encoder (K-Encoder) entity embeddings are pre-trained by TransEè´Ÿè´£å°†çŸ¥è¯†å›¾è°±é›†æˆåˆ°åº•å±‚çš„æ–‡æœ¬ä¿¡æ¯ä¸­ Knowledgeable Encoder the knowledgeable encoder K-Encoder consists of stacked aggregators designed for encoding both tokens and entities as well as fusing their heterogeneous features. In the i-th aggregator the input: token embeddings: $\left\{\boldsymbol{w}_{1}^{(i-1)}, \ldots, \boldsymbol{w}_{n}^{(i-1)}\right\}$ entity embeddings :$\left\{\boldsymbol{e}_{1}^{(i-1)}, \ldots, \boldsymbol{e}_{m}^{(i-1)}\right\}$ fed into two multi-head self-attentions(MH-ATTs) $\left\{\tilde{\boldsymbol{w}}_{1}^{(i)}, \ldots, \tilde{\boldsymbol{w}}_{n}^{(i)}\right\}=\mathrm{MH}-\operatorname{ATT}\left(\left\{\boldsymbol{w}_{1}^{(i-1)}, \ldots, \boldsymbol{w}_{n}^{(i-1)}\right\}\right)$ $\left\{\tilde{\boldsymbol{e}}_{1}^{(i)}, \ldots, \tilde{\boldsymbol{e}}_{m}^{(i)}\right\}=\mathrm{MH}-\operatorname{ATT}\left(\left\{\boldsymbol{e}_{1}^{(i-1)}, \ldots, \boldsymbol{e}_{m}^{(i-1)}\right\}\right)$ an information fusion layer \begin{aligned} \boldsymbol{h}_{j} &=\sigma\left(\tilde{\boldsymbol{W}}_{t}^{(i)} \tilde{\boldsymbol{w}}_{j}^{(i)}+\tilde{\boldsymbol{W}}_{e}^{(i)} \tilde{\boldsymbol{e}}_{k}^{(i)}+\tilde{\boldsymbol{b}}^{(i)}\right) \\ \boldsymbol{w}_{j}^{(i)} &=\sigma\left(\boldsymbol{W}_{t}^{(i)} \boldsymbol{h}_{j}+\boldsymbol{b}_{t}^{(i)}\right) \\ \boldsymbol{e}_{k}^{(i)} &=\sigma\left(\boldsymbol{W}_{e}^{(i)} \boldsymbol{h}_{j}+\boldsymbol{b}_{e}^{(i)}\right) \end{aligned} $h_j$ is the inner hidden state For the tokens without corresponding entities \begin{aligned} \boldsymbol{h}_{j} &=\sigma\left(\tilde{\boldsymbol{W}}_{t}^{(i)} \tilde{\boldsymbol{w}}_{j}^{(i)}+\tilde{\boldsymbol{b}}^{(i)}\right) \\ \boldsymbol{w}_{j}^{(i)} &=\sigma\left(\boldsymbol{W}_{t}^{(i)} \boldsymbol{h}_{j}+\boldsymbol{b}_{t}^{(i)}\right) \end{aligned}Pre-training for Injecting KnowledgeIn order to inject knowledge into language rep- resentation by informative entities. Randomly masks some token-entity alignments and then requires the system to predict all corresponding entities based on aligned tokens. denoising entity auto-encoder (dEA) define the aligned entity distribution for the token $w_i$ as follows: p\left(e_{j} | w_{i}\right)=\frac{\exp \left(\text { linear }\left(\boldsymbol{w}_{i}^{o}\right) \cdot \boldsymbol{e}_{j}\right)}{\sum_{k=1}^{m} \exp \left(\text { linear }\left(\boldsymbol{w}_{i}^{o}\right) \cdot \boldsymbol{e}_{k}\right)} linear(Â·) is a linear layer For dEA, perform the following operations: in 5% of the time, replace the entity with another random aims to train model to correct the errors that the token is aligned with a wrong entity; In 15% of the time, mask token-entity alignments aims to train model to correct the errors that entity alignment system doesnâ€™t extract all existing alignments; in the rest of the time, keep token-entity alignments unchanged aims to encourage our model to integrate the entity information into token representations for better language understanding. Fine-tuning for Specific Tasks We can take the final output embedding for the first token, which corresponds to the special [CLS] token, as the representation of the input sequence for specific tasks. For some knowledge-driven tasks, we design special fine-tuning procedure: relation classification design different tokens [HD] and [TL] for head entities and tail entities respectively a similar role like position embeddings in the conventional relation classification models (Zeng et al., 2015) entity typing the mention mark token [ENT] è¿™é‡Œçš„CLSä¸çŸ¥é“æœ‰ä»€ä¹ˆä½œç”¨ï¼Œæ‰€æœ‰çš„ä»»åŠ¡éƒ½æœ‰ï¼Œæ˜¯ä¸åŒçš„ä»»åŠ¡é‡CLSçš„embeddingæœ‰æ‰€ä¸åŒå—ï¼Ÿä¸ªäººç›®å‰è§‰å¾—æ˜¯è¿™æ ·çš„ã€‚ ä½œè€…è¿™é‡Œé‡‡ç”¨çš„mark tokençš„æ–¹æ³•ä»£æ›¿position embeddingï¼Œä¸çŸ¥é“ä¸¤ä¸ªå¯¹æ¯”é‚£ç§æ•ˆæœä¼šæ›´å¥½ä¸€äº›ã€‚ç›´è§‚è§‰å¾—éƒ½æ˜¯æ ‡è®°ä½ç½®ä¿¡æ¯ã€‚ ExperimentsPre-training Dataset we use English Wikipedia as our pre-training corpus and align text to Wiki-data 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities before pre-training ERINE, entity embeddings by TransE sample part of Wikidata which contains 5, 040, 986 entities and 24, 267, 796 fact triples Training Details We also fine-tune ERNIE on the distant supervised dataset, i.e., FIGER (Ling et al., 2015) we use TAGME (Ferragina and Scaiella, 2010) to extract the entity mentions in the sentences and link them to their corresponding entities in KGs Entity Typingdatasettwo well-established datasets FIGER (Ling et al., 2015) and Open Entity (Choi et al., 2018). The training set of FIGER is labeled with distant supervision, and its test set is annotated by human. Open Entity is a completely manually-annotated dataset. Comparble model NFGEC NFGEC is a hybrid model proposed by Shimaoka et al. (2016) UFET (Choi et al., 2018) The results on FIGER:However, BERT has lower accuracy than the best NFGEC model. As strict accuracy is the ratio of instances whose predictions are identical to human annotations, it illustrates some wrong labels from distant supervision are learned by BERT due to its powerful fitting ability. Relation Classificationdatasettwo well-established datasets FewRel (Han et al., 2018b) and TACRED (Zhang et al., 2017). FewRel As FewRel does not have any null instance where there isnâ€™t any relation between entities, we adopt macro averaged metrics to present the model performances. Since FewRel is built by checking whether the sentences contain facts in Wiki-data, we drop the related facts in KGs before pretraining for fair comparison TACRED In TACRED, there are nearly 80% null instances so that we follow the previous work (Zhang et al., 2017) to adopt micro averaged metrics to represent the model performances instead of the macro Comparble model CNN:(Zeng et al., 2015). PA-LSTM C-GCN :Zhang et al. (2018) adopt the graph convolution operations to model dependency trees for relation classification. GLUEThe General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is a collection of diverse natural language understanding tasks Ablation Studyexplore the effects of the informative entities and the knowledgeable pretraining task (dEA) for ERNIE using FewRel dataset å®éªŒéƒ¨åˆ†åšçš„å¾ˆä¸°å¯Œï¼Œæ—¢æœ‰ä¸¤ä¸ªä»»åŠ¡çš„å¯¹æ¯”å®éªŒï¼Œä¹Ÿæœ‰å¯¹è‡ªèº«æ¨¡å—çš„å¯¹æ¯”å®éªŒï¼Œå¹¶ä¸”è¿˜å¯¹æ¯”äº†bertæ¥æ£€æµ‹è‡ªå·±æ¨¡å‹æ˜¯å¦å¯¹GLUEä»»åŠ¡æ•ˆæœæœ‰é™ä½ã€‚ future research1) inject knowledge into feature-based pre-training models such as ELMo (Peters et al., 2018); (2) introduce diverse structured knowledge into language representation models such as ConceptNet (Speer and Havasi, 2012) which is different from world knowledge database Wikidata; (3) annotate more real-world corpora heuristically for larger pre-training data å‚è€ƒé“¾æ¥ https://blog.csdn.net/summerhmh/article/details/91042273]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
        <category>KG</category>
      </categories>
      <tags>
        <tag>KGR</tag>
        <tag>BERT</tag>
        <tag>KG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Incorporating Literals into Knowledge Graph Embeddingsé˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FIncorporating_Literals_into_Knowledge_Graph_Embeddings%2F</url>
    <content type="text"><![CDATA[è¯»å®Œäº†å‰ä¸¤ç« ï¼Œç®€å•çš„çœ‹äº†ä¸€ä¸‹ä½œè€…æå‡ºçš„æ¨¡å‹ï¼Œæ„Ÿè§‰å¹¶æ²¡æœ‰å¤ªå¤§ä»·å€¼ï¼Œå°±æ˜¯ç»™å®ä½“è¾“å…¥å¤šåŠ å…¥äº†ä¸€ä¸ªliteralçš„ä¿¡æ¯ï¼ˆåŠ å…¥æ–¹æ³•å¯ä»¥é‡‡ç”¨çº¿æ€§ã€éçº¿æ€§æˆ–è€…ç¥ç»ç½‘ç»œï¼‰ã€‚ è¯»è®ºæ–‡å‰éœ€è¦å…ˆç†Ÿæ‚‰DistMultã€ComlLExå’ŒConvEæ¨¡å‹ï¼Œæ­¤è®ºæ–‡æ–¹æ³•æ˜¯æ·»åŠ åœ¨è¿™äº›æ–¹æ³•ä¸Šçš„ã€‚]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
        <category>KG</category>
      </categories>
      <tags>
        <tag>KGE</tag>
        <tag>link prediction</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Learning Knowledge Embeddings by Combining Limit-based Scoring Lossé˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FLearning_Knowledge_Embeddings_by_Combining_Limit-based_Scoring_Loss%2F</url>
    <content type="text"><![CDATA[æ­¤ç¯‡æ–‡ç« æœ€ä¸ºé‡è¦çš„å°±æ˜¯ä½œè€…è®¾è®¡çš„ margin-based ranking loss çš„æ”¹è¿›ï¼Œå¯¹ä¸¤ä¸ªè¶…å‚æ•°$\lambda$å’Œ$\gamma$çš„å®éªŒï¼Œå¯¹äºå®éªŒç»“æœæœ‰å¾ˆå¤šå€¼å¾—åˆ†æä¸æ€è€ƒçš„åœ°æ–¹ã€‚ è®ºæ–‡ä¸‹è½½åœ°å€ Problem StatementThe margin-based ranking loss function cannot ensure the fact that the scoring of correct triplets must be low enough to fulfill the translation. research objectivereduce the scoring of correct triplets to fulfill the translation by mending the margin-based ranking loss function Contributions proposing a limit-based ranking loss item combined with margin-based ranking loss extending TransE and TransH to TransE-RS and TransH-RS ModelMargin-based Tanking Lossformula: L_{R}=\sum_{(h, r, t) \in \Delta} \sum_{\left(h^{\prime}, r, t^{\prime}\right) \in \Delta^{\prime}}\left[\gamma_{1}+f_{r}(h, t)-f_{r}\left(h^{\prime}, t^{\prime}\right)\right) ]_{+} The margin-based ranking loss function aims to make the score $f_{r}\left(h^{\prime}, t^{\prime}\right)$ of corrupted triplet higher by at least $\gamma_{1}$ than of positive triplet. cannot be proved $f_{r}(h, t)&lt;\varepsilon$ Limit-based Scoring Lossformula: L_{S}=\sum_{(h, r, t) \in \Delta}\left[f_{r}(h, t)-\gamma_{2}\right]_{+}Finally lossformula: L_{R S}=L_{R}+\lambda L_{S}, \quad(\lambda>0)detail is : \begin{array}{c}{L_{R S}=\sum_{(h, r, t) \in \Delta} \sum_{\left(h^{\prime}, r, t^{\prime}\right) \in \Delta^{\prime}}\left\{\left[\gamma_{1}+f_{r}(h, t)-f_{r}\left(h^{\prime}, t^{\prime}\right)\right]_{+}\right.} \\ {+\lambda\left[f_{r}(h, t)-\gamma_{2}\right]_{+} \}}\end{array}Experimentsdataset Link prediction æ€è€ƒ ä½œè€…åªæ˜¯å¯¹è¡¨æ ¼çš„æ•°æ®è¿›è¡Œäº†é™ˆè¿°ï¼Œæœ‰ä¸€äº›é—®é¢˜å¹¶æ²¡æœ‰è¿›è¡Œåˆ†æè§£é‡Š å¹¶æ²¡æœ‰åˆ†ææ¯”å¦‚è¯´ä¸ºä»€ä¹ˆæ”¹è¿›lossåçš„transEä¸ºä»€ä¹ˆä¼šæ¯”TransHï¼ˆRã€Dï¼‰æ•ˆæœè¦å¥½ï¼Ÿ ä¸ºä»€ä¹ˆåœ¨n-to-1ä¸­çš„è¡¨ç°æ•ˆæœæ²¡æœ‰è¾¾åˆ°æœ€å¥½ï¼ˆå…¶ä»–çš„éƒ½è¾¾åˆ°äº†æœ€å¥½ï¼‰ï¼Ÿ é€šè¿‡è¿™ç§æ”¹è¿›å¯ä»¥å‘ç°ï¼ŒtransHç›¸æ¯”äºTransEå¹¶æ²¡æœ‰æ˜¾è‘—æå‡ï¼ŒåŸå› æ˜¯ä»€ä¹ˆï¼Ÿ Triple Classification TransE-RS and TransH-RS have same parameter and operation complexities as TransE and TransH, which is lower than TransR and TransD. Our models randomly initial the entities, not use the learned embeddings by TransE as TransR and TransD. It means that our models have much better ability to overcome the problem of overfitting Distributions of Tripletsâ€™ Scoresaimanalyze the difference between $L_R$ Loss and our $L_RS$ Loss Parameters æ€è€ƒï¼š å¯¹äºæˆ‘è‡ªå·±æ­£åœ¨åšçš„å®éªŒï¼šæ˜¯ä¸æ˜¯æˆ‘è‡ªå·±ç”¨çš„é—´éš”å¤ªå°äº† result æ€è€ƒ è¿™éƒ¨åˆ†çš„å®éªŒå€¼å¾—å€Ÿé‰´ï¼Œå®ƒå¯ä»¥ç›¸å¯¹äºç›´è§‚çš„å¯ä»¥å±•ç¤ºå‡ºä¸ºä»€ä¹ˆæ•ˆæœä¼šå¥½ã€‚ æ¯”å¦‚å¯¹äºä¸Šè¿°ä¸ºä»€ä¹ˆæ”¹è¿›åçš„transEçš„æ•ˆæœä¼šæ›´å¥½ çœ‹åˆ°æœ€åçš„åˆ†æ•°åˆ†å¸ƒtransE-RSçš„åˆ†å¸ƒæ•ˆæœå’ŒTrans-Hçš„ååˆ†æ¥è¿‘ï¼Œ è€ŒtransEçš„æ¨¡å‹è¾ƒä¸ºç®€å•ï¼Œå¯èƒ½æœ€ç»ˆlossæœ€å°åŒ–ä¼šä½¿å¾—æ¨¡å‹å……åˆ†è¡¨è¾¾ï¼Œè€Œå…¶ä»–æ¨¡å‹å¼•å…¥äº†æ›´å¤šçš„å‡è®¾å¯èƒ½ä¼šå¸¦æ¥æ›´å¤šçš„å™ªå£° ä¹Ÿå¯èƒ½å½“losså¾ˆå°æ—¶ï¼Œå…¶ä»–çš„å‡è®¾æ¡ä»¶å‘æŒ¥ä½œç”¨çš„å¾ˆå°ï¼ˆè‡³å°‘ä»å®éªŒç»“æœæ¥çœ‹æ˜¯çš„ï¼Œä½†æ˜¯è¿˜æœ‰å¾…äºè¿›ä¸€æ­¥è®¾è®¡å®éªŒéªŒè¯ï¼‰ Discussion of ParametersDiscussion on Î³1 and Î³2. We find that Î³2 = 3Î³1 or Î³2 = 4Î³1 is better for link prediction, but for triplet classification there are not obvious characteristics on Î³1 and Î³2. a lower Î³2 is expected to ensure the golden condition $\mathbf{h}+\mathbf{r} \approx \mathbf{t}$ for positive triplets, but an entity needs to satisfy many golden coditions at the same time. æ€è€ƒ æ—¢ç„¶å¦‚ä½œè€…è¯´ï¼Œé‚£ä¹ˆç†è®ºä¸ŠtransHçš„æ•ˆæœåº”è¯¥å¾ˆå¥½æ‰å¯¹ï¼Œä½†æ˜¯ç»“æœå¹¶ä¸æ˜¯è¿™æ ·çš„ï¼Œè¿™åˆäº§ç”ŸçŸ›ç›¾ã€‚ Discussion on Î» æ€è€ƒ çœ‹åˆ°Î»å¹¶æ²¡æœ‰å¯¹æ¨¡å‹å½±å“å¹¶æ²¡å¾ˆå¤§ Î»åœ¨1å·¦å³æ˜¯æ•ˆæœä¼šæ¯”è¾ƒå¥½ Î»å’Œmarginä¼šä¸ä¼šäº§ç”Ÿå…³è”ï¼Ÿ]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
        <category>KG</category>
      </categories>
      <tags>
        <tag>transH</tag>
        <tag>KG</tag>
        <tag>margin loss</tag>
        <tag>transE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Knowledge Graph Embedding by Translating on Hyperplanesé˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FKnowledge%20Graph%20Embedding%20by%20Translating%20on%20Hyperplanes%2F</url>
    <content type="text"><![CDATA[ä½œä¸ºtransç³»åˆ—ç»å…¸æ–‡çŒ®ï¼Œå¿…è¯»ã€‚æ–‡ç« ä¸»è¦ç²¾ååœ¨äºè¿™ç§è¶…å¹³é¢æƒ³æ³•çš„ç”±æ¥è§£å†³äº†åŒä¸€å®ä½“çš„å¤šå…³ç³»é—®é¢˜ã€‚ Authors proposed TransH which models a relation as a hyperplane together with a translation operation on it. It solves the problem of multi-relation and makes a good trade-off between model capacity and efficiency. æ¨æµ‹transHçš„æƒ³æ³•æ¥æº æ—¢ç„¶å®é™…æ˜¯è¡¨è¾¾åŒä¸€å…³ç³»ä¸åŒå®ä½“æœ€åé€šè¿‡TransEåä¼šè¶‹äºä¸€è‡´ï¼Œé‚£ä¹ˆæˆ‘ç›´æ¥é€šè¿‡ä¸€ä¸ªä¸­ä»‹æ¥è¿›è¡Œæ˜ å°„å°†åŒä¸€è¡¨ç¤ºæ˜ å°„æˆä¸åŒå‘é‡è¡¨ç¤ºï¼Œé‚£ä¹ˆè¿™äº›å‘é‡è¡¨ç¤ºå°±å¯ä»¥ä»£è¡¨ä¸åŒçš„å®ä½“ï¼Œå°±è¾¾åˆ°äº†ä¸åŒå®ä½“æ‹¥æœ‰ä¸åŒè¡¨ç¤ºçš„ç›®çš„ã€‚å› ä¸ºå…³ç³»æ˜¯ä¸å˜çš„æ‰€ä»¥æƒ³åˆ°äº†å°†å…³ç³»ä½œä¸ºæ˜ å°„å¹³é¢ï¼Œè®©å®ä½“å‘é‡å‘å…¶ä¸­æ˜ å°„ã€‚ research objective solves the problem of multi-relation makes a good trade-off between model capacity and efficiency Problem Statement TransE canâ€™t deal with reflexive, one-to-many, many-to-many and many -to-one relations some complex model sacrifice efficiency in the process(although can deal with transEâ€™s problem) Contribution proposing a method named translation on hyperplanes(TransH) interpreting a relation as a translating operation on a hyperplane proposing a simple trick to reduce the chance of false negative labeling Embedding by Translating on HyperplanesRelationsâ€™ Mapping Properties in EmbeddingtransE the representation of an entity is the same when involved in any relations, ignoring distributed representations of entities when invovled in different relaions Translating on Hyperplanes (TransH)åŒä¸€ä¸ªå®ä½“åœ¨ä¸åŒå…³ç³»ä¸­çš„æ„ä¹‰ä¸åŒï¼ŒåŒæ—¶ä¸åŒå®ä½“ï¼Œåœ¨åŒä¸€å…³ç³»ä¸­çš„æ„ä¹‰ï¼Œä¹Ÿå¯ä»¥ç›¸åŒã€‚ å°†æ¯ä¸ªå…³ç³»å®šä¹‰åœ¨ä¸€ä¸ªç‹¬ç‰¹çš„å¹³é¢å‘¢ï¼Œåœ¨è¯¥å¹³é¢å†…æœ‰ç¬¦åˆè¯¥å…³ç³»çš„transEçš„è¡¨ç¤ºï¼ˆh,r,t)ï¼Œå¤šåŠ å…¥çš„ä»£è¡¨è¯¥å¹³é¢çš„æ³•å‘é‡å®Œæˆäº†å°†ä¸åŒå®ä½“å‘å¹³é¢å†…å’Œhï¼Œtè½¬åŒ–çš„ä»»åŠ¡ï¼Œä½¿å¾—åŒä¸€å…³ç³»çš„ä¸åŒå®ä½“æ‹¥æœ‰ä¸åŒçš„è¡¨ç¤ºï¼Œä½†æ˜¯åœ¨å…³ç³»å¹³é¢å†…çš„æŠ•å½±ç›¸åŒï¼›åŒä¸€å®ä½“å¯ä»¥åœ¨ä¸åŒçš„å…³ç³»å¹³é¢å†…æ‹¥æœ‰ä¸åŒçš„å«ä¹‰ï¼ˆå¹³é¢å†…çš„æŠ•å½±ï¼‰ å¦‚å›¾æ‰€ç¤ºï¼Œå¯¹äºæ­£ç¡®çš„ä¸‰å…ƒç»„æ¥è¯´$(h, r, t) \in \Delta$ï¼Œæ‰€éœ€æ»¡è¶³çš„å…³ç³»å¦‚å›¾æ‰€ç¤ºã€‚é‚£ä¹ˆå¯¹äºä¸€ä¸ªå®ä½“$hâ€™â€™$å¦‚æœæ»¡è¶³$\left(h^{\prime \prime}, r, t\right) \in \Delta $ï¼Œåœ¨transEä¸­æ˜¯éœ€è¦$hâ€™â€™=h$ï¼Œè€Œåœ¨transHä¸­åˆ™å°†çº¦æŸæ”¾å®½åˆ°$h,hâ€™â€™$åœ¨$W_r$ä¸Šçš„æŠ•å½±ç›¸åŒå°±å¯ä»¥äº†ï¼Œä¹Ÿå¯ä»¥å®ç°å°†$h,hâ€™â€™$åŒºåˆ†å¼€å¹¶ä¸”å…·æœ‰ä¸åŒçš„è¡¨ç¤ºã€‚ ç›®æ ‡å‡½æ•°scoring functionï¼š d(h+r, t)=f_{r}(h, t)=\left\|h_{\perp}+d_{r}-t_{\perp}\right\|_{2}^{2}As the hyperplane $W_r$, the $w_r$ is the normal vector of it, and $\left|w_{r}\right|_{2}^{2}=1$, so the projection $h$ in $w_r$ is: h_{w_{r}}=w_r^{T} h w_rå…¶ä¸­ï¼Œ$w_r^{T} h=|w_r||h| \cos \theta$å¯ä»¥è¡¨ç¤º$h$åœ¨$w_r$ä¸Šçš„æŠ•å½±çš„é•¿åº¦å’Œ$w_r$é•¿åº¦çš„ä¹˜ç§¯ï¼Œå› ä¸º$\left|w_{r}\right|_{2}^{2}=1$,æ‰€ä»¥å¯ä»¥ä»£è¡¨æŠ•å½±çš„é•¿åº¦ï¼Œå†ä¹˜ä¸Šå•ä½å‘é‡å³å¯è¡¨ç¤ºæŠ•å½±å‘é‡ã€‚æ‰€ä»¥ï¼š \mathbf{h}_{\perp}=\mathbf{h}-\mathbf{w}_{r}^{\top} \mathbf{h w}_{r}, \quad \mathbf{t}_{\perp}=\mathbf{t}-\mathbf{w}_{r}^{\top} \mathbf{t} \mathbf{w}_{r}å¦‚å›¾æ‰€ç¤ºï¼š the score function is: f_{r}(\mathbf{h}, \mathbf{t})=\left\|\left(\mathbf{h}-\mathbf{w}_{r}^{\top} \mathbf{h w}_{r}\right)+\mathbf{d}_{r}-\left(\mathbf{t}-\mathbf{w}_{r}^{\top} \mathbf{t} \mathbf{w}_{r}\right)\right\|_{2}^{2}Trainingloss function consists of margin-based ranking loss and some constraints: \begin{aligned} \mathcal{L} &=\sum_{(h, r, t) \in \Delta\left(h^{\prime}, r^{\prime}, t^{\prime}\right) \in \Delta_{(h, r, t)}}\left[f_{r}(\mathbf{h}, \mathbf{t})+\gamma-f_{r^{\prime}}\left(\mathbf{h}^{\prime}, \mathbf{t}^{\prime}\right)\right]_{+} \\ &+C\left\{\sum_{e \in E}\left[\|\mathbf{e}\|_{2}^{2}-1\right]_{+}+\sum_{r \in R}\left[\frac{\left(\mathbf{w}_{r}^{\top} \mathbf{d}_{r}\right)^{2}}{\left\|\mathbf{d}_{r}\right\|_{2}^{2}}-\epsilon^{2}\right]_{+}\right\}, \text { (4) } \end{aligned}the constraints: \forall e \in E,\|\mathrm{e}\|_{2} \leq 1, // \text { scale }\\ \forall r \in R,\left|\mathbf{w}_{r}^{\top} \mathbf{d}_{r}\right| /\left\|\mathbf{d}_{r}\right\|_{2} \leq \epsilon, / / \text { orthogonal }\\ \forall r \in R,\left\|\mathbf{w}_{r}\right\|_{2}=1, / / \text { unit normal vector } the second grantees the translation vectot $d_r$ is in the hyperplane they project each $w_r$ to unit $l_2$-ball before visiting each mini-batch æ—¢ç„¶transHå¯ä»¥å®Œæˆå°†åŒä¸€å®ä½“æ˜ å°„åˆ°ä¸åŒçš„å…³ç³»å¹³é¢æ¥è·å¾—ä¸åŒçš„å«ä¹‰ï¼Œé‚£ä¹ˆæˆ‘è§‰å¾— æ˜¯ä¸æ˜¯ä¸åŒä»£è¡¨åŒä¸€å«ä¹‰çš„æŠ•å½±è¡¨ç¤ºåº”è¯¥ç›¸åŒæˆ–è€…ç›¸ä¼¼ è¿™æ ·æ˜¯ä¸æ˜¯å¯ä»¥è§£å†³åŒä¸€ä¸ªå®ä½“çš„å¤šä¹‰æ€§é—®é¢˜ã€‚ Reducing Ralse Negative LabelsAuthors set different probabilities for replacing the head or tail entity depending on the mapping property of the relation (one-to-many, many-to-one, many-to-many) give more chance to replacing the head entity if the relation is one-to-many åˆ†åˆ«ç»Ÿè®¡æ¯ä¸ªå¤´å®ä½“å¯¹åº”å°¾å®ä½“çš„æ•°é‡ï¼ˆåä¹‹äº¦ç„¶ï¼‰ï¼ŒæŒ‰å æ¯”è¿›è¡Œç”Ÿæˆè´Ÿæ ·ä¾‹ é€šè¿‡è¿™æ ·çš„æ–¹å¼ï¼Œä¾‹å¦‚one-manyå…³ç³»ï¼Œæ›¿æ¢å¤´å®ä½“æ˜¾ç„¶æ›´ä¸å®¹æ˜“å¾—åˆ°æ­£æ ·ä¾‹ï¼ˆå› ä¸ºåªæœ‰ä¸€ç§å¤´å®ä½“æ˜¯å¯¹çš„ï¼Œç„¶è€Œæ›¿æ¢å°¾å®ä½“å› ä¸ºå¯¹äºå¤´å®ä½“å¯¹åº”è¯¥å…³ç³»çš„å°¾å®ä½“æ›´å¤šï¼Œè¯´ä¸å®šå°±æœ‰å…¶ä»–ä¸åœ¨æ­¤manyä¸­çš„å°¾å®ä½“ç¬¦åˆè¿™ä¸ªå…³ç³»ã€‚ ç›¸æ¯”ä¹‹ä¸‹æˆ‘è®¤ä¸ºåœ¨ã€ŠBootstrapping-Entity-Alignment-with-Knowledge-Graph-Embeddingã€‹é‡‡ç”¨çš„å‡åŒ€æˆªæ–­è´Ÿé‡‡æ ·æ•ˆæœä¼šæ›´å¥½ä¸€äº› Experimentsthe detail can be seen in the paper Link predictionoutperform TransE in one-to-oneAuthors explain: entities are connected with relations so that better embeddings of some parts lead to better results on the whole. æˆ‘æ˜¯è§‰å¾—æœ‰äº›ç‰µå¼ºï¼Œä¸è¿‡è¦æ˜¯ç¡¬ç†è§£ä¹Ÿæ˜¯å¯ä»¥ï¼Œæ¯•ç«Ÿé€šè¿‡æŠ•å½±ç›¸å½“äºæŠŠå®ä½“å’Œå…³ç³»è¿›è¡Œäº†ä¸€ä¸ªè”ç³»ï¼Œå¯èƒ½è¿™ä¸ªå¢å¼ºäº†æ•ˆæœã€‚ Triplets ClassificationThis means FB13 is a very dense subgraph where strong correlations exist between entities Relational Fact Extraction from Text Actually, knowledge graph embedding is able to score a candidate fact, without observing any evidence from ex- ternal text corpus å¯ä»¥çœ‹åˆ°ä»14å¹´å¼€å§‹å°±æœ‰åˆ©ç”¨çŸ¥è¯†å›¾è°±æ¥ä»æ–‡æœ¬æŠ½å–å…³ç³»ï¼Œæœ€è¿‘è¿™ä¸ªåº”ç”¨å¥½åƒåˆæœ‰èµ·è‰²ï¼Œè¿™ä¸ªä¹Ÿå¯ä½œä¸ºè‡ªå·±å®éªŒçš„ä¸€éƒ¨åˆ†ã€‚ Reference https://blog.csdn.net/MonkeyDSummer/article/details/85273843]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
        <category>KG</category>
      </categories>
      <tags>
        <tag>KGE</tag>
        <tag>transH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Attention Is All You Needé˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FAttention%20Is%20All%20You%20Need%2F</url>
    <content type="text"><![CDATA[transformer æ˜¯ä¸€ä¸ªå®Œå…¨ç”±æ³¨æ„åŠ›æœºåˆ¶ç»„æˆçš„æ­å»ºçš„æ¨¡å‹ï¼Œæ¨¡å‹å¤æ‚åº¦ä½ï¼Œå¹¶å¯ä»¥è¿›è¡Œå¹¶è¡Œè®¡ç®—ï¼Œä½¿å¾—è®¡ç®—é€Ÿåº¦å¿«ã€‚åœ¨ç¿»è¯‘æ¨¡å‹ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ•ˆæœã€‚æœ¬ç¯‡è®ºæ–‡å±äºç»å…¸å¿…è¯»è®ºæ–‡ï¼Œé˜…è¯»ç¬”è®°ä¸­å¯¹ä¸€äº›ä¸æ¸…æ¥šçš„åœ°æ–¹è¿›è¡Œäº†æ±‰è¯­è§£é‡Šï¼Œè¯»å®Œè®ºæ–‡åé˜…è¯»å‚è€ƒé“¾æ¥ä»¥åŠ æ·±ç†è§£ã€‚ è®ºæ–‡ä¸‹è½½åœ°å€ research objectivebased solely on attention mechanisms, increase parallezable computation and decrease train time Problem Statementrecurrent models hidden states depended on previous hidden state and the input for position precludes parallelization contribution Transformer, eschewing recurrence and instead relying entirely on an attention mechanism, solve the long dependency problem. draw global dependecies between input and output allow for significantly more parallelization Model ArchitectureThe Transformer uses stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. Encoder and Decoder StacksEncoder compose of a stack of N identical layers each layers has two sub-layers multi-head self-attention mechanism position-wise fully connected feed forward network employ a residual connection around each of the two sub-layers, followed by layer normalization the output of each sub-layer is $\text { LayerNorm }(x+\text { Sublayer }(x))$ encoderä¸­çš„Qï¼ŒKï¼ŒVéƒ½æ˜¯å­¦å‡ºæ¥çš„ Decoder composed of a stack of N identical layers has the same two sub-layers as the encoder the third sub-layer between the two sub-layers perform multi-head attention over the output of the encoder stack add a mask to modify the self-attention sub-layer to ensure that the predictions for position $i$ can depend only the known outputs at positions less than $i$ é™¤äº†ç¬¬ä¸€å­å±‚ä¸­Qï¼ŒKï¼ŒVæ˜¯è‡ªå·±å­¦å‡ºæ¥çš„ï¼Œç¬¬äºŒä¸ªå­å±‚åˆ©ç”¨äº†encoderä¸­çš„Kï¼ŒVã€‚ Attention Scaled Dot-Product Attentionthe calculation process as the left at the figure 2. formulaï¼š \text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V where $\sqrt{d_{k}}$ is to prevent value from getting too large, which will push the softmax function into regions where it has extremely small gradients. å› ä¸ºé‡çº§å¤ªå¤§ï¼Œsoftmaxåå°±é0å³1äº†ï¼Œä¸å¤Ÿâ€œsoftâ€äº†ã€‚ä¹Ÿä¼šå¯¼è‡´softmaxçš„æ¢¯åº¦éå¸¸å°ã€‚ä¹Ÿå°±æ˜¯è®©softmaxç»“æœä¸ç¨€ç–(é—®å·è„¸ï¼Œé€šå¸¸äººä»¬å¸Œæœ›å¾—åˆ°æ›´ç¨€ç–çš„attentionå§)ã€‚ $Q, K,V$ is a matrix needed to learn from input. Multi-Head Attentionhelps the encoder look at other words in the input sentence as it encodes a specific word in the figure 2 right. itâ€™s beneficial to lineraly project the quries, keys and values $h$ times with different, learned projections to $d_k, d_k, d_v$ dimensions, respectively concatenate the output \begin{aligned} \text { MultiHead }(Q, K, V) &=\text { Concat (head }_{1}, \ldots, \text { head }_{h} ) W^{O} \\ \text { where head }_{i} &=\text { Attention }\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right) \end{aligned}where $W_{i}^{Q} \in \mathbb{R}^{d_{\text { model }} \times d_{k}}, W_{i}^{K} \in \mathbb{R}^{d_{\text { model }} \times d_{k}}, W_{i}^{V} \in \mathbb{R}^{d_{\text { model }} \times d_{v}}, W^{O} \in \mathbb{R}^{h d_{v} \times d_{\mathrm{model}}}$ Applications of Attention in our Model the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. mimicing the seq-to-seq self -attention can make that each position in the encoder can attend to all positions in the previous layer of the encoder We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to âˆ’âˆ) all values in the input of the softmax which correspond to illegal connections. See Figure 2ã€‚å³æˆ‘ä»¬åªèƒ½attendåˆ°å‰é¢å·²ç»ç¿»è¯‘è¿‡çš„è¾“å‡ºçš„è¯è¯­ï¼Œå› ä¸ºç¿»è¯‘è¿‡ç¨‹æˆ‘ä»¬å½“å‰è¿˜å¹¶ä¸çŸ¥é“ä¸‹ä¸€ä¸ªè¾“å‡ºè¯è¯­ï¼Œè¿™æ˜¯æˆ‘ä»¬ä¹‹åæ‰ä¼šæ¨æµ‹åˆ°çš„ã€‚å³å°†$QK^T$ä¸­æ¯è¡Œè¯¥å•è¯ä¹‹åçš„æ•°å€¼åšå¤„ç†ï¼Œä½¿å¾—å‰é¢çš„å•è¯çœ‹ä¸åˆ°åé¢å•è¯æ‰€å çš„é‡è¦æ€§ç¨‹åº¦ã€‚ Position-wise Feed-Forward Networks applied to each position separately and identically feed-forward network consists of tow linear transformations with a ReLU activation. formula: \mathrm{FFN}(x)=\max \left(0, x W_{1}+b_{1}\right) W_{2}+b_{2} å°ç»“ ä¸ºä»€ä¹ˆå«å¼ºè°ƒposition-wise? è§£é‡Šä¸€: è¿™é‡ŒFFNå±‚æ˜¯æ¯ä¸ªpositionè¿›è¡Œç›¸åŒä¸”ç‹¬ç«‹çš„æ“ä½œï¼Œæ‰€ä»¥å«position-wiseã€‚å¯¹æ¯ä¸ªpositionç‹¬ç«‹åšFFNã€‚ è§£é‡ŠäºŒï¼šä»å·ç§¯çš„è§’åº¦è§£é‡Šï¼Œè¿™é‡Œçš„FFNç­‰ä»·äºkernel_size=1çš„å·ç§¯ï¼Œè¿™æ ·æ¯ä¸ªpositionéƒ½æ˜¯ç‹¬ç«‹è¿ç®—çš„ã€‚å¦‚æœkernel_size=2ï¼Œæˆ–è€…å…¶ä»–ï¼Œpositionä¹‹é—´å°±å…·æœ‰ä¾èµ–æ€§äº†ï¼Œè²Œä¼¼å°±ä¸èƒ½å«åšposition-wiseäº† ä¸ºä»€ä¹ˆè¦é‡‡ç”¨å…¨è¿æ¥å±‚ï¼Ÿ ç›®çš„: å¢åŠ éçº¿æ€§å˜æ¢ å¦‚æœä¸é‡‡ç”¨FFNå‘¢ï¼Ÿæœ‰ä»€ä¹ˆæ›¿ä»£çš„è®¾è®¡ï¼Ÿ ä¸ºä»€ä¹ˆé‡‡ç”¨2å±‚å…¨è¿æ¥ï¼Œè€Œä¸”ä¸­é—´å‡ç»´ï¼Ÿ è¿™ä¹Ÿæ˜¯æ‰€è°“çš„bottle neckï¼Œåªä¸è¿‡ä½ç»´åœ¨IOä¸Šï¼Œä¸­é—´é‡‡ç”¨high rank Embeddings and SoftmaxSharing the same weight maatrix between the two embedding layers and the pre-softmax linear transformation Positional EncodingUsing sine and xosine functions of different frequencies: P E_{(p o s, 2 i)}=\sin \left(p o s / 10000^{2 i / d_{\text { model }}}\right) \\ P E_{(p o s, 2 i+1)}=\cos \left(p o s / 10000^{2 i / d_{\mathrm{model}}}\right) where $pos$ is the postiiton and $i$ is the dimension Authors hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$, $PE_{pos+k}$can be represented as a linear function of $PE_{pos}$ ä½†åœ¨è¯­è¨€ä¸­ï¼Œç›¸å¯¹ä½ç½®ä¹Ÿå¾ˆé‡è¦ï¼ŒGoogleé€‰æ‹©å‰è¿°çš„ä½ç½®å‘é‡å…¬å¼çš„ä¸€ä¸ªé‡è¦åŸå› æ˜¯ï¼šç”±äºæˆ‘ä»¬æœ‰$\sin (\alpha+\beta)=\sin \alpha \cos \beta+\cos \alpha \sin \beta$ä»¥åŠ$\cos (\alpha+\beta)=\cos \alpha \cos \beta-\sin \alpha \sin \beta$ï¼Œè¿™è¡¨æ˜ä½ç½®$p+k$çš„å‘é‡å¯ä»¥è¡¨ç¤ºæˆä½ç½®$p$çš„å‘é‡çš„çº¿æ€§å˜æ¢ï¼Œè¿™æä¾›äº†è¡¨è¾¾ç›¸å¯¹ä½ç½®ä¿¡æ¯çš„å¯èƒ½æ€§ã€‚ Compared with using learned positional embeddings, the sinusoidal version may allow the model to extrapolate to sequence lengths longer than the ones encountered during training. æ³¨æ„ç”±äºè¯¥æ¨¡å‹æ²¡æœ‰recurrenceæˆ–convolutionæ“ä½œï¼Œæ‰€ä»¥æ²¡æœ‰æ˜ç¡®çš„å…³äºå•è¯åœ¨æºå¥å­ä¸­ä½ç½®çš„ç›¸å¯¹æˆ–ç»å¯¹çš„ä¿¡æ¯ï¼Œä¸ºäº†æ›´å¥½çš„è®©æ¨¡å‹å­¦ä¹ ä½ç½®ä¿¡æ¯ï¼Œæ‰€ä»¥æ·»åŠ äº†position encodingå¹¶å°†å…¶å åŠ åœ¨word embeddingä¸Šã€‚ Why Self-Attention total computational complexity per layer the amount of computation that can be parallelized the path between long-range dependencies in the network self-attention|ï¼š $QK^TV$ç›¸ä¹˜ï¼Œæ ¹æ®çŸ©é˜µå¤§å°ï¼ˆåˆ†åˆ«ä¸º$nd, nd, nd$éœ€è¦çš„å¤æ‚åº¦ä¸º$O(n^2d2)$ï¼ˆå¿½ç•¥softmaxï¼‰ maximum path lengthï¼šå›¾è¯´æ˜äº†ï¼Œ å¯¹äºself-attention, target node (ç”Ÿæˆçš„é‚£ä¸ªç‚¹) å®é™…ä¸Šå’Œ è¾“å…¥ä¸­çš„ä»»æ„ä¸€ç‚¹çš„è·ç¦»æ˜¯ç›¸åŒçš„ convolutional: æ¯å±‚æœ‰kä¸ªå·ç§¯å’Œï¼Œå¯¹äºinput matixï¼ˆ$nd$)çŸ©é˜µæ‰§è¡Œå·ç§¯éœ€è¦è¿ç®—å¤æ‚åº¦æ˜¯$nd*(d-m)$, mä¸ºå·ç§¯å’Œå®½åº¦æ˜¯ä¸€ä¸ªæ¯”è¾ƒå°çš„å¸¸æ•°ï¼Œæ‰€ä»¥æ€»å¤æ‚åº¦ä¸º$O\left(k \cdot n \cdot d^{2}\right)$,ä½œè€…æåˆ°å¯åˆ†ç¦»çš„å·åŸºå±‚æš‚æ—¶è¿˜ä¸äº†è§£ï¼Œå¯ä»¥ä»¥åæŸ¥é˜…ã€‚ maximum path length: æ­£å¸¸å·ç§¯å’Œçš„è·ç¦»æ˜¯$O(n/k)$, ä½†å¦‚æœæ˜¯å †å å·ç§¯å¦‚å›¾ï¼š å°±å¯ä»¥å‡å°åˆ°$O\left(\log _{k}(n)\right)$ recurrent: è®¡ç®—æ˜¯æ¯ä¸ªè¯å‘é‡ä¹˜éšè—æƒé‡($d*d$)ï¼Œæ‰€ä»¥æ˜“å¾—è®¡ç®—å¤æ‚åº¦ï¼š$O\left(n \cdot d^{2}\right)$ maximum path length: é•¿åº¦å°±æ˜¯nã€‚ æ“ä½œæ­¥éª¤è¦ä»ç¬¬ä¸€ä¸ªåˆ°ç¬¬nä¸ªä¸ºnæ­¥ï¼Œæ˜¯æœ‰é¡ºåºçš„ã€‚å…¶ä»–çš„éƒ½æ²¡æœ‰é¡ºåºè¦æ±‚ self-attentin(restricted) ç›¸å½“äºåªè¾“å…¥ré‚»è¿‘çš„å¥å­é•¿åº¦ï¼Œè‡ªç„¶å¯ä»¥å¾—åˆ°å¦‚å›¾ç»“æœ TrainOptimizer \text { lrate }=d_{\text { model }}^{-0.5} \cdot \min \left(\text {step}_{-} n u m^{-0.5}, \text { step }_{-} n u m \cdot \text { warmup steps }^{-1.5}\right) increasing the learning rate linearly for the first warmup_steps training steps decreasing it thereafter proportionally to the inverse square root of the step number RegularizationResidual Dropout apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks Label Smoothing This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score Resultmachine TranslationEven our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models Model Variations ç¼ºç‚¹ç¼ºç‚¹åœ¨åŸæ–‡ä¸­æ²¡æœ‰æåˆ°ï¼Œæ˜¯åæ¥åœ¨Universal Transformersä¸­æŒ‡å‡ºçš„ï¼Œåœ¨è¿™é‡ŒåŠ ä¸€ä¸‹å§ï¼Œä¸»è¦æ˜¯ä¸¤ç‚¹ï¼š å®è·µä¸Šï¼šæœ‰äº›rnnè½»æ˜“å¯ä»¥è§£å†³çš„é—®é¢˜transformeræ²¡åšåˆ°ï¼Œæ¯”å¦‚å¤åˆ¶stringï¼Œå°¤å…¶æ˜¯ç¢°åˆ°æ¯”è®­ç»ƒæ—¶çš„sequenceæ›´é•¿çš„æ—¶ ç†è®ºä¸Šï¼štransformersécomputationally universalï¼ˆå›¾çµå®Œå¤‡ï¼‰ï¼Œï¼ˆæˆ‘è®¤ä¸ºï¼‰å› ä¸ºæ— æ³•å®ç°â€œwhileâ€å¾ªç¯ æ€»ç»“Transformeræ˜¯ç¬¬ä¸€ä¸ªç”¨çº¯attentionæ­å»ºçš„æ¨¡å‹ï¼Œä¸ä»…è®¡ç®—é€Ÿåº¦æ›´å¿«ï¼Œåœ¨ç¿»è¯‘ä»»åŠ¡ä¸Šä¹Ÿè·å¾—äº†æ›´å¥½çš„ç»“æœã€‚ Googleç°åœ¨çš„ç¿»è¯‘åº”è¯¥æ˜¯åœ¨æ­¤åŸºç¡€ä¸Šåšçš„ï¼Œä½†æ˜¯æ•°æ®é‡å¤§å¯èƒ½ç”¨transformerå¥½ä¸€äº›ï¼Œå°çš„è¯è¿˜æ˜¯ç»§ç»­ç”¨rnn-based modelã€‚ èŠ±äº†ä¸å°‘æ—¶é—´ï¼Œç®—æ˜¯ç†è§£äº†attentionå’Œtransformerï¼Œå¯¹å…¶ä¸­ä¸æ˜¯å¾ˆæ¸…æ¥šçš„ç‚¹å¦‚attentionçš„å†…éƒ¨ä¸­Qï¼ŒKï¼ŒVå…·ä½“æ˜¯ä»€ä¹ˆåœ¨self-attentionå’Œmulti-head attentionä¸­å¤§å°æ˜¯ä¸åŒçš„ï¼Œå¦‚ä½•maskï¼Œå¦‚ä½•è®¡ç®—å¤æ‚ï¼Œç­‰è¿›è¡ŒæŸ¥é˜…èµ„æ–™å¼„æ‡‚äº†ã€‚æ€»ä½“æ¥è¯´è¿˜æ˜¯æ”¶è·å¾ˆå¤§çš„ã€‚å‡†å¤‡åœ¨çœ‹ä¸€äº›ä»£ç è®²è§£ã€‚ reference Attentionæœºåˆ¶è¯¦è§£ï¼ˆäºŒï¼‰â€”â€”Self-Attentionä¸Transformer - å·é™€å­¦è€…çš„æ–‡ç«  - çŸ¥ä¹https://zhuanlan.zhihu.com/p/47282410 https://jalammar.github.io/illustrated-transformer/ï¼ˆè¿™ä¸ªè®²çš„æ¯”è¾ƒè¯¦ç»†ï¼Œå»ºè®®çœ‹å®Œè®ºæ–‡åå†çœ‹ä¸€éè¿™ä¸ªä¼šåŠ æ·±ç†è§£ï¼‰ ã€NLPã€‘Transformerè¯¦è§£ - æå¦‚çš„æ–‡ç«  - çŸ¥ä¹https://zhuanlan.zhihu.com/p/44121378 https://blog.eson.org/pub/664e9bad/ https://mp.weixin.qq.com/s/J-anyCuwLd5UYjTsUFNT1g]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
        <category>classical</category>
      </categories>
      <tags>
        <tag>attention</tag>
        <tag>transformer</tag>
        <tag>translation</tag>
        <tag>classical</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Graph Neural Networks with Generated Parameters for Relation Extractioné˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FGraph_Neural_Networks_with_Generated_Parameters_for_Relation%2F</url>
    <content type="text"><![CDATA[æœ¬æ–‡å°†GNNsåº”ç”¨åˆ°å¤„ç†éç»“æ„åŒ–æ–‡æœ¬çš„ï¼ˆå¤šè·³ï¼‰å…³ç³»æ¨ç†ä»»åŠ¡æ¥è¿›è¡Œå…³ç³»æŠ½å–ã€‚é‡‡ç”¨ä»å¥å­åºåˆ—ä¸­è·å–çš„å®ä½“æ„å»ºå…¨é“¾æ¥å›¾ï¼Œåº”ç”¨ç¼–ç ï¼ˆsequence modelï¼‰ï¼Œä¼ æ’­ï¼ˆèŠ‚ç‚¹é—´ä¿¡æ¯ï¼‰å’Œåˆ†ç±»ï¼ˆé¢„æµ‹ï¼‰ä¸‰ä¸ªæ¨¡å—æ¥å¤„ç†å…³ç³»æ¨ç†ã€‚æœ¬æ–‡æä¾›äº†ä¸‰ä¸ªæ•°æ®é›†ã€‚ problem statement existing relation extraction models fail to infer the relationship without multi-hop relational reasoning. existing GNNs canâ€™t process multi-hop relational reasoning in natural language relational reasoning research objectiveenable GNNs to porcess relational reasoning on unstructed text inputs contribution extend a GNN with generated parameters, which could be applied to process relational reasoning on unstructured inputs verify GP-GNNs in the taks of relation extraction from text; present three datasets GP-GNNs construct a fully-connected graph with the entities in the sequence of text employs three models to process relational reasoning an encoding modul: enable edges to encode rich information from natural languages a propagation modul: propagates realtional information among various nodes a classification modul: make prediction with node representations As compared to tradtional GNNs, GP-GNNs could learn edgesâ€™ parameters from natural lanuages Related workGraph Neural Networks(GNNs) existing models still perfom message-passing on predefined graphs Learning Graphical State Transitions is most related introduecs a nove lnerual architecture to generate a graph based on the textal input dynamically update the relationship during the learning process relational reasoning existing models could not make full use of the multi-hop inference patterns among multiple entity pair and their relaitons within the sentence LEARNING GRAPHICAL STATE TRANSITIONS is the most related work the proposed model incorporates contextual relations with attention mechanism when predicting the relation of a target entity pair Graph Neural Network with Grenerated Parameters(GP-GNNs)The picture is overall architecture: encoding module, propagation module and classification module Encoding Moduleformula: \mathcal{A}_{i, j}^{(n)}=f\left(E\left(x_{0}^{i, j}\right), E\left(x_{1}^{i, j}\right), \cdots, E\left(x_{l-1}^{i, j}\right) ; \theta_{e}^{n}\right)where $f(\cdot)$ could be any model that could sequential(such as LSTMs); $E(\cdot)$ indicates an embedding function. $x^{i, j}$ is the word in sentence labeled( $i,j$) Porpagation Modulethe representations of layer n + 1 are calculated by: \mathbf{h}_{i}^{(n+1)}=\sum_{v_{j} \in \mathcal{N}\left(v_{i}\right)} \sigma\left(\mathcal{A}_{i, j}^{(n)} \mathbf{h}_{j}^{(n)}\right)where $\mathcal{N}\left(v_{i}\right)$ denotes the neighbors of node $v_i$ Classification Modulethe loss of GP-GNNs: \mathcal{L}=g\left(\mathbf{h}_{0 :|\mathcal{V}|-1}^{0}, \mathbf{h}_{0 :|\mathcal{V}|-1}^{1}, \ldots, \mathbf{h}_{0 :|\mathcal{V}|-1}^{K}, Y ; \theta_{c}\right)Relation Extraction with GP-GNNsAuthors introduce how to apply GP-GNNs to relation extraction Encoding Moduleencoding then context of entity pairs (or edges in the graph) E\left(x_{t}^{i, j}\right)=\left[\boldsymbol{x}_{t} ; \boldsymbol{p}_{t}^{i, j}\right]where $x_t$ denotes the word embedding; $\boldsymbol{p}_{t}^{i, j}$denotes the position embedding of word posistion t relative to the entity pairâ€™s position $i, j$. position embeddingwe mark each token in the sentence as either belonging to the first entity $v_i$, the second entity $v_j$ or to neither of those Propagation Module the formula is the same as the front The Initial Embeddings of Nodes when extracting the relationship between entity $v_i$ and entity $v_j$, the initial embeddings of them are annotated as $\mathbf{h}_{v_{i}}^{(0)}=a_{\text { subject }}$, and $h_{v_{j}}^{(0)}=a_{\text { object }}$, while the intial embeddings of other entities are set to all zeros. In our experiments, we generalize the idea of Gated Graph Neural Networks (Li et al., 2016) by setting $a_{\text { subject }}=[1 ; 0]^{\top}$and $a_{\text { object }}=[0 ; 1]^{\top}$. classification ModuleAs the target entity pair $(v_i, v_j)$: \boldsymbol{r}_{v_{i}, v_{j}}=\left[\left[\boldsymbol{h}_{v_{i}}^{(1)} \odot \boldsymbol{h}_{v_{j}}^{(1)}\right]^{\top} ;\left[\boldsymbol{h}_{v_{i}}^{(2)} \odot \boldsymbol{h}_{v_{j}}^{(2)}\right]^{\top} ; \ldots ;\left[\boldsymbol{h}_{v_{i}}^{(K)} \odot \boldsymbol{h}_{v_{j}}^{(K)}\right]^{\top}\right]where $\odot$ represents element-wise multiplication classification: \mathbb{P}\left(r_{v_{i}, v_{j}} | h, t, s\right)=\operatorname{softmax}\left(M L P\left(\boldsymbol{r}_{v_{i}, v_{j}}\right)\right)loss: \mathcal{L}=\sum_{s \in S} \sum_{i \neq j} \log \mathbb{P}\left(r_{v_{i}, v_{j}} | i, j, s\right)Experimentsaim showing their best models could improve the performance of relation extraction under a variety of settings illlustrating that how the number of layers affect the performance of their model performing a qualitiative investigation to highlight the diference between their models and baseline models designas the first and second aim show that our models could improve instance-level relation extraction on a human annotated test set we will show that our models could also help enhance the performance of bag-level relation extraction on a distantly labeled test set split a subset of distantly labeled test set, where the number of entities and edges is large Datasetdistantly label set Sorokin and Gurevych (2017) proposed modify their dataset added reversed edges for all of the entity pairs with no relations, added â€œNAâ€ labels to them Human annotated test set Sorokin and Gurevych (2017) select the distantly lablel pairs which all 5 annotaters are accepted. There are 350 sentences and 1,230 triples in this test set Dense distantly labeled test set criteria the number of entities should be strictly larger than 2 there must be at least one circle (with at least three entities) in the ground-truth label of the sentence There are 1,350 sentences and more than 17,915 triples and 7,906 relational facts in this test set. Models for comparison Context-aware RE Multi-Window CNN PCNN LSTM or GP-GNN with K = 1 layer GP-GNN with K = 2 or K = 3 layerss Evaluation DetailsTo evaluation models in bag-level: E\left(r | v_{i}, v_{j}, S\right)=\max _{s \in S} \mathbb{P}\left(r_{v_{i}, v_{j}} | i, j, s\right)result: Effectiveness of Reasoning Mechanism Context-Aware RE may introduce more noise, for it may mistakenly increase the probability of a relation with the similar topic with the context relations sentences from Wikipedia corpus are always complex, which may be hard to model for CNN and PCNN The Effectiveness of the Number of Layers the improvement of the third layer is much smaller on the overall distantly supervised test set than the one on the dense subset This observation reveals that the reasoning mechanism could help us identify relations especially on sentences where there are more entities as the number of layers grows, the curves get higher and higher precision, indicating considering more hops in reasoning leads to better performance Qualitative Results: Case Study Context-Aware RE makes a mistake by predicting (Kentucky, share boarder with, Ohio). As we have discussed before, this is due to its mechanism to model co-occurrence of multiple relations æ€è€ƒæ–‡ç« æ˜¯åˆ˜çŸ¥è¿œç»„çš„è®ºæ–‡ï¼Œé’ˆå¯¹çš„æ–¹å‘æ˜¯å…³ç³»æŠ½å–ï¼Œåœ¨å…¶ä¸­ç»“åˆäº†å…³ç³»æ¨ç†ï¼Œæœ€è¿‘è®¸å¤šä»»åŠ¡éƒ½åœ¨ç»“åˆæ¨ç†çš„æ€æƒ³ã€‚æ–‡ç« æ•´ä½“çš„ç»“æ„ï¼Œé€»è¾‘ååˆ†æ¸…æ™°ï¼Œè®ºè¿°çš„ä¹Ÿæ¯”è¾ƒè¯¦ç»†ï¼Œå±äºæ ‡å‡†è®ºæ–‡ã€‚æ„Ÿè§‰æ–‡ç« ä¸­GP-GNNsç»“æ„å›¾è¿˜å¯ä»¥ç”»çš„æ›´å¥½ä¸€ç‚¹ï¼Œå±•ç°ä¸€ä¸‹encoding moduleçš„å±‚ï¼Œå¯ä»¥æ›´å¥½ç†è§£ã€‚æ–‡ç« çš„ç²¾é«“åº”è¯¥æ˜¯è¿™ä¸ªpropagation moduleçš„éƒ¨åˆ†ï¼Œè¿˜éœ€è¦æ¶ˆåŒ–ä¸€ä¸‹ï¼Œä¸è¿‡è¿™éƒ¨åˆ†å¯èƒ½æ˜¯æœ‰å…ˆå‰çš„çŸ¥è¯†æ”¯æ’‘çš„ã€‚]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>GNNs</tag>
        <tag>relation extraction</tag>
        <tag>relation reasoning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[allennlpå®‰è£…è¸©å‘]]></title>
    <url>%2Fpost%2Fallennlp_install%2F</url>
    <content type="text"><![CDATA[å®‰è£…allennlpçš„è¸©å‘ä¹‹è·¯ï¼Œè¸©äº†ä¸å°‘å‘æœ€åé€‰æ‹©â€™Installing from sourceâ€™çš„å®‰è£…æ–¹æ³•ï¼Œæ’å‘åä¸‹é¢æ–¹æ³•äº²æµ‹å¯ç”¨ Installing from sourceå®‰è£…æ­¥éª¤ï¼š 1.ä¸‹è½½GitHubæ–‡ä»¶git clone https://github.com/allenai/allennlp.git 2.åˆ›å»ºcondaç¯å¢ƒconda create -n allennlp python=3.6 3.æ¿€æ´»ç¯å¢ƒä¸‹è½½ä¾èµ–æ–‡ä»¶ æ¿€æ´»ç¯å¢ƒ source activate allennlp è¿›å…¥githubä¸Šä¸‹è½½çš„æ–‡ä»¶å¤¹ ä¸‹è½½ä¾èµ–æ–‡ä»¶ pip install -r requirements.txt é‡åˆ°æŠ¥é”™é—®é¢˜ï¼Œå‚è€ƒä¸‹ä¸€å°èŠ‚ï¼Œæ‰€æ¬²é—®é¢˜è§£å†³ã€‚ 4.å®‰è£…allennlppip install --editable . 5.æµ‹è¯•allennlp æˆåŠŸåæ•ˆæœå¦‚ä¸‹ï¼š $ allennlp 2019-05-22 21:58:42,297 - INFO - pytorch_pretrained_bert.modeling - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex . usage: allennlp Run AllenNLP optional arguments: -h, --help show this help message and exit --version show program's version number and exit Commands: configure Run the configuration wizard. train Train a model. evaluate Evaluate the specified model + dataset. predict Use a trained model to make predictions. make-vocab Create a vocabulary. elmo Create word vectors using a pretrained ELMo model. fine-tune Continue training a model on a new dataset. dry-run Create a vocabulary, compute dataset statistics and other training utilities. test-install Run the unit tests. find-lr Find a learning rate range. print-results Print results from allennlp serialization directories to the console. é‡åˆ°çš„é—®é¢˜é—®é¢˜1æŠ¥é”™ä¿¡æ¯ï¼šERROR: Failed building wheel for jsonnet è§£å†³æ–¹æ³•ï¼šconda install -c conda-forge jsonnet é—®é¢˜2æŠ¥é”™ä¿¡æ¯ï¼šæŠ¥çš„éƒ½æ˜¯æŸäº›åŒ…çš„ç‰ˆæœ¬é—®é¢˜ ERROR: botocore 1.12.152 has requirement urllib3=1.20; python_version >= "3.4", but you'll have urllib3 1.25.2 which is incompatible. ERROR: aws-sam-translator 1.11.0 has requirement jsonschema~=2.6, but you'll have jsonschema 3.0.1 which is incompatible. ERROR: cfn-lint 0.20.3 has requirement jsonschema~=2.6, but you'll have jsonschema 3.0.1 which is incompatible. ERROR: cfn-lint 0.20.3 has requirement requests=2.15.0, but you'll have requests 2.22.0 which is incompatible è§£å†³æ–¹æ³•æ ¹æ®æŠ¥é”™ä¿¡æ¯ä¸‹è½½ç›¸åº”å®‰è£…åŒ…å³å¯ é—®é¢˜3æŠ¥é”™ä¿¡æ¯ï¼šImportError: dlopen: cannot load any more object with static TLS ___________________________________________________________________________ Contents of /home/minelab/anaconda3/envs/allennlp/lib/python3.6/site-packages/sklearn/__check_build: __init__.py setup.py _check_build.cpython-36m-x86_64-linux-gnu.so __pycache__ ___________________________________________________________________________ It seems that scikit-learn has not been built correctly. If you have installed scikit-learn from source, please do not forget to build the package before using it: run `python setup.py install` or `make` in the source directory. If you have used an installer, please check that it is suited for your Python version, your operating system and your platform. è§£å†³æ–¹æ³•ï¼šä¸‹è½½æ›´ä½ç‰ˆæœ¬çš„scikit-learn,ä¾‹å¦‚ pip install scikit-learn=0.20.3 å‚è€ƒé“¾æ¥ https://github.com/pytorch/pytorch/issues/10443 https://github.com/pypa/pip/issues/4330 å®‰è£…çš„å¯ç¤ºç¯å¢ƒé—®é¢˜ æœ€åŸºæœ¬çš„å°±æ˜¯å…ˆå»ç½‘ä¸ŠæŸ¥è¿™ä¸ªé”™è¯¯çš„è§£å†³æ–¹æ³• ç½‘ä¸Šçš„è§£å†³ä¸äº†çš„ï¼Œå…ˆçŒœçŒœå¤§æ¦‚ç‡æ˜¯å“ªæ–¹é¢çš„é—®é¢˜ã€‚ æ¯”å¦‚å¤§æ¦‚ç‡æ˜¯å„ç§ç‰ˆæœ¬äº’ç›¸ä¹‹é—´ä¸é€‚é…çš„é—®é¢˜ï¼Œé‚£å°±è°ƒè¯•ç‰ˆæœ¬ï¼Œä¸€èˆ¬éƒ½ä¼šå‘Šè¯‰ä½ å“ªä¸ªæœ‰é—®é¢˜ï¼Œæ¯”å¦‚ä¸Šé¢çš„scikit-learné—®é¢˜ã€‚]]></content>
      <categories>
        <category>install</category>
      </categories>
      <tags>
        <tag>allennlp</tag>
        <tag>åŒ…å®‰è£…</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Triple Trustworthiness Measurement for Knowledge Graphé˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FTriple%20Trustworthiness%20Measurement%20for%20Knowledge%20Graph%2F</url>
    <content type="text"><![CDATA[æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡è®¡ç®—triple trustworthinessæ¥è¯„ä¼°çŸ¥è¯†å›¾è°±çš„å‡†ç¡®ç¨‹åº¦çš„æ–¹æ³•ã€‚æ¨¡å‹åˆ©ç”¨ç¥ç»ç½‘ç»œç»¼åˆæ¥è‡ªå®ä½“ï¼ˆå€Ÿé‰´Resource allocationï¼‰ã€å…³ç³»ï¼ˆå€Ÿé‰´ç¿»è¯‘æ¨¡å‹çš„æ€æƒ³ï¼Œå¦‚TransEï¼‰å’ŒKGå…¨å±€ï¼ˆå€Ÿé‰´å…³ç³»è·¯å¾„ï¼ŒRNNï¼‰ä¸‰ä¸ªå±‚é¢çš„è¯­ä¹‰å’Œå…¨å±€ä¿¡æ¯ï¼Œè¾“å‡ºæœ€åçš„ trustworthinessä½œä¸ºåˆ¤æ–­ä¾æ®ã€‚ ä¸‹è½½åœ°å€ SummaryThis paper proposed a method for estimating the accuracy of a knowledge graph by computing triple trustworthiness. The model uses neural network to synthesize semantic and global information from three levels: entity(resource allocation), relationship(translation model ideas, such as TransE)m and KG global(relationship path, RNN) and outputting the final trustworthiness as the basis for judgment. Problem statementpossible noises and conflicts are inevitably intoduced in the process of constructing the KG research objectivequantify the KGâ€™s semantic correctness and the true degree of the facts expressed Contribution Knowledge graph triple trustworthiness measurement use the triple semantic information and globally inferring information three levels measurement and an intergration of confidence value experiment result verified the model valid on large-scale KG Freebase the KGTtm could be utilized in knowledge graph construction or improvement THE TRIPLE TRUSTWORTHINESS MEASUREMENT MODEL Longitudinally, the model can be divided into two level. the upper is a pool of multiple trustworthiness estimate cells(estimator) the output of these Estimator forms the input of lower-level fusion device(Fusioner) Viewed laterally, three progressive levels are be considered, as following. Is there a possible relationship between the entity pairs? ResourceRank: The algorithm assumes that the association between entity paires $(h,t)$ will be stronger, and more resource is passed from the head $h$ through all associated paths to the tail $t$ in a graph The amount of resource aggregated into $t$ ingeniously indicateds the association strength from $h$ to $t$. As pair $(e_1,e_2)$, there only one directed edge from $e_1$ to $e_2$ in the graph, where the different bandwidth of the edge indicates the number of the multiple relations. output: \left\{\begin{array}{c}{u=\alpha\left(W_{1} V+b_{1}\right)} \\ {R R(h, t)=W_{2} u+b_{2}}\end{array}\right.Authors constructed a $V$ vector by combining six characteristics. R (t | h); In-degree of head node ID(h); Out-degree of head node OD(h); In-degree of tail node ID(t); Out-degree of tail node OD(t); The depth from head node to tail node Dep As for 1. the formula: R(t | h)=(1-\theta) \sum_{e_{i} \in M_{t}} \frac{R\left(e_{i} | h\right) \cdot B W_{e_{i} t}}{O D\left(e_{i}\right)}+\frac{\theta}{N} $M_t$is the set of all nodes that have outgoing links to the node $t$, $OD (e_i)$ is the out-degree of the node eiand the $BW_{e_it}$ is the bandwidth from the $e_i$ to $t$. In order to improve the model fault-tolerance, we assume that the resource fow from each node may directly jump to a random node with the same probability Î¸ Can the determined relationship $r$ occur between the entity pair $(h,t)$ ? Translation-based energy function (TEF)ï¼šdepended on TransE $E(h, r, t)=|\mathbf{h}+\mathbf{r}-\mathbf{t}|$ output: P(E(h, r, t))=\frac{1}{1+e^{-\lambda\left(\delta_{r}-E(h, r, t)\right)}}Can the relevant triples in the KG infer that the triple is trustworthy? Reachable paths inference (RPI): There two challenges to exploit the reachable paths for inferring triple trustworthiness: reachable paths selectionSemantic distance-based path selection Reachable Paths Representationusing a RNN to deal with the embeddings of the three elements of each triple in the selected path Fusing the Estimatorsa classifer based on a multi-layer perceptron EXPERIMENTSdatasetFB15K Interpreting the Validity of the Trustworthiness The left picture shows that the positives examples are mainly concentrated in the upper region, vice versa. As for the right picture only if the value of a triple is higher than the threshold can it be considered trustworthy shows that the positive examples universally have higher confidence values Comparing With Other Models on The Knowledge Graph Error Detection Task Authorsâ€™ model has beter results in terms of accuracy and the F1-score than the other models. Analyzing the ability of models to tackle the three type noises. a higher recall shows that authorsâ€™ model can more accurately find the right from noisy triples higher average trustworthiness values show that authorsâ€™ model can better identify the correct instances and with high confidence the worst among the $(h, ?, t)$, because the various relations between a certain entity increase the difficulty of model judgment. Analyzing the Efects of Single Estimators It can be found that the accuracy obtained by each model is above 0.8, which proves the effectiveness of each Estimator æ€è€ƒæœ¬æ–‡åœ¨æ–¹æ³•ä¸Šå‡ ä¹æ²¡æœ‰ä»€ä¹ˆåˆ›æ–°ï¼Œæœ¬è´¨ä¸Šå°±æ˜¯ä¸€ä¸ªè€æ–¹æ³•çš„å¤šä¸ªç»„åˆã€‚æœ€å¤§äº®ç‚¹å°±æ˜¯ä½œè€…èƒ½æå‡ºtrustworthinessæ¥æŠŠè¿™ä¸ªè¯„ä»·çŸ¥è¯†å›¾è°±å‡†ç¡®åº¦çš„é—®é¢˜è¿›è¡Œäº†é‡åŒ–ã€‚è¿™ç§èƒ½åŠ›æ¯”æå‡ºæ–¹æ³•ä¸Šçš„åˆ›æ–°æ›´åŠ å‰å®³ï¼Œä¹Ÿæ˜¯éœ€è¦å­¦ä¹ çš„åœ°æ–¹ã€‚]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>KG</tag>
        <tag>Knowledge Graph</tag>
        <tag>Triple</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GloVe: Global Vectors for Word Representationé˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FGloVe%3AGlobal%20Vectors%20for%20Word%20Representation%2F</url>
    <content type="text"><![CDATA[è®ºæ–‡ä¸‹è½½åœ°å€ï¼ŒGloVeæ˜¯ä¸€ä¸ªæ–°çš„å…¨çƒå¯¹æ•°åŒçº¿æ€§å›å½’æ¨¡å‹ï¼Œå±äºç»å…¸çš„è¯å‘é‡è¡¨ç¤ºæ–¹æ³•ä¹‹ä¸€ã€‚ Introductionevaluate the intrinsic quality Most word vector methods rely on the distance or angle between pairs of word vectors Mikolov et al. (2013c) introduced word analogies that examines word vectorâ€™s various dimensions of difference. two main model families for learning vectors: global matrix factorization methods local context window methods Authors propose a specific weighted least squares model that trains on globla word-word co-occurrence counts and thus makes efficient use of statistics. Related WorkMatix Facroization MethodsThese methods utilize low-rank approximations to decompose large matrices that capture statistical information about a corpus. shortcomingthe most frequent words contribute a dispropoertionate amount to the similarity measure. Shallow Window-Based MethodsAnother approach is to learn word representations that aid in making predictins within local context windows. shortcomingdo not operate directly on the co-occurrence statistics of the corpus and fails to take advantage of the vast amount of repetition in the data. The GloVe ModelGloVe: Global Vectorsthe global corpus statistics are captured directly by the model the question about the model using the statistics of word occurrences in a corpus how meaning is generated from these statistics how the resulting word vectors might represent that meaning some notation$X_{ij}$ : the number of times word j occurs in the context of word i $X_i = \sum_{k} X_{i k}$ : the number of times any word appears in the context of word i $P_{i j}=P(j | i)=X_{i j} / X_{i}$: the probability that word j appear in the context of word i above that, werd vector learning should be with ratios of co-occurrence probabilities: $w \in \mathbb{R}^{d}$are word vectors and $\tilde{w} \in \mathbb{R}^{d}$are separate context word vectors For F, we should select a unique choice by enforcing a few desiderata. encoding the information present the ratio $P_{i k} / P_{j k}$ in the word vector space. Since vector spaces are inherently linear structures put F to be a compicated function parameterized, and avoiding bofuscating the linear structure the word-word co-occurrence matrices, we can exchange a word and a context word(because a word can also be a context word) F should be a homomorphism by Eqn.(3) F = exp or the Eqn(6) would have the exchange symmetry if not $\log \left(X_{i}\right)$ and $\log \left(X_{i}\right)$ is independent of k, so it can be absorbed into a bias $b_i$ for avoiding diverge, $\log \left(X_{i k}\right) \rightarrow \log \left(1+X_{i k}\right)$ a new weighted least squares regression model to address the problem that LSA wirhts all co-occuttences equally. cost function: Relationship to Other ModelsIn this subsection authors show how these models are related to their proposed model. the defect of cross entropy it has the unfortunate property that distributions with long tails are often modeled poorly with too much wieght given to the unlikely events. Complexity of the modelthe computational complexity of the model depends on the number of nonzero elects in the matrix $X$ some assumptions about the distribution of word co-occurrences the number of co-occurrences of word $i$ with word $j$, $X_{ij}$, can be modeled as a power-law function of the frequency rank of that word pair, $r_{ij}$: $X_{i j}=\frac{k}{\left(r_{i j}\right)^{\alpha}}$ ExperimentsEvaluation methodsauthors conduct experiments on the word analogy taks of Mikolov et al. (2013a) Word analogiesThe word analogy task consists of questions like, â€œa is to b as c is to ?â€ Word similarity Named entity recognitionResultsTable 2 shows the CloVe model performs significantly better than the other baslines, often with smaller vector sizes and smaller corpora. Table 3 shows results on five different word similarity datasets. Table 4 shows results on the NER task with the CRF-based model. Model Analysis: Vector Length and Context Size Model Analysis: Corpus Size On the syntactic subtask, larger corpora typically produce better statistics so that there is a monotonic increase in performance as the cor- pus size increases. But the same trend is not true for the semantic subtask, which is probably because of analogy dataset Model Analysis: Run-time Model Analysis: Comparison with word2vecFor the same corpus, vocabulary, window size, and training time, GloVe consistently outperforms word2vec å‚è€ƒé“¾æ¥ https://blog.csdn.net/coderTC/article/details/73864097]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>word vector</tag>
        <tag>GloVe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep contextualized word representations é˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FDeep%20contextualized%20word%20representations%2F</url>
    <content type="text"><![CDATA[è®ºæ–‡ä¸‹è½½åœ°å€ï¼ŒELMoäº‹å…ˆç”¨è¯­è¨€æ¨¡å‹å­¦å¥½ä¸€ä¸ªå•è¯çš„ Word Embeddingï¼Œæ­¤æ—¶å¤šä¹‰è¯æ— æ³•åŒºåˆ†ï¼Œä¸è¿‡è¿™æ²¡å…³ç³»ã€‚åœ¨æˆ‘å®é™…ä½¿ç”¨ Word Embedding çš„æ—¶å€™ï¼Œå•è¯å·²ç»å…·å¤‡äº†ç‰¹å®šçš„ä¸Šä¸‹æ–‡äº†ï¼Œè¿™ä¸ªæ—¶å€™æˆ‘å¯ä»¥æ ¹æ®ä¸Šä¸‹æ–‡å•è¯çš„è¯­ä¹‰å»è°ƒæ•´å•è¯çš„ Word Embedding è¡¨ç¤ºï¼Œè¿™æ ·ç»è¿‡è°ƒæ•´åçš„ Word Embedding æ›´èƒ½è¡¨è¾¾åœ¨è¿™ä¸ªä¸Šä¸‹æ–‡ä¸­çš„å…·ä½“å«ä¹‰ï¼Œè‡ªç„¶ä¹Ÿå°±è§£å†³äº†å¤šä¹‰è¯çš„é—®é¢˜äº†ã€‚æ‰€ä»¥ ELMO æœ¬èº«æ˜¯ä¸ªæ ¹æ®å½“å‰ä¸Šä¸‹æ–‡å¯¹ Word Embedding åŠ¨æ€è°ƒæ•´çš„æ€è·¯ã€‚ IntroductionELMo(Embedddings from Language Models):why call ELMo:Using vectors derived from a bidirectional LSTM that is trained with a coupled language model(LM) objective on a large text corups. characteristics ELMo representations are a function of all of the internal layers of the biLM. learn a linear combination of the vectors stacked above each input word for each end task the higher-level LSTM states capture context-dependent aspects of word meaning the lower-level states model aspects of syntax Extensive experiments EMLo representations can be easily added to existing models improve the state of art in every case ELMo outperform those derived from just the top layer of a LSTM Related work Some approaches for learning word vectors only allow a single context-independent representation for each word. to overcome some shortcomings of traditional word vectors: enriching them with subword information learning separate vectors for each word sense Authors uses subword units through the use of character convolutions, seamlessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes. context-depends representations Authors take full advantage of access to plentiful monolingual data Previous work also shown that different layers of deep biRNNs encode different types of information introducing multi-task syntactic supervision at the lower levels of a deep LSTM can improve overall performance of higher level tasks the top layer of an LSTM for encoding word context (Melamud et al., 2016) has been shown to learn representations of word sense. ELMo representations can also induce similar signals. ELMo: Embeddings from Language ModelsBidirectional language models model the probability of token $t_k$ given the history($t_1, â€¦ , t_{k-1}$): a backward LM: Authorsâ€™ formulation jointly maximizes the log likelihood of the forward and backward directions: ELMo For each token $t_k$, a L-layer biLM computes a set of 2L + 1 representations: For a downstream model, ELMo collapses all layers in R into a single vector. In the simplest case, ELMo just selects the top layer. For a task specific weighting of all biLM layers: $s^{task}$ are softmax-normalized weithts and the scalar parameter $Î³^{task}$ allows the task model to scale the entire ELMo vector Using biLMs for supervised NLP tasks Given a pre-trained biLM and a supervised architecture for a target NLP task let the end task model learn a linear combination of these representations consider the lowest layers of th supervised model without the biLM add ELMo to the supervised model freeze the weights of the biLM concatenate the ELMo vector $ELMo^{task}_k$ with $x_k$ and pass the ELMo enhanced representation $[x_k,;ELMo^{task}_k ]$ into the task RNN. for some tasks, authors also include ELMo ar the output of task RNN by introducing another set of out put specific linear weights and replacing $h_k$ with $[h_k,;ELMo^{task}_k ]$ add a moderate amount of dropout to ELMo and in some case to regularize the ELMo weights Pre-trained bidirectional language model architecture the biLM provides three layers of representations for each input token, both directions and a residual connection between LSTM layers fine tuning the biLM on domain specific data Evaluationthe following picture shows the performance of ELMo in Question answering, Textual entailment, Semantic role labeling, Corefrence resolution, Named entity extraction, Sentiment analysis. In every task considered, simply adding ELMo establishes a new state-of-the-art result. AnalysisAlternate layer weighting schemes the following picture compares these alternatives. Including representations from all layers improves overall performance over just using the last layer, and including contextual representations from the last layer improves performace over the baseline. Also shows the $\lambda$ is important. Where to include ELMo?The ELMo can be included in both the input and output. the results show including the ELMo in both input and output can preform better. What information is captured by the biLMâ€™s representations?Intuitively, the biLM must be disambiguating the meaning of words using their context. The GloVe can only capure the speech. but the biLM is able to disambiguate both the part of speech and word sense in the source sentence. Word sense disambiguationgiven a sentence, predicting the sense of a target word using a simple 1-nearst negihbor approach POS taggingto examine whether the biLM captures basic syntax. Sample efficiencyAdding ELMo to a model increases the sample efficiency considerably, both in terms of number of parameter updates to reach state-of-the-art performance and the overall training set size. Visualization of learned weights å‚è€ƒé“¾æ¥ NAACL2018:é«˜çº§è¯å‘é‡(ELMo)è¯¦è§£(è¶…è¯¦ç»†) ç»å…¸ï¼Œè¿™ç¯‡æ–‡ç« ä¸­é˜è¿°äº†ä¸€äº›ä½¿ç”¨çš„ç»†èŠ‚ï¼Œå¹¶ç”¨å›¾æ¥è¡¨ç¤ºï¼Œæ›´åŠ æ¸…æ™°ã€‚ ELMoç®—æ³•ä»‹ç»ï¼Œè¿™ç¯‡åšå®¢ä¸­è‡ªå·±å¯¹æ•´ä¸ªè®ºæ–‡çš„æ¦‚è¿°å’Œæ€»ç»“å’Œå¥½ï¼Œéœ€è¦å­¦ä¹ ã€‚]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>embedding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ã€ŠEfficient Estimation of Word Representations in Vector Spaceã€‹é˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FEfficient%20Estimation%20of%20Word%20Representations%20in%20Vector%20Space%2F</url>
    <content type="text"><![CDATA[è®ºæ–‡ä¸‹è½½åœ°å€ï¼Œè¯¥ç¯‡è®ºæ–‡çš„å¤§ç¯‡å¹…éƒ½åœ¨è®¨è®ºå®éªŒç»“æœçš„åˆ†æï¼Œæ¨¡å‹çš„éƒ¨åˆ†æ¯”è¾ƒç®€å•ï¼Œæ²¡æœ‰è¯¦ç»†åˆ†æï¼Œæœ¬æ¥æ˜¯æƒ³è¯»ä¸€ä¸‹CBOWå’Œskip-gramçš„åŸå§‹è®ºæ–‡ï¼Œå‘ç°å¹¶æ²¡æœ‰æƒ³è±¡ä¸­çš„é‚£ä¹ˆå¤§çš„ç”¨å¤„ã€‚ Goals of paper å¼€å‘äº†ä¸¤ç§æ–°æ¨¡å‹ï¼Œå¹¶ä¿ç•™äº†å•è¯ä¹‹é—´çš„çº¿æ€§è§„å¾‹ è®¾è®¡äº†ä¸€ä¸ªæ–°çš„ç»¼åˆæµ‹è¯•é›†ï¼Œç”¨äºæµ‹é‡å¥æ³•å’Œè¯­ä¹‰è§„å¾‹ è®¨è®ºäº†è®­ç»ƒæ—¶é—´å’Œå‡†ç¡®æ€§å¦‚ä½•å–å†³äºå•è¯å‘é‡çš„ç»´åº¦å’Œè®­ç»ƒæ•°æ®çš„æ•°é‡ Model Architecturesè®­ç»ƒå¤æ‚åº¦ï¼š å…¶ä¸­ï¼ŒEæ˜¯è®­ç»ƒæ¬¡æ•°ï¼ŒTæ˜¯è®­ç»ƒé›†å•è¯æ•°é‡ï¼ŒQæ˜¯æ¨¡å‹ç»“æ„ã€‚ Feedforward Neural Net Language Model (NNLM)å®ƒç”±è¾“å…¥ï¼Œæ˜ å°„ï¼Œéšè—å’Œè¾“å‡ºå±‚ç»„æˆã€‚é€šè¿‡ç®€åŒ–æ–¹æ³•ï¼ŒQ= N x D x H Recurrent Neural Net Language Model (RNNLM)å…‹æœäº†æ¨¡å‹éœ€è¦å›ºå®šçš„ä¸Šä¸‹æ–‡é•¿åº¦çš„é—®é¢˜ï¼Œå¹¶ä¸”åªæœ‰è¾“å…¥ï¼Œéšè—å’Œè¾“å‡ºå±‚ã€‚ Q= H x H + H x Vï¼Œå…¶ä¸­H = Dï¼ˆå•è¯è¡¨ç¤ºï¼‰ï¼ŒH x V å¯ä»¥é€šè¿‡åˆ†çº§softmaxè¢«ç®€åŒ–ä¸ºH x log_2(V)ã€‚æ‰€ä»¥ä¸»è¦çš„å¤æ‚åº¦æ¥è‡ªäºH x Hã€‚ Parallel Training of Neural Networksæ¨¡å‹ä½¿ç”¨çš„DistBeliefæ¡†æ¶å…è®¸æˆ‘ä»¬å¹¶è¡Œè¿è¡ŒåŒä¸€æ¨¡å‹çš„å¤šä¸ªå‰¯æœ¬ï¼Œæ¯ä¸ªå‰¯æœ¬é€šè¿‡é›†ä¸­çš„æœåŠ¡å™¨åŒæ­¥å…¶æ¢¯åº¦æ›´æ–°ï¼Œè¯¥æœåŠ¡å™¨ä¿ç•™æ‰€æœ‰å‚æ•° New Log-linear Modelså¤§å¤šæ•°å¤æ‚æ€§æ˜¯ç”±äºæ¨¡å‹ä¸­çš„éçº¿æ€§éšè—å±‚å¼•èµ·çš„ã€‚æ¨¡å‹ç»“æ„å¦‚ä¸‹ï¼š Continuous Bag-of-Words Model(CBOW)ç¬¬ä¸€ä¸ªæå‡ºçš„ä½“ç³»ç»“æ„ç±»ä¼¼äºå‰é¦ˆNNLMï¼Œå…¶ä¸­å»é™¤äº†éçº¿æ€§éšè—å±‚ï¼Œå¹¶ä¸”æ‰€æœ‰å•è¯ï¼ˆä¸ä»…ä»…æ˜¯æŠ•å½±çŸ©é˜µï¼‰å…±äº«æŠ•å½±å±‚ã€‚ å› æ­¤ï¼Œæ‰€æœ‰å•è¯éƒ½è¢«æŠ•å°„åˆ°ç›¸åŒçš„ä½ç½®ï¼ˆå®ƒä»¬çš„å‘é‡è¢«å¹³å‡ï¼‰ã€‚ å°†è¿™ä¸ªæ¶æ„ç§°ä¸ºè¯è¢‹æ¨¡å‹ï¼Œå› ä¸ºå†å²ä¸­çš„å•è¯é¡ºåºä¸ä¼šå½±å“æŠ•å½±ã€‚ æ¨¡å‹çš„å¤æ‚åº¦ï¼šQ = N Ã— D + D Ã— log_2(V ) Continuous Skip-gram ModelåŸºäºåŒä¸€å¥å­ä¸­çš„å¦ä¸€ä¸ªå•è¯æœ€å¤§åŒ–å•è¯çš„åˆ†ç±»ã€‚ æ›´å‡†ç¡®åœ°è¯´ï¼Œä½¿ç”¨æ¯ä¸ªå½“å‰å•è¯ä½œä¸ºå…·æœ‰è¿ç»­æŠ•å½±å±‚çš„å¯¹æ•°çº¿æ€§åˆ†ç±»å™¨çš„è¾“å…¥ï¼Œå¹¶é¢„æµ‹å½“å‰å•è¯ä¹‹å‰å’Œä¹‹åçš„ç‰¹å®šèŒƒå›´å†…çš„å•è¯ã€‚ æ¨¡å‹çš„å¤æ‚åº¦ï¼šQ = C Ã— (D + D Ã— log2(V ))ï¼Œå…¶ä¸­Cæ˜¯å•è¯çš„æœ€å¤§è·ç¦»ã€‚ å®éªŒä»»åŠ¡æè¿°ä¸ºäº†åº¦é‡è¯å‘é‡çš„è´¨é‡ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªå¤æ‚çš„æµ‹è¯•é›†ï¼Œå®ƒåŒ…æ‹¬äº†äº”ç§ç±»å‹çš„è¯­ä¹‰é—®é¢˜ã€‚ä¹ä¸ªç±»å‹çš„å¥æ³•é—®é¢˜ã€‚åŒ…æ‹¬æ¯ä¸ªç±»åˆ«çš„ä¸¤ä¸ªæ ·æœ¬é›†åœ¨ä¸Šè¡¨å±•ç¤ºï¼›æ€»ä¹‹ï¼Œå…±æ‹¥æœ‰8869ä¸ªè¯­ä¹‰é—®é¢˜å’Œ10675ä¸ªå¥æ³•é—®é¢˜ ä½œè€…é€šè¿‡ï¼šæœ€å¤§åŒ–ç²¾ç¡®åº¦ ï¼Œæ¨¡å‹ä½“ç³»ç»“æ„çš„æ¯”è¾ƒï¼Œæ¨¡å‹çš„å¤§è§„æ¨¡å¹¶è¡Œè®­ç»ƒæ¥è¯æ˜æå‡ºæ¨¡å‹çš„è¿é€Ÿåº¦å’Œç²¾ç¡®çš„ä¼˜åŠ¿ã€‚]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>word2vec</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shared Embedding Based Neural Networks for Knowledge Graph Completioné˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FShared%20Embedding%20Based%20Neural%20Networks%20for%20Knowledge%20Graph%20Completion%2F</url>
    <content type="text"><![CDATA[åŸæ–‡ä¸‹è½½é“¾æ¥ï¼ŒçŸ¥è¯†å›¾è°±è¡¥å…¨ï¼ˆKGCï¼ŒKnowledge Graph Completion)æ˜¯ä¸€ç§è‡ªåŠ¨å»ºç«‹å›¾è°±å†…éƒ¨çŸ¥è¯†å…³è”çš„å·¥ä½œã€‚ç›®æ ‡æ˜¯è¡¥å…¨çŸ¥è¯†å›¾è°±ä¸­ä¸‰å…ƒç»„çš„ç¼ºå¤±éƒ¨åˆ†ã€‚ä¸»è¦æ–¹æ³•ä¸ºåŸºäºå¼ é‡ï¼ˆæˆ–è€…çŸ©é˜µï¼‰å’ŒåŸºäºç¿»è¯‘ä¸¤ç±»ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºå…±äº«åµŒå…¥çš„ç¥ç»ç½‘ç»œçš„æ¨¡å‹ï¼ˆSENNï¼‰æ¥å¤„ç†KGCã€‚ Contribulation æå‡ºäº†SENNæ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ˜ç¡®åŒºåˆ†å¤´å®ä½“ã€å…³ç³»å’Œä¸ºå®ä½“é¢„æµ‹ä»»åŠ¡ï¼Œå¹¶æŠŠå®ƒä»¬æ•´åˆåˆ°ä¸€ä¸ªåŸºäºå…¨è¿æ¥ç¥ç»ç½‘ç»œæ¡†æ¶ä¸­ï¼Œè¯¥æ¡†æ¶å…±äº«çš„å®ä½“å’Œå…³ç³»åµŒå…¥ã€‚ SENNæå‡ºäº†ä¸€ä¸ªè‡ªé€‚åº”å…¨ä¸­æŸå¤±æœºåˆ¶ï¼Œè¯¥æ–¹æ³•å¯ä»¥å¾ˆå¥½çš„å¤„ç†å…·æœ‰ä¸åŒæ˜ å°„å±æ€§çš„ä¸‰å…ƒç»„ï¼Œå¹¶å¤„ç†ä¸åŒçš„é¢„æµ‹ä»»åŠ¡ã€‚ ç”±äºå…³ç³»é¢„æµ‹é€šå¸¸æ¯”å¤´å°¾å®ä½“é¢„æµ‹å…·æœ‰æ›´å¥½çš„æ€§èƒ½ï¼Œæˆ‘ä»¬æŠŠSENNåº”ç”¨åˆ°å¤´å°¾å®ä½“é¢„æµ‹ï¼Œä»è€Œå°†SENNæ‰©å±•åˆ°SENN+ã€‚ Related worksTensor/Matrix Based MethodsRESCALæ˜¯ä¸€ä¸ªå…¸å‹çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŸºäºä¸‰å‘å¼ é‡å› å­åˆ†è§£çš„æ–¹æ³•ã€‚ ç›®æ ‡å‡½æ•°ä¸ºï¼š $M_r$æ˜¯rçš„å…³ç³»çŸ©é˜µï¼Œå¤§å°ä¸ºk x kã€‚ ComlExæ˜¯æœ€è¿‘æå‡ºçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŸºäºçŸ©é˜µåˆ†è§£ï¼Œå¹¶ä¸”å®ƒä½¿ç”¨å¤æ•°å€¼æ¥å®šä¹‰å®ä½“å’Œå…³ç³»çš„åµŒå…¥ã€‚ ç›®æ ‡å‡½æ•°ä¸ºï¼š Re(x)è¿”å›xçš„å®éƒ¨ã€‚ Translation Based Methodsä»£è¡¨æ¨¡å‹ä¸ºç»å…¸çš„TransEæ¨¡å‹ï¼ˆè¿™é‡Œä¸å†èµ˜è¿°ï¼‰ Translation Based MethodsER-MLPä½¿ç”¨å¤šå±‚æ„ŸçŸ¥å™¨æ¥æ•è·å¤´å®ä½“ï¼Œå…³ç³»å’Œå°¾å®ä½“ä¹‹é—´çš„éšå¼äº¤äº’ã€‚ ç›®æ ‡å‡½æ•°ä¸ºï¼š ProjEä½¿ç”¨å…·æœ‰ç»„åˆå±‚å’ŒæŠ•å½±å±‚çš„ç¥ç»ç½‘ç»œæ¥å¯¹å¤´å°¾å®ä½“é¢„æµ‹å»ºæ¨¡ã€‚ THE SENN METHODæ¨¡å‹ç»“æ„å¦‚å›¾æ‰€ç¤ºï¼š ä½œè€…å°†æ¡†æ¶åˆ’åˆ†ä¸ºä»¥ä¸‹å››ä¸ªéƒ¨åˆ†ï¼š ä¸‰å…ƒç»„çš„æ‰¹é‡é¢„å¤„ç† çŸ¥è¯†å›¾è°±çš„Shared embeddingsè¡¨ç¤ºå­¦ä¹  ç‹¬ç«‹çš„å¤´å°¾å®ä½“åŠå…³ç³»é¢„æµ‹å­æ¨¡å‹è®­ç»ƒä¸èåˆ è”åˆæŸå¤±å‡½æ•°æ„æˆ æ•´ä¸ªKGCçš„æµç¨‹å¯ä»¥æè¿°å¦‚ä¸‹ï¼š å°†è®­ç»ƒæ•°æ®ä¸­çš„å®Œæ•´ä¸‰å…ƒç»„ï¼ˆçŸ¥è¯†å›¾è°±ï¼‰åˆ’åˆ†æ‰¹é‡åä½œä¸ºæ¨¡å‹çš„è¾“å…¥ å¯¹äºè¾“å…¥çš„ä¸‰å…ƒç»„ï¼Œåˆ†åˆ«è®­ç»ƒå¾—åˆ°å®ä½“ï¼ˆåŒ…æ‹¬å¤´å°¾å®ä½“ï¼‰åµŒå…¥çŸ©é˜µä¸å…³ç³»åµŒå…¥çŸ©é˜µï¼ˆembeddingsï¼‰ å°†å¤´å°¾å®ä½“åŠå…³ç³»embeddingsåˆ†åˆ«è¾“å…¥åˆ°ä¸‰ä¸ªé¢„æµ‹æ¨¡å‹ä¸­ï¼ˆå¤´å®ä½“é¢„æµ‹ï¼ˆ?, r, tï¼‰ï¼Œå…³ç³»é¢„æµ‹(h, ?, t)ï¼Œå°¾å®ä½“é¢„æµ‹(h, r, ?)ï¼‰ The Three Substructuresé¢„æµ‹å­æ¨¡å‹å…·æœ‰ç›¸ä¼¼çš„ç»“æ„å¦‚ä¸‹å›¾ï¼Œæ¨¡å‹è¾“å…¥å…³ç³»å‘é‡ä¸å®ä½“å‘é‡åï¼Œè¿›å…¥nå±‚å…¨è¿æ¥å±‚ï¼Œå¾—åˆ°é¢„æµ‹å‘é‡ï¼Œå†ç»è¿‡ä¸€ä¸ªsigmoidï¼ˆæˆ–è€…softmaxï¼‰å±‚ï¼Œè¾“å‡ºé¢„æµ‹æ ‡ç­¾å‘é‡ã€‚ å¤´å®ä½“é¢„æµ‹ç›®æ ‡å‡½æ•°ï¼š f(x)= max(0,x). é¢„æµ‹æ ‡ç­¾ï¼š å…¶å®ƒä¸¤ç§ä¸æ­¤å¤´å®ä½“ç±»ä¼¼ã€‚ Model TrainingThe General Loss Functionæ¨¡å‹ç›®æ ‡æ ‡ç­¾å‘é‡è¡¨ç¤ºä¸ºï¼š $I_h$æ˜¯åœ¨è®­ç»ƒé›†ä¸­ç»™å®šrå’Œtçš„æ‰€æœ‰æœ‰æ•ˆå¤´å®ä½“é›†ã€‚ ä¸‰è€…çš„å¹³æ»‘å‘é‡è¡¨ç¤ºä¸ºï¼š ä¸‰ä¸ªé¢„æµ‹ä»»åŠ¡çš„æŸå¤±å‡½æ•°ä¸ºï¼š æ€»æŸå¤±å‡½æ•°ä¸ºï¼š The Adaptively Weighted Loss Mechanism.è¯¥æ–¹æ³•çš„åŠ¨æœºï¼š åœ¨çŸ¥è¯†å›¾è°±ä¸­çš„ä¸‰å…ƒç»„æœ‰4ç§ç±»å‹ï¼š1-TO-1, 1-TO-M, M-TO-1 and M-TO-Mã€‚æ‰€ä»¥é¢„æµ‹åœ¨è®­ç»ƒé›†ä¸­å…·æœ‰çš„æœ‰æ•ˆå®ä½“/å…³ç³»è¶Šå¤šï¼Œå®ƒå°±è¶Šä¸ç¡®å®šã€‚æ‰€ä»¥ä½œè€…å°†å¯¹åº”äºå¤´éƒ¨å®ä½“é¢„æµ‹ï¼Œå…³ç³»é¢„æµ‹å’Œå°¾éƒ¨å®ä½“é¢„æµ‹çš„æŸå¤±çš„æƒé‡ä¸æœ‰æ•ˆå®ä½“çš„æ•°é‡ç›¸å…³è”ã€‚ å› ä¸ºå…³ç³»é¢„æµ‹æ¯”å®ä½“é¢„æµ‹æ›´åŠ å®¹æ˜“ã€‚æ‰€ä»¥ä½œè€…åŠ å¤§å¯¹å¤´å°¾å®ä½“çš„é”™è¯¯é¢„æµ‹çš„æƒ©ç½šã€‚ æ‰€ä»¥ä½œè€…å¾—åˆ°æ–°çš„æŸå¤±å‡½æ•°ï¼š æ€»æŸå¤±å‡½æ•°å˜ä¸ºï¼š THE SENN+METHODä½œè€…ç›¸ä¿¡å¯ä»¥è¿›ä¸€æ­¥åˆ©ç”¨å…³ç³»é¢„æµ‹çš„ç›¸å½“å¥½çš„æ€§èƒ½æ¥è¾…åŠ©æµ‹è¯•è¿‡ç¨‹ä¸­çš„å¤´éƒ¨å’Œå°¾éƒ¨å®ä½“é¢„æµ‹ã€‚ ç»™å®šå¤´éƒ¨é¢„æµ‹ä»»åŠ¡ï¼ˆï¼Ÿï¼Œrï¼Œtï¼‰å¹¶å‡è®¾hæ˜¯æœ‰æ•ˆçš„å¤´éƒ¨å®ä½“ã€‚ å¦‚æœæˆ‘ä»¬é‡‡ç”¨SENNæ–¹æ³•æ¥é¢„æµ‹hå’Œtä¹‹é—´çš„å…³ç³»ï¼Œå³æ‰§è¡Œå…³ç³»é¢„æµ‹ä»»åŠ¡ï¼ˆhï¼Œï¼Ÿï¼Œtï¼‰ï¼Œåˆ™å…³ç³»ræœ€æœ‰å¯èƒ½å…·æœ‰ é¢„æµ‹æ ‡ç­¾é«˜äºå…¶ä»–å…³ç³»ï¼Œå› æ­¤åº”æ’åé«˜äºå…¶ä»–å…³ç³»ã€‚ å…¶ä¸­Valueï¼ˆxï¼Œrï¼‰è¿”å›å¯¹åº”äºå…³ç³»rçš„å‘é‡xçš„æ¡ç›®; Rankï¼ˆxï¼Œrï¼‰ä»¥é™åºè¿”å›å¯¹åº”äºå…³ç³»rçš„å‘é‡xçš„æ¡ç›®çš„ç­‰çº§ã€‚ æœ€åSENN+ç§é¢„æµ‹æ ‡ç­¾ä¸ºï¼š å…¶ä¸­ EXPERIMENTSDatasets Entity Prediction Relation Prediction è®ºæ–‡è¿˜è¿›è¡Œäº†å…±äº«åµŒå…¥å’Œè‡ªé€‚åº”æƒé‡æŸå¤±æœºåˆ¶æœ‰æ•ˆæ€§çš„éªŒè¯ã€‚ å‚è€ƒé“¾æ¥ http://blog.openkg.cn/%E8%AE%BA%E6%96%87%E6%B5%85%E5%B0%9D-%E9%9D%A2%E5%90%91%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E8%A1%A5%E5%85%A8%E7%9A%84%E5%85%B1%E4%BA%AB%E5%B5%8C%E5%85%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>çŸ¥è¯†å›¾è°±</tag>
        <tag>ç¥ç»ç½‘ç»œ</tag>
        <tag>çŸ¥è¯†å›¾è°±è¡¥å…¨</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ã€ŠBootstrapping Entity Alignment with Knowledge Graph Embeddingã€‹é˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FBootstrapping%20Entity%20Alignment%20with%20Knowledge%20Graph%20Embedding%2F</url>
    <content type="text"><![CDATA[è®ºæ–‡ä¸‹è½½åœ°å€ï¼Œé‡‡ç”¨äº†bootstrappingæ–¹æ³•æ¥è§£å†³ç¼ºä¹è®­ç»ƒæ•°æ®çš„è¿‡ç¨‹ï¼Œæå‡ºäº†æˆªæ–­å‡åŒ€è´Ÿé‡‡æ ·æ¥æé«˜è´Ÿæ ·ä¾‹å¯¹äºç›®æ ‡å‡½æ•°çš„è´¡çŒ®ï¼Œé‡‡ç”¨åŸºäºé™åˆ¶çš„ç›®æ ‡å‡½æ•°æ¥æŒ‰éœ€è°ƒæ•´æ­£è´Ÿæ ·ä¾‹çš„å¾—åˆ†ã€‚ åŸºäºåµŒå…¥çš„å®ä½“å¯¹é½å°†ä¸åŒçš„çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰è¡¨ç¤ºä¸ºä½ç»´åµŒå…¥ï¼Œå¹¶é€šè¿‡æµ‹é‡å®ä½“åµŒå…¥ä¹‹é—´çš„ç›¸ä¼¼æ€§æ¥æŸ¥æ‰¾å®ä½“å¯¹é½ã€‚å…¶ä¸­ï¼Œå¤§é‡æ–¹æ³•æ‰€é¢ä¸´çš„ä¸€ä¸ªæŒ‘æˆ˜æ˜¯ï¼šç¼ºä¹è¶³å¤Ÿçš„å…ˆå‰å¯¹é½ä½œä¸ºæ ‡è®°çš„è®­ç»ƒæ•°æ®ã€‚ è´¡çŒ® ä½œè€…æŠŠå®ä½“å¯¹é½å»ºæ¨¡ä¸ºä¸€ä¸ªåˆ†ç±»é—®é¢˜ï¼Œå…¶åŸºäºKGåµŒå…¥æ¥å¯»æ±‚æœ€å¤§åŒ–æ‰€æœ‰æ ‡è®°å’Œæœªæ ‡è®°çš„å®ä½“å¯¹é½å¯èƒ½æ€§ å¯¹äºé¢å‘å¯¹é½çš„KGåµŒå…¥ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºé™åˆ¶çš„ç›®æ ‡å‡½æ•°ï¼›ä¸ºäº†å¯¹ä¸å¤ªå¯èƒ½åŒºåˆ†çš„è´Ÿä¸‰å…ƒç»„è¿›è¡ŒæŠ½æ ·ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æˆªæ–­å‡åŒ€çš„è´ŸæŠ½æ ·æ–¹æ³•ã€‚ ä½œè€…æå‡ºäº†ä¸€ä¸ªè‡ªä¸¾è¿‡ç¨‹ï¼ˆbootstrappingï¼‰æ¥å…‹æœç¼ºä¹è¶³å¤Ÿè®­ç»ƒæ•°æ®ï¼Œé€šè¿‡æ ‡è®°å¯èƒ½çš„å¯¹é½å¹¶è¿­ä»£åœ°å°†å…¶æ·»åŠ åˆ°è®­ç»ƒæ•°æ®ä¸­æ¥æ›´æ–°é¢å‘å¯¹é½çš„åµŒå…¥ã€‚ ä½œè€…åœ¨ä¸‰ä¸ªè·¨è¯­è¨€å’Œä¸¤ä¸ªå¤§å‹æ•°æ®é›†ä¸Šè¯„ä¼°äº†æ‰€æå‡ºçš„æ–¹æ³•ï¼Œè¡¨æ˜æ‰€æå‡ºçš„æ–¹æ³•æ˜æ˜¾ä¼˜äºä¸‰ç§æœ€å…ˆè¿›çš„å®ä½“å¯¹é½æ–¹æ³•ã€‚ é—®é¢˜æè¿°æœ€å¤§ä¼¼ç„¶å‡†åˆ™æŒ‡å¯¼é€‰æ‹©å®ç°æœ€é«˜å¯¹é½å¯èƒ½æ€§çš„æœ€ä½³Î¸ å…¶ä¸­ï¼ŒL_xä»£è¡¨å®ä½“xçš„çœŸå®æ ‡ç­¾ï¼Œ1_[]æ˜¯ä¸€ä¸ªæŒ‡ç¤ºå‡½æ•°ï¼Œè¡¨ç¤ºç»™å®šå‘½é¢˜çš„çœŸå€¼ï¼ˆ0æˆ–1ï¼‰ã€‚ä½†æ˜¯å¯¹äºæ²¡æœ‰æ ‡ç­¾çš„å®ä½“ï¼Œæƒ³è¦é€šè¿‡ä¸Šè¿°æ¥å¾—åˆ°thetaå°±å¾ˆå›°éš¾ã€‚ æ¨¡å‹é¢å‘å¯¹é½çš„KGåµŒå…¥ä½œè€…æå‡ºäº†ä¸€ä¸ªç›®æ ‡å‡½æ•°ï¼š è¯¥ç›®æ ‡å‡½æ•°æœ‰ä¸¤ä¸ªæœŸæœ›çš„å±æ€§ï¼š é¢„æœŸæ­£ä¸‰å…ƒç»„å¾—åˆ†è¾ƒä½ï¼Œè€Œè´Ÿä¸‰å…ƒç»„å¾—åˆ†è¾ƒé«˜ã€‚ä¾‹å¦‚f(r)&lt;= r_1 å¹¶ä¸” f(râ€™)&gt;=r_2ï¼Œè®¾ç½®æ—¶r_2&gt;r_1,ä¸”r_1æ˜¯ä¸€ä¸ªå°çš„æ­£å€¼ã€‚ ä»ç„¶å¯ä»¥å¾—åˆ°f(râ€™)-f(r)&gt;=r_2 - r_1ï¼Œè¿™è¡¨æ˜æ‰€æå‡ºçš„ç›®æ ‡å‡½æ•°ä»ç„¶ä¿ç•™äº†åŸºäºè¾¹é™…æ’åºæŸå¤±çš„ç‰¹å¾ã€‚ æˆªæ–­å‡åŒ€è´Ÿé‡‡æ ·å¦‚æœæ ·ä¾‹å¤ªå®¹æ˜“åŒºåˆ†ï¼Œé‚£ä¹ˆå¯¹æ•´ä¸ªçš„åµŒå…¥å­¦ä¹ çš„è´¡çŒ®ä¼šå¾ˆå°ã€‚ æ‰€ä»¥ï¼Œä½œè€…é‡‡ç”¨åœ¨åµŒå…¥ç©ºé—´ä¸­sæœ€è¿‘çš„é‚»å±…ä½œä¸ºå€™é€‰é›†ï¼Œå‰”é™¤é‚£äº›å’Œå®ä½“xç›¸ä¼¼åº¦è¿‡ä½çš„æ•°æ®ã€‚ å¼•å¯¼å¯¹é½ï¼ˆBootstrapping Alignmentï¼‰ä½œè€…è¿­ä»£åœ°å°†å¯èƒ½çš„å¯¹é½æ ‡è®°ä½œä¸ºè®­ç»ƒæ•°æ®ï¼Œå¹¶ä½¿ç”¨å®ƒæ¥è¿›ä¸€æ­¥æ”¹è¿›å®ä½“åµŒå…¥å’Œå¯¹é½ã€‚ å¯èƒ½çš„å¯¹é½æ ‡ç­¾å’Œç¼–è¾‘ä½œè€…ä¸ºäº†å®ç°æœ€å¤§åŒ–å¯¹é½å¯èƒ½æ€§å¹¶éµå®ˆä¸€å¯¹ä¸€å¯¹é½çº¦æŸï¼Œæå‡ºä»¥ä¸‹ä¼˜åŒ–é—®é¢˜æ¥æ ‡è®°ç¬¬tæ¬¡è¿­ä»£ï¼š Yâ€™_x = {y|y âˆˆ Yâ€™ and Ï€(y|x; Î˜^(t)) &gt; Î³3}ä»£è¡¨æ ‡ç­¾xçš„å€™é€‰é›†ï¼›Ïˆ^(t)(Â·)æ˜¯ä¸€ä¸ªæŒ‡ç¤ºå‡½æ•°ï¼Œåªæœ‰å½“xåœ¨ç¬¬tæ¬¡è¿­ä»£æ—¶æ ‡ç­¾ä¸ºyæ—¶ä¸º1ï¼Œå…¶å®ƒæƒ…å†µä¸º0ã€‚ä¸¤ä¸ªé™åˆ¶æ¡ä»¶ä¿è¯äº†ä¸€å¯¹ä¸€çš„æ ‡ç­¾ã€‚è¿™æ—¶å¾—åˆ°äº†ä¸€ä¸ªæ–°çš„æ ‡ç­¾å¯¹é½ï¼š ä¸ºäº†æé«˜æ ‡ç­¾è´¨é‡å¹¶æ»¡è¶³ä¸€å¯¹ä¸€çš„å¯¹é½çº¦æŸï¼Œåœ¨è‡ªä¸¾è¿‡ç¨‹ä¸­ï¼Œä¸€æ—¦è¢«æ ‡è®°çš„å®ä½“å¯ä»¥åœ¨éšåçš„æ ‡è®°ä¸­é‡æ–°æ ‡è®°æˆ–å˜ä¸ºæœªæ ‡è®°çš„å®ä½“ã€‚ å½“å‘ç”Ÿä¸¤ä¸ªæ ‡ç­¾å†²çªæ—¶ï¼Œæˆ‘ä»¬é€šè¿‡è®¡ç®—ä¸‹é¢çš„ä¼¼ç„¶å·®å¼‚æ¥ç¡®å®šä¿ç•™å“ªä¸ªï¼š å½“è¯¥å€¼å¤§äº0è¯´æ˜å‰è€…å…·æœ‰æ›´å¤§çš„å¯¹é½æ¦‚ç‡ã€‚ ä»æ•´ä½“è§’åº¦å­¦ä¹ ä¸ºäº†è·å¾—æ ‡è®°å’Œæœªæ ‡è®°å®ä½“çš„æ•´ç†è§‚å¯Ÿï¼Œä½œè€…å®šä¹‰äº†æ¦‚ç‡åˆ†å¸ƒÏ†xæ¥æè¿°æ‰€æœ‰xå¯èƒ½çš„æ¦‚ç‡åˆ†å¸ƒã€‚ ç”±æ­¤ï¼Œä½œè€…å¾—åˆ°äº†æœ€å°åŒ–ä¸‹é¢çš„ä¼¼ç„¶å‡½æ•°æ¥å¾—åˆ°Î˜ï¼š å› ä¸ºï¼ŒåµŒå…¥ä¸ä»…åº”è¯¥æ•è·å¯¹é½å¯èƒ½æ€§ï¼Œè¿˜åº”è¯¥æ¨¡æ‹ŸKGçš„è¯­ä¹‰ï¼Œæ‰€ä»¥ä½œè€…æœ€åå®šä¹‰è”åˆç›®æ ‡å‡½æ•°ï¼š å®éªŒæ•°æ®é›† DBP15K [Sun et al., 2017]åŒ…å«ä¸‰ä¸ªè·¨è¯­è¨€æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†æ˜¯ä»DBpediaçš„å¤šè¯­è¨€ç‰ˆæœ¬æ„å»ºçš„ã€‚DBPZH-EN(Chinese to English), DBPJA-EN(Japanese to English) and DBPFR-EN(French to English)æ¯ä¸ªæ•°æ®é›†åŒ…å«15ï¼Œ000ä¸ªå‚è€ƒå®ä½“å¯¹é½ã€‚ DWY100KåŒ…å«ä»DBpediaï¼ŒWikidataå’ŒYAGO3ä¸­æå–çš„ä¸¤ä¸ªå¤§å‹æ•°æ®é›†ï¼Œç”±DBP-WDå’ŒDBP-YGè¡¨ç¤ºã€‚ æ¯ä¸ªæ•°æ®é›†éƒ½æœ‰10ä¸‡ä¸ªå‚è€ƒå®ä½“å¯¹é½ å®éªŒè®¾ç½®ä½œè€…é€‰å–äº†ä¸‰ç§æœ€å…ˆè¿›çš„åŸºäºåµŒå…¥çš„æ–¹æ³•æ¥å®ç°å®ä½“å¯¹é½ã€‚ MTransE [Chen et al., 2017]ï¼Œé€‰å–äº†ç¬¬å››ç§å˜ä½“ï¼ˆè¡¨ç°æœ€ä½³ï¼‰ã€‚ IPTransE[Zhu et al., 2017]æ˜¯ä¸€ä¸ªè¿­ä»£æ–¹æ³• JAPE [Sun et al., 2017]ç»“åˆäº†å®ä½“å¯¹é½çš„å…³ç³»å’Œå±æ€§åµŒå…¥ AlignEé¢å‘å¯¹é½çš„KGåµŒå…¥æ¨¡å‹çš„å®ç°ï¼Œå…·æœ‰æˆªæ–­çš„å‡åŒ€è´Ÿé‡‡æ ·å’Œå‚æ•°äº¤æ¢ï¼Œå®ƒä¼˜åŒ–äº†å…¬å¼ï¼ˆ3ï¼‰ï¼Œä½†æ˜¯æ²¡æœ‰è‡ªä¸¾ å®éªŒç»“æœè¡¨2ä¸­æˆ‘ä»¬è§‚å¯Ÿåˆ°AlignEæ˜æ˜¾ä¼˜äºMTransEï¼ŒIPTransEå’ŒJAPEï¼Œå› ä¸ºå®ƒé‡‡ç”¨é¢å‘å¯¹é½çš„åµŒå…¥ã€‚è€ŒBootEAæ˜¾ç€æ”¹å–„äº†AlignEçš„ç»“æœï¼Œè¡¨æ˜äº†è‡ªä¸¾çš„è‰¯å¥½æ€§èƒ½æ˜¯ç”±äºå…¶èƒ½å¤Ÿå‡†ç¡®åœ°å°†å¯èƒ½çš„å¯¹é½æ ‡è®°ä¸ºè®­ç»ƒæ•°æ®ã€‚ åˆ†ææˆªæ–­å‡åŒ€è´ŸæŠ½æ ·çš„æœ‰æ•ˆæ€§ä»å›¾ä¸­å¯ä»¥çœ‹å‡ºï¼Œä¸MTransEï¼ŒIPTransEå’ŒJAPEç›¸æ¯”ï¼Œå…·æœ‰å‡åŒ€è´Ÿé‡‡æ ·çš„AlignEä»ç„¶è·å¾—äº†ä¼˜å¼‚çš„ç»“æœï¼Œå¹¶ä¸”éšç€é‡‡æ ·ç¦»xæ›´åŠ æ¥è¿‘ï¼Œæ•ˆæœå‘ˆä¸Šå‡è¶‹åŠ¿ã€‚ å¯èƒ½å¯¹é½çš„å‡†ç¡®æ€§å¯ä»¥çœ‹åˆ°ä»¥ä½œè€…çš„æ ‡è®°æ–¹æ³•S3è¡¨ç°æœ€ä½³ã€‚è¿™äº›ç»“æœè¯å®ä½œè€…çš„æ–¹æ³•å¯ä»¥ä¿è¯ä½¿ç”¨æœªæ ‡è®°æ•°æ®çš„å®‰å…¨æ€§ã€‚ å¯¹å…ˆå‰å¯¹å‡†æ¯”ä¾‹çš„æ•æ„Ÿæ€§ æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œéšç€æ¯”ä¾‹çš„å¢åŠ ï¼Œæ‰€æœ‰äº”ä¸ªæ•°æ®é›†çš„ç»“æœéƒ½å˜å¾—æ›´å¥½ï¼Œå› ä¸ºæ›´å¤šçš„å…ˆå‰å¯¹é½å¯ä»¥æä¾›æ›´å¤šä¿¡æ¯æ¥å¯¹é½ä¸¤ä¸ªKGã€‚ F1-score w.r.t. å…³ç³»ä¸‰å…ƒæ•°çš„åˆ†å¸ƒ BootEAåœ¨æ‰€æœ‰æ—¶é—´é—´éš”éƒ½ä¼˜äºMTransEï¼ŒIPTransEå’ŒJAPEï¼Œè¿™å†æ¬¡è¯å®äº†BootEAçš„æœ‰æ•ˆæ€§ã€‚è€Œä¸”BootEAå¯ä»¥åœ¨ç¨€ç–æ•°æ®ä¸Šå–å¾—æœ‰å¸Œæœ›çš„ç»“æœã€‚]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>çŸ¥è¯†å›¾è°±</tag>
        <tag>çŸ¥è¯†å›¾è°±åµŒå…¥</tag>
        <tag>å®ä½“å¯¹é½</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ã€ŠEntity Alignment between Knowledge Graphs Using Attribute Embeddingsã€‹é˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FEntity%20Alignment%20between%20Knowledge%20Graphs%20Using%20Attribute%20Embeddings%2F</url>
    <content type="text"><![CDATA[è®ºæ–‡ä¸‹è½½åœ°å€ï¼ŒçŸ¥è¯†å›¾ä¹‹é—´çš„å®ä½“å¯¹é½çš„ä»»åŠ¡æ—¨åœ¨åœ¨ä»£è¡¨ç›¸åŒç°å®ä¸–ç•Œå®ä½“çš„ä¸¤ä¸ªçŸ¥è¯†å›¾ä¸­æ‰¾åˆ°å®ä½“ã€‚æœ¬æ–‡æœ€ä¸»è¦å°±æ˜¯æå‡ºäº†å±æ€§å­—ç¬¦åµŒå…¥(attribute character embeddings)çš„æ–¹æ³•ã€‚ Abstractæˆ‘ä»¬çš„æ¨¡å‹åˆ©ç”¨çŸ¥è¯†å›¾ä¸­å­˜åœ¨çš„å¤§é‡å±æ€§ä¸‰å…ƒç»„(attribute triples)å¹¶ç”Ÿæˆå±æ€§å­—ç¬¦åµŒå…¥ã€‚ å±æ€§å­—ç¬¦åµŒå…¥(attribute character embeddings)é€šè¿‡åŸºäºå®ä½“çš„å±æ€§è®¡ç®—å®ä½“ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œå°†å®ä½“åµŒå…¥ä»ä¸¤ä¸ªçŸ¥è¯†å›¾ç§»ä½åˆ°åŒä¸€ç©ºé—´ä¸­ã€‚æˆ‘ä»¬ä½¿ç”¨ä¼ é€’è§„åˆ™æ¥è¿›ä¸€æ­¥ä¸°å¯Œå®ä½“çš„å±æ€§æ•°é‡ä»¥å¢å¼ºå±æ€§å­—ç¬¦åµŒå…¥ã€‚ Contribution æå‡ºäº†ä¸¤ä¸ªKGä¹‹é—´å®ä½“å¯¹é½çš„æ¡†æ¶ï¼Œå®ƒç”±è°“è¯å¯¹é½æ¨¡å—ï¼ŒåµŒå…¥å­¦ä¹ æ¨¡å—å’Œå®ä½“å¯¹é½æ¨¡å—ç»„æˆã€‚ æå‡ºäº†ä¸€ç§æ–°é¢–çš„åµŒå…¥æ¨¡å‹ï¼Œå®ƒå°†å®ä½“åµŒå…¥ä¸å±æ€§åµŒå…¥é›†æˆåœ¨ä¸€èµ·ï¼Œä»¥ä¾¿ä¸ºä¸¤ä¸ªKGå­¦ä¹ ç»Ÿä¸€çš„åµŒå…¥ç©ºé—´ã€‚ æˆ‘ä»¬åœ¨ä¸‰ä¸ªçœŸæ­£çš„KGå¯¹ä¸Šè¯„ä¼°å»ºè®®çš„æ¨¡å‹ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å®ä½“å¯¹é½ä»»åŠ¡ä¸Šå§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œhits@1è¶…è¿‡50ï¼…ã€‚ æ¨¡å‹æ¨¡å‹æ€»è§ˆpredicate alignment, embedding learning, and entity alignment Predicate Alignmentè°“è¯å¯¹é½æ¨¡å—é€šè¿‡ä½¿ç”¨ç»Ÿä¸€çš„å‘½åæ–¹æ¡ˆé‡å‘½åä¸¤ä¸ªKGçš„è°“è¯æ¥åˆå¹¶ä¸¤ä¸ªKGï¼Œä»¥ä¾¿ä¸ºå…³ç³»åµŒå…¥æä¾›ç»Ÿä¸€çš„å‘é‡ç©ºé—´ã€‚dbp:bornIn vs. yago:wasBornIn ç»Ÿä¸€å‘½åä¸º :bornInã€‚ ä¸ºäº†æ‰¾åˆ°éƒ¨åˆ†åŒ¹é…çš„è°“è¯ï¼Œä½œè€…è®¡ç®—è°“è¯URIçš„æœ€åéƒ¨åˆ†çš„ç¼–è¾‘è·ç¦»ï¼ˆä¾‹å¦‚ï¼ŒbornInä¸wasBornInï¼‰å¹¶å°†0.95è®¾ç½®ä¸ºç›¸ä¼¼æ€§é˜ˆå€¼ã€‚ Embedding LearningStructure Embeddingä½œè€…é‡‡ç”¨TransEæ¥å­¦ä¹ å¯¹äºå®ä½“çš„ç»“æ„åµŒå…¥ã€‚ä¸TransEä¸åŒçš„æ˜¯ï¼Œæ¨¡å‹å¸Œæœ›æ›´å…³æ³¨å·²å¯¹é½çš„ä¸‰å…ƒç»„ï¼Œä¹Ÿå°±æ˜¯åŒ…å«å¯¹é½è°“è¯çš„ä¸‰å…ƒç»„ã€‚æ¨¡å‹é€šè¿‡æ·»åŠ æƒé‡æ¥å®ç°è¿™ä¸€ç›®çš„ã€‚Structure embeddingçš„ç›®æ ‡å‡½æ•°å¦‚ä¸‹ï¼š count(r)æ˜¯å…³ç³»rå‡ºç°çš„æ•°é‡ã€‚ Attribute Character Embeddingå¯¹äºå±æ€§å­—ç¬¦åµŒå…¥ï¼Œä¹Ÿå‚è€ƒTransEçš„æ€æƒ³ï¼Œå°†è°“è¯rè§£é‡Šä¸ºä»å¤´éƒ¨å®ä½“håˆ°å±æ€§açš„è½¬æ¢ã€‚ä½†æ˜¯ï¼Œç›¸åŒçš„å±æ€§aå¯ä»¥åœ¨ä¸¤ä¸ªKGä¸­ä»¥ä¸åŒçš„å½¢å¼å‡ºç°ï¼Œä¾‹å¦‚50.9989å¯¹50.9988888889ä½œä¸ºå®ä½“çš„çº¬åº¦; â€œBarack Obamaâ€ä¸â€œBarack Hussein Obamaâ€ä½œä¸ºäººåç­‰ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºä½¿ç”¨ç»„åˆå‡½æ•°å¯¹å±æ€§å€¼è¿›è¡Œç¼–ç ï¼Œå¹¶å°†å±æ€§ä¸‰å…ƒç»„ä¸­æ¯ä¸ªå…ƒç´ çš„å…³ç³»å®šä¹‰ä¸ºh +râ‰ˆfaï¼ˆaï¼‰ã€‚ è¿™é‡Œï¼Œfaï¼ˆaï¼‰æ˜¯ç»„åˆå‡½æ•°ï¼Œaæ˜¯å±æ€§å€¼a = {c1ï¼Œc2ï¼Œc3ï¼Œâ€¦ï¼Œct}çš„å­—ç¬¦åºåˆ—ã€‚ ç»„åˆå‡½æ•°å°†å±æ€§å€¼ç¼–ç ä¸ºå•ä¸ªå‘é‡ï¼Œå¹¶å°†ç±»ä¼¼çš„å±æ€§å€¼æ˜ å°„åˆ°ç±»ä¼¼çš„å‘é‡è¡¨ç¤ºã€‚ ä½œè€…å®šä¹‰äº†ä¸‰ä¸ªç»„æˆå‡½æ•°å¦‚ä¸‹ã€‚ Sum compositional function (SUM)å­˜åœ¨é—®é¢˜ï¼šåŒ…å«ç›¸åŒå­—ç¬¦ä¸åŒé¡ºåºçš„å±æ€§å€¼ä¼šæœ‰ç›¸åŒçš„å‘é‡è¡¨ç¤º LSTM-based compositional function (LSTM). N-gram-based compositional function (N-gram) æœ€åattribute character embeddingç›®æ ‡å‡½æ•°ï¼š Joint Learning of Structure Embedding and Attribute Character Embeddingä½œè€…ä½¿ç”¨å±æ€§å­—ç¬¦åµŒå…¥é€šè¿‡æœ€å°åŒ–ä»¥ä¸‹ç›®æ ‡å‡½æ•°å°†ç»“æ„åµŒå…¥ç§»åŠ¨åˆ°ç›¸åŒçš„å‘é‡ç©ºé—´ï¼š æœ¬æ–‡æ•´ä½“æŸå¤±å‡½æ•°ï¼š Entity Alignmentåœ¨ç»è¿‡ä¸Šè¿°è®­ç»ƒè¿‡ç¨‹ä¹‹åï¼Œæ¥è‡ªä¸åŒKGçš„ç›¸ä¼¼çš„å®ä½“å°†ä¼šæœ‰ç›¸ä¼¼çš„å‘é‡è¡¨ç¤ºï¼Œå› æ­¤å¯é€šè¿‡ è·å¾—æ½œåœ¨å®ä½“å¯¹é½å¯¹ã€‚æ­¤å¤–ï¼Œæ¨¡å‹è®¾å®šç›¸ä¼¼åº¦é˜ˆå€¼æ¥è¿‡æ»¤æ½œåœ¨å®ä½“å¯¹é½å¯¹ï¼Œå¾—åˆ°æœ€ç»ˆçš„å¯¹é½ç»“æœã€‚ Triple Enrichment via Transitivity Ruleä½œè€…åˆ©ç”¨ä¸€é˜¶é€»è¾‘ä¼ é€’å…³ç³»æ¥ä¸°å¯Œä¸‰å…ƒç»„ã€‚å³ï¼šå­˜åœ¨å’Œåˆ™å¯ä»¥æ¨ç†å‡ºh_1+ (r_1.r_2) â‰ˆ t_2 Databaseæœ¬æ–‡ä» DBpedia (DBP)ã€LinkedGeoData (LGD)ã€Geonames (GEO) å’Œ YAGO å››ä¸ª KG ä¸­æŠ½å–æ„å»ºäº†ä¸‰ä¸ªæ•°æ®é›†ï¼Œåˆ†åˆ«æ˜¯DBP-LGDã€DBP-GEOå’ŒDBP-YAGOã€‚å…·ä½“çš„æ•°æ®ç»Ÿè®¡å¦‚ä¸‹ï¼š ExperimentsEntity Alignment Resultsæœ¬æ–‡å¯¹æ¯”äº†ä¸‰ä¸ªç›¸å…³çš„æ¨¡å‹ï¼Œåˆ†åˆ«æ˜¯ TransEã€MTransE å’Œ JAPEã€‚è¯•éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ¨¡å‹åœ¨å®ä½“å¯¹é½ä»»åŠ¡ä¸Šå–å¾—äº†å…¨é¢çš„è¾ƒå¤§çš„æå‡ï¼Œåœ¨ä¸‰ç§ç»„åˆå‡½æ•°ä¸­ï¼ŒN-gramå‡½æ•°çš„ä¼˜åŠ¿è¾ƒä¸ºæ˜æ˜¾ã€‚æ­¤å¤–ï¼ŒåŸºäºä¼ é€’è§„åˆ™çš„ä¸‰å…ƒç»„ä¸°å¯Œæ¨¡å‹å¯¹ç»“æœä¹Ÿæœ‰ä¸€å®šçš„æå‡ã€‚å…·ä½“ç»“æœå¦‚ä¸‹ Rule-based Entity Alignment Resultsä¸ºäº†è¿›ä¸€æ­¥è¡¡é‡ attribute character embedding æ•è·å®ä½“é—´ç›¸ä¼¼ä¿¡æ¯çš„èƒ½åŠ›ï¼Œæœ¬æ–‡è®¾è®¡äº†åŸºäºè§„åˆ™çš„å®ä½“å¯¹é½æ¨¡å‹ã€‚æœ¬å®éªŒå¯¹æ¯”äº†ä¸‰ç§ä¸åŒçš„æ¨¡å‹ï¼šä»¥labelçš„å­—ç¬¦ä¸²ç›¸ä¼¼åº¦ä½œä¸ºåŸºç¡€æ¨¡å‹ï¼›é’ˆå¯¹æ•°æ®é›†ç‰¹ç‚¹ï¼Œåœ¨åŸºç¡€æ¨¡å‹çš„åŸºç¡€ä¹‹ä¸Šå¢åŠ äº†åæ ‡å±æ€§ï¼Œä»¥æ­¤ä½œä¸ºç¬¬äºŒä¸ªæ¨¡å‹ï¼›ç¬¬ä¸‰ä¸ªæ¨¡å‹æ˜¯æŠŠæœ¬æ–‡æå‡ºçš„æ¨¡å‹ä½œä¸ºé™„åŠ æ¨¡å‹ï¼Œä¸åŸºç¡€æ¨¡å‹ç›¸ç»“åˆã€‚å…·ä½“ç»“æœå¦‚ä¸‹ï¼š KG Completion Resultsæœ¬æ–‡è¿˜åœ¨KGè¡¥å…¨ä»»åŠ¡ä¸ŠéªŒè¯äº†æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚æ¨¡å‹ä¸»è¦æµ‹è¯•äº†é“¾æ¥é¢„æµ‹å’Œä¸‰å…ƒç»„åˆ†ç±»ä¸¤ä¸ªæ ‡å‡†ä»»åŠ¡ï¼Œåœ¨è¿™ä¸¤ä¸ªä»»åŠ¡ä¸­ï¼Œæ¨¡å‹ä¹Ÿå–å¾—äº†ä¸é”™çš„æ•ˆæœã€‚å…·ä½“ç»“æœå¦‚ä¸‹ï¼š]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>çŸ¥è¯†å›¾è°±</tag>
        <tag>çŸ¥è¯†å›¾è°±åµŒå…¥</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ã€ŠRelNN A Deep Neural Model for Relational Learningã€‹é˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FRelNN%20A%20Deep%20Neural%20Model%20for%20Relational%20Learning%2F</url>
    <content type="text"><![CDATA[è®ºæ–‡ä¸‹è½½åœ°å€ï¼Œè¿™ç¯‡æ–‡ç« ç›¸å½“äºç»“åˆäº†ç»Ÿè®¡å­¦ä¹ å’Œæ·±åº¦ç¥ç»ç½‘ç»œã€‚é‡Œé¢æœ‰äº›å…¬å¼æ²¡æœ‰ç†è§£ï¼Œåº”è¯¥æ˜¯æœ‰è®¸å¤šå…ˆå‰è®ºæ–‡éœ€è¦é˜…è¯»ã€‚ä½†æ˜¯æœ¬ç¯‡è®ºæ–‡æ‰©å±•äº†æ€è·¯å¦‚ä½•ç»“åˆç»Ÿè®¡å­¦å’Œæ·±åº¦å­¦ä¹ ï¼Œå¹¶ä¸”åŸºäºå…¶ä½™æ•°æ®æ¥é¢„æµ‹ä¸€ä¸ªç±»ä¸­å¯¹è±¡çš„ä¸€ä¸ªå±æ€§ï¼Œæƒ³æ³•ä¹Ÿæ¯”è¾ƒå¥½ã€‚ Introductionä½œè€…ä¸»è¦é›†ä¸­äºåŸºäºå…¶ä½™æ•°æ®æ¥é¢„æµ‹ä¸€ä¸ªç±»ä¸­å¯¹è±¡çš„ä¸€ä¸ªå±æ€§ã€‚ Challengeå½“ç±»ä¸­æ¯ä¸ªå¯¹è±¡çš„å±æ€§ä¾èµ–äºä¸åŒæ•°é‡çš„å…¶ä»–å¯¹è±¡çš„å±æ€§å’Œå…³ç³»æ—¶ï¼Œæ­¤é—®é¢˜å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ åœ¨StarAIç¤¾åŒºä¸­ï¼Œæ­¤é—®é¢˜ç§°ä¸ºèšåˆï¼ˆaggregationï¼‰ã€‚ Relational Logistic Regression and Markov Logic NetworksStarAIæ¨¡å‹æ—¨åœ¨æ¨¡æ‹Ÿå¯¹è±¡ä¹‹é—´å…³ç³»çš„æ¦‚ç‡ã€‚ Relational logistic regression (RLR) (Kazemi et al. 2014)å®šä¹‰çš„æ¦‚ç‡å…¬å¼å¦‚ä¸‹ï¼š ä¸Šé¢å®šä¹‰çš„RLRæ¨¡å‹ä»…é€‚ç”¨äºå¸ƒå°”å€¼æˆ–å¤šå€¼çˆ¶é¡¹ã€‚ä½œè€…é‡‡ç”¨çš„æ˜¯è¿ç»­çš„åŸå­ï¼ˆcontinuous atomsï¼‰ï¼ˆFatemi, Kazemi, and Poole (2016)ï¼‰ Relational Neural Networksä½œè€…é€šè¿‡è®¾è®¡ç¥ç»ç½‘ç»œä¸­çº¿æ€§å±‚ï¼ˆLLï¼‰ï¼Œæ¿€æ´»å±‚ï¼ˆALï¼‰å’Œè¯¯å·®å±‚ï¼ˆELï¼‰çš„å…³ç³»å¯¹åº”ç‰©ï¼Œå¯¹å…·æœ‰åˆ†å±‚æ¶æ„çš„RLR / MLNæ¨¡å‹è¿›è¡Œç¼–ç ã€‚ å…³ç³»ç¥ç»ç½‘ç»œï¼ˆRelNNï¼‰æ˜¯åŒ…å«ä½œä¸ºå›¾å½¢å½¼æ­¤è¿æ¥çš„è‹¥å¹²RLLå’ŒRALçš„ç»“æ„ã€‚ Motivations for hidden layers ä½¿å–œæ¬¢çœ‹åŠ¨ä½œç”µå½±çš„äººæ•°å¢åŠ æ—¶ï¼Œç”·æ€§çš„æ¦‚ç‡å˜ä¸º[0, 1]é‡çš„ä»»ä½•æ•°å€¼ï¼Œä¸è‡³äºç›´æ¥å˜ä¸º0æˆ–è€…1ã€‚ å› æ­¤ï¼Œéšè—å±‚é€šè¿‡ä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ é€šç”¨è§„åˆ™å¹¶ç›¸åº”åœ°å¯¹å¯¹è±¡è¿›è¡Œåˆ†ç±»ï¼Œç„¶åä»¥ä¸åŒæ–¹å¼å¤„ç†ä¸åŒç±»åˆ«çš„å¯¹è±¡ï¼Œä»è€Œæé«˜äº†å»ºæ¨¡èƒ½åŠ›ã€‚ ä½¿ç”¨RLRè¡¨ç¤ºä¸åŒç±»å‹çš„ç°æœ‰æ˜¾å¼èšåˆå™¨ï¼Œç„¶è€Œæœ‰äº›æƒ…å†µéœ€è¦ä½¿ç”¨2ä¸ªRLLså’Œ2ä¸ªRALs Learning latent properties directlyå¯¹è±¡å¯èƒ½åŒ…å«æ— æ³•ä½¿ç”¨å¸¸è§„è§„åˆ™æŒ‡å®šçš„æ½œåœ¨å±æ€§ï¼Œä½†å¯ä»¥åœ¨è®­ç»ƒæœŸé—´ç›´æ¥ä»æ•°æ®ä¸­å­¦ä¹ ã€‚ è€ƒè™‘å›¾2ä¸­çš„æ¨¡å‹ï¼Œè®©Latentï¼ˆmï¼‰æˆä¸ºç”µå½±çš„æ•°å­—æ½œåœ¨å±æ€§ï¼Œå…¶å€¼å°†åœ¨è®­ç»ƒæœŸé—´å­¦ä¹ ã€‚ From ConvNet Primitives to RelNNsæˆ‘ä»¬è§£é‡Šä¸ºä»€ä¹ˆRelNNä¹Ÿå¯ä»¥è¢«è§†ä¸ºConvNetsçš„ä¸€ä¸ªå®ä¾‹ã€‚ ConvNetsçš„è¾“å…¥çŸ©é˜µä¸­çš„å•å…ƒï¼ˆä¾‹å¦‚ï¼Œå›¾åƒåƒç´ ï¼‰å…·æœ‰ç©ºé—´ç›¸å…³æ€§å’Œç©ºé—´å†—ä½™ï¼šå½¼æ­¤æ›´æ¥è¿‘çš„å•å…ƒæ¯”æ›´è¿œçš„å•å…ƒæ›´ä¾èµ–ã€‚ ä¾‹å¦‚ï¼Œå¦‚æœMè¡¨ç¤ºå›¾åƒçš„è¾“å…¥é€šé“ï¼Œåˆ™M [iï¼Œj]å’ŒM [i + 1ï¼Œj + 1]ä¹‹é—´çš„ä¾èµ–æ€§å¯èƒ½è¿œå¤§äºM [iï¼Œj]å’ŒM [iï¼Œj+20]ä¹‹é—´çš„ä¾èµ–æ€§ã€‚ å¯¹äºå…³ç³»æ•°æ®ï¼Œè¾“å…¥çŸ©é˜µä¸­çš„ä¾èµ–å…³ç³»ï¼ˆå…³ç³»ï¼‰æ˜¯ä¸åŒçš„ï¼šåŒä¸€è¡Œæˆ–åˆ—ä¸­çš„å•å…ƒï¼ˆå³åŒä¸€å¯¹è±¡çš„å…³ç³»ï¼‰å…·æœ‰æ¯”ä¸åŒè¡Œå’Œåˆ—ä¸­çš„å•å…ƒæ›´é«˜çš„ä¾èµ–æ€§ï¼ˆå³ä¸åŒå¯¹è±¡çš„å…³ç³»ï¼‰ã€‚ å› æ­¤ï¼Œä¸ºäº†ä½¿ConvNetsé€‚åº”å…³ç³»æ•°æ®ï¼Œæˆ‘ä»¬éœ€è¦çŸ¢é‡å½¢çŠ¶çš„è¿‡æ»¤å™¨ï¼Œè¿™äº›è¿‡æ»¤å™¨å¯¹è¡Œå’Œåˆ—äº¤æ¢æ˜¯ä¸å˜çš„ï¼Œå¹¶ä¸”æ›´å¥½åœ°æ•è·å…³ç³»ä¾èµ–æ€§å’Œå¯äº¤æ¢æ€§å‡è®¾ã€‚ DatasetsMovielens 1M dataset (Harper and Konstan 2015)ç¬¬ä¸€ä¸ªæ•°æ®é›†æ˜¯Movielens 1M dataset (Harper and Konstan 2015)ï¼Œå¿½ç•¥äº†å®é™…çš„è¯„çº§ï¼Œåªè€ƒè™‘ç”µå½±æ˜¯å¦è¢«è¯„çº§ï¼Œåªè€ƒè™‘åŠ¨ä½œå’Œæˆå‰§ç±»å‹ã€‚ PAKDD15è·å–åœ°å€ all Chinese and Mexican restaurants in Yelp dataset challengeè·å–åœ°å€ Empirical Resultsä½œè€…æå‡ºäº†ä¸‰ä¸ªé—®é¢˜æ¥è¿›è¡Œå®éªŒï¼š Q1ï¼šRelNNçš„æ€§èƒ½ä¸å…¶ä»–ä¼—æ‰€å‘¨çŸ¥çš„å…³ç³»å­¦ä¹ ç®—æ³•ç›¸æ¯”å¦‚ä½•ï¼Ÿ Q2ï¼šåŸºäºæ•°å­—å’Œè§„åˆ™çš„æ½œåœ¨å±æ€§å¦‚ä½•å½±å“RelNNçš„æ€§èƒ½?æ›´æ”¹äº†RelNNä¸­éšè—å›¾å±‚å’Œæ•°å­—æ½œåœ¨å±æ€§çš„æ•°é‡ï¼Œä»¥æŸ¥çœ‹å®ƒä»¬å¦‚ä½•å½±å“æ€§èƒ½ã€‚ è¯·æ³¨æ„ï¼Œæ·»åŠ å›¾å±‚åªä¼šæ·»åŠ ä¸€å®šæ•°é‡çš„å‚æ•°ï¼Œä½†æ·»åŠ kä¸ªæ•°å­—æ½œåœ¨å±æ€§ä¼šå¢åŠ k * |Î”m|å‚æ•°ã€‚ Q3ï¼šRelNNå¦‚ä½•æ¨æ–­å‡ºçœ‹ä¸è§çš„æ¡ˆä¾‹å¹¶è§£å†³æŒ‡å‘çš„è§„æ¨¡å¤§å°é—®é¢˜ ï¼ˆPoole et al.2014ï¼‰?ä½œè€…å®æ–½äº†ä¸¤ä¸ªå®éªŒï¼š æˆ‘ä»¬åœ¨å¤§é‡æ•°æ®ä¸­è®­ç»ƒä¸€ä¸ªRelNNï¼Œå¹¶åœ¨ä¸€å°æ•°æ®ä¸Šè¿›è¡Œæµ‹è¯•ï¼šè¯¥å®éªŒå¯ä»¥çœ‹ä½œæ¯ä¸ªæ¨¡å‹å—å†·å¯åŠ¨é—®é¢˜çš„ä¸¥é‡ç¨‹åº¦ ç„¶åæˆ‘ä»¬åœ¨ä¸€å°æ•°æ®ä¸Šè®­ç»ƒä¸€ä¸ªRelNNå¹¶åœ¨ä¸€å¤§æ•°æ®ä¸Šè¿›è¡Œæµ‹è¯•ï¼šå¯ä»¥çœ‹ä½œè¿™äº›æ¨¡å‹å¯¹æ›´å¤§ç¾¤ä½“çš„æ¨æ–­ Futureä½œè€…å°†ä»æ•°æ®ä¸­è‡ªåŠ¨å­¦ä¹ è¿™äº›ç»“æ„çš„é—®é¢˜ç•™ä½œæœªæ¥çš„å·¥ä½œã€‚]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>å…³ç³»æŠ½å–</tag>
        <tag>ç»Ÿè®¡å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ã€ŠInteraction Embeddings for Prediction and Explanationã€‹é˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FInteraction%20Embeddings%20for%20Prediction%20and%20Explanation%2F</url>
    <content type="text"><![CDATA[è®ºæ–‡ä¸‹è½½åœ°å€ï¼Œæ­¤è®ºæ–‡ä¸»è¦æå‡ºäº†å®ä½“å’Œå…³ç³»çš„äº¤äº’ä½œç”¨å¯¹äºçŸ¥è¯†å›¾è°±åµŒå…¥çš„å½±å“ï¼Œå’Œæå‡ºäº†æ–°çš„åµŒå…¥è¯„ä¼°æ–¹æ¡ˆ - æœç´¢é¢„æµ‹è§£é‡Šã€‚ è®ºæ–‡è´¡çŒ® æå‡ºäº†CrossEï¼Œä¸€ç§é€šè¿‡å­¦ä¹ ä¸€ä¸ªäº¤äº’çŸ©é˜µæ¥ç»™å®ä½“å’Œå…³ç³»çš„äº¤äº’å»ºæ¨¡çš„æ–°å‹çŸ¥è¯†å›¾è°±åµŒå…¥ã€‚ æˆ‘ä»¬ä½¿ç”¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†è¯„ä¼°CrossEä¸é“¾æ¥é¢„æµ‹ä»»åŠ¡ä¸Šçš„å„ç§å…¶ä»–KGEçš„æ¯”è¾ƒï¼Œå¹¶æ˜¾ç¤ºCrossEåœ¨å…·æœ‰é€‚åº¦å‚æ•°å¤§å°çš„å¤æ‚ä¸”æ›´å…·æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šå®ç°æœ€å…ˆè¿›çš„ç»“æœã€‚ æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åµŒå…¥è¯„ä¼°æ–¹æ¡ˆ - æœç´¢é¢„æµ‹è§£é‡Šï¼Œå¹¶è¡¨æ˜CrossEèƒ½å¤Ÿç”Ÿæˆæ¯”å…¶ä»–æ–¹æ³•æ›´å¯é çš„è§£é‡Šã€‚ è¿™è¡¨æ˜äº¤äº’åµŒå…¥æ›´èƒ½åœ¨ä¸åŒçš„ä¸‰å…ƒç»„ç¯å¢ƒä¸­æ•æ‰å®ä½“å’Œå…³ç³»ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚ ä»‹ç»ç»™å®šçŸ¥è¯†å›¾è°±å’Œä¸€ä¸ªè¦é¢„æµ‹çš„ä¸‰å…ƒç»„çš„å¤´å®ä½“å’Œå…³ç³»ï¼Œåœ¨é¢„æµ‹å°¾å®ä½“çš„è¿‡ç¨‹ä¸­ï¼Œå¤´å®ä½“å’Œå…³ç³»ä¹‹é—´æ˜¯æœ‰äº¤å‰äº¤äº’çš„crossover interaction, å³å…³ç³»å†³å®šäº†åœ¨é¢„æµ‹çš„è¿‡ç¨‹ä¸­å“ªäº›å¤´å®ä½“çš„ä¿¡æ¯æ˜¯æœ‰ç”¨çš„ï¼Œè€Œå¯¹é¢„æµ‹æœ‰ç”¨çš„å¤´å®ä½“çš„ä¿¡æ¯åˆå†³å®šäº†é‡‡ç”¨ä»€ä¹ˆé€»è¾‘å»æ¨ç†å‡ºå°¾å®ä½“ï¼Œæ–‡ä¸­é€šè¿‡ä¸€ä¸ªæ¨¡æ‹Ÿçš„çŸ¥è¯†å›¾è°±è¿›è¡Œäº†è¯´æ˜å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š ç›¸å…³å·¥ä½œè®ºæ–‡ä¸­åœ¨è¿™éƒ¨åˆ†å¯¹KGEï¼ˆKnowledge graph embeddingï¼‰è¿›è¡Œäº†åˆ†ç±»æ€»ç»“ï¼š KGEs with general embeddings KGEs with multiple embeddings. KGEs that utilize extra information. è¿™éƒ¨åˆ†æ€»ç»“ä¸­å¯¹å¤§é‡çš„æ–¹æ³•è¿›è¡Œæè¿°ï¼Œå¯ä»¥ä½œä¸ºèƒŒæ™¯çŸ¥è¯†è¿›è¡Œé˜…è¯»ã€‚ CrossEæ¨¡å‹åŸºäºå¯¹å¤´å®ä½“å’Œå…³ç³»ä¹‹é—´äº¤å‰äº¤äº’çš„è§‚å¯Ÿï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„çŸ¥è¯†å›¾è°±è¡¨ç¤ºå­¦ä¹ æ¨¡å‹CrossE. CrossEé™¤äº†å­¦ä¹ å®ä½“å’Œå…³ç³»çš„å‘é‡è¡¨ç¤ºï¼ŒåŒæ—¶è¿˜å­¦ä¹ äº†ä¸€ä¸ªäº¤äº’çŸ©é˜µCï¼ŒCä¸å…³ç³»ç›¸å…³ï¼Œå¹¶ä¸”ç”¨äºç”Ÿæˆå®ä½“å’Œå…³ç³»ç»è¿‡äº¤äº’ä¹‹åçš„å‘é‡è¡¨ç¤ºï¼Œæ‰€ä»¥åœ¨CrossEä¸­å®ä½“å’Œå…³ç³»ä¸ä»…ä»…æœ‰é€šç”¨å‘é‡è¡¨ç¤ºï¼ŒåŒæ—¶è¿˜æœ‰å¾ˆå¤šäº¤äº’å‘é‡è¡¨ç¤ºã€‚CrossEæ ¸å¿ƒæƒ³æ³•å¦‚ä¸‹å›¾ï¼š ç›®æ ‡å‡½æ•°ç²‰å››æ­¥ç”Ÿæˆï¼š Interaction Embedding for Entitiesï¼šæ ¹æ®å¤´å®ä½“å‘é‡å’Œäº¤äº’çŸ©é˜µï¼ˆä»¥å…³ç³»ç¡®å®šçš„ï¼‰æ¥ç¡®å®šå¤´å®ä½“çš„äº¤äº’è¡¨ç¤ºã€‚ Interaction Embedding for Relationsï¼šæ ¹æ®å¤´å®ä½“çš„äº¤äº’è¡¨ç¤ºå’Œå…³ç³»ä½œç”¨ç”Ÿæˆå…³ç³»çš„äº¤äº’è¡¨ç¤º Combination Operatorï¼šå°†å¤´å®ä½“çš„äº¤äº’è¡¨ç¤ºå’Œå…³ç³»çš„äº¤äº’è¡¨ç¤ºç›¸ç»“åˆï¼Œå¹¶è¿›è¡Œéçº¿æ€§å¤„ç†ï¼ˆtanhï¼‰ Similarity Operatorï¼šè®¡ç®—ç»“åˆåè¡¨ç¤ºå’Œå°¾å®ä½“è¡¨ç¤ºä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚ æœ€ååˆ†æ•°å‡½æ•°ï¼š æŸå¤±å‡½æ•°ï¼šï¼ˆè¿™é‡Œå°±æ˜¯ä¸€ä¸ªäº¤å‰ç†µå‡½æ•°ï¼Œä½†æ˜¯å†™çš„æœ‰é—®é¢˜f(x)é¡¹åº”è¯¥åœ¨æ‹¬å·å¤–ï¼‰ å¯¹äºé¢„æµ‹çš„è§£é‡Šè¿™éƒ¨åˆ†ä½œè€…æè¿°äº†å¦‚ä½•ç”Ÿæˆé¢„æµ‹ä¸‰å…ƒç»„çš„è§£é‡Šï¼Œå¹¶ä»‹ç»äº†åŸºäºåµŒå…¥çš„è·¯å¾„æœç´¢ç®—æ³•ï¼Œä¸»è¦æ­¥éª¤å¦‚ä¸‹ï¼š Search for similar relationsï¼šä¿®å‰ªæ‰ä¸åˆç†è·¯å¾„ Search for paths between h and tï¼šä½œè€…å®šä¹‰äº†6ç§è·¯å¾„ï¼ˆç­æ±‰ä¸€ä¸ªæˆ–ä¸¤ä¸ªå…³ç³»ï¼‰ Search similar entitiesï¼šæ•è·å®ä½“ä¹‹é—´çš„ç›¸ä¼¼æ€§æ–¹é¢è¶Šæœ‰èƒ½åŠ›ï¼Œå°±è¶Šæœ‰å¯èƒ½å­˜åœ¨ï¼ˆhsï¼Œrï¼Œtsï¼‰ : Search for similar structures as supportsï¼šæˆ‘ä»¬åªå°†çŸ¥è¯†å›¾ä¸­è‡³å°‘æœ‰ä¸€ä¸ªæ”¯æŒçš„è·¯å¾„è§†ä¸ºè§£é‡Šã€‚ å®éªŒæ•°æ®é›† é“¾æ¥é¢„æµ‹ ä»å®éªŒç»“æœä¸­æˆ‘ä»¬å¯ä»¥çœ‹å‡ºï¼ŒCrossEå®ç°äº†è¾ƒå¥½çš„é“¾æ¥é¢„æµ‹ç»“æœã€‚æˆ‘ä»¬å»é™¤CrossEä¸­çš„å¤´å®ä½“å’Œå…³ç³»çš„äº¤å‰äº¤äº’ï¼Œæ„é€ äº†æ¨¡å‹ CrossESï¼ŒCrossE å’Œ CrossES çš„æ¯”è¾ƒè¯´æ˜äº†äº¤å‰äº¤äº’çš„æœ‰æ•ˆæ€§ã€‚ ç”Ÿæˆè§£é‡Šæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç›¸ä¼¼ç»“æ„é€šè¿‡çŸ¥è¯†å›¾è°±çš„è¡¨ç¤ºå­¦ä¹ ç»“æœç”Ÿæˆé¢„æµ‹ç»“æœè§£é‡Šçš„æ–¹æ³•ï¼Œå¹¶æå‡ºäº†ä¸¤ç§è¡¡é‡è§£é‡Šç»“æœçš„æŒ‡æ ‡ï¼ŒAvgSupportå’ŒRecallã€‚Recallæ˜¯æŒ‡æ¨¡å‹èƒ½ç»™å‡ºè§£é‡Šçš„é¢„æµ‹ç»“æœçš„å æ¯”ï¼Œå…¶ä»‹äº0å’Œ1ä¹‹é—´ä¸”å€¼è¶Šå¤§è¶Šå¥½ï¼›AvgSupportæ˜¯æ¨¡å‹èƒ½ç»™å‡ºè§£é‡Šçš„é¢„æµ‹ç»“æœçš„å¹³å‡supportä¸ªæ•°ï¼ŒAvgSupportæ˜¯ä¸€ä¸ªå¤§äº0çš„æ•°ä¸”è¶Šå¤§è¶Šå¥½ã€‚å¯è§£é‡Šçš„è¯„ä¼°ç»“æœå¦‚ä¸‹ï¼š é“¾æ¥é¢„æµ‹å’Œå¯è§£é‡Šçš„å®éªŒä»ä¸¤ä¸ªä¸åŒçš„æ–¹é¢è¯„ä¼°äº†çŸ¥è¯†å›¾è°±è¡¨ç¤ºå­¦ä¹ çš„æ•ˆæœï¼ŒåŒæ—¶ä¹Ÿè¯´æ˜äº†é“¾æ¥é¢„æµ‹çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§æ²¡æœ‰å¿…ç„¶è”ç³»ï¼Œé“¾æ¥é¢„æµ‹æ•ˆæœå¥½çš„æ¨¡å‹å¹¶ä¸ä¸€å®šèƒ½å¤Ÿæ›´å¥½åœ°æä¾›è§£é‡Šï¼Œåä¹‹äº¦ç„¶ã€‚ å‚è€ƒé“¾æ¥ http://blog.openkg.cn/%E8%AE%BA%E6%96%87%E6%B5%85%E5%B0%9D-interaction-embeddings-for-prediction-and-explanation/]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>çŸ¥è¯†å›¾è°±</tag>
        <tag>çŸ¥è¯†å›¾è°±åµŒå…¥</tag>
        <tag>çŸ¥è¯†å›¾è°±æ¨ç†</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ã€ŠDifferentiable Learning of Logical Rules for Knowledge Base Reasoningã€‹é˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FDifferentiable%20Learning%20of%20Logical%20Rules%20for%20Knowledge%20Base%20Reasoning%2F</url>
    <content type="text"><![CDATA[è®ºæ–‡ä¸‹è½½åœ°å€ï¼Œæœ¬æ–‡ç ”ç©¶ç”¨äºäºçŸ¥è¯†å›¾è°±æ¨ç†çš„å­¦ä¹ æ¦‚ç‡ä¸€é˜¶é€»è¾‘è§„åˆ™çš„é—®é¢˜ï¼Œæå‡ºäº†Neural Logic Programmingï¼ˆNeural-LPï¼‰æ¡†æ¶ï¼Œå®ƒç»“åˆäº†ç«¯åˆ°ç«¯å¯å¾®åˆ†æ¨¡å‹ä¸­ä¸€é˜¶é€»è¾‘è§„åˆ™çš„å‚æ•°å’Œç»“æ„å­¦ä¹ ã€‚ä¸ºäº†åœ¨å¯å¾®åˆ†çš„æ¡†æ¶ä¸­åŒæ—¶å­¦ä¹ å‚æ•°å’Œç»“æ„ï¼Œä½œè€…è®¾è®¡äº†ä¸€ä¸ªå…·æœ‰æ³¨æ„æœºåˆ¶å’Œè®°å¿†çš„ç¥ç»æ§åˆ¶å™¨ç³»ç»Ÿï¼Œä»¥å­¦ä¹ é¡ºåºç»„æˆTensorLogä½¿ç”¨çš„åŸå§‹å¯å¾®æ“ä½œã€‚ä½œè€…é‡‡ç”¨çš„æ³¨æ„æœºåˆ¶æ˜¯ä½œä¸ºé€»è¾‘è§„åˆ™çš„ç½®ä¿¡åº¦å¹¶ä¸”æœ‰å¯“æ„å«ä¹‰çš„ã€‚ ä¸‹å›¾å±•ç¤ºäº†ä¸€ä¸ªä½¿ç”¨é€»è¾‘è§„åˆ™è¿›è¡ŒçŸ¥è¯†å›¾è°±æ¨ç†çš„ä¾‹å­ ä½¿ç”¨æ¦‚ç‡é€»è¾‘çš„ä¼˜ç‚¹æ˜¯é€šè¿‡ä¸ºé€»è¾‘è§„åˆ™é…å¤‡æ¦‚ç‡ï¼Œå¯ä»¥æ›´å¥½åœ°æ¨¡æ‹Ÿç»Ÿè®¡å¤æ‚å’Œå™ªå£°æ•°æ®ã€‚ statistical relational learningï¼ˆç»Ÿè®¡å…³ç³»å­¦ä¹ ï¼‰ï¼šå­¦ä¹ å…³ç³»è§„åˆ™çš„é›†åˆ ==inductive logic programmingï¼ˆå½’çº³é€»è¾‘è§„åˆ’ï¼‰ï¼š==å½“å­¦ä¹ æ¶‰åŠæå‡ºæ–°çš„é€»è¾‘è§„åˆ™æ—¶ã€‚ï¼ˆè¿™åº”è¯¥å’Œæˆ‘æ­£åœ¨åšçš„æ–¹å‘æ˜¯ç›¸å…³çš„ï¼Œéƒ½æ˜¯å¸¦æœ‰å½’çº³æ€§è´¨çš„ï¼Œæœ‰æ–°çš„ä¸œè¥¿äº§ç”Ÿï¼‰ã€‚ FrameworkKnowledge base reasoningä¸ºäº†æ¨ç†çŸ¥è¯†åº“ï¼Œå¯¹äºæ¯ä¸ªæŸ¥è¯¢æˆ‘ä»¬éƒ½æœ‰å…´è¶£å­¦ä¹ ä»¥ä¸‹å½¢å¼çš„åŠ æƒé“¾å¼é€»è¾‘è§„åˆ™ï¼Œç±»ä¼¼äº==éšæœºé€»è¾‘ç¨‹åº==ï¼š å…¶ä¸­$\alpha$æ˜¯å’Œè§„åˆ™æœ‰å…³çš„ç½®ä¿¡åº¦ï¼ŒRæ˜¯çŸ¥è¯†åº“ä¸­çš„å…³ç³»ï¼Œquery(Y,X) è¡¨ç¤ºä¸€ä¸ªä¸‰å…ƒç»„ï¼Œquery è¡¨ç¤ºä¸€ä¸ªå…³ç³»ã€‚ TensorLog for KB reasoningå°†å®ä½“è½¬æ¢æˆone-hotå˜é‡ï¼›å¹¶ç”¨ä¸€ä¸ªçŸ©é˜µ$M_R$è¡¨ç¤ºå…³ç³»ï¼Œè¯¥çŸ©é˜µåªåœ¨ï¼ˆiï¼Œjï¼‰å¤„ä¸º1ï¼Œiã€jä¸ºç¬¬iã€jä¸ªå®ä½“ã€‚ ç»“åˆä¸¤ä¸ªæ“ä½œï¼Œé€»è¾‘è§„åˆ™æ¨ç†$R(Y,X) \gets P(Y,X) \bigwedge Q(Z,X)$å¯ä»¥è¢«è¡¨ç¤ºä¸ºï¼š$M_P \cdot M_P \cdot v_x \doteq s$ï¼Œå‘é‡sä¸­ä¸º1çš„ä½ç½®å°±æ˜¯Yçš„ç­”æ¡ˆã€‚ å¯¹äºä¸€æ¡æŸ¥è¯¢ï¼Œæ‰€æœ‰çš„é€»è¾‘è§„åˆ™çš„å³è¾¹éƒ¨åˆ†è¢«è¡¨ç¤ºä¸ºä»¥ä¸‹å½¢å¼ï¼š å…¶ä¸­ï¼Œlè¡¨ç¤ºæ‰€æœ‰çš„å¯èƒ½è§„åˆ™çš„ä¸ªæ•°ï¼Œ$\alpha_l$æ˜¯è§„åˆ™lçš„ç½®ä¿¡åº¦ï¼Œ$\beta_l$æ˜¯æŸç‰¹å®šå…³ç³»é‡Œçš„æœ‰åºå…³ç³»åˆ—è¡¨ï¼Œæ‰€ä»¥åœ¨inferenceæ—¶ï¼Œç»™å®šå®ä½“$v_xâ€‹$ï¼Œå®ä½“yçš„scoreç­‰äºå‘é‡sä¸­çš„å¯¹åº”yçš„ä½ç½®çš„å€¼ã€‚å¯¹äºæ¨ç†ï¼Œç»™å®šå®ä½“xï¼Œå®ä½“yçš„scoreç­‰äºå‘é‡sä¸­çš„å¯¹åº”yçš„ä½ç½®çš„å€¼ã€‚ æ‰€ä»¥æ€»ç»“æœ¬æ–‡å…³å¿ƒçš„ä¼˜åŒ–é—®é¢˜å¦‚ä¸‹ï¼š Learning the logical rulesåœ¨ä¸Šå¼çš„ä¼˜åŒ–é—®é¢˜ä¸­ï¼Œç®—æ³•éœ€è¦å­¦ä¹ çš„éƒ¨åˆ†åˆ†ä¸ºä¸¤ä¸ªï¼šä¸€ä¸ªæ˜¯è§„åˆ™çš„ç»“æ„ï¼Œå³ä¸€ä¸ªè§„åˆ™æ˜¯ç”±å“ªäº›æ¡ä»¶ç»„åˆè€Œæˆçš„ï¼›å¦ä¸€ä¸ªæ˜¯è§„åˆ™çš„ç½®ä¿¡åº¦ã€‚ç”±äºæ¯ä¸€æ¡è§„åˆ™çš„ç½®ä¿¡åº¦éƒ½æ˜¯ä¾èµ–äºå…·ä½“çš„è§„åˆ™å½¢å¼ï¼Œè€Œè§„åˆ™ç»“æ„çš„ç»„æˆä¹Ÿæ˜¯ä¸€ä¸ªç¦»æ•£åŒ–çš„è¿‡ç¨‹ï¼Œå› æ­¤ä¸Šå¼æ•´ä½“æ˜¯ä¸å¯å¾®çš„ã€‚å› æ­¤ä½œè€…å¯¹å‰é¢çš„å¼å­åšäº†ä»¥ä¸‹æ›´æ”¹ï¼š å¯¹æ¯”ä¸å¼ï¼ˆ2ï¼‰ï¼šä¸»è¦äº¤æ¢äº†è¿ä¹˜å’Œç´¯åŠ çš„è®¡ç®—é¡ºåºï¼Œå¯¹é¢„ä¸€ä¸ªå…³ç³»çš„ç›¸å…³çš„è§„åˆ™ï¼Œä¸ºæ¯ä¸ªå…³ç³»åœ¨æ¯ä¸ªæ­¥éª¤éƒ½å­¦ä¹ äº†ä¸€ä¸ªæƒé‡ï¼Œå³ä¸Šå¼çš„ $a_t^k$ã€‚ ç”±äºä¸Šå¼å›ºå®šäº†æ¯ä¸ªè§„åˆ™çš„é•¿åº¦éƒ½ä¸º Tï¼Œè¿™æ˜¾ç„¶æ˜¯ä¸åˆé€‚çš„ã€‚ä¸ºäº†èƒ½å¤Ÿå­¦ä¹ åˆ°å˜é•¿çš„è§„åˆ™ï¼ŒNeural LPä¸­è®¾è®¡äº†è®°å¿†å‘é‡ $u_t$,è¡¨ç¤ºæ¯ä¸ªæ­¥éª¤è¾“å‡ºçš„ç­”æ¡ˆâ€”æ¯ä¸ªå®ä½“ä½œä¸ºç­”æ¡ˆçš„æ¦‚ç‡åˆ†å¸ƒï¼Œè¿˜è®¾è®¡äº†ä¸¤ä¸ªæ³¨æ„åŠ›å‘é‡ï¼šä¸€ä¸ªä¸ºè®°å¿†æ³¨æ„åŠ›å‘é‡ $b_t$ â€”â€”è¡¨ç¤ºåœ¨æ­¥éª¤ t æ—¶å¯¹äºä¹‹å‰æ¯ä¸ªæ­¥éª¤çš„æ³¨æ„åŠ›ï¼›ä¸€ä¸ªä¸ºç®—å­æ³¨æ„åŠ›å‘é‡ $a_t$ â€”â€”è¡¨ç¤ºåœ¨æ­¥éª¤ t æ—¶å¯¹äºæ¯ä¸ªå…³ç³»ç®—å­çš„æ³¨æ„åŠ›ã€‚æ¯ä¸ªæ­¥éª¤çš„è¾“å‡ºç”±ä¸‹é¢ä¸‰ä¸ªå¼å­ç”Ÿæˆï¼š å…¶ä¸­$b_t$å’Œ$a_t$ç”±ä»¥ä¸‹å…¬å¼é€šè¿‡RNNè·å¾—ï¼š æ¨ç†æœºçš„æ•´ä½“æ¡†æ¶æ˜¯ï¼š å…¶ä¸­memoryå­˜çš„å°±æ˜¯æ¯æ­¥çš„æ¨ç†ç»“æœï¼ˆå®ä½“ï¼‰ï¼Œæœ€åçš„è¾“å‡ºï¼ˆä¾‹å¦‚$u_{T+1}$ï¼Œç›®æ ‡å°±æ˜¯æœ€å¤§åŒ– $logv_y^Tu$ï¼ŒåŠ logæ˜¯å› ä¸ºéçº¿æ€§èƒ½è®©æ•ˆæœå˜å¥½ã€‚ æ•´ä¸ªç®—æ³•å¦‚ä¸‹ï¼š å®éªŒï¼ˆ1ï¼‰ ä¸¤ä¸ªæ ‡å‡†æ•°æ®é›†ä¸Šçš„ç»Ÿè®¡å…³ç³»å­¦ä¹ ç›¸å…³çš„å®éªŒ Unified Medical Language System (UMLS)ï¼šThe entities are biomedical concepts (e.g. disease, antibiotic) and relations are like treats and diagnoses. Kinshipï¼šcontains kinship relationships among members of the Alyawarra tribe from Central Australia [ ï¼ˆ2ï¼‰ åœ¨$16*16$çš„ç½‘æ ¼ä¸Šçš„è·¯å¾„å¯»æ‰¾çš„å®éªŒ ï¼ˆ3ï¼‰ çŸ¥è¯†åº“è¡¥å…¨å®éªŒå®éªŒæ‰€ç”¨æ•°æ®é›†ä¿¡æ¯ï¼š FB15KSelectedï¼šè¿™æ˜¯é€šè¿‡ä»FB15Kä¸­å»é™¤è¿‘ä¼¼é‡å¤å’Œåå‘å…³ç³»è€Œæ„é€ çš„ å®éªŒç»“æœï¼š ä¸ºäº†è¯æ˜Neural LPçš„å½’çº³æ¨ç†çš„èƒ½åŠ›ï¼Œæœ¬æ–‡è¿˜ç‰¹åˆ«è®¾è®¡äº†ä¸€ä¸ªå®éªŒï¼Œåœ¨è®­ç»ƒæ•°æ®é›†ä¸­å»æ‰æ‰€æœ‰æ¶‰åŠæµ‹è¯•é›†ä¸­åŒ…å«çš„å®ä½“çš„ä¸‰å…ƒç»„ï¼Œç„¶åè®­ç»ƒå¹¶é¢„æµ‹ï¼Œå¾—åˆ°ç»“æœå¦‚ä¸‹ï¼š ï¼ˆ4ï¼‰ çŸ¥è¯†åº“é—®ç­”çš„å®éªŒ æ€»ç»“æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå¯å¾®çš„è§„åˆ™å­¦ä¹ æ¨¡å‹ï¼Œå¹¶å¼ºè°ƒäº†çŸ¥è¯†åº“ä¸­çš„è§„åˆ™åº”è¯¥æ˜¯å®ä½“æ— å…³çš„ï¼Œå¯¹äºæˆ‘ç›®å‰åœ¨åšçš„æ–¹å‘ï¼Œæœ¬ä½“è®ºä¹Ÿæ˜¯ä¸å®ä½“æ— å…³çš„ï¼Œè¿™ç§è§„åˆ™å­¦ä¹ æœ‰ä¸€å®šçš„å€Ÿé‰´æ€§ï¼Œä½†æ˜¯å¥½åƒæ‰€åŒºåˆ«ã€‚è¿™ä¸ªè§„åˆ™æ¨ç†ä¹Ÿå¯ä»¥çœ‹æˆæŸäº›å…³ç³»ä¹‹é—´çš„åŒ…å«å…³ç³»3.1ä¸­ä¸¾çš„HasOfficeInCity(New York,Uber) and CityInCountry(USA,New York)çš„ä¾‹å­ï¼Œå¯ä»¥çœ‹ä½œæ˜¯2å¯¹äº1æœ‰åŒ…å«å…³ç³»ã€‚å¹¶ä¸”å¯ä»¥çœ‹åˆ°æœ¬ç¯‡è®ºæ–‡ä¸­ï¼Œä½œè€…è®¾è®¡äº†ä¸°å¯Œçš„å®éªŒã€‚ å‚è€ƒé“¾æ¥ https://toutiao.io/posts/wrxf4z/preview https://zhuanlan.zhihu.com/p/46024825]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>çŸ¥è¯†å›¾è°±</tag>
        <tag>çŸ¥è¯†å›¾è°±æ¨ç†</tag>
        <tag>è§„åˆ™å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ã€ŠOK Google, What Is Your Ontology? Or/ Exploring Freebase Classification to Understand Googleâ€™s Knowledge Graphï¼Ÿã€‹é˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FOK%20Google%2C%20What%20Is%20Your%20Ontology%3F%20Or%2F%20Exploring%20Freebase%20Classification%20to%20Understand%20Google%E2%80%99s%20Knowledge%20Graph%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[æœ¬è®ºæ–‡è¯¦ç»†é˜è¿°Freebaseä¸­çš„æ•°æ®æ ¼å¼ï¼Œå¹¶è¿›è¡Œäº†é‡æ„ã€‚é€šè¿‡è€ƒè™‘æ•´ä½“æ¶æ„çš„ä¸‰ä¸ªéƒ¨åˆ†ï¼šFreebaseç±»å‹ç³»ç»ŸåŠå…¶ç¼ºä¹ç»§æ‰¿å’Œä¾èµ–äºä¸å…¼å®¹æ€§ï¼Œå…è®¸è¡¨ç¤ºå€¼çš„ä¸ç¡®å®šæ€§çš„å®ç°ï¼Œä»¥åŠåˆå¹¶å’Œæ‹†åˆ†å¯¹è±¡çš„å®ç°ã€‚æ¥å¯¹æœ¬ä½“è¿›è¡Œé˜è¿°ã€‚è®ºæ–‡ä¸‹è½½åœ°å€ è¿™ç¯‡è®ºæ–‡é‡æ„äº†Freebaseæ•°æ®è½¬å‚¨æ¥ç†è§£è°·æ­Œè¯­ä¹‰æœç´¢ç‰¹å¾èƒŒåçš„æœ¬ä½“ã€‚è®ºæ–‡å°†ä¼šæ¢ç´¢Freebaseæœ¬ä½“å¦‚ä½•ç”±è®¸å¤šåŠ›é‡å¡‘é€ çš„ï¼Œè¿™äº›åŠ›é‡ä¹Ÿé€šè¿‡æ·±å…¥ç ”ç©¶æœ¬ä½“è®ºå’Œä¸€ä¸ªå°çš„ç›¸å…³æ€§ç ”ç©¶æ¥å½¢æˆåˆ†ç±»ç³»ç»Ÿã€‚è¿™äº›å‘ç°å°†ä¼šæä¾›çŸ¥è¯†å›¾è°±ä¸“æœ‰é»‘ç›’çš„ä¸€ç¥ã€‚ The structures found in the Freebase/Knowledge Graph ontology will be analyzed in light of the findings on classification systems in a key text by Bowker and Star (2000) [5]. æœ¯è¯­å®šä¹‰ ObjectFreebaseå¯¹è±¡æ˜¯ä¸€ä¸ªå…¨å±€å”¯ä¸€çš„æ ‡è¯†ç¬¦ï¼Œå®ƒæ˜¯Freebaseä¸­ä¸–ç•Œä¸ŠæŸç§ä¸œè¥¿çš„è¡¨ç¤ºã€‚ TypeFreebaseç±»å‹ç”¨æ¥è¡¨è¾¾ç±»çš„æ¦‚å¿µã€‚ PropertyFreebaseå±æ€§æ˜¯æè¿°å¯¹è±¡å¦‚ä½•é“¾æ¥åˆ°å…¶ä»–å€¼æˆ–å¯¹è±¡çš„å…³ç³»ã€‚ Property Detailå±æ€§è¯¦ç»†ä¿¡æ¯æŒ‡çš„æ˜¯å¯ä»¥é€šè¿‡å±æ€§é“¾æ¥çš„å¯¹è±¡æˆ–å€¼çš„çº¦æŸã€‚ RDF tripleèµ„æºæè¿°æ ¼å¼ï¼ˆRDFï¼‰æ˜¯ç”¨äºâ€œä¸‰å…ƒç»„â€ï¼ˆæˆ–N = 3å…ƒç»„ï¼‰æ ¼å¼çš„æ•°æ®è¡¨ç¤ºçš„è§„èŒƒ[17]ã€‚ Ontologyå¯¹äºæœ¬æ–‡ï¼ŒFreebaseæœ¬ä½“æ˜¯ç±»å‹ï¼Œå±æ€§å’Œå±æ€§è¯¦ç»†ä¿¡æ¯çš„æ­£å¼ç»“æ„å’Œæè¿°ï¼Œç”¨äºæŒ‡å®šå¯¹è±¡å¦‚ä½•ç›¸äº’å…³è”ã€‚ Architectureåœ¨æœ¬æ–‡ä¸­ï¼Œæ¶æ„æŒ‡çš„æ˜¯å¯ä»¥åœ¨æœ¬ä½“ä¸­æ‰¾åˆ°çš„ä¸€èˆ¬æ¨¡å¼å’Œå…³ç³»ã€‚ ==æœ¬ä½“æ˜¯å¦å…è®¸ç±»ï¼ˆæˆ–Freebaseç”¨è¯­ä¸­çš„ç±»å‹ï¼‰ä¹‹é—´çš„ç»§æ‰¿ï¼Ÿ æ˜¯å¦æœ‰ä¸å±æ€§ç›¸å…³çš„é»˜è®¤å€¼ï¼Ÿ å¦‚ä½•å¤„ç†â€œé›¶â€æˆ–ç©ºå€¼ï¼Ÿ è¿™äº›ç±»å‹çš„é—®é¢˜ä¸ä¸€å®šå…³æ³¨æœ¬ä½“ï¼ˆé£æœºï¼Œç«è½¦æˆ–æ±½è½¦ï¼‰ä¸­å…·ä½“è¡¨è¾¾çš„å†…å®¹ï¼Œè€Œæ˜¯å…³äºæœ¬ä½“è¡¨è¾¾æ–¹å¼çš„æ›´å¤šé—®é¢˜åº”è¯¥é€šè¿‡æ£€æŸ¥æ¶æ„æ¥è§£å†³ã€‚== Methodologyä½œè€…æŠŠæ•°æ®è¿›è¡Œåˆ‡åˆ†ï¼šæŒ‰ç…§RDFä¸­ä¸‰å…ƒç»„çš„è°“è¯­è¿›è¡Œåˆ†ç±»ï¼Œä¾‹å¦‚ï¼š Freebase Ontology and Classification As Bowker and Star note, â€œInformation infrastructure is a tricky thing to analyzeâ€¦the easier they are to use, the harder they are to see.â€ [5]. What does the system make sense of? What is left out? What is privileged and by extension what is ignored by Google? è™½ç„¶Freebaseæœ¬ä½“å¯èƒ½ä¸ä¼šç«‹å³çœ‹èµ·æ¥åƒä¸€ä¸ªåˆ†ç±»ç³»ç»Ÿï¼Œä½†ç±»å‹ï¼ˆç±»ï¼‰å’Œå±æ€§çš„ç»“æ„æ˜¯ä¸€ä¸ªåŸºäºå¯¹å„ç§äº‹ç‰©è¿›è¡Œåˆ†ç±»çš„ç³»ç»Ÿã€‚ ä½œä¸ºå¯¹ä¸–ç•Œäº‹ç‰©è¡¨å¾è¿›è¡Œæ’åºå’Œåˆ†ç±»çš„ç³»ç»Ÿï¼Œå°†æ ¹æ®Bowkerå’ŒStarçš„åˆ†ç±»ç»“æœè®¨è®ºFreebaseæœ¬ä½“ã€‚ä»–ä»¬å°†å¯¹äºšé‡Œå£«å¤šå¾·å’ŒåŸå‹åˆ†ç±»ï¼ˆAristotelian and prototype classificationï¼‰è¿›è¡Œäº†åŒºåˆ†ã€‚ äºšé‡Œå£«å¤šå¾·çš„åˆ†ç±»â€œæŒ‰ç…§ä¸€ç»„äºŒå…ƒç‰¹å¾è¿›è¡Œæ“ä½œï¼Œè¢«åˆ†ç±»çš„ç‰©ä½“å‘ˆç°æˆ–ä¸å‘ˆç°â€ï¼Œè€ŒåŸå‹åˆ†ç±»åˆ™è®¤ä¸ºâ€œåœ¨æˆ‘ä»¬å¿ƒç›®ä¸­å¯¹äºæ¤…å­æ˜¯ä»€ä¹ˆçš„å¹¿æ³›æè¿°; æˆ‘ä»¬ç”¨éšå–»å’Œç±»æ¯”æ¥æ‰©å±•è¿™å¼ å›¾ç‰‡â€œ 5.1. Freebaseâ€™s Type Systemä¸å…¼å®¹æ€§çš„æ¦‚å¿µå‡ºç°åœ¨Freebaseç³»ç»Ÿä¸­ï¼Œç”¨äºè¡¨ç¤ºå¯¹è±¡å¦‚ä½•å…·æœ‰æŸäº›ç±»å‹ï¼Œè€Œè¿™äº›ç±»å‹å¿…é¡»å°†å…¶æ’é™¤åœ¨å…¶ä»–ç±»å‹ä¹‹å¤–ã€‚ æ²¡æœ‰ç»§æ‰¿ï¼ˆnot implement inheritanceï¼‰ï¼šä¸Šè¿°ä¸å…¼å®¹æ€§åœ¨ç¡®ä¿æ•°æ®ä¸è¡¨è¾¾å¯èƒ½åœ¨Google KPä¸­æä¾›çš„ä»¤äººå°´å°¬ï¼Œæœ‰å®³æˆ–ä¸æ­£ç¡®çš„é™ˆè¿°æ–¹é¢å‘æŒ¥äº†è¶³å¤Ÿå¼ºå¤§çš„ä½œç”¨ã€‚ ç¼ºä¹ç»§æ‰¿ä¹Ÿå¯èƒ½æ˜¯ä¸€ç§å…è®¸å®ä½“å…·æœ‰æ›´å¤§çµæ´»æ€§çš„ç‰¹å¾ã€‚è¿™é‡Œä½œè€…ä¸¾äº†ä¸€ä¸ªç‹—ä¸ºç”µå½±æ¼”å‘˜çš„ä¾‹å­ã€‚ 5.2. Has Value or Has No Value?ä¸‰å…ƒç»„å¦‚ä½•è¡¨è¾¾ä¼°è®¡å€¼ï¼Œä¸ç¡®å®šå€¼æˆ–ç©ºå€¼ï¼Ÿå®é™…å¤„ç†æ—¶ç”¨â€œHas Valueâ€ (HV) and â€œHas No Valueâ€ (HNV)æ¥åˆ†åˆ«è¡¨è¾¾ä¸ç¡®å®šå€¼å’Œç©ºå€¼ã€‚ ä»¥è¿™ç§æ–¹å¼è¡¨è¾¾æœªçŸ¥æ•°å’Œç©ºå€¼çš„æœ‰è¶£å®ç°å¯èƒ½è¡¨æ˜Freebase / KGæœ€åˆå¹¶ä¸æ˜¯ä¸ºäº†æ”¯æŒè¿™ç§ä¸ç¡®å®šæ€§è€Œå»ºç«‹çš„ã€‚Googleçš„æ•°æ®ç¼–ç æŸäº›ä¸ç¡®å®šæ€§çš„æ¦‚å¿µå¹¶æœªå‘æœ€ç»ˆç”¨æˆ·å…¬å¼€ï¼Œå°½ç®¡å®ƒè‚¯å®šä»¥è¿™ç§ç‹¬ç‰¹çš„æ–¹å¼å®ç°ã€‚ 5.3. Dealing with Doppelgangers and Chimerasæ¶‰åŠFreebaseå¦‚ä½•å¤„ç†â€œåˆå¹¶â€é‡å¤å¯¹è±¡ï¼ˆdoppelgangersï¼‰å’Œâ€œæ‹†åˆ†â€æ··åˆå¯¹è±¡ï¼ˆåµŒåˆä½“ï¼‰ã€‚ the property â€œ/dataworld/gardening hint/replaced byâ€ is used to implement merges be- tween various objects (e.g. by saying â€œ/m/xyz123 - Replaced By - /m/abc123â€). A Small Correlational Studyä¸»è¦æ¢ç´¢è¿™ä¸ªé—®é¢˜ï¼šåŸŸçš„æœ¬ä½“çš„å¤æ‚æ€§ï¼ˆäººç‰©ï¼Œç”µå½±ç­‰é¢†åŸŸçš„ç±»å‹ï¼Œå±æ€§ç­‰ï¼‰ä¸è¡¨è¾¾ä¸æœ¬ä½“ç›¸å…³çš„äº‹å®ï¼ˆâ€œçŸ¥è¯†åº“â€ï¼‰çš„ä¸‰å…ƒç»„æ•°é‡ä¹‹é—´æ˜¯å¦å­˜åœ¨å…³è”ï¼Ÿ å¯¹äºæœ¬ç ”ç©¶ï¼Œé€šè¿‡è€ƒè™‘ä¸åŸŸç›¸å…³çš„å±æ€§è¯¦ç»†ä¿¡æ¯é‡ï¼ˆå¤šå°‘æè¿°ï¼Œçº¦æŸç­‰ï¼‰æ¥å®ç°â€œå¤æ‚æ€§â€å’Œâ€œæˆç†Ÿåº¦â€ã€‚ å¯¹äº89ä¸ªåŸŸä¸­çš„æ¯ä¸€ä¸ªï¼Œè·å¾—äº†å…³äºæ¯ä¸ªåŸŸçš„æœ¬ä½“çš„ä»¥ä¸‹ç»Ÿè®¡ï¼š ==åŸŸä¸­çš„ç±»å‹å’Œå±æ€§æ•°== ==æ¯ç§ç±»å‹å’Œå±æ€§çš„æè¿°æ•°== ==æ¯ç§ç±»å‹å’Œå±æ€§çš„å±æ€§è¯¦ç»†ä¿¡æ¯æ•°== é€šè¿‡è·å–åŸŸä¸­æ¯ç§ç±»å‹å’Œå±æ€§çš„å¹³å‡æè¿°æ•°å’Œå±æ€§è¯¦ç»†ä¿¡æ¯æ¥è®¡ç®—ç®€å•çš„å¤æ‚æ€§åˆ†æ•°ã€‚ æ‰€æœ‰åŸŸçš„RDFä¸‰å…ƒç»„è®¡æ•°ä¸æ­¤å¤æ‚æ€§å¾—åˆ†ä¹‹é—´çš„Pearsonç›¸å…³ç³»æ•°ä¸0.2824å‘ˆæ­£ç›¸å…³ï¼Œç®€å•çº¿æ€§å›å½’çš„æ–œç‡ä¸º78,424.08ï¼ˆè§å›¾6ï¼‰ã€‚ å½“æ’é™¤å¼‚å¸¸éŸ³ä¹åˆ‡ç‰‡æ—¶ï¼Œç›¸å…³æ€§å’Œæ–œç‡åˆ†åˆ«å˜ä¸º0.6680å’Œ33,899.53ã€‚ è™½ç„¶éœ€è¦è¿›ä¸€æ­¥çš„å·¥ä½œæ¥æ¢ç´¢è¿™ä¸ªç ”ç©¶é—®é¢˜ï¼Œä½†è¿™ä¸ªå°çš„ç›¸å…³æ€§ç ”ç©¶ä¸ºè¿›ä¸€æ­¥çš„å®éªŒæä¾›äº†ä¸€äº›æœ‰å¸Œæœ›çš„åˆæ­¥ç»“æœ discussionè€ƒè™‘æ•´ä½“æ¶æ„çš„ä¸‰ä¸ªéƒ¨åˆ†ï¼šFreebaseç±»å‹ç³»ç»ŸåŠå…¶ç¼ºä¹ç»§æ‰¿å’Œä¾èµ–äºä¸å…¼å®¹æ€§ï¼Œå…è®¸è¡¨ç¤ºå€¼çš„ä¸ç¡®å®šæ€§çš„å®ç°ï¼Œä»¥åŠåˆå¹¶å’Œæ‹†åˆ†å¯¹è±¡çš„å®ç°ã€‚æ­¤å¤–ï¼Œè¿˜è¿›è¡Œäº†ä¸€é¡¹å°å‹ç›¸å…³ç ”ç©¶ï¼Œä»¥æ£€éªŒåŸºäºBowkerå’ŒStaræ¨åŠ¨çš„é¢„æ„Ÿçš„å‡è®¾ã€‚åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šï¼Œåˆ†ç±»ç³»ç»Ÿä¸­çš„è®¸å¤šç‰¹å¾ä¹Ÿå¯ä»¥åœ¨Freebaseçš„æœ¬ä½“å’Œä½“ç³»ç»“æ„ä¸­æ‰¾åˆ°ã€‚ æœ¬æ–‡å…·ä½“è€Œè¨€ï¼Œæ¢è®¨äº†æ”¯æŒæ•´ä¸ªäº¤ä»˜æµç¨‹çš„åŸºç¡€ç»“æ„ï¼ˆæœ¬ä½“å’Œä½“ç³»ç»“æ„ï¼‰ï¼Œè€Œä¸æ˜¯Freebase / KGä¸­è¡¨ç¤ºçš„ç‰¹å®šäº‹å®ã€‚ conclusion åº”é€šè¿‡æ¢ç´¢Freebaseæœ¬ä½“å’Œä½“ç³»ç»“æ„çš„å…¶ä»–æ–¹é¢ä»¥åŠå¯¹Freebaseè¿›è¡Œæ›´å…¨é¢çš„å®éªŒåˆ†ææ¥è¿›è¡Œè¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>çŸ¥è¯†å›¾è°±</tag>
        <tag>Ontology</tag>
        <tag>freebase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ­£åˆ™è¡¨è¾¾å¼]]></title>
    <url>%2Fpost%2FRegular%20expression%2F</url>
    <content type="text"><![CDATA[æœ¬æ–‡å‚è€ƒäº†ä¸€äº›é“¾æ¥ï¼Œè®°å½•äº†ä¸€äº›å¸¸ç”¨æ­£åˆ™è¡¨è¾¾å¼çš„è¯¦ç»†ä½¿ç”¨æ–¹æ³•ã€‚ åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼Œå¾ˆå¤šæ—¶å€™æˆ‘ä»¬éƒ½éœ€è¦ä»æ–‡æœ¬æˆ–å­—ç¬¦ä¸²ä¸­æŠ½å–å‡ºæƒ³è¦çš„ä¿¡æ¯ï¼Œå¹¶è¿›ä¸€æ­¥åšè¯­ä¹‰ç†è§£æˆ–å…¶å®ƒå¤„ç†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œä½œè€…ç”±åŸºç¡€åˆ°é«˜çº§ä»‹ç»äº†å¾ˆå¤šæ­£åˆ™è¡¨è¾¾å¼ï¼Œè¿™äº›è¡¨è¾¾å¼æˆ–è§„åˆ™åœ¨å¾ˆå¤šç¼–ç¨‹è¯­è¨€ä¸­éƒ½æ˜¯é€šç”¨çš„ã€‚ ä¹¦å†™æ­£åˆ™è¡¨è¾¾å¼ç½‘ç«™ æ­£åˆ™è¡¨è¾¾å¼ï¼ˆregex æˆ– regexpï¼‰å¯¹äºä»æ–‡æœ¬ä¸­æŠ½å–ä¿¡æ¯æå…¶æœ‰ç”¨ï¼Œå®ƒä¸€èˆ¬ä¼šæœç´¢åŒ¹é…ç‰¹å®šæ¨¡å¼çš„è¯­å¥ï¼Œè€Œè¿™ç§æ¨¡å¼åŠå…·ä½“çš„ ASCII åºåˆ—æˆ– Unicode å­—ç¬¦ã€‚ä»è§£æ/æ›¿ä»£å­—ç¬¦ä¸²ã€é¢„å¤„ç†æ•°æ®åˆ°ç½‘é¡µçˆ¬å–ï¼Œæ­£åˆ™è¡¨è¾¾å¼çš„åº”ç”¨èŒƒå›´éå¸¸å¹¿ã€‚ å…¶ä¸­ä¸€ä¸ªæ¯”è¾ƒæœ‰æ„æ€çš„åœ°æ–¹æ˜¯ï¼Œåªè¦æˆ‘ä»¬å­¦ä¼šäº†æ­£åˆ™è¡¨è¾¾å¼çš„è¯­å¥ï¼Œæˆ‘ä»¬å‡ ä¹å¯ä»¥å°†å…¶åº”ç”¨äºå¤šæœ‰çš„ç¼–ç¨‹è¯­è¨€ï¼ŒåŒ…æ‹¬ JavaScriptã€Pythonã€Ruby å’Œ Java ç­‰ã€‚åªä¸è¿‡å¯¹äºå„ç¼–ç¨‹è¯­è¨€æ‰€æ”¯æŒçš„æœ€é«˜çº§ç‰¹å¾ä¸è¯­æ³•æœ‰ç»†å¾®çš„åŒºåˆ«ã€‚ ä¸‹é¢æˆ‘ä»¬å¯ä»¥å…·ä½“è®¨è®ºä¸€äº›æ¡ˆä¾‹ä¸è§£é‡Šã€‚ åŸºæœ¬è¯­å¥é”šç‚¹ï¼š^ å’Œ $^The åŒ¹é…ä»»ä½•ä»¥â€œTheâ€å¼€å¤´çš„å­—ç¬¦ä¸² end$ åŒ¹é…ä»¥â€œendâ€ä¸ºç»“å°¾çš„å­—ç¬¦ä¸² ^The end$ æŠ½å–åŒ¹é…ä»â€œTheâ€å¼€å§‹åˆ°â€œendâ€ç»“æŸçš„å­—ç¬¦ä¸² roar åŒ¹é…ä»»ä½•å¸¦æœ‰æ–‡æœ¬â€œroarâ€çš„å­—ç¬¦ä¸² æ•°é‡ç¬¦ï¼š*ã€+ã€ï¼Ÿå’Œ {}**abc* åŒ¹é…åœ¨â€œabâ€åé¢è·Ÿç€é›¶ä¸ªæˆ–å¤šä¸ªâ€œcâ€çš„å­—ç¬¦ä¸² abc+ åŒ¹é…åœ¨â€œabâ€åé¢è·Ÿç€ä¸€ä¸ªæˆ–å¤šä¸ªâ€œcâ€çš„å­—ç¬¦ä¸² abc? åŒ¹é…åœ¨â€œabâ€åé¢è·Ÿç€é›¶ä¸ªæˆ–ä¸€ä¸ªâ€œcâ€çš„å­—ç¬¦ä¸² abc{2} åŒ¹é…åœ¨â€œabâ€åé¢è·Ÿç€ä¸¤ä¸ªâ€œcâ€çš„å­—ç¬¦ä¸² abc{2,} åŒ¹é…åœ¨â€œabâ€åé¢è·Ÿç€ä¸¤ä¸ªæˆ–æ›´å¤šâ€œcâ€çš„å­—ç¬¦ä¸² abc{2,5} åŒ¹é…åœ¨â€œabâ€åé¢è·Ÿç€2åˆ°5ä¸ªâ€œcâ€çš„å­—ç¬¦ä¸² a(bc)* åŒ¹é…åœ¨â€œaâ€åé¢è·Ÿç€é›¶ä¸ªæˆ–æ›´å¤šâ€œbcâ€åºåˆ—çš„å­—ç¬¦ä¸² a(bc){2,5} åŒ¹é…åœ¨â€œaâ€åé¢è·Ÿç€2åˆ°5ä¸ªâ€œbcâ€åºåˆ—çš„å­—ç¬¦ä¸² æˆ–è¿ç®—ç¬¦ï¼š| ã€ []a(b|c) åŒ¹é…åœ¨â€œaâ€åé¢è·Ÿç€â€œbâ€æˆ–â€œcâ€çš„å­—ç¬¦ä¸² a[bc] åŒ¹é…åœ¨â€œaâ€åé¢è·Ÿç€â€œbâ€æˆ–â€œcâ€çš„å­—ç¬¦ä¸² å­—ç¬¦ç±»ï¼š\dã€\wã€\s å’Œ .**\d åŒ¹é…æ•°å­—å‹çš„å•ä¸ªå­—ç¬¦ \w åŒ¹é…å•ä¸ªè¯å­—ï¼ˆå­—æ¯åŠ ä¸‹åˆ’çº¿ï¼‰ \s åŒ¹é…å•ä¸ªç©ºæ ¼å­—ç¬¦ï¼ˆåŒ…æ‹¬åˆ¶è¡¨ç¬¦å’Œæ¢è¡Œç¬¦ï¼‰ . åŒ¹é…ä»»æ„å­—ç¬¦ ä½¿ç”¨ã€Œ.ã€è¿ç®—ç¬¦éœ€è¦éå¸¸å°å¿ƒï¼Œå› ä¸ºå¸¸è§ç±»æˆ–æ’é™¤å‹å­—ç¬¦ç±»éƒ½è¦æ›´å¿«ä¸ç²¾ç¡®ã€‚\dã€\w å’Œ\s åŒæ ·æœ‰å®ƒä»¬å„è‡ªçš„æ’é™¤å‹å­—ç¬¦ç±»ï¼Œå³\Dã€\W å’Œ\Sã€‚ä¾‹å¦‚\D å°†æ‰§è¡Œä¸\d å®Œå…¨ç›¸åçš„åŒ¹é…æ–¹æ³•ï¼š \D åŒ¹é…å•ä¸ªéæ•°å­—å‹çš„å­—ç¬¦ ä¸ºäº†æ­£ç¡®åœ°åŒ¹é…ï¼Œæˆ‘ä»¬å¿…é¡»ä½¿ç”¨è½¬ä¹‰ç¬¦åæ–œæ ã€Œ\ã€å®šä¹‰æˆ‘ä»¬éœ€è¦åŒ¹é…çš„ç¬¦å·ã€Œ^.[$()|*+?{\ã€ï¼Œå› ä¸ºæˆ‘ä»¬å¯èƒ½è®¤ä¸ºè¿™äº›ç¬¦å·åœ¨åŸæ–‡æœ¬ä¸­æœ‰ç‰¹æ®Šçš„å«ä¹‰ã€‚ \$\d åŒ¹é…åœ¨å•ä¸ªæ•°å­—å‰æœ‰ç¬¦å·â€œ$â€çš„å­—ç¬¦ä¸² -&gt; Try it! (https://regex101.com/r/cO8lqs/9) æ³¨æ„æˆ‘ä»¬åŒæ ·èƒ½åŒ¹é… non-printable å­—ç¬¦ï¼Œä¾‹å¦‚ Tab ç¬¦ã€Œ\tã€ã€æ¢è¡Œç¬¦ã€Œ\nã€å’Œå›è½¦ç¬¦ã€Œ\rã€ Flagsæˆ‘ä»¬å·²ç»äº†è§£å¦‚ä½•æ„å»ºæ­£åˆ™è¡¨è¾¾å¼ï¼Œä½†ä»ç„¶é—æ¼äº†ä¸€ä¸ªéå¸¸åŸºç¡€çš„æ¦‚å¿µï¼šflagsã€‚ æ­£åˆ™è¡¨è¾¾å¼é€šå¸¸ä»¥/abc/è¿™ç§å½¢å¼å‡ºç°ï¼Œå…¶ä¸­æœç´¢æ¨¡å¼ç”±ä¸¤ä¸ªåæ–œæ ã€Œ/ã€åˆ†ç¦»ã€‚è€Œåœ¨æ¨¡å¼çš„ç»“å°¾ï¼Œæˆ‘ä»¬é€šå¸¸å¯ä»¥æŒ‡å®šä»¥ä¸‹ flag é…ç½®æˆ–å®ƒä»¬çš„ç»„åˆï¼š gï¼ˆglobalï¼‰åœ¨ç¬¬ä¸€æ¬¡å®ŒæˆåŒ¹é…åå¹¶ä¸ä¼šè¿”å›ç»“æœï¼Œå®ƒä¼šç»§ç»­æœç´¢å‰©ä¸‹çš„æ–‡æœ¬ã€‚ mï¼ˆmulti lineï¼‰å…è®¸ä½¿ç”¨^å’Œ$åŒ¹é…ä¸€è¡Œçš„å¼€å§‹å’Œç»“å°¾ï¼Œè€Œä¸æ˜¯æ•´ä¸ªåºåˆ—ã€‚ iï¼ˆinsensitiveï¼‰ä»¤æ•´ä¸ªè¡¨è¾¾å¼ä¸åŒºåˆ†å¤§å°å†™ï¼ˆä¾‹å¦‚/aBc/i å°†åŒ¹é… AbCï¼‰ã€‚ ä¸­çº§è¯­å¥åˆ†ç»„å’Œæ•è·ï¼š()a(bc) åœ†æ‹¬å¼§ä¼šåˆ›å»ºä¸€ä¸ªæ•è·æ€§åˆ†ç»„ï¼Œå®ƒä¼šæ•è·åŒ¹é…é¡¹â€œbcâ€ a(?:bc)* ä½¿ç”¨ â€œ?:â€ ä¼šä½¿æ•è·åˆ†ç»„å¤±æ•ˆï¼Œåªéœ€è¦åŒ¹é…å‰é¢çš„â€œaâ€ a(?&lt;foo&gt;bc) ä½¿ç”¨ â€œ?&lt;foo&gt;â€ ä¼šä¸ºåˆ†ç»„é…ç½®ä¸€ä¸ªåç§° æ•è·æ€§åœ†æ‹¬å· () å’Œéæ•è·æ€§åœ†æ‹¬å¼§ (?:) å¯¹äºä»å­—ç¬¦ä¸²æˆ–æ•°æ®ä¸­æŠ½å–ä¿¡æ¯éå¸¸é‡è¦ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Python ç­‰ä¸åŒçš„ç¼–ç¨‹è¯­è¨€å®ç°è¿™ä¸€åŠŸèƒ½ã€‚ä»å¤šä¸ªåˆ†ç»„ä¸­æ•è·çš„å¤šä¸ªåŒ¹é…é¡¹å°†ä»¥ç»å…¸çš„æ•°ç»„å½¢å¼å±•ç¤ºï¼šæˆ‘ä»¬å¯ä»¥ä½¿ç”¨åŒ¹é…ç»“æœçš„ç´¢å¼•è®¿é—®å®ƒä»¬çš„å€¼ã€‚ å¦‚æœéœ€è¦ä¸ºåˆ†ç»„æ·»åŠ åç§°ï¼ˆä½¿ç”¨ (?â€¦)ï¼‰ï¼Œæˆ‘ä»¬å°±èƒ½å¦‚å­—å…¸é‚£æ ·ä½¿ç”¨åŒ¹é…ç»“æœæ£€ç´¢åˆ†ç»„çš„å€¼ï¼Œå…¶ä¸­å­—å…¸çš„é”®ä¸ºåˆ†ç»„çš„åç§°ã€‚ æ–¹æ‹¬å¼§è¡¨è¾¾å¼ï¼š[][abc] åŒ¹é…å¸¦æœ‰ä¸€ä¸ªâ€œaâ€ã€â€œabâ€æˆ–â€œacâ€çš„å­—ç¬¦ä¸² -&gt; ä¸ a|b|c ä¸€æ · [a-c] åŒ¹é…å¸¦æœ‰ä¸€ä¸ªâ€œaâ€ã€â€œabâ€æˆ–â€œacâ€çš„å­—ç¬¦ä¸² -&gt; ä¸ a|b|c ä¸€æ · [a-fA-F0-9] åŒ¹é…ä¸€ä¸ªä»£è¡¨16è¿›åˆ¶æ•°å­—çš„å­—ç¬¦ä¸²ï¼Œä¸åŒºåˆ†å¤§å°å†™ [0-9]% åŒ¹é…åœ¨%ç¬¦å·å‰é¢å¸¦æœ‰0åˆ°9è¿™å‡ ä¸ªå­—ç¬¦çš„å­—ç¬¦ä¸² [^a-zA-Z] åŒ¹é…ä¸å¸¦aåˆ°zæˆ–Aåˆ°Zçš„å­—ç¬¦ä¸²ï¼Œå…¶ä¸­^ä¸ºå¦å®šè¡¨è¾¾å¼ è®°ä½åœ¨æ–¹æ‹¬å¼§å†…ï¼Œæ‰€æœ‰ç‰¹æ®Šå­—ç¬¦ï¼ˆåŒ…æ‹¬åæ–œæ \ï¼‰éƒ½ä¼šå¤±å»å®ƒä»¬åº”æœ‰çš„æ„ä¹‰ã€‚ ==Greedy å’Œ Lazy åŒ¹é…==æ•°é‡ç¬¦ï¼ˆ* + {}ï¼‰æ˜¯ä¸€ç§è´ªå¿ƒè¿ç®—ç¬¦ï¼Œæ‰€ä»¥å®ƒä»¬ä¼šéå†ç»™å®šçš„æ–‡æœ¬ï¼Œå¹¶å°½å¯èƒ½åŒ¹é…ã€‚ä¾‹å¦‚ï¼Œ&lt;.+&gt; å¯ä»¥åŒ¹é…æ–‡æœ¬ã€ŒThis is a simple div testã€ä¸­çš„ã€Œsimple divã€ã€‚ä¸ºäº†ä»…æ•è· div æ ‡ç­¾ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨ã€Œï¼Ÿã€ä»¤è´ªå¿ƒæœç´¢å˜å¾— Lazy ä¸€ç‚¹ï¼š &lt;.+?&gt; ä¸€æ¬¡æˆ–å¤šæ¬¡åŒ¹é… â€œ&lt;â€ å’Œ â€œ&gt;â€ é‡Œé¢çš„ä»»ä½•å­—ç¬¦ï¼Œå¯æŒ‰éœ€æ‰©å±• æ³¨æ„æ›´å¥½çš„è§£å†³æ–¹æ¡ˆåº”è¯¥éœ€è¦é¿å…ä½¿ç”¨ã€Œ.ã€ï¼Œè¿™æœ‰åˆ©äºå®ç°æ›´ä¸¥æ ¼çš„æ­£åˆ™è¡¨è¾¾å¼ï¼š &lt;[^&lt;&gt;]+&gt; ä¸€æ¬¡æˆ–å¤šæ¬¡åŒ¹é… â€œ&lt;â€ å’Œ â€œ&gt;â€ é‡Œé¢çš„ä»»ä½•å­—ç¬¦ï¼Œé™¤å» â€œ&lt;â€ æˆ– â€œ&gt;â€ å­—ç¬¦ é«˜çº§è¯­å¥ è¾¹ç•Œç¬¦ï¼š\b å’Œ \B \babc\b æ‰§è¡Œæ•´è¯åŒ¹é…æœç´¢ \b å¦‚æ’å…¥ç¬¦å·é‚£æ ·è¡¨ç¤ºä¸€ä¸ªé”šç‚¹ï¼ˆå®ƒä¸$å’Œ^ç›¸åŒï¼‰æ¥åŒ¹é…ä½ç½®ï¼Œå…¶ä¸­ä¸€è¾¹æ˜¯ä¸€ä¸ªå•è¯ç¬¦å·ï¼ˆå¦‚\wï¼‰ï¼Œå¦ä¸€è¾¹ä¸æ˜¯å•è¯ç¬¦å·ï¼ˆä¾‹å¦‚å®ƒå¯èƒ½æ˜¯å­—ç¬¦ä¸²çš„èµ·å§‹ç‚¹æˆ–ç©ºæ ¼ç¬¦å·ï¼‰ã€‚ å®ƒåŒæ ·èƒ½è¡¨è¾¾ç›¸åçš„éå•è¯è¾¹ç•Œã€Œ\Bã€ï¼Œå®ƒä¼šåŒ¹é…ã€Œ\bã€ä¸ä¼šåŒ¹é…çš„ä½ç½®ï¼Œå¦‚æœæˆ‘ä»¬å¸Œæœ›æ‰¾åˆ°è¢«å•è¯å­—ç¬¦ç¯ç»•çš„æœç´¢æ¨¡å¼ï¼Œå°±å¯ä»¥ä½¿ç”¨å®ƒã€‚ \Babc\B åªè¦æ˜¯è¢«å•è¯å­—ç¬¦ç¯ç»•çš„æ¨¡å¼å°±ä¼šåŒ¹é… å‰å‘åŒ¹é…å’Œåå‘åŒ¹é…ï¼š(?=) å’Œ (?&lt;=)d(?=r) åªæœ‰åœ¨åé¢è·Ÿç€â€œrâ€çš„æ—¶å€™æ‰åŒ¹é…â€œdâ€ï¼Œä½†æ˜¯â€œrâ€å¹¶ä¸ä¼šæˆä¸ºæ•´ä¸ªæ­£åˆ™è¡¨è¾¾å¼åŒ¹é…çš„ä¸€éƒ¨åˆ† (?&lt;=r)d åªæœ‰åœ¨å‰é¢è·Ÿç€â€œrâ€æ—¶æ‰åŒ¹é…â€œdâ€ï¼Œä½†æ˜¯â€œrâ€å¹¶ä¸ä¼šæˆä¸ºæ•´ä¸ªæ­£åˆ™è¡¨è¾¾å¼åŒ¹é…çš„ä¸€éƒ¨åˆ† æˆ‘ä»¬åŒæ ·èƒ½ä½¿ç”¨å¦å®šè¿ç®—å­ï¼š d(?!r) åªæœ‰åœ¨åé¢ä¸è·Ÿç€â€œrâ€çš„æ—¶å€™æ‰åŒ¹é…â€œdâ€ï¼Œä½†æ˜¯â€œrâ€å¹¶ä¸ä¼šæˆä¸ºæ•´ä¸ªæ­£åˆ™è¡¨è¾¾å¼åŒ¹é…çš„ä¸€éƒ¨åˆ† (?&lt;!r)d åªæœ‰åœ¨å‰é¢ä¸è·Ÿç€â€œrâ€æ—¶æ‰åŒ¹é…â€œdâ€ï¼Œä½†æ˜¯â€œrâ€å¹¶ä¸ä¼šæˆä¸ºæ•´ä¸ªæ­£åˆ™è¡¨è¾¾å¼åŒ¹é…çš„ä¸€éƒ¨åˆ† ç»“è¯­ æ­£å¦‚ä¸Šæ–‡æ‰€ç¤ºï¼Œæ­£åˆ™è¡¨è¾¾å¼çš„åº”ç”¨é¢†åŸŸéå¸¸å¹¿ï¼Œå¾ˆå¯èƒ½å„ä½è¯»è€…åœ¨å¼€å‘çš„è¿‡ç¨‹ä¸­å·²ç»é‡åˆ°äº†å®ƒï¼Œä¸‹é¢æ˜¯æ­£åˆ™è¡¨è¾¾å¼å¸¸ç”¨çš„é¢†åŸŸï¼š æ•°æ®éªŒè¯ï¼Œä¾‹å¦‚æ£€æŸ¥æ—¶é—´å­—ç¬¦ä¸²æ˜¯å¦ç¬¦åˆæ ¼å¼ï¼› æ•°æ®æŠ“å–ï¼Œä»¥ç‰¹å®šé¡ºåºæŠ“å–åŒ…å«ç‰¹å®šæ–‡æœ¬æˆ–å†…å®¹çš„ç½‘é¡µï¼› æ•°æ®åŒ…è£…ï¼Œå°†æ•°æ®ä»æŸç§åŸæ ¼å¼è½¬æ¢ä¸ºå¦å¤–ä¸€ç§æ ¼å¼ï¼› å­—ç¬¦ä¸²è§£æï¼Œä¾‹å¦‚æ•è·æ‰€æ‹¥æœ‰ URL çš„ GET å‚æ•°ï¼Œæˆ–æ•è·ä¸€ç»„åœ†æ‹¬å¼§å†…çš„æ–‡æœ¬ï¼› å­—ç¬¦ä¸²æ›¿ä»£ï¼Œå°†å­—ç¬¦ä¸²ä¸­çš„æŸä¸ªå­—ç¬¦æ›¿æ¢ä¸ºå…¶å®ƒå­—ç¬¦ã€‚ å‚è€ƒé“¾æ¥https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650749657&amp;idx=4&amp;sn=da4852cb0c4919316d801fe19a64901d&amp;chksm=871afea7b06d77b1bda42ac134c5dddad5af24647f62a5a8e7bdcef4499f40fd4c97045a6f3d&amp;mpshare=1&amp;scene=23&amp;srcid=1009dbNFxJJahsQK6NGw4wS3%23rd pythonæ­£åˆ™è¡¨è¾¾å¼çš„ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼matchå’Œsearchçš„åŒºåˆ«]]></content>
      <categories>
        <category>çˆ¬è™«</category>
      </categories>
      <tags>
        <tag>æ­£åˆ™è¡¨è¾¾å¼</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019å¹´è®¡åˆ’]]></title>
    <url>%2Fpost%2F2019-plans%2F</url>
    <content type="text"><![CDATA[è¯»ä¸ªåšå‰ä»–å¯ä»¥å¼¹å”±å‡ é¦–æµè¡Œæ­Œæ›²å‡æœŸæŠ¥ä¸ªç­ï¼Œè¯·è€å¸ˆæŒ‡ç‚¹ä¸€ä¸‹ ç²¾è¯»50ç¯‡è®ºæ–‡å¹³å‡ä¸‹æ¥ï¼Œæ¯å‘¨éƒ½éœ€è¦è¯»ä¸€ç¯‡ï¼Œæ¯ä¸€ç¯‡éƒ½è¦å‹¾åˆ’ï¼Œå‘åšå®¢ã€‚ å‘2ç¯‡paperä¸ŠåŠå¹´ç§¯ç´¯çŸ¥è¯†ï¼Œåšå®éªŒï¼Œæš‘å‡å¼€å§‹å†™ è€ƒæ‰˜ç¦è¿‡90ä¸ŠåŠå¹´èƒŒå•è¯ç­‰ ä¹°ä¸ªå¾®å•å­¦ä¸ªæ‹ç…§ä¸æ€¥ï¼Œç°åœ¨æ²¡é’±ï¼Œä¸‹åŠå¹´å…¥æ‰‹ä¸€ä¸ªå§ è‡ªå·±å‡ºé—¨æ—…è¡Œä¸€æ¬¡å—¯ï¼Œè¦æ˜¯è®ºæ–‡å†™å®Œå°±å»å§ï¼ç›®å‰è®¡åˆ’å»å±±ä¸œï¼Œé¡ºä¾¿çœ‹ä¸€ä¸‹æˆ‘äºŒå§‘ã€‚5å¤©å·¦å³çš„æ—…è¡Œå§ï¼ å­¦ä¹ æ»‘æ¿å¼€å§‹å¥èº«æ¯å‘¨å»ä¸‰æ¬¡å¥èº«æˆ¿]]></content>
      <categories>
        <category>annual plans</category>
      </categories>
      <tags>
        <tag>plan</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ã€ŠAn Automatic Knowledge Graph Construction System for K-12 Educationã€‹é˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FAn%20Automatic%20Knowledge%20Graph%20Construction%20System%20for%20K-12%20Education%2F</url>
    <content type="text"><![CDATA[è®ºæ–‡åŸå€ã€‚æœ¬ç¯‡æ–‡ç« ä¸»è¦æå‡ºäº†ä¸€ä¸ªè‡ªåŠ¨åŒ–æ„å»ºæ•°å­¦é¢†åŸŸçŸ¥è¯†å›¾è°±çš„ç³»ç»Ÿï¼Œä¸»è¦åº”ç”¨çš„äº‹NERå’Œæ•°æ®æŒ–æ˜æŠ€æœ¯ï¼Œå…¶ä¸­NERä¸»è¦æ˜¯æŠ½å–æ•°å­¦æ¦‚å¿µï¼Œæ¦‚å¿µé—´çš„å…³ç³»æ˜¯ä½œè€…è‡ªå·±æ„å»ºçš„ï¼ˆä¾‹å¦‚å…ˆä¿®å…³ç³»ï¼‰ã€‚å¯¹äºæ•°æ®é›†ï¼Œä½œè€…ä¸»è¦ä»the Chinese curriculum standards of mathematicsä¸Šæå–çš„æ¦‚å¿µå®ä½“ï¼Œä»è‡ªå·±çš„SLPå¹³å°ä¸Šï¼Œé€šè¿‡å¯¹å­¦ç”Ÿè¡¨ç°æ¥æå–å…³ç³»ï¼ˆæŠŠè¿™éƒ¨åˆ†ä½œä¸ºæ•°æ®æŒ–æ˜ï¼‰ã€‚æœ¬ç¯‡æ–‡ç« å®é™…ä¸Šå¯ä»¥ä½œä¸ºæ„å»ºç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†å›¾è°±çš„ä¸€ä¸ªå‚è€ƒã€‚ challenges the desired educational concept entities are more abstract than real world entities like PERSON, ORGANIZATION, LOCATION the desired relations are more cognitive and implicit, so cannot be derived from the literal meanings of text like generic knowledge graphs contributions a novel but practical system entity recognition (NER) &amp; association rule mining algorithms demonstrate an exemplary case with constructing a knowledge graph for the subject of mathematics SYSTEM OVERVIEW Educational Concept Extraction Module: Implicit Relation Identification Module CONCEPT EXTRACTION çº¿æ€§é“¾å¼CRFæ¨¡å‹ æ ‡ç­¾é¢„æµ‹ RELATION IDENTIFICATIONä¸¤ç§æ–¹æ³• support confidence From the perspective of prerequisite relation, if concept si is a prerequisite of concept sj, learners who do not master sivery likely do not master sj, and learners who master sjmost likely master si. EXEMPLARY CASE AND SYSTEM EVALUATIONConcept ExtractionDatasetthe Chinese curriculum standards of mathematics published by the ministry of education as the main data source Evaluation adopt precision, recall and F1- score The ground truth is manually labeled by two domain experts. Relation IdentificationDataset studentsâ€™ performance data collected by our SLP platform. EvaluationThe ground truth of the prerequisite relations between selected 9 concepts are annotated manually by two domain experts.]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>çŸ¥è¯†å›¾è°±</tag>
        <tag>NER</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ã€ŠRECURRENT NEURAL NETWORK REGULARIZATIONã€‹é˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FRECURRENT%20NEURAL%20NETWORK%20REGULARIZATION%2F</url>
    <content type="text"><![CDATA[è®ºæ–‡é“¾æ¥ã€‚è¿™ç¯‡è®ºæ–‡æå‡ºäº†LSTMçš„dropoutç­–ç•¥æ¥é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œå³åªåœ¨éå¾ªç¯é“¾æ¥å¤„é‡‡å–dropoutã€‚åœ¨BasicLSTMCellçš„æ¥å£å°±æ˜¯ä¾æ®è¿™ç¯‡è®ºæ–‡å®ç°çš„ã€‚ æ–‡ç« æ•´ä½“æ¶æ„å’Œé‡ç‚¹ contribution present a simple regularization technique background dropout Srivastava(2013)ï¼Œå¯¹äºå‰å‘åé¦ˆç½‘ç»œæœ€æœ‰åŠ›çš„æ­£åˆ™åŒ–æ–¹æ³•å¹¶ä¸èƒ½å¾ˆå¥½çš„åº”ç”¨åœ¨RNNsä¸Šã€‚è¿™å¯¼è‡´RNNsè§„æ¨¡éƒ½å¾ˆå°ï¼Œå› ä¸ºå¤ªå¤§ä¼šè¿‡æ‹Ÿåˆã€‚ Bayer et al. (2013)æŒ‡å‡ºäº†å·ç§¯dropoutä¸èƒ½åœ¨RNNsä¸Šå¾ˆå¥½å·¥ä½œçš„åŸå› æ˜¯å¾ªç¯ä¼šæ”¾å¤§å™ªéŸ³ã€‚ modelä»¿å°„å˜æ¢ï¼ˆaffine transformï¼‰å…³äºä»¿å°„å˜æ¢ï¼šçº¿æ€§å˜æ¢åŠ ä¸Šå¹³ç§»ï¼Œç›—ä¸ªçŸ¥ä¹ä¸Šçš„å›¾ï¼ˆåŸæ–‡é“¾æ¥ï¼‰ æ¨¡å‹ä¸»ä½“é‡‡ç”¨çš„æ˜¯Graves et al. (2013) dropoutç­–ç•¥ The main contribution of this paper is a recipe for applying dropout to LSTMs in a way that success- fully reduces overfitting. The main idea is to apply the dropout operator only to the non-recurrent connections. è§‚å¯Ÿå…¬å¼ï¼Œå®é™…ä¸Šå°±æ˜¯é€šè¿‡åœ¨å±‚é—´ä¼ é€’ä¸­åº”ç”¨dropoutã€‚å¦‚ä¸‹å›¾ä¸­è™šçº¿æ‰€ç¤ºã€‚ ä»ä¸Šå›¾ä¸­ä¹Ÿå¯ä»¥çœ‹åˆ°ï¼Œè¯¥dropoutçš„æ¬¡æ•°åªå’Œç½‘ç»œæ·±åº¦æœ‰å…³ï¼ˆæ•°å€¼ä¸ºç½‘ç»œæ·±åº¦+1ï¼‰ã€‚ experimentså®éªŒéƒ¨åˆ†ä½œè€…åšäº†4éƒ¨åˆ†å®éªŒæ¥è¯æ˜è‡ªå·±é‡‡ç”¨çš„æ–¹æ³•æœ‰æ•ˆï¼Œåˆ†åˆ«ä¸ºlanguage modeling, speech recognition, machine translation, image caption generationã€‚è¿™éƒ¨åˆ†æ²¡ä»€ä¹ˆéœ€è¦è§£é‡Šäº†ï¼Œæ„Ÿå…´è¶£å¯ä»¥è‡ªå·±çœ‹ä¸€ä¸‹å®éªŒã€‚]]></content>
      <categories>
        <category>ç¥ç»ç½‘ç»œ</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Understanding LSTM Networks]]></title>
    <url>%2Fpost%2FUnderstanding%20LSTM%20Networks%2F</url>
    <content type="text"><![CDATA[åŸæ–‡é“¾æ¥ã€‚è¿™ç¯‡æ–‡ç« å¾ˆå¥½å¾ˆç»†çš„ä¸€æ­¥ä¸€æ­¥çš„åˆ†è§£è®²è§£äº†LSTMï¼Œä¹‹å‰çœ‹è¿‡ä¸€ç¯‡ç¿»è¯‘çš„åšå®¢ï¼Œç°åœ¨è‡ªå·±ç¿»è¯‘ä¸€éï¼Œæ„Ÿè§‰å¯¹LSTMçš„è®¤è¯†åŠ æ·±äº†è®¸å¤šï¼Œè™½ç„¶è¿˜æ˜¯å¯¹LSTMä¸­å­˜æœ‰ä¸€äº›é—®é¢˜ï¼Œæ¯”å¦‚ä¸ºä»€ä¹ˆç”¨tanhï¼Œsigmoidï¼Œä¸ºä»€ä¸é‡‡ç”¨å…¶ä»–çš„ï¼Ÿï¼Œä½†æ˜¯çœ‹è¿‡ä¹‹åè‡³å°‘å¯¹LSTMæ²¡æœ‰é‚£ä¹ˆç•æƒ§ï¼Œä¸è§‰å¾—è¿‡äºå¤æ‚äº†ã€‚ LSTMRecurrent Neural Networks å¾ªç¯å…è®¸ä¿¡æ¯ä»ä¸€ä¸ªç½‘ç»œä¼ å…¥ä¸‹ä¸€ä¸ªã€‚ ä¸€ä¸ªå¾ªç¯ç½‘ç»œå¯ä»¥è¢«è®¤ä¸ºæ˜¯ç›¸åŒç½‘ç»œçš„å¤šä¸ªå¤åˆ¶ï¼Œæ¯ä¸€ä¸ªç½‘ç»œéƒ½å°†ä¿¡æ¯ä¼ é€’ç»™åç»§è€…ã€‚è¿™ç§ç±»ä¼¼é“¾çš„æ€§è´¨è¡¨æ˜ï¼Œé€’å½’ç¥ç»ç½‘ç»œä¸åºåˆ—å’Œåˆ—è¡¨å¯†åˆ‡ç›¸å…³ã€‚ The Problem of Long-Term DependenciesRNNçš„ä¸€ä¸ªå¸å¼•åŠ›æ˜¯ä»–ä»¬å¯èƒ½èƒ½å¤Ÿå°†å…ˆå‰ä¿¡æ¯è¿æ¥åˆ°å½“å‰ä»»åŠ¡ã€‚ æœ‰æ—¶ï¼Œæˆ‘ä»¬åªéœ€è¦æŸ¥çœ‹æœ€è¿‘çš„ä¿¡æ¯æ¥æ‰§è¡Œå½“å‰ä»»åŠ¡ã€‚ä¾‹å¦‚ï¼š If we are trying to predict the last word in â€œthe clouds are in the sky,â€ we donâ€™t need any further context â€“ itâ€™s pretty obvious the next word is going to be sky. åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¦‚æœç›¸å…³ä¿¡æ¯ä¸å¾…é¢„æµ‹åœ°æ–¹ä¹‹é—´çš„å·®è·å¾ˆå°ï¼ŒRNNå¯ä»¥å­¦ä¹ ä½¿ç”¨è¿‡å»çš„ä¿¡æ¯ã€‚ ä½†æ˜¯ï¼Œå¯¹äºä¸€äº›æƒ…å†µï¼Œæˆ‘ä»¬éœ€è¦æ›´å¤šçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ Consider trying to predict the last word in the text â€œI grew up in Franceâ€¦ I speak fluent French.â€ è¿™æ—¶ï¼Œç›¸å…³ä¿¡æ¯ä¸éœ€è¦å˜å¾—éå¸¸å¤§çš„ç‚¹ä¹‹é—´çš„å·®è·å®Œå…¨æœ‰å¯èƒ½ã€‚ä¸å¹¸çš„æ˜¯ï¼Œéšç€å·®è·çš„æ‰©å¤§ï¼ŒRNNæ— æ³•å­¦ä¼šè¿æ¥ä¿¡æ¯ã€‚ The problem was explored in depth by Hochreiter (1991) [German] and Bengio, et al. (1994), who found some pretty fundamental reasons why it might be difficult. LSTM Networks LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn! æ ‡å‡†RNNsçš„é‡å¤æ¨¡å—åªæœ‰ä¸€ä¸ªç®€å•çš„ç»“æ„ï¼Œæ¯”å¦‚tanhå±‚ã€‚ LSTMsä¹Ÿæœ‰åƒè¿™ç§é“¾å¼ç»“æ„ï¼Œä½†æ˜¯å®ƒçš„é‡å¤æ¨¡å—å…·æœ‰ä¸åŒçš„ç»“æ„ã€‚ æœ‰å››ä¸ªï¼Œè€Œä¸æ˜¯ä¸€ä¸ªç¥ç»ç½‘ç»œå±‚ï¼Œä»¥ä¸€ç§éå¸¸ç‰¹æ®Šçš„æ–¹å¼è¿›è¡Œäº¤äº’ã€‚ åŸºæœ¬ç¬¦å·å¦‚ä¸‹ï¼š åœ¨ä¸Šå›¾ä¸­ï¼Œæ¯ä¸€è¡Œéƒ½æºå¸¦ä¸€ä¸ªå®Œæ•´çš„å‘é‡ï¼Œä»ä¸€ä¸ªèŠ‚ç‚¹çš„è¾“å‡ºåˆ°å…¶ä»–èŠ‚ç‚¹çš„è¾“å…¥ã€‚ ç²‰è‰²åœ†åœˆè¡¨ç¤ºé€ç‚¹è¿ç®—ï¼Œå¦‚çŸ¢é‡åŠ æ³•ï¼Œè€Œé»„è‰²æ¡†è¡¨ç¤ºç¥ç»ç½‘ç»œå±‚ã€‚ è¡Œåˆå¹¶è¡¨ç¤ºè¿æ¥ï¼Œè€Œè¡Œåˆ†å‰è¡¨ç¤ºå…¶å†…å®¹è¢«å¤åˆ¶ï¼Œå‰¯æœ¬å°†è½¬ç§»åˆ°ä¸åŒçš„ä½ç½®ã€‚ The Core Idea Behind LSTMsLSTMçš„å…³é”®æ˜¯å•å…ƒçŠ¶æ€ï¼Œæ°´å¹³çº¿è´¯ç©¿å›¾çš„é¡¶éƒ¨ã€‚ å•å…ƒçŠ¶æ€æœ‰ç‚¹åƒä¼ é€å¸¦ã€‚ å®ƒç›´æ¥æ²¿ç€æ•´ä¸ªé“¾è¿è¡Œï¼Œåªæœ‰ä¸€äº›å¾®å°çš„çº¿æ€§ç›¸äº’ä½œç”¨ã€‚ ä¿¡æ¯å¾ˆå®¹æ˜“æ²¿ç€å®ƒä¸å˜åœ°æµåŠ¨ã€‚ LSTMç¡®å®èƒ½å¤Ÿç§»é™¤æˆ–æ·»åŠ ä¿¡æ¯åˆ°ç»†èƒçŠ¶æ€ï¼Œç”±ç§°ä¸ºé—¨çš„ç»“æ„ç²¾å¿ƒè°ƒèŠ‚ã€‚ é—¨æ˜¯ä¸€ç§å¯é€‰æ‹©é€šè¿‡ä¿¡æ¯çš„æ–¹å¼ã€‚ å®ƒä»¬ç”±Sigmoidç¥ç»ç½‘ç»œå±‚å’Œé€ç‚¹ä¹˜æ³•è¿ç®—ç»„æˆã€‚ sigmoidå±‚è¾“å‡º0åˆ°1ä¹‹é—´çš„æ•°å­—ï¼Œæè¿°æ¯ä¸ªç»„ä»¶åº”è¯¥é€šè¿‡å¤šå°‘ã€‚ å€¼ä¸ºé›¶æ„å‘³ç€â€œä¸è®©ä»»ä½•ä¸œè¥¿é€šè¿‡â€ï¼Œè€Œå€¼ä¸º1åˆ™æ„å‘³ç€â€œè®©ä¸€åˆ‡éƒ½é€šè¿‡ï¼â€ LSTMå…·æœ‰ä¸‰ä¸ªè¿™æ ·çš„é—¨ï¼Œç”¨äºä¿æŠ¤å’Œæ§åˆ¶å•å…ƒçŠ¶æ€ã€‚ Step-by-Step LSTM Walk Throughæˆ‘ä»¬çš„ç¬¬ä¸€æ­¥å°±æ˜¯ç¡®å®šæˆ‘ä»¬å°†ä»å•å…ƒçŠ¶æ€ä¸­ä¸¢å¼ƒçš„ä¿¡æ¯ã€‚è¿™ä¸ªå†³å®šæ˜¯ç”±ä¸€ä¸ªç§°ä¸ºâ€œé—å¿˜é—¨å±‚â€çš„sigmoidå±‚å†³å®šçš„ã€‚å®ƒæŸ¥çœ‹$h_{t-1}$å’Œ$x_t$ï¼Œå¹¶ä¸ºå•å…ƒçŠ¶æ€$C_{t-1}$ä¸­çš„æ¯ä¸€ä¸ªæ•°å­—è¾“å‡ºä¸€ä¸ªä»‹äº0å’Œ1ä¹‹é—´çš„æ•°å­—ã€‚1ä»£è¡¨â€œå®Œå…¨ä¿ç•™è¿™ä¸ªâ€ï¼Œè€Œ0ä»£è¡¨â€œå®Œå…¨èˆå¼ƒè¿™ä¸ªâ€ã€‚ è®©æˆ‘ä»¬å›åˆ°æˆ‘ä»¬çš„è¯­è¨€æ¨¡å‹ç¤ºä¾‹ï¼Œè¯•å›¾æ ¹æ®ä»¥å‰çš„æ‰€æœ‰å•è¯é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ã€‚ åœ¨è¿™æ ·çš„é—®é¢˜ä¸­ï¼Œå•å…ƒçŠ¶æ€å¯èƒ½åŒ…æ‹¬å½“å‰å—è¯•è€…çš„æ€§åˆ«ï¼Œå› æ­¤å¯ä»¥ä½¿ç”¨æ­£ç¡®çš„ä»£è¯ã€‚ å½“æˆ‘ä»¬çœ‹åˆ°ä¸€ä¸ªæ–°ä¸»é¢˜æ—¶ï¼Œæˆ‘ä»¬æƒ³è¦å¿˜è®°æ—§ä¸»é¢˜çš„æ€§åˆ«ã€‚ ä¸‹ä¸€æ­¥æ˜¯ç¡®å®šæˆ‘ä»¬å°†åœ¨å•å…ƒçŠ¶æ€ä¸­å­˜å‚¨å“ªäº›æ–°ä¿¡æ¯ã€‚ è¿™æœ‰ä¸¤ä¸ªéƒ¨åˆ†ã€‚ é¦–å…ˆï¼Œç§°ä¸ºâ€œè¾“å…¥é—¨å±‚â€çš„sigmoidå±‚å†³å®šæˆ‘ä»¬å°†æ›´æ–°å“ªäº›å€¼ã€‚ æ¥ä¸‹æ¥ï¼Œtanhå±‚åˆ›å»ºå¯ä»¥æ·»åŠ åˆ°çŠ¶æ€çš„æ–°å€™é€‰å€¼$\tilde{C}_t$çš„å‘é‡ã€‚ åœ¨ä¸‹ä¸€æ­¥ä¸­ï¼Œæˆ‘ä»¬å°†ç»“åˆè¿™ä¸¤ä¸ªæ¥åˆ›å»ºçŠ¶æ€æ›´æ–°ã€‚ åœ¨æˆ‘ä»¬çš„è¯­è¨€æ¨¡å‹çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬æƒ³è¦å°†æ–°ä¸»é¢˜çš„æ€§åˆ«æ·»åŠ åˆ°å•å…ƒæ ¼çŠ¶æ€ï¼Œä»¥æ›¿æ¢æˆ‘ä»¬å¿˜è®°çš„æ—§ä¸»é¢˜ã€‚ ç°åœ¨æ˜¯æ—¶å€™å°†æ—§çš„å•å…ƒçŠ¶æ€$C_{T-1}$æ›´æ–°ä¸ºæ–°çš„å•å…ƒçŠ¶æ€$C_t$ã€‚ å‰é¢çš„æ­¥éª¤å·²ç»å†³å®šè¦åšä»€ä¹ˆï¼Œæˆ‘ä»¬åªéœ€è¦å®é™…åšåˆ°è¿™ä¸€ç‚¹ã€‚ æˆ‘ä»¬å°†æ—§çŠ¶æ€ä¹˜ä»¥$f_t$ï¼Œå¿˜è®°æˆ‘ä»¬ä¹‹å‰å†³å®šå¿˜è®°çš„äº‹æƒ…ã€‚ ç„¶åæˆ‘ä»¬æ·»åŠ $i_t * \tilde{C}_t$ã€‚ è¿™æ˜¯æ–°çš„å€™é€‰å€¼ï¼Œæ ¹æ®æˆ‘ä»¬å†³å®šæ›´æ–°æ¯ä¸ªçŠ¶æ€çš„å€¼æ¥ç¼©æ”¾ã€‚ åœ¨è¯­è¨€æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å®é™…ä¸Šæ”¾å¼ƒäº†å…³äºæ—§ä¸»é¢˜çš„æ€§åˆ«çš„ä¿¡æ¯å¹¶æ·»åŠ æ–°ä¿¡æ¯ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨å‰é¢çš„æ­¥éª¤ä¸­æ‰€åšçš„é‚£æ ·ã€‚ æœ€åï¼Œæˆ‘ä»¬éœ€è¦å†³å®šæˆ‘ä»¬è¦è¾“å‡ºçš„å†…å®¹ã€‚ æ­¤è¾“å‡ºå°†åŸºäºæˆ‘ä»¬çš„å•å…ƒçŠ¶æ€ï¼Œä½†å°†æ˜¯è¿‡æ»¤ç‰ˆæœ¬ã€‚ é¦–å…ˆï¼Œæˆ‘ä»¬è¿è¡Œä¸€ä¸ªsigmoidå±‚ï¼Œå®ƒå†³å®šæˆ‘ä»¬è¦è¾“å‡ºçš„å•å…ƒçŠ¶æ€çš„å“ªäº›éƒ¨åˆ†ã€‚ ç„¶åï¼Œæˆ‘ä»¬å°†å•å…ƒæ ¼çŠ¶æ€è®¾ç½®ä¸ºtanhï¼ˆå°†å€¼æ¨åˆ°ä»‹äº-1å’Œ1ä¹‹é—´ï¼‰å¹¶å°†å…¶ä¹˜ä»¥sigmoidé—¨çš„è¾“å‡ºï¼Œä»¥ä¾¿æˆ‘ä»¬åªè¾“å‡ºæˆ‘ä»¬å†³å®šçš„éƒ¨åˆ†ã€‚ å¯¹äºè¯­è¨€æ¨¡å‹ç¤ºä¾‹ï¼Œç”±äºå®ƒåªæ˜¯çœ‹åˆ°ä¸€ä¸ªä¸»é¢˜ï¼Œå®ƒå¯èƒ½æƒ³è¦è¾“å‡ºä¸åŠ¨è¯ç›¸å…³çš„ä¿¡æ¯ï¼Œä»¥é˜²æ¥ä¸‹æ¥ä¼šå‘ç”Ÿä»€ä¹ˆã€‚ ä¾‹å¦‚ï¼Œå®ƒå¯èƒ½è¾“å‡ºä¸»è¯­æ˜¯å•æ•°è¿˜æ˜¯å¤æ•°ï¼Œä»¥ä¾¿æˆ‘ä»¬çŸ¥é“åŠ¨è¯åº”è¯¥ä¸ä»€ä¹ˆå½¢å¼å…±è½­ï¼Œå¦‚æœæ¥ä¸‹æ¥çš„è¯ã€‚ Variants on Long Short Term Memoryåˆ°ç›®å‰ä¸ºæ­¢æˆ‘æ‰€æè¿°çš„æ˜¯ä¸€ä¸ªéå¸¸æ­£å¸¸çš„LSTMã€‚ ä½†å¹¶éæ‰€æœ‰LSTMéƒ½ä¸ä¸Šè¿°ç›¸åŒã€‚ äº‹å®ä¸Šï¼Œä¼¼ä¹å‡ ä¹æ‰€æœ‰æ¶‰åŠLSTMçš„è®ºæ–‡éƒ½ä½¿ç”¨ç•¥æœ‰ä¸åŒçš„ç‰ˆæœ¬ã€‚ å·®å¼‚å¾ˆå°ï¼Œä½†å€¼å¾—ä¸€æçš„æ˜¯å…¶ä¸­ä¸€äº›ã€‚ One popular LSTM variant, introduced by Gers &amp; Schmidhuber (2000), is adding â€œpeephole connections.â€ This means that we let the gate layers look at the cell state. ä¸Šé¢çš„å›¾è¡¨ä¸ºæ‰€æœ‰é—¨å¢åŠ äº†çª¥è§†å­”ï¼ˆpeepholeï¼‰ï¼Œä½†æ˜¯è®¸å¤šè®ºæ–‡ä¼šç»™ä¸€äº›çª¥è§†å­”è€Œä¸æ˜¯å…¶ä»–çš„ã€‚ å¦ä¸€ç§å˜åŒ–æ˜¯ä½¿ç”¨è€¦åˆçš„é—å¿˜å’Œè¾“å…¥é—¨ã€‚ æˆ‘ä»¬ä¸æ˜¯å•ç‹¬å†³å®šå¿˜è®°ä»€ä¹ˆä»¥åŠåº”è¯¥æ·»åŠ æ–°ä¿¡æ¯ï¼Œè€Œæ˜¯ä¸€èµ·åšå‡ºè¿™äº›å†³å®šã€‚æˆ‘ä»¬ä»…ä»…ä¼šå½“æˆ‘ä»¬åœ¨å½“å‰ä½ç½®å°†è¦è¾“å…¥æ—¶å¿˜è®°ã€‚æˆ‘ä»¬ä»…ä»…è¾“å…¥æ–°çš„å€¼åˆ°é‚£äº›æˆ‘ä»¬å·²ç»å¿˜è®°æ—§çš„ä¿¡æ¯çš„é‚£äº›çŠ¶æ€ ã€‚ å¦ä¸€ä¸ªæ”¹åŠ¨è¾ƒå¤§çš„å˜ä½“æ˜¯ Gated Recurrent Unit (GRU)ï¼Œè¿™æ˜¯ç”± Cho, et al. (2014) æå‡ºã€‚å®ƒå°†é—å¿˜å’Œè¾“å…¥é—¨ç»„åˆæˆä¸€ä¸ªâ€œæ›´æ–°é—¨â€ã€‚å®ƒè¿˜åˆå¹¶äº†å•å…ƒçŠ¶æ€å’Œéšè—çŠ¶æ€ï¼Œå¹¶è¿›è¡Œäº†ä¸€äº›å…¶ä»–æ›´æ”¹ã€‚ ç”±æ­¤äº§ç”Ÿçš„æ¨¡å‹æ¯”æ ‡å‡†LSTMæ¨¡å‹ç®€å•ï¼Œå¹¶ä¸”è¶Šæ¥è¶Šå—æ¬¢è¿ã€‚ è¿™äº›åªæ˜¯æœ€ç€åçš„LSTMå˜ç§ä¸­çš„ä¸€å°éƒ¨åˆ†ã€‚ è¿˜æœ‰å¾ˆå¤šå…¶ä»–çš„ä¸œè¥¿ï¼Œå¦‚ Yao, et al. (2015) æå‡ºçš„ Depth Gated RNNã€‚ è¿˜æœ‰ä¸€äº›å®Œå…¨ä¸åŒçš„è§£å†³é•¿æœŸä¾èµ–å…³ç³»çš„æ–¹æ³•ï¼Œå¦‚ Koutnik, et al. (2014) æå‡ºçš„ Clockwork RNNã€‚ å“ªç§å˜ä½“æœ€å¥½ï¼Ÿ å·®å¼‚æ˜¯å¦é‡è¦ï¼Ÿ Greff, et al. (2015) å¯¹æµè¡Œå˜ä½“è¿›è¡Œäº†å¾ˆå¥½çš„æ¯”è¾ƒï¼Œå‘ç°å®ƒä»¬å‡ ä¹å®Œå…¨ç›¸åŒã€‚Jozefowicz, et al. (2015) æµ‹è¯•äº†è¶…è¿‡ä¸€ä¸‡ä¸ªRNNæ¶æ„ï¼Œæ‰¾åˆ°äº†ä¸€äº›åœ¨æŸäº›ä»»åŠ¡ä¸Šæ¯”LSTMæ›´å¥½çš„æ¶æ„ã€‚ Conclusionæ—©äº›æ—¶å€™ï¼Œæˆ‘æåˆ°äº†äººä»¬ç”¨RNNå–å¾—çš„æ˜¾ç€æˆæœã€‚åŸºæœ¬ä¸Šæ‰€æœ‰è¿™äº›éƒ½æ˜¯ä½¿ç”¨LSTMå®ç°çš„ã€‚å¯¹äºå¤§å¤šæ•°ä»»åŠ¡æ¥è¯´ï¼Œå®ƒä»¬ç¡®å®å·¥ä½œå¾—æ›´å¥½ï¼ ä½œä¸ºä¸€ç»„æ–¹ç¨‹å†™ä¸‹æ¥ï¼ŒLSTMçœ‹èµ·æ¥éå¸¸ä»¤äººç”Ÿç•ã€‚å¸Œæœ›ï¼Œåœ¨è¿™ç¯‡æ–‡ç« ä¸­é€æ­¥èµ°è¿‡å®ƒä»¬ä½¿ä»–ä»¬æ›´åŠ å¹³æ˜“è¿‘äººã€‚ LSTMæ˜¯æˆ‘ä»¬ç”¨RNNå®ç°çš„é‡è¦ä¸€æ­¥ã€‚å¾ˆè‡ªç„¶åœ°æƒ³çŸ¥é“ï¼šè¿˜æœ‰å¦ä¸€ä¸ªé‡è¦çš„ä¸€æ­¥å—ï¼Ÿç ”ç©¶äººå‘˜çš„å…±åŒè§‚ç‚¹æ˜¯ï¼šâ€œæ˜¯çš„ï¼ä¸‹ä¸€æ­¥æ˜¯å®ƒçš„æ³¨æ„ï¼â€œæˆ‘ä»¬çš„æƒ³æ³•æ˜¯è®©RNNçš„æ¯ä¸€æ­¥éƒ½ä»ä¸€äº›æ›´å¤§çš„ä¿¡æ¯é›†ä¸­é€‰æ‹©ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨ä½¿ç”¨RNNåˆ›å»ºæè¿°å›¾åƒçš„æ ‡é¢˜ï¼Œåˆ™å¯èƒ½ä¼šé€‰æ‹©å›¾åƒçš„ä¸€éƒ¨åˆ†æ¥æŸ¥çœ‹å…¶è¾“å‡ºçš„æ¯ä¸ªå•è¯ã€‚ äº‹å®ä¸Šï¼Œ Xu, et al.(2015) åšåˆ°è¿™ä¸€ç‚¹ - å¦‚æœä½ æƒ³æ¢ç´¢æ³¨æ„åŠ›ï¼Œè¿™å¯èƒ½æ˜¯ä¸€ä¸ªæœ‰è¶£çš„èµ·ç‚¹ï¼ä½¿ç”¨æ³¨æ„åŠ›å·²ç»å–å¾—äº†è®¸å¤šéå¸¸ä»¤äººå…´å¥‹çš„ç»“æœï¼Œä¼¼ä¹è¿˜æœ‰æ›´å¤šçš„äº‹æƒ…å³å°†æ¥ä¸´â€¦â€¦ æ³¨æ„åŠ›ä¸æ˜¯RNNç ”ç©¶ä¸­å”¯ä¸€ä»¤äººå…´å¥‹çš„é—®é¢˜ã€‚ä¾‹å¦‚ï¼ŒKalchbrenner, et al. (2015) çš„Grid LSTMsä¼¼ä¹éå¸¸æœ‰å¸Œæœ›ã€‚åœ¨ç”Ÿæˆæ¨¡å‹ä¸­ä½¿ç”¨RNNå·¥ä½œ - ä¾‹å¦‚Gregor, et al. (2015), Chung, et al. (2015), æˆ–è€… Bayer &amp; Osendorfer (2015) ä¼¼ä¹ä¹Ÿå¾ˆæœ‰è¶£ã€‚è¿‡å»å‡ å¹´å¯¹äºåå¤å‡ºç°çš„ç¥ç»ç½‘ç»œæ¥è¯´æ˜¯ä¸€ä¸ªæ¿€åŠ¨äººå¿ƒçš„æ—¶åˆ»ï¼Œå³å°†åˆ°æ¥çš„é‚£äº›æ‰¿è¯ºåªä¼šæ›´åŠ å¦‚æ­¤ï¼]]></content>
      <categories>
        <category>ç¥ç»ç½‘ç»œ</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ã€ŠLearning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translationã€‹é˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FLearning_Phrase_Representations_using_RNN_Encoder%E2%80%93Decoder_for_Statistical_Machine_Translation%2F</url>
    <content type="text"><![CDATA[åŸæ–‡é“¾æ¥ã€‚è¯¥è®ºæ–‡æ˜¯Sequence to Sequenceå­¦ä¹ çš„æœ€æ—©åŸå‹ï¼Œè®ºæ–‡ä¸­æå‡ºä¸€ç§å´­æ–°çš„RNN(GRU) Encoder-Decoderç®—æ³•ï¼Œè™½ç„¶æ–‡ç« å±äºæ¯”è¾ƒæ—§çš„æ–‡ç« ï¼Œä½†ä½œä¸ºseq2seqçš„åŸºç¡€åŸå‹ï¼Œè¿˜æ˜¯éœ€è¦é˜…è¯»äº†è§£ä¸€ä¸‹çš„ã€‚æ–‡ç« å†™çš„æ¯”è¾ƒè¯¦ç»†ï¼Œå„éƒ¨åˆ†ç»†èŠ‚éƒ½æœ‰è®²è§£ã€‚ æ–‡ç« çš„ä¸»è¦ç»“æ„ contribution a novel RNN Encoderâ€“Decoder èƒ½å¤Ÿå¤„ç†å˜é•¿åºåˆ— a novel hidden unit reset gate update gate RNN Encoderâ€“Decoderæ¨¡å‹ç»“æ„å›¾å¦‚ä¸‹ï¼š æ–‡ä¸­ä½œè€…å¯¹é½è¿›è¡Œæ€»ä½“æ¦‚è¿°ä¸ºï¼š From a probabilistic perspective, this new model is a general method to learn the conditional distribution over a variable-length sequence conditioned on yet another variable-length sequence ä»æ¦‚ç‡çš„è§’åº¦æ¥çœ‹ï¼Œè¿™ä¸ªæ–°æ¨¡å‹æ˜¯å­¦ä¹ åœ¨å¦ä¸€ä¸ªå¯å˜é•¿åº¦åºåˆ—æ¡ä»¶ä¸‹çš„å¯å˜é•¿åº¦åºåˆ—ä¸Šçš„æ¡ä»¶åˆ†å¸ƒçš„ä¸€èˆ¬æ–¹æ³• Encoderè¿™éƒ¨åˆ†æ˜¯ä¸€ä¸ªRNNå•å…ƒã€‚æ¯ä¸ªæ—¶é—´æ­¥ï¼Œæˆ‘ä»¬å‘Encoderä¸­è¾“å…¥ä¸€ä¸ªå­—/è¯ï¼ˆä¸€èˆ¬ä¸ºå‘é‡å½¢å¼ï¼‰ï¼Œç›´åˆ°æˆ‘ä»¬è¾“å…¥è¿™ä¸ªå¥å­çš„æœ€åä¸€ä¸ªå­—/è¯$X_T$ï¼Œç„¶åè¾“å…¥æ•´ä¸ªå¥å­çš„è¯­ä¹‰å‘é‡cã€‚ç”±äºRNNçš„ç‰¹å¸¦ä½ å°±æ˜¯æŠŠå‰é¢æ¯ä¸€æ­¥çš„è¾“å…¥ä¿¡æ¯éƒ½è€ƒè™‘è¿›æ¥ï¼Œæ‰€ä»¥ç†è®ºä¸Šè¿™ä¸ªcå°±åŒ…å«äº†æ•´ä¸ªå¥å­çš„æ‰€æœ‰ä¿¡æ¯ã€‚æˆ‘ä»¬å¯ä»¥æŠŠå½“æˆè¿™ä¸ªå¥å­çš„ä¸€ä¸ªè¯­ä¹‰è¡¨ç¤ºã€‚ DecoderDecoderæ˜¯å¦ä¸€ä¸ªRNNï¼Œå…¶è¢«è®­ç»ƒå‡ºæ¥ä»¥é€šè¿‡é¢„æµ‹éšè—çŠ¶æ€$h_t$çš„ä¸‹ä¸€ä¸ªç¬¦å·$y_t$æ¥ç”Ÿæˆè¾“å‡ºåºåˆ—ã€‚è®¡ç®—å…¬å¼å¦‚ä¸‹ h_t = f(h_{t-1},y_{t-1},c)ä¸‹ä¸€ä¸ªåºåˆ—çš„è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š P(y_t|y_{t-1},y_{t-2},\dots,y_1,c)=g(h_t,y_{t-1},c)Hidden Unitè¯¥éƒ¨åˆ†æ˜¯å¯¹å„éƒ¨åˆ†å…·ä½“çš„å…¬å¼è®²è§£ï¼Œå®é™…æ˜¯GRUçš„å…·ä½“å…¬å¼ç®—æ³•ï¼Œä¸å†æ­¤è¯¦ç»†å™è¿°äº†ã€‚ reset gate In this formulation, when the reset gate is close to 0, the hidden state is forced to ignore the pre- vious hidden state and reset with the current input only. This effectively allows the hidden state to drop any information that is found to be irrelevant later in the future, thus, allowing a more compact representation. è¿™æ®µåŸæ–‡ä¸»è¦è®²è§£äº†å¤ä½é—¨çš„ä½œç”¨ï¼šæœ‰æ•ˆåœ°å…è®¸éšè—çŠ¶æ€ä¸¢å¼ƒåœ¨å°†æ¥ç¨åå‘ç°ä¸ç›¸å…³çš„ä»»ä½•ä¿¡æ¯ï¼Œä»è€Œå…è®¸æ›´ç´§å‡‘çš„è¡¨ç¤ºã€‚ å½“æ•è·çŸ­æœŸä¾èµ–æ—¶ï¼Œå¤ä½é—¨æ´»è·ƒ update gate the update gate controls how much information from the previous hidden state will carry over to the current hidden state. æ›´æ–°é—¨æ§åˆ¶æ¥è‡ªå…ˆå‰éšè—çŠ¶æ€çš„å¤šå°‘ä¿¡æ¯å°†è½¬ç§»åˆ°å½“å‰éšè—çŠ¶æ€ã€‚ å½“æ•è·é•¿æœŸä¾èµ–æ—¶ï¼Œæ›´æ–°é—¨æ´»è·ƒ Statistical Machine Translation(SMT) Experimentsè¿™éƒ¨åˆ†ä½œè€…ä¸»è¦åšäº†é‡åŒ–åˆ†æå’Œæ€§è´¨åˆ†æï¼Œä¸»è¦å°±æ˜¯è¯´ä»–çš„æ¨¡å‹æ€ä¹ˆå‰å®³ã€‚ã€‚ã€‚ï¼ˆæ²¡æœ‰å…·ä½“çš„æ•°å€¼æŒ‡æ ‡ï¼Œç¿»è¯‘çš„è¿˜ä¸æ˜¯ä¸­è‹±ç¿»è¯‘ï¼Œæƒ³çœ‹çš„è¯å¯ä»¥å»çœ‹ä¸€ä¸‹ï¼Œå°±ä¸è´´å®éªŒç»“æœäº†ï¼‰ã€‚ futureè¿™é‡Œä½œè€…æå‡ºäº†å¯ä»¥ç”¨decoderç”Ÿæˆçš„ç›®æ ‡çŸ­è¯­æ¥æ›¿æ¢åŸå¥ä¸­çŸ­è¯­çš„æ€è·¯ï¼Œå¦‚æœæ²¡è®°é”™çš„è¯ï¼Œè¿™ä¸ªæƒ³æ³•å¥½åƒå¯¹åé¢çš„æœºå™¨ç¿»è¯‘æœ‰å¾ˆå¤§çš„æŒ‡å¯¼ä½œç”¨ã€‚]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>seq2seq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ã€ŠDifferentiating Concepts and Instances for Knowledge Graph Embeddingã€‹é˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2Fread_Differentiating_Concepts_and_Instances_for_Knowledge_Graph_Embedding%2F</url>
    <content type="text"><![CDATA[è®ºæ–‡è·å–åœ°å€ã€‚è¿™ç¯‡æ–‡ç« æœ€å¤§çš„äº®ç‚¹å°±æ˜¯æŠŠconceptæ˜ å°„ä¸ºä¸€ä¸ªçƒé¢ï¼Œç„¶åæŠŠinstanceæ˜ å°„ä¸ºä¸€ä¸ªå‘é‡ï¼Œé€šè¿‡è¿™ç§ç©ºé—´å…³ç³»æ¥è¿›è¡Œembeddingã€‚å¦‚æœinstanceå’Œconceptæ»¡è¶³InstanceOfçš„å…³ç³»ï¼Œåˆ™instanceåº”è¯¥åœ¨çƒå†…ï¼›å¦‚æœä¸¤ä¸ªconceptæ»¡è¶³SubClassOfçš„å…³ç³»ï¼Œåˆ™ä¸€ä¸ªçƒä¼šåœ¨å¦ä¸€ä¸ªçƒé¢å†…ã€‚ conceptA concept is a fundamental category of existence (Rosch, 1973) and can be reified by all of its actual or potential instances.Concepts, which represent a group of different instances sharing common properties, are essential information in knowledge representation. drawback of the previous method ignore to distinguish between concepts and instances will lead to two drawbacks: Insufficient concept representationï¼š cannot explicitly represent the difference between concepts and instances Lack transitivity of both isA relations: instanceOf and subClassOf (generally known as isA)isA relations exhibit transitivity contributions the first to propose and formalize the problem of knowledge graph embedding which differentiates between concepts and instances a novel knowledge embedding method named TransC state-of-the-art on link prediction and triple classification Translation-based ModelsTransE triple (h, r, t) should satisfy h + r â‰ˆ t loss function:$f_r(h,t) = ||h + r - t||^2_2$ suitable for 1-to-1 relations TransH It regards a relation vector r as a translation on a hyperplane with $w_r$ as the normal vector. loss function:$f_r(h,t) = ||h_{\bot} + r - t_{\bot}||^2_2$ï¼Œå…¶ä¸­$h_{\bot}=h-w^{\top}_r h w_r$ï¼Œ$t_{\bot}=t-w^{\top}_r t w_r$ suitable for 1-to-N, N-to-1, and N-to-N relations TransR/CTransR addresses the issue in TransE and TransH that some entities are similar in the entity space but comparably different in other specific aspects. loss function:$f_r(h,t) = ||M_rh +r -M_rt||^2_2$ï¼Œ$M_r$ for each relation r TransD considers the different types of entities and relations at the same time loss function:$f_r(h,t) = ||h_{\bot} + r - t_{\bot}||^2_2$ï¼Œ$h_{\bot} = M_{rh}h$å’Œ$t_{\bot} = M_{rt}t$ï¼Œ$M_{r,e}$ for each relation-entity pair (r, e) Bilinear ModelsRESCAL the first bilinear model It associates each entity with a vector to capture its latent semantics. Each relation is represented as a matrix which models pairwise interactions between latent factors. External Information Learning Models textual information entity descriptions Problem Formulationè¿™éƒ¨åˆ†ä¸­ä½œè€…è¯¦ç»†ä»‹ç»äº†çŸ¥è¯†å›¾è°±çš„ç»„æˆéƒ¨åˆ†ï¼šæ¦‚å¿µå’Œå®ä¾‹é›†ã€å…³ç³»é›†ï¼ˆåŒ…æ‹¬instanceOfã€subClassOfå’Œinstance relationï¼‰ï¼Œä¸‰å…ƒç»„é›†ï¼ˆæŒ‰ç…§å…³ç³»é›†åŒæ ·åˆ†ä¸ºä¸‰ä¸ªéƒ¨åˆ†ï¼‰ã€‚ä¸ºäº†èƒ½è¡¨è¾¾is Aå…³ç³»çš„ä¼ é€’æ€§ï¼Œä½œè€…å°†instanceOfå’ŒsubClassofä¸¤ç§å…³ç³»è¿›è¡Œäº†ç²¾å¿ƒçš„è®¾è®¡ï¼Œä¹Ÿæ˜¯è¯¥è®ºæ–‡çš„é‡ç‚¹ã€‚ For each concept c âˆˆ C, we learn a sphere s(p, m) with $p \in R^k$ and m denoting the sphere center and radius. TranCSpecifically, TransC encodes each concept in knowledge graph as a sphere and each instance as a vector in the same semantic space. InstanceOf Triple Representationloss functionï¼š$f_e(i,c) = ||i-p||_2 - m$ï¼Œå½“è¯¥å‡½æ•°å¤§äº0æ—¶è¿›è¡Œä¼˜åŒ–ï¼Œä½¿å…¶å°äºé›¶ã€‚ SubClassOf Triple Representation å¦‚å›¾æ‰€ç¤ºï¼Œå…¶ä¸­å­å›¾ï¼ˆaï¼‰ä¸ºç›®æ ‡çŠ¶æ€ã€‚ä¸¤ä¸ªæ¦‚å¿µçš„çš„åœ†å¿ƒè·ç¦»ï¼š$d = ||p_i - p_j||_2$ã€‚éœ€è¦åšåˆ°çš„å°±æ˜¯$d-(m_j -m_i) \leq 0$å¹¶ä¸”$ (m_j &gt; m_i)$ã€‚ Relational Triple Representationè¿™éƒ¨åˆ†æŒ‰ç…§TranEçš„æ€è·¯è¿›è¡Œå¤„ç†ï¼Œ$||h+r-t||^2_3$ train modelmargin based lossè¯¦è§£unit and bern Regarding the strategy of constructing negative labels, we use â€œunifâ€ to denote the traditional way of replacing head or tail with equal probability, and use â€œbern.â€ to denote reducing false negative labels by replacing head or tail with different probabilities. the following research directions find a more expressive model instead of spheres to represent concepts A concept may have different meanings in different triples. use several typical vectors of instances as a conceptâ€™s centers to represent different meanings of a concept.]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>çŸ¥è¯†å›¾è°±</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Discourse-Level Named Entity Recognition and Relation Extraction Dataset for Chinese Literature Text é˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2FREAD_A_Discourse-Level_Named_Entity_Recognition_and_Relation_Extraction_Dataset_for_Chinese_Literature_Text%2F</url>
    <content type="text"><![CDATA[è¯¥è®ºæ–‡æœ€ä¸»è¦çš„è´¡çŒ®å°±æ˜¯è¿™ä¸ªæ•°æ®ï¼Œæ•°æ®é›†åœ°å€ã€‚è®ºæ–‡ä¸­æåˆ°çš„æ ‡æ ‡ç­¾è¿‡ç¨‹ä¹Ÿæ˜¯ä¸€ä¸ªåˆ›æ–°ç‚¹ï¼Œè¿ç”¨äº†å¯å‘å¼å’Œæœºå™¨è¾…åŠ©æ ‡æ ‡ç­¾ï¼Œè¿™æ ·å¯ä»¥æé«˜å‡†ç¡®åº¦å¹¶å‡å°‘æ ‡æ³¨äººå‘˜å·¥ä½œã€‚ contribution provide a new dataset for joint learning of NER and RE for Chinese literature text the proposed dataset is based on the discourse level which provides additional context information introduce some widely used models to conduct experiments tagging processtwo methods:one is a heuristic tagging method and another is a machine auxiliary tagging method. Step 1: First Tagging Processfind a problem of data inconsistency. Step 2: Heuristic Tagging Based on Generic disambiguating Rules For example, remove all adjective words and only tag â€œentity headerâ€ . re-annotate all articles and correct all inconsistency entities and relations based on the heuristic rules. Step 3: Machine Auxiliary Tagging The core idea is to train a model to learn annotation guidelines on the subset of the corpus and produce predicted tags on the rest data. CRF tagging set Annotation FormatEntityEach entity is identified by T tag, which takes several attributes. Id: a unique number identifying the entity within the document. It starts at 0, and is incremented every time a new entity is identified within the same document. Type: one of the entity tags. Begin Index: the begin index of an entity. It starts at 0, and is incremented every character. End Index: the end index of an entity. It starts at 0, and is incremented every character. Value: words being referred to an identifiable object. RelationEach relation is identified by R tag, which can take several attributes: Id: a unique number identifying the relation within the document. It starts at 0, and is incremented every time a new relation is identified within the same document. Arg1 and Arg2: two entities associated with a relation. Type: one of the relation tags.]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>NER</tag>
        <tag>RE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandasçš„æ•°æ®ç±»å‹æ“ä½œ]]></title>
    <url>%2Fpost%2FPandas_data_type_manipulation%2F</url>
    <content type="text"><![CDATA[åœ¨åŸæ–‡é“¾æ¥ä¸­æ‘˜æŠ„å‡ºéƒ¨åˆ†ä¿¡æ¯ä½œä¸ºè®°å½•å½¢æˆæœ¬æ–‡ã€‚ æ•°æ®ç±»å‹ Pandas dtype Python ç±»å‹ NumPy ç±»å‹ ç”¨é€” object str string_, unicode_ æ–‡æœ¬ int64 int int_, int8, int16, int32, int64, uint8, uint16, uint32, uint64 æ•´æ•° float64 float float_, float16, float32, float64 æµ®ç‚¹æ•° bool bool bool_ å¸ƒå°”å€¼ datetime64 NA NA æ—¥æœŸæ—¶é—´ timedelta[ns] NA NA æ—¶é—´å·® category NA NA æœ‰é™é•¿åº¦çš„æ–‡æœ¬å€¼åˆ—è¡¨ æ•°æ®ç±»å‹æ“ä½œ ä½¿ç”¨df.dtypeså¯ä»¥æ˜¾ç¤ºæ•°æ®æ‰€æœ‰åˆ—çš„ç±»å‹ df.infoï¼ˆï¼‰ å‡½æ•°å¯ä»¥æ˜¾ç¤ºæ›´æœ‰ç”¨çš„ä¿¡æ¯ ä½¿ç”¨ astype() å‡½æ•°ä½¿ç”¨æ¡ä»¶ æ•°æ®æ˜¯å¹²å‡€çš„ï¼Œå¯ä»¥ç®€å•åœ°è§£é‡Šä¸ºä¸€ä¸ªæ•°å­— ä½ æƒ³è¦å°†ä¸€ä¸ªæ•°å€¼è½¬æ¢ä¸ºä¸€ä¸ªå­—ç¬¦ä¸²å¯¹è±¡ å¦‚æœæ•°æ®å…·æœ‰éæ•°å­—å­—ç¬¦æˆ–å®ƒä»¬é—´ä¸åŒè´¨ï¼ˆhomogeneousï¼‰ï¼Œé‚£ä¹ˆ astype() å¹¶ä¸æ˜¯ç±»å‹è½¬æ¢çš„å¥½é€‰æ‹©ã€‚ä½ éœ€è¦è¿›è¡Œé¢å¤–çš„å˜æ¢æ‰èƒ½å®Œæˆæ­£ç¡®çš„ç±»å‹è½¬æ¢ã€‚ ä½¿ç”¨æ–¹å¼ä¸ºäº†çœŸæ­£ä¿®æ”¹åŸå§‹ dataframe ä¸­æ•°æ®ç±»å‹ï¼Œè®°å¾—æŠŠ astype() å‡½æ•°çš„è¿”å›å€¼é‡æ–°èµ‹å€¼ç»™ dataframeï¼Œå› ä¸º astype() ä»…è¿”å›æ•°æ®çš„å‰¯æœ¬è€Œä¸åŸåœ°ä¿®æ”¹ã€‚ å‚è€ƒé“¾æ¥ https://juejin.im/post/5acc36e66fb9a028d043c2a5]]></content>
      <categories>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>pandas</tag>
        <tag>python</tag>
        <tag>æ•°æ®åˆ†æ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pythonçˆ¬å–ä¸­æ–‡ç½‘é¡µæ—¶ä¸­æ–‡å­—ç¬¦å˜è‹±æ–‡çš„è§£å†³æ–¹æ³•]]></title>
    <url>%2Fpost%2Fsolution_of_python_for_Chinese_characters_to_become_English_when_crawling_Chinese_web_pages%2F</url>
    <content type="text"><![CDATA[ä½¿ç”¨pythonçš„scrapyçˆ¬å–ç½‘é¡µæ—¶ï¼Œæºä»£ç ä¸­çš„ä¸­æ–‡å­—ç¬¦åœ¨çˆ¬å–ä¸‹æ¥åå˜æˆäº†è‹±æ–‡å­—ç¬¦ã€‚ é—®é¢˜ä¸¾ä¾‹ä¾‹å¦‚ï¼ŒåŸç½‘é¡µä¸ºï¼š çˆ¬å–ç»“æœä¸ºï¼š è§£å†³æ–¹æ³•ä¿®æ”¹è¯·æ±‚å¤´ï¼šåœ¨settings.pyæ–‡ä»¶ä¸­æ‰¾åˆ°ä¸‹å±ä»£ç ï¼š # Override the default request headers: #DEFAULT_REQUEST_HEADERS = { # 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', # 'Accept-Language': 'en', #} æ”¹ä¸ºï¼š # Override the default request headers: DEFAULT_REQUEST_HEADERS = { 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Language': 'zh-CN', } ä¿®æ”¹ç»“æœå±•ç¤ºï¼š å‚è€ƒé“¾æ¥ https://blog.csdn.net/wuqili_1025/article/details/79690103]]></content>
      <categories>
        <category>çˆ¬è™«</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>çˆ¬è™«</tag>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pythonåŠ¨æ€ç½‘é¡µçˆ¬å–ä¹‹å®‰è£…dockerå’Œsplash]]></title>
    <url>%2Fpost%2FPython_dynamic_web_crawler_installed_docker_and_splash%2F</url>
    <content type="text"><![CDATA[åˆ©ç”¨pythonè¿›è¡ŒåŠ¨æ€ç½‘é¡µçˆ¬å–æ—¶ï¼Œåœ¨å®‰è£…dockerå’Œsplashæ—¶è¸©è¿‡çš„å‘ï¼Œè®°å½•äº†ä¸€ä¸‹è‡ªå·±çš„å®‰è£…è¿‡ç¨‹ã€‚ç”¨çš„ç³»ç»Ÿæ˜¯mac osã€‚ å®‰è£…scrapy-splash åˆ©ç”¨pipå®‰è£…scrapy-splashåº“ï¼š$ pip install scrapy-splash å®‰è£…Docker==ä¸‹é¢ğŸ‘‡è¿™æ ·å®‰ä¸ä¸‹å»äº†== å¦‚æœæ˜¯Macçš„è¯éœ€è¦ä½¿ç”¨brewå®‰è£…ï¼Œå¦‚ä¸‹ï¼šbrew install docker æŠ¥é”™ï¼š Error: Failure while executing; `git config --local --replace-all homebrew.private true` exited with 1. è§£å†³æ–¹æ³•ï¼š xcode-select --install ç„¶ååœ¨æ‰§è¡Œï¼š brew install docker å†ç»§ç»­ï¼š service docker start æŠ¥é”™ï¼š -bash: service: command not foundä¸Šç½‘ä¸ŠæŸ¥ä¸€å †ä¹±ä¸ƒå…«ç³Ÿçš„è§£å†³æ–¹å¼ï¼Œè¯¥è·¯å¾„å•¥çš„ï¼ŒçœŸçš„ä¸æƒ³æ”¹è·¯å¾„ï¼Œæ€•æŠŠå…¶ä»–çš„æ”¹å´©äº†ã€‚æœ€åæ”¾å¼ƒè¿™ç§æ–¹å¼ï¼Œå¦‚æœæœ‰å…´è¶£ä¹Ÿå¯ä»¥å°è¯•è§£å†³ã€‚ ==å°è¯•å¦‚ä¸‹å®‰è£…DOCKERæ–¹æ³•== å»å®˜ç½‘ä¸‹è½½è¿™ç§æ–¹æ³•ä¸‹è½½dockerå®¢æˆ·ç«¯éœ€è¦ä»æœåŠ¡å™¨ä¸‹è½½ï¼Œè‡ªå·±ç”µè„‘ä¸‹è½½12k/sï¼Œç®€ç›´æ…¢æ­»äº†ã€‚ æ‹‰å–é•œåƒ(pull the image)ï¼šdocker pull scrapinghub/splash ç”¨dockerè¿è¡Œscrapinghub/splashï¼š docker run -p 8050:8050 scrapinghub/splash åœ¨æµè§ˆå™¨ä¸­è¾“å…¥localhost:8050 ==å®‰è£…æˆåŠŸ== å‚è€ƒé“¾æ¥ http://www.morecoder.com/article/1001249.html https://www.jianshu.com/p/e54a407c8a0a]]></content>
      <categories>
        <category>çˆ¬è™«</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>çˆ¬è™«</tag>
        <tag>splash</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ç”¨python3è¯»å†™csvæ–‡æ¡£]]></title>
    <url>%2Fpost%2Fread%20the%20CSV%20document%20using%20python3%2F</url>
    <content type="text"><![CDATA[å¯¹äºå¤§å¤šæ•°çš„CSVæ ¼å¼çš„æ•°æ®è¯»å†™é—®é¢˜ï¼Œéƒ½å¯ä»¥ä½¿ç”¨ csv åº“ã€‚ ä¾‹å¦‚ï¼šå‡è®¾ä½ åœ¨ä¸€ä¸ªåå«stocks.csvæ–‡ä»¶ä¸­æœ‰ä¸€äº›è‚¡ç¥¨å¸‚åœºæ•°æ®ï¼Œå°±åƒè¿™æ ·ï¼š Symbol,Price,Date,Time,Change,Volume "AA",39.48,"6/11/2007","9:36am",-0.18,181800 "AIG",71.38,"6/11/2007","9:36am",-0.15,195500 "AXP",62.58,"6/11/2007","9:36am",-0.46,935000 "BA",98.31,"6/11/2007","9:36am",+0.12,104800 "C",53.08,"6/11/2007","9:36am",-0.25,360900 "CAT",78.29,"6/11/2007","9:36am",-0.23,225400 csvæ–‡æ¡£çš„è¯»å–1. å¸¸è§„è¯»å–ä¸‹é¢å‘ä½ å±•ç¤ºå¦‚ä½•å°†è¿™äº›æ•°æ®è¯»å–ä¸ºä¸€ä¸ªå…ƒç»„çš„åºåˆ—ï¼š import csv with open('stocks.csv') as f: f_csv = csv.reader(f) headers = next(f_csv) for row in f_csv: # Process row ... åœ¨ä¸Šé¢çš„ä»£ç ä¸­ï¼Œ row ä¼šæ˜¯ä¸€ä¸ªåˆ—è¡¨ã€‚å› æ­¤ï¼Œä¸ºäº†è®¿é—®æŸä¸ªå­—æ®µï¼Œä½ éœ€è¦ä½¿ç”¨ä¸‹æ ‡ï¼Œå¦‚ row[0]è®¿é—®Symbolï¼Œ row[4] è®¿é—®Changeã€‚==è¿™æ ·å¯ä»¥é€šè¿‡å¤–å»ºå­—å…¸æ¥å­˜å‚¨è¯»å‡ºçš„csvæ•°æ®ã€‚== 2. å‘½åå…ƒç»„ç”±äºè¿™ç§ä¸‹æ ‡è®¿é—®é€šå¸¸ä¼šå¼•èµ·æ··æ·†ï¼Œä½ å¯ä»¥è€ƒè™‘ä½¿ç”¨==å‘½åå…ƒç»„==ã€‚ä¾‹å¦‚ï¼š from collections import namedtuple with open('stock.csv') as f: f_csv = csv.reader(f) headings = next(f_csv) Row = namedtuple('Row', headings) for r in f_csv: row = Row(*r) # Process row ... å®ƒå…è®¸ä½ ä½¿ç”¨åˆ—åå¦‚ row.Symbol å’Œ row.Change ä»£æ›¿ä¸‹æ ‡è®¿é—®ã€‚ éœ€è¦æ³¨æ„çš„æ˜¯è¿™ä¸ªåªæœ‰åœ¨åˆ—åæ˜¯åˆæ³•çš„Pythonæ ‡è¯†ç¬¦çš„æ—¶å€™æ‰ç”Ÿæ•ˆã€‚å¦‚æœä¸æ˜¯çš„è¯ï¼Œ ä½ å¯èƒ½éœ€è¦ä¿®æ”¹ä¸‹åŸå§‹çš„åˆ—å(å¦‚å°†éæ ‡è¯†ç¬¦å­—ç¬¦æ›¿æ¢æˆä¸‹åˆ’çº¿ä¹‹ç±»çš„)ã€‚ 3. å­—å…¸å¦å¤–ä¸€ä¸ªé€‰æ‹©å°±æ˜¯å°†æ•°æ®è¯»å–åˆ°ä¸€ä¸ªå­—å…¸åºåˆ—ä¸­å»ã€‚å¯ä»¥è¿™æ ·åšï¼š import csv with open('stocks.csv') as f: f_csv = csv.DictReader(f) for row in f_csv: # process row ... åœ¨è¿™ä¸ªç‰ˆæœ¬ä¸­ï¼Œä½ å¯ä»¥ä½¿ç”¨åˆ—åå»è®¿é—®æ¯ä¸€è¡Œçš„æ•°æ®äº†ã€‚æ¯”å¦‚ï¼Œrow[&#39;Symbol&#39;] æˆ–è€… row[&#39;Change&#39;]ã€‚ fieldnames æ˜¯dict_readerçš„ä¸€ä¸ªå±æ€§ï¼Œè¡¨ç¤ºCSVæ–‡æ¡£çš„æ•°æ®åç§°ã€‚å¯ä»¥é€šè¿‡f_csv.fieldnamesæ¥è®¿é—®æ•°æ®åç§°é‚£ä¸€è¡Œã€‚ CSVæ–‡ä»¶å†™å…¥ä¸ºäº†å†™å…¥CSVæ•°æ®ï¼Œä½ ä»ç„¶å¯ä»¥ä½¿ç”¨csvæ¨¡å—ï¼Œä¸è¿‡è¿™æ—¶å€™å…ˆåˆ›å»ºä¸€ä¸ª writer å¯¹è±¡ã€‚ä¾‹å¦‚: headers = ['Symbol','Price','Date','Time','Change','Volume'] rows = [('AA', 39.48, '6/11/2007', '9:36am', -0.18, 181800), ('AIG', 71.38, '6/11/2007', '9:36am', -0.15, 195500), ('AXP', 62.58, '6/11/2007', '9:36am', -0.46, 935000), ] with open('stocks.csv','w') as f: f_csv = csv.writer(f) f_csv.writerow(headers) f_csv.writerows(rows) å¦‚æœä½ æœ‰ä¸€ä¸ªå­—å…¸åºåˆ—çš„æ•°æ®ï¼Œå¯ä»¥åƒè¿™æ ·åšï¼š headers = ['Symbol', 'Price', 'Date', 'Time', 'Change', 'Volume'] rows = [{'Symbol':'AA', 'Price':39.48, 'Date':'6/11/2007', 'Time':'9:36am', 'Change':-0.18, 'Volume':181800}, {'Symbol':'AIG', 'Price': 71.38, 'Date':'6/11/2007', 'Time':'9:36am', 'Change':-0.15, 'Volume': 195500}, {'Symbol':'AXP', 'Price': 62.58, 'Date':'6/11/2007', 'Time':'9:36am', 'Change':-0.46, 'Volume': 935000}, ] with open('stocks.csv','w') as f: f_csv = csv.DictWriter(f, headers) f_csv.writeheader() f_csv.writerows(rows) å…¶ä¸­f_csv.writeheader()ä¹Ÿå¯ä»¥æ›¿æ¢æˆf_csv.writerow(dict(zip(headers, headers))) å‚è€ƒé“¾æ¥ https://python3-cookbook.readthedocs.io/zh_CN/latest/c06/p01_read_write_csv_data.html https://blog.csdn.net/guoziqing506/article/details/52014506]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>æ–‡ä»¶è¯»å–</tag>
        <tag>csv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neo4jåˆå§‹åŒ–èŠ‚ç‚¹æ˜¾ç¤ºè®¾ç½®]]></title>
    <url>%2Fpost%2FNeo4j_initializes_the_node_display_Settings%2F</url>
    <content type="text"><![CDATA[é—®é¢˜æè¿°ï¼šneo4jä¸­æœ‰é»˜è®¤çš„åˆå§‹åŒ–èŠ‚ç‚¹æ˜¾ç¤ºè®¾ç½®ä¸º300ä¸ªèŠ‚ç‚¹ï¼Œå¦‚æœæƒ³è¦æ˜¾ç¤ºçš„èŠ‚ç‚¹å¤šäº300ä¸ªï¼Œåˆ™ä¼šåªæ˜¾ç¤º300ä¸ªï¼Œå¹¶ç»™äºˆä»¥ä¸‹æç¤ºè¯­å¥ï¼š Not all return nodes are being displayed due to Initial Node Display setting. Only 300 of 300 nodes are being displayed. è§£å†³æ–¹æ³•ï¼šåœ¨å¦‚å›¾æ‰€ç¤ºinitial Node Displayå¤„å¯ä»¥ä¿®æ”¹ï¼Œåœ¨æ­¤å¤„ä¿®æ”¹ä¸º300000.]]></content>
      <categories>
        <category>Neo4j</category>
      </categories>
      <tags>
        <tag>Neo4j</tag>
        <tag>æ•°æ®åº“</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ã€ŠBidirectional LSTM-CRF Models for Sequence Taggingã€‹é˜…è¯»ç¬”è®°]]></title>
    <url>%2Fpost%2Fread_Bidirectional_LSTM-CRF_Models_for_Sequence_Tagging%2F</url>
    <content type="text"><![CDATA[è¿™ç¯‡è®ºæ–‡å¯ä»¥ä½œä¸ºä¸€ä¸ªRNNå’ŒLSTMå­¦ä¹ çš„ä¸€ä¸ªä¾‹å­æ¥çœ‹ï¼Œæœ‰åˆ©äºæ–°æ‰‹å¯¹LSTMçš„ç†è§£ã€‚å¯¹äºNERçš„å¤„ç†ä¸»è¦æ˜¯ä½œä¸ºä¸€ä¸ªåºåˆ—æ ‡æ³¨é—®é¢˜ã€‚ä½†æ˜¯ä½œä¸ºç»å…¸æ–‡ç« è¿˜æ˜¯å¯ä»¥è¯»ä¸€è¯»äº†è§£ä¸€ä¸‹çš„ã€‚ åœ¨æœ¬ç¯‡è®ºæ–‡ä¸­ï¼Œä½œè€…æå‡ºäº†4ç§æ¨¡å‹ï¼šLSTMã€BI-LSTMã€LSTM-CRFå’ŒBI-LSTM-CRFã€‚ contribution(è´¡çŒ®) ä½œè€…åœ¨NLPæ ‡æ³¨æ•°æ®é›†ä¸Šç³»ç»Ÿçš„å¯¹æ¯”äº†ä»¥ä¸Šå››ä¸ªæ¨¡å‹ï¼› ä½œè€…æ˜¯é¦–å…ˆæå‡ºæŠŠBI-LSTM-CRFæ¨¡å‹ç”¨äºNLPåºåˆ—æ ‡æ³¨ï¼Œå¹¶ä¸”è¾¾åˆ°äº†state-of-the-artçš„æ°´å¹³ï¼› ä½œè€…å±•ç¤ºäº†BI-LSTM-CRFæ˜¯robustï¼Œå¹¶ä¸”æå°‘ä¾èµ–äºè¯å‘é‡ã€‚ model(æ¨¡å‹)LSTMé¦–å…ˆï¼Œä½œè€…å…ˆä»‹ç»äº†RNNçš„ç»“æ„å’Œå·¥ä½œåŸç†ï¼Œå¦‚å›¾ï¼š å…¶ä¸­è¾“å…¥ä¸ºå¥å­ï¼šEU rejects German call to boycott British lambã€‚è¾“å‡ºä¸ºæ ‡ç­¾ï¼šB-ORG O B-MISC O O O B-MISC O Oï¼Œå…¶ä¸­B-ï¼ŒI-è¡¨ç¤ºå®ä½“å¼€å§‹å’Œä¸­é—´ä½ç½®ã€‚æ ‡ç­¾ç§ç±»ä¸ºï¼šother (O)å’Œå››ç§å®ä½“æ ‡ç­¾ï¼šPerson (PER), Location (LOC), Organization (ORG), and Miscellaneous (MISC). è¾“å…¥å±‚è¡¨ç¤ºåœ¨æ—¶é—´æ­¥ t çš„ç‰¹å¾ã€‚å®ƒä»¬å¯ä»¥æ˜¯ one-hot-encoding çš„è¯ç‰¹å¾ï¼Œç¨ å¯†æˆ–è€…ç¨€ç–çš„å‘é‡ç‰¹å¾ã€‚è¾“å…¥å±‚ä¸ç‰¹å¾æœ‰ç›¸åŒå¤§å°çš„ç»´åº¦ã€‚è¾“å‡ºå±‚è¡¨ç¤ºåœ¨æ—¶é—´æ­¥ t çš„æ ‡ç­¾ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒï¼Œç»´åº¦ä¸æ ‡æ³¨æ•°é‡ç›¸åŒã€‚ç›¸æ¯”å‰é¦ˆç¥ç»ç½‘ç»œï¼ŒRNN å¼•å…¥å‰ä¸€ä¸ªéšè—çŠ¶æ€å’Œå½“å‰éšè—çŠ¶æ€çš„ç»“åˆï¼Œå› æ­¤å¯ä»¥å‚¨å­˜å†å²ä¿¡æ¯ã€‚ æ¶‰åŠå…¬å¼ä¸ºï¼š å…¶ä¸­ï¼ŒUï¼ŒWï¼ŒVéƒ½æ˜¯æƒé‡ï¼Œå‡½æ•°fï¼Œgåˆ†åˆ«ä¸ºsigmoidå’Œsoftmaxå‡½æ•°ã€‚ æ¥ä¸‹æ¥ï¼Œä½œè€…å±•ç¤ºäº†LSTMçš„ç»“æ„å’ŒåŸç†ï¼Œå¦‚å›¾ï¼š å…¬å¼ï¼š å…¶ä¸­ï¼ŒÏƒæ˜¯é€»è¾‘sigmoidå‡½æ•°ï¼Œi, f, o å’Œ cåˆ†åˆ«æ˜¯è¾“å…¥é—¨ï¼Œå¿˜è®°é—¨ï¼Œè¾“å‡ºé—¨å’Œç»†èƒå‘é‡ï¼Œæ‰€æœ‰çš„å¤§å°éƒ½å’Œå‘é‡hä¸€æ ·ã€‚wæƒé‡çš„å«ä¹‰å¦‚å…¶ä¸‹è¡¨æ‰€ç¤ºã€‚ LSTMåºåˆ—æ ‡æ³¨æ¨¡å‹å¦‚å›¾æ‰€ç¤ºï¼š å…¶ä¸­ï¼Œä¸­é—´çš„ç”»æ–œçº¿çš„æ ¼å­å³ä¸ºå›¾2ä¸­æ‰€ç¤ºéƒ¨åˆ†ã€‚ Bidirectional LSTM(åŒå‘LSTM)ä½œè€…å±•ç¤ºäº†åŒå‘LSTMçš„ç»“æ„ï¼Œå¦‚å›¾æ‰€ç¤ºï¼š åŒå‘LSTMç½‘ç»œå¯ä»¥æœ‰æ•ˆåˆ©ç”¨è¿‡å»ç‰¹å¾å’Œæœªæ¥ç‰¹å¾ã€‚åœ¨ä½œè€…çš„å®ç°ä¸­ï¼Œå¯¹äºæ•´ä¸ªå¥å­çš„å‰å‘å’Œåå‘æ“ä½œï¼Œä½œè€…åªéœ€è¦åœ¨æ¯ä¸ªå¥å­å¼€å§‹æ—¶å°†éšè—çŠ¶æ€é‡ç½®ä¸º0ã€‚ä½œè€…é‡‡ç”¨æ‰¹å¤„ç†ï¼Œä½¿å¾—å¯ä»¥åŒæ—¶å¤„ç†å¤šä¸ªå¥å­ã€‚ CRFä½¿ç”¨é‚»å±…æ ‡è®°ä¿¡æ¯é¢„æµ‹å½“å‰æ ‡è®°æœ‰ä¸¤ç§ä¸åŒçš„æ–¹æ³•ï¼š é¢„æµ‹æ¯ä¸ªæ—¶é—´æ­¥é•¿çš„æ ‡ç­¾åˆ†å¸ƒï¼Œç„¶åä½¿ç”¨æ³¢æŸå¼è§£ç æ¥æ‰¾åˆ°æœ€ä¼˜çš„æ ‡ç­¾åºåˆ—ï¼Œä»£è¡¨æ–¹æ³•ï¼šMEMMs æ³¨é‡å¥å­å±‚æ¬¡è€Œä¸æ˜¯ä¸ªä½“ä½ç½®ï¼Œä»£è¡¨æ–¹æ³•ï¼šCRFï¼Œè¾“å…¥å’Œè¾“å‡ºæ˜¯ç›´æ¥ç›¸è¿çš„ï¼›å¦‚å›¾ï¼š ç ”ç©¶è¡¨æ˜ï¼ŒCRFsä¸€èˆ¬èƒ½å¤Ÿäº§ç”Ÿæ›´é«˜çš„æ ‡ç­¾ç²¾åº¦ã€‚ LSTM-CRFä½œè€…å±•ç¤ºäº†LSTM-CRFçš„ç»“æ„ï¼Œå¦‚å›¾ï¼š è¿™ç½‘ç»œå¯ä»¥æœ‰æ•ˆåœ°é€šè¿‡ LSTM åˆ©ç”¨è¿‡å»çš„è¾“å…¥ç‰¹å¾å’Œé€šè¿‡ CRF åˆ©ç”¨å¥å­çº§çš„æ ‡æ³¨ä¿¡æ¯ã€‚å›¾ä¸­CRFå±‚ç”±è¿æ¥è¿ç»­è¾“å‡ºå±‚çš„çº¿è¡¨ç¤ºã€‚CRFå±‚æœ‰ä¸€ä¸ªçŠ¶æ€è½¬ç§»çŸ©é˜µä½œä¸ºå‚æ•°ã€‚ å…¬å¼ä¸ºï¼š å‡½æ•°fä¸ºç½‘ç»œçš„è¾“å‡ºåˆ†æ•°ï¼Œ[x]ä¸ºè¾“å…¥ï¼Œ [fÎ¸]i,t ä¸ºå¸¦æœ‰å‚æ•°Î¸ï¼ˆå¥å­xï¼Œç¬¬i ä¸ªæ ‡ç­¾ï¼Œç¬¬tä¸ªå•è¯ï¼‰çš„ç½‘ç»œè¾“å‡ºï¼› [A]i,jä¸ºè½¬ç§»åˆ†æ•°ï¼Œä»è¿ç»­çš„æ—¶é—´æ­¥içŠ¶æ€åˆ°jçŠ¶æ€çš„è½¬ç§»åˆ†æ•°ã€‚æ³¨æ„ï¼Œè¯¥è½¬æ¢çŸ©é˜µæ˜¯ä½ç½®æ— å…³çš„ã€‚ BI-LSTM-CRFä½œè€…å±•ç¤ºäº†BI-LSTM-CRFçš„ç»“æ„ï¼Œå¦‚å›¾æ‰€ç¤ºï¼š ä½œè€…åœ¨å®éªŒä¸­å±•ç¤ºäº†é¢å¤–çš„æœªæ¥ç‰¹å¾å¯ä»¥æé«˜æ ‡ç­¾çš„å‡†ç¡®ç‡ã€‚ è®­ç»ƒè¿‡ç¨‹æœ¬æ–‡ä½¿ç”¨çš„æ‰€æœ‰æ¨¡å‹éƒ½å…±äº«ä¸€ä¸ªé€šç”¨SGDå‰å‘å’Œåå‘è®­ç»ƒè¿‡ç¨‹ã€‚ä½œè€…å±•ç¤ºäº†BI-LSTM-CRFçš„ç®—æ³•ï¼Œå¦‚å›¾ ä½œè€…è®¾ç½®äº†æ‰¹æ¬¡å¤§å°ä¸º100ã€‚ å®éªŒdataä½œè€…åœ¨ä»¥ä¸‹ä¸‰ä¸ªæ•°æ®é›†ä¸Šæµ‹è¯•è‡ªå·±çš„æ¨¡å‹ï¼šPenn TreeBank (PTB) POS tagging, CoNLL 2000 chunking, and CoNLL 2003 named entity tagging.æ•°æ®é›†ä¿¡æ¯å±•ç¤ºå¦‚ä¸‹ï¼š Featuresä½œè€…ä»ä¸‰ä¸ªæ•°æ®é›†ä¸­æå–å‡ºå…¶å…¬å…±ç‰¹å¾ã€‚ç‰¹å¾å¯ä»¥åˆ†ä¸ºæ‹¼å†™ç‰¹å¾å’Œä¸Šä¸‹æ–‡ç‰¹å¾ã€‚æœ€ç»ˆï¼Œä½œè€…å¯¹äºPOSï¼ˆè¯æ€§æ ‡æ³¨ï¼‰ã€chunkingï¼ˆç»„å—ï¼‰å’ŒNERï¼ˆå‘½åå®ä½“è¯†åˆ«ï¼‰åˆ†åˆ«æå–401Kï¼Œ76Kå’Œ341Kä¸ªç‰¹å¾ã€‚ spelling featuresï¼ˆæ‹¼å†™ç‰¹å¾ï¼‰é™¤äº†å°å†™å­—æ¯ç‰¹å¾ä¹‹å¤–ï¼Œæˆ‘ä»¬æå–ç»™å®šå•è¯çš„ä»¥ä¸‹ç‰¹å¾ã€‚ context featursï¼ˆä¸Šä¸‹æ–‡ç‰¹å¾ï¼‰å¯¹äºå•è¯ç‰¹å¾ï¼Œä½œè€…ä½¿ç”¨unigramå’Œbi-gramsç‰¹å¾ã€‚å¯¹äºåœ¨CoNLL2000æ•°æ®é›†ä¸­çš„POSç‰¹å¾å’Œåœ¨CoNLL2003æ•°æ®é›†ä¸­çš„ POS &amp; CHUNKç‰¹å¾ï¼Œä½œè€…ä½¿ç”¨äº†unigramï¼Œbi-gramå’Œtri-gramç‰¹å¾ã€‚ è¯å‘é‡è¯å‘é‡åœ¨æ”¹è¿›åºåˆ—æ ‡æ³¨ä»»åŠ¡çš„è¡¨ç°æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œæˆ‘ä»¬ä½¿ç”¨ 130K è¯æ±‡å¹¶ä¸”æ¯ä¸ªè¯æ±‡çš„è¯å‘é‡ç»´åº¦æ˜¯ 50 ç»´ã€‚æˆ‘ä»¬å°† one-hot-encodingè¯è¡¨ç¤ºæ›¿æ¢æ¯ä¸ªè¯å¯¹åº”çš„è¯å‘é‡ã€‚ Features connection tricksæˆ‘ä»¬å¯ä»¥å°†æ‹¼å†™å’Œä¸Šä¸‹æ–‡ç‰¹å¾ä¸å•è¯ç‰¹å¾ä¸€æ ·å¯¹å¾…ã€‚è¿™æ ·ç½‘ç»œçš„è¾“å…¥åŒ…æ‹¬å•è¯ï¼Œå•è¯çš„æ‹¼å†™å’Œä¸Šä¸‹æ–‡ç‰¹å¾ã€‚ç„¶è€Œï¼Œ==æˆ‘ä»¬å‘ç°å°†æ‹¼å†™å’Œä¸Šä¸‹æ–‡ç‰¹å¾ä¸è¾“å‡ºç›´æ¥è¿æ¥å¯ä»¥åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ï¼ŒåŒæ—¶ä¹Ÿèƒ½ä¿æŒæ ‡æ³¨çš„å‡†ç¡®ç‡ï¼Œ==å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š æˆ‘ä»¬æ³¨æ„åˆ°ï¼Œè¿™ç§ç‰¹å¾çš„ä½¿ç”¨ä¸ä½¿ç”¨çš„æœ€å¤§ç†µç‰¹å¾ç±»ä¼¼ã€‚åŒºåˆ«åœ¨äºé‡‡ç”¨ç‰¹å¾ä¸‰åˆ—æŠ€æœ¯å¯èƒ½ä¼šå‘ç”Ÿç‰¹å¾å†²çªã€‚ç”±äºåºåˆ—æ ‡æ³¨æ•°æ®é›†ä¸­çš„è¾“å‡ºæ ‡ç­¾å°äºè¯­è¨€æ¨¡å‹ï¼ˆé€šå¸¸ä¸ºæ•°åä¸‡ï¼‰ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥åœ¨ç‰¹å¾å’Œè¾“å‡ºä¹‹é—´å»ºç«‹å®Œæ•´çš„è¿æ¥ï¼Œä»¥é¿å…æ½œåœ¨çš„ç‰¹å¾å†²çªã€‚ ç»“æœåœ¨ç›¸åŒçš„æ•°æ®é›†ä¸Šåˆ†åˆ«è®­ç»ƒLSTMï¼ŒBI-LSTMï¼ŒCRFï¼ŒLSTM-CRFå’ŒBI-LSTM-CRFæ¨¡å‹ï¼Œå¹¶ä¸”é‡‡ç”¨ä¸¤ç§æ–¹å¼åˆå§‹åŒ–word embeddingï¼šéšæœºå’ŒSennaæ–¹å¼ã€‚æ¨¡å‹çš„è®­ç»ƒé€Ÿç‡ä¸º0.1ï¼Œéšè—å±‚æ•°é‡ä¸º300.ä¸åŒæ¨¡å‹åœ¨ä¸åŒword embeddingä¸‹çš„ç»“æœå¦‚è¡¨2æ‰€ç¤ºï¼ŒåŒæ—¶åˆ—å‡ºäº†ä¹‹å‰æœ€å¥½æ¨¡å‹Cov-CRFã€‚ ä¸Cov-CRFæ¯”è¾ƒ å®éªŒä¸­è®¾ç½®äº†3ä¸ªåŸºå‡†æ¨¡å‹ï¼Œåˆ†åˆ«ä¸ºLSTMã€BI-LSTMå’ŒCRFï¼Œç»“æœä¸­LSTMåœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸­æ•ˆæœæœ€å·®ï¼ŒBI-LSTMè·ŸCRFåœ¨POSå’Œchunkingä¸­æ•ˆæœæ¥è¿‘ï¼Œä½†æ˜¯åœ¨NERä¸­åè€…è¦ä¼˜äºå‰è€…ã€‚æœ‰è¶£çš„æ˜¯è¡¨ç°æœ€å¥½çš„æ¨¡å‹BI-LSTM-CRFç›¸å¯¹äºCov-CRFæ¥è¯´å¯¹Senna embeddingçš„ä¾èµ–ç¨‹åº¦æ›´å°ã€‚ (robustness)æ¨¡å‹é²æ£’æ€§ ä¸ºéªŒè¯æ¨¡å‹çš„é²æ£’æ€§ï¼Œå¯¹ä¸åŒæ¨¡å‹åªé‡‡ç”¨word featureç‰¹å¾è¿›è¡Œè®­ç»ƒï¼Œè®­ç»ƒç»“æœå¦‚è¡¨3ï¼Œæ‹¬å·ä¸­æ•°å­—è¡¨ç¤ºç›¸æ¯”äºå…¨éƒ¨ç‰¹å¾ï¼Œæ¨¡å‹çš„ç»“æœä¸‹é™æ•°å€¼ã€‚ ä¸å…¶ä»–ç³»ç»Ÿçš„æ¯”è¾ƒ è¿™é‡Œå°±ä¸è´´å›¾äº†ï¼Œæ€»ä¹‹å°±æ˜¯é˜è¿°ä½œè€…è‡ªå·±æ¨¡å‹å¥½ã€‚ ç»“è®ºæ€»ä¹‹ä½œè€…çš„æ¨¡å‹æ˜¯åŸºäºä¹‹å‰æ¨¡å‹çš„ä¸€äº›æ”¹è¿›ï¼Œä¸»è¦è¿ç”¨äº†IBI-LSTMå’ŒCRFçš„ç»“åˆã€‚ è®ºæ–‡ä¸‹è½½åœ°å€]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>NER</tag>
        <tag>LSTM</tag>
        <tag>BI-LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CN-DBpedia A Never-Ending Chinese Knowledge Extraction SystemCN-DBpedia System]]></title>
    <url>%2Fpost%2Fessay%2F</url>
    <content type="text"><![CDATA[å‰è¨€ æœ¬ç¯‡è®ºæ–‡ä¸º2016å¹´çš„ä¸€ç¯‡è®ºæ–‡ï¼Œä¸»è¦ä»‹ç»äº†ä½œè€…æ„å»ºä¸­æ–‡çŸ¥è¯†å›¾è°±æ‰€é‡åˆ°çš„ä¸€äº›é—®é¢˜å’Œè§£å†³æ–¹æ³•ã€‚ challenge å¦‚ä½•é™ä½äººåŠ›æˆæœ¬ï¼Ÿ å¦‚ä½•ä¿æŒçŸ¥è¯†åº“çš„æ–°é²œåº¦ï¼Ÿ è´¡çŒ® åœ¨æ„å»ºä¸­æ–‡çŸ¥è¯†åº“ä¸­é™ä½äº†äººåŠ›æˆæœ¬ï¼š é‡å¤åˆ©ç”¨å·²ç»å­˜åœ¨çš„æœ¬ä½“è®º æå‡ºäº†ä¸€ä¸ªä¸ç”¨äººå·¥ç›‘ç£çš„ç«¯åˆ°ç«¯çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ æå‡ºäº†ä¸€ä¸ªæ™ºèƒ½ä¸»åŠ¨æ›´æ–°ç­–ç•¥ ç³»ç»Ÿç»“æ„ æé«˜çŸ¥è¯†åº“è´¨é‡ï¼š Normalizationï¼š normalize the attributes and values Enrichmentï¼šreuse the ontology Correctionï¼štwo steps error detection: rule-based detection based on user feedbacks error correction crowd-sourcing é™ä½äººåŠ›æˆæœ¬è¿™éƒ¨åˆ†ä½œè€…é‡‡ç”¨äº†ä¸¤ç§æ–¹æ³•ï¼š é‡å¤åˆ©ç”¨å·²ç»å­˜åœ¨åœ¨çŸ¥è¯†åº“çš„æœ¬ä½“è®ºå’Œç±»å‹åŒ–çš„ä¸­æ–‡å®ä½“ æ„å»ºä¸€ä¸ªç«¯åˆ°ç«¯æå–å™¨ Cross-Lingual Entity Typingï¼ˆè·¨è¯­è¨€çš„å®ä½“ç±»å‹ï¼‰ ç¬¬ä¸€æ­¥æ˜¯é€šè¿‡ç”¨è‹±æ–‡DBpediaç±»å‹æ¥ç±»å‹åŒ–ä¸­æ–‡å®ä½“ã€‚ä¸ºäº†è¾¾åˆ°è¿™ä¸ªç›®çš„ï¼Œä½œè€…æå‡ºäº†å¦‚ä¸‹ç³»ç»Ÿï¼šç³»ç»Ÿå»ºç«‹äº†ç›‘ç£å±‚æ¬¡åˆ†ç±»æ¨¡å‹ï¼Œç³»ç»Ÿè¾“å…¥ä¸ºæ²¡æœ‰æ ‡è®°ç±»å‹çš„ä¸­æ–‡å®ä½“ï¼Œè¾“å‡ºä¸ºåœ¨DBä¸­æ‰€æœ‰æœ‰æ•ˆçš„è‹±æ–‡ç±»å‹ã€‚ä½œè€…å°†ä¸­æ–‡å®ä½“ä¸å…±äº«ç›¸åŒä¸­æ–‡æ ‡ç­¾åç§°çš„è‹±è¯­å®ä½“é…å¯¹ï¼Œè¿™æ ·ä¸­æ–‡å®ä½“ä»¥åŠé…å¯¹è‹±è¯­å®ä½“çš„ç±»å‹è‡ªç„¶æ˜¯æ ‡è®°æ ·æœ¬ã€‚ ç”¨ä¸Šè¿°æ–¹æ³•å¾—åˆ°çš„è®­ç»ƒé›†å¯èƒ½å‡ºç°ä¸‹é¢ä¸€äº›é—®é¢˜ï¼š è‹±æ–‡DBpediaå®ä½“ç±»å‹åœ¨è®¸å¤šæƒ…å†µä¸‹å¯èƒ½ä¸å®Œå…¨ï¼› è‹±æ–‡DBpediaå®ä½“ç±»å‹åœ¨è®¸å¤šæƒ…å†µä¸‹å¯èƒ½æ˜¯é”™è¯¯çš„ï¼› ä¸­è‹±æ–‡é“¾æ¥å¯èƒ½å‡ºé”™ï¼› ä¸­æ–‡å®ä½“çš„ç‰¹å¾é€šå¸¸ä¸å®Œæ•´ã€‚ ä¸ºäº†è§£å†³ä»¥ä¸Šé—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸¤ç§æ–¹æ³•ï¼š å®Œå–„è‹±æ–‡DBpediaå®ä½“ç±»å‹ï¼› è®¾è®¡ä¸€ä¸ªè¿‡æ»¤æ­¥éª¤æ¥å‰”é™¤é”™è¯¯æ ·æœ¬ã€‚ infobox completion Infobox completion is a task to extract object for a given pair of entity and predicate from encyclopedia articles. ä½œè€…å»ºæ¨¡äº†ä¸€ä¸ªseq2seqæ¨¡å‹ï¼Œè¾“å…¥ä¸ºåŒ…å«tokensçš„è‡ªç„¶è¯­è¨€å¥å­ï¼Œè¾“å‡ºä¸ºæ¯ä¸ªtokençš„æ ‡ç­¾ã€‚å¯¹äºæ ‡ç­¾ä¸º0æˆ–1ã€‚ å¯¹äºå»ºç«‹ä¸€ä¸ªæœ‰æ•ˆçš„æå–å™¨æœ‰ä»¥ä¸‹ä¸¤ä¸ªå…³é”®ï¼š å¦‚ä½•æ„å»ºè®­ç»ƒé›†ï¼šä½œè€…é‡‡ç”¨è¿œç¨‹ç›‘ç£æ–¹æ³•ï¼ˆåˆ©ç”¨Wikipediaï¼‰ å¦‚ä½•é€‰å–æœŸæœ›çš„æå–æ¨¡å‹ï¼šLSTM-RNNï¼Œå¦‚å›¾æ‰€ç¤º çŸ¥è¯†åº“æ›´æ–°ä½œè€…é‡‡ç”¨åŠ¨æ€æ›´æ–°ï¼šè¯†åˆ«æ–°å®ä½“æˆ–å¯èƒ½åŒ…å«æ–°äº‹å®çš„æ—§å®ä½“ ä½œè€…æ ¹æ®ä»¥ä¸‹ä¸¤æ–¹é¢æ¥è¾¨åˆ«è¿™äº›å®ä½“ï¼š è¿‘æœŸçƒ­ç‚¹æ–°é—»ä¸­æåŠçš„å®ä½“ åœ¨æœç´¢å¼•æ“çš„æµè¡Œæœç´¢å…³é”®å­—æˆ–å…¶ä»–æµè¡Œç½‘é¡µä¸­æåˆ°çš„å®ä½“ å¯¹äºå¦‚ä½•ä»æ–°é—»æ ‡é¢˜å’Œæœç´ æŒ‡ä»¤ä¸­æå–å®ä½“åå­—ï¼Œä½œè€…é‡‡ç”¨ç®€å•çš„è¯åˆ†å‰²æ–¹æ³•ï¼Œä»ç™¾ç§‘å…¨ä¹¦ä¸­åˆ¤æ–­å…¶æ˜¯å¦ä¸ºå®ä½“ï¼Œå¹¶æå‡ºIDFå€¼ä½çš„åˆ†å‰²å­ä¸²ã€‚ ç»Ÿè®¡æ•°æ® è®ºæ–‡ä¸‹è½½é“¾æ¥]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>çŸ¥è¯†å›¾è°±</tag>
        <tag>æœºå™¨å­¦ä¹ </tag>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ontology reasoning with deep neural networks]]></title>
    <url>%2Fpost%2Fessay%2F</url>
    <content type="text"><![CDATA[Ontology reasoning with deep neural networksï¼ˆåŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„æœ¬ä½“æ¨ç†ï¼‰å‰è¨€ æœ¬è®ºæ–‡å±äºçŸ¥è¯†å›¾è°±çš„ä¸‹æ¸¸ä»»åŠ¡ï¼Œä¹Ÿå°±æ˜¯çŸ¥è¯†å›¾è°±çš„åº”ç”¨çš„ä¸€ä¸ªä¾‹å­ã€‚è¿™ç¯‡è®ºæ–‡çš„æ–¹æ³•æ ¹æ®ä½œè€…æè¿°RRNæ˜¯ç¬¬ä¸€ä¸ªåŸºäºæ·±åº¦å­¦ä¹ çš„å…¨é¢æœ¬ä½“æ¨ç†æ–¹æ³•ã€‚ ç›®æ ‡è·å¾—ä¸€ä¸ªå¯ä»¥åœ¨ä¸åŒçš„åœºæ™¯è¿›è¡Œæœ‰æ•ˆæ¨ç†çš„æ¨¡ é—®é¢˜æè¿°åŸºäºæœºå™¨å­¦ä¹ çš„æ¨ç†æ–‡ç« é€šå¸¸å‡è®¾äº†ä¸€ä¸ªç‰¹å®šçš„åº”ç”¨æ¡ˆä¾‹ï¼šè‡ªç„¶è¯­è¨€æˆ–è§†è§‰è¾“å…¥çš„æ¨ç†ã€‚ä½œè€…é‡‡ç”¨ä¸€ä¸ªä¸åŒçš„æ–¹æ³•ï¼šå°†æ­£å¼çš„æ¨ç†é—®é¢˜ä½œä¸ºèµ·ç‚¹ã€‚å¯¹äºç‰¹å®šçš„é—®é¢˜é€‰æ‹©ï¼šé€‰æ‹©ä¸€ç§åœ¨è¡¨ç°åŠ›ä¸å¦ä¸€æ–¹é¢å¤æ‚æ€§ä¹‹é—´å–å¾—é€‚å½“å¹³è¡¡çš„æ–¹æ³•é€šå¸¸æ˜¯æ˜æ™ºçš„ã€‚OWL RLæœ¬ä½“æ¨ç†æ˜¯æŒ‡ä¸€ç§å¸¸è§çš„åœºæ™¯ï¼Œåœ¨è¿™ç§åœºæ™¯ä¸­ï¼Œç”¨äºæ¨ç†çš„æ¨ç†è§„åˆ™ï¼ˆåœ¨æ­¤ä¸Šä¸‹æ–‡ä¸­ç§°ä¸ºæœ¬ä½“ï¼‰ä¸æˆ‘ä»¬å¯»æ±‚æ¨ç†çš„äº‹å®ä¿¡æ¯ä¸€èµ·æŒ‡å®šã€‚ æœ¬ä½“æ¨ç†æ˜¯ä¸€ç§éå¸¸çµæ´»çš„å·¥å…·ï¼Œå®ƒå…è®¸å¯¹å¤§é‡ä¸åŒçš„åœºæ™¯è¿›è¡Œå»ºæ¨¡ï¼Œå› æ­¤æ»¡è¶³äº†æˆ‘ä»¬å¯¹é€‚ç”¨äºå„ç§åº”ç”¨çš„ç³»ç»Ÿçš„éœ€æ±‚ã€‚==é¦–å…ˆå¼•å‡ºäº†ä»€ä¹ˆæ˜¯æœ¬è´¨æ¨ç†ï¼Œç„¶åè¿›ä¸€æ­¥é˜è¿°ä¸ºä»€ä¹ˆè¦ç”¨æœºå™¨å­¦ä¹ == ä»Šå¤©ç”¨äºæ¨ç†çš„å¤§å¤šæ•°KRRå½¢å¼éƒ½æ¤æ ¹äºç¬¦å·é€»è¾‘,è¿™äº›æ–¹æ³•åœ¨å®è·µä¸­ä¼šé‡åˆ°è®¸å¤šé—®é¢˜ï¼šä¾‹å¦‚å¤„ç†ä¸å®Œæ•´ï¼Œå†²çªæˆ–ä¸ç¡®å®šæ•°æ®çš„å›°éš¾æœºå™¨å­¦ä¹ æ¨¡å‹é€šå¸¸å…·æœ‰é«˜åº¦å¯æ‰©å±•æ€§ï¼Œæ›´èƒ½æŠµæŠ—æ•°æ®ä¸­çš„å¹²æ‰°ï¼Œå¹¶ä¸”å³ä½¿æ‰€æä¾›çš„å½¢å¼æ˜¯é”™è¯¯çš„ä¹Ÿèƒ½å¤Ÿæä¾›é¢„æµ‹ã€‚ ä½œè€…çš„ç›®æ ‡æ˜¯é€šè¿‡é‡‡ç”¨å°–ç«¯çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œç›®æ ‡æ˜¯åœ¨è¿‘ä¼¼äºå½¢å¼æ–¹æ³•çš„é«˜åº¦æœŸæœ›ï¼ˆç†è®ºï¼‰å±æ€§å’Œå¦ä¸€æ–¹é¢åˆ©ç”¨æœºå™¨å­¦ä¹ çš„ç¨³å¥æ€§ä¹‹é—´ç®¡ç†å¹³è¡¡è¡Œä¸ºã€‚ å¯¹äºç”¨äºæ¨ç†çš„çŸ¥è¯†å›¾è°±ï¼šä½œè€…é‡‡ç”¨çš„æ˜¯ç”±ä¸ªä½“ã€ç±»å’ŒäºŒå…ƒå…³ç³»ç»„æˆçš„ä¿¡æ¯æ„æˆï¼Œå…¶ä¸­ä¸ªä½“å¯¹åº”äºé¡¶ç‚¹ï¼Œå…³ç³»å¯¹åº”äºè¢«æ ‡è®°çš„æœ‰å‘è¾¹ç¼˜ï¼Œç±»å¯¹åº”äºäºŒè¿›åˆ¶é¡¶ç‚¹æ ‡ç­¾ã€‚å…³ç³»æ˜¯ä¸»ä½“å’Œå®¢ä½“ä¹‹é—´çš„å…³ç³»æˆ–è€…ä¸ªäººå’Œç±»ä¹‹é—´çš„å…³ç³»ã€‚è¿™ä¸å…³ç³»å­¦ä¹ ä¸åŒï¼šåœ¨å…³ç³»å­¦ä¹ çš„èƒŒæ™¯ä¸‹ï¼ŒçŸ¥è¯†å›¾é€šå¸¸é€šè¿‡å°†ç±»è§†ä¸ºä¸ªäººä»¥åŠå°†æˆå‘˜è§†ä¸ºæ™®é€šå…³ç³»æ¥ç®€åŒ–ã€‚ç„¶è€Œï¼Œå°±ä½œè€…çš„ç›®çš„è€Œè¨€ï¼Œæ˜ç¡®åŒºåˆ†ç±»å’Œå…³ç³»æ˜¯å¾ˆé‡è¦çš„ï¼Œå› ä¸ºåœ¨ç”¨äºæ¨ç†çš„çŸ¥è¯†å›¾è°±ä¸­ç±»å’Œå…³ç³»å¯èƒ½ä¸åŒã€‚ æ¨¡å‹æ€»è§ˆæ•´ä¸ªæ¨¡å‹æ˜¯ä»¥RRNä¸ºåŸºç¡€è¿›è¡Œæ„å»ºçš„ï¼Œæ¯ä¸ªRRNéƒ½é’ˆå¯¹ç‰¹å®šçš„æœ¬ä½“è¿›è¡Œè®­ç»ƒã€‚å½“è®­ç»ƒæ¨¡å‹åº”ç”¨äºä¸€ç»„ç‰¹å®šçš„äº‹å®æ—¶ï¼Œå®ƒåˆ†ä¸ºå¦‚ä¸‹ä¸¤ä¸ªæ­¥éª¤ï¼š å®ƒä¸ºæ‰€æœ‰çš„æ­¥éª¤ç”ŸæˆçŸ¢é‡è¡¨ç¤ºï¼Œä¹Ÿå°±æ˜¯åµŒå…¥åœ¨æ‰€è€ƒè™‘æ•°æ®ä¸­å‡ºç°çš„ä¸ªä½“ã€‚ å®ƒä»…åŸºäºè¿™äº›ç”Ÿæˆå‘é‡è®¡ç®—æŸ¥è¯¢é¢„æµ‹ åœ¨å›¾ä¸­ï¼Œ aä¸­å®ƒè€ƒè™‘ä¸€ä¸ªäº‹å®ä¸‰å…ƒç»„ï¼Œå¹¶æ ¹æ®æ•°æ®é›†é‡å¤å¤šæ¬¡ã€‚ bä¸­å®ƒæ¯è¯»å–ä¸€ä¸ªäº‹å®å°±è·å–ä¸‰å…ƒç»„ä¸­çš„ä¸ªä½“æ½œå…¥ï¼Œå¹¶å°†ä»–ä»¬çš„åé¦ˆé€å…¥æ›´æ–°å±‚ï¼Œè¯¥å±‚äº§ç”Ÿå·²æä¾›çš„åµŒå…¥çš„æ›´æ–°ç‰ˆæœ¬ï¼Œç„¶åå°†å…¶å­˜å‚¨åœ¨å‰ä¸€ä¸ªç‰ˆæœ¬çš„ä½ç½®ã€‚ cä¸­ä»éšæœºç”Ÿæˆçš„å‘é‡å¼€å§‹ï¼Œé€æ­¥æ›´æ–°åµŒå…¥ï¼Œä»¥ä¾¿å¯¹å…³äºå®ƒä»¬æ‰€ä»£è¡¨çš„ä¸ªä½“çš„äº‹å®å’Œæ¨è®ºè¿›è¡Œç¼–ç ã€‚ è¯„ä¼°ä½œè€…åœ¨å››ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šè®­ç»ƒå’Œè¯„ä¼°äº†RRNï¼Œå…¶ä¸­ä¸¤ä¸ªæ˜¯äººå·¥ç”Ÿæˆçš„ç©å…·æ•°æ®é›†ï¼Œä¸¤ä¸ªæ˜¯ä»ç°å®ä¸–ç•Œçš„æ•°æ®åº“ä¸­æå–çš„ã€‚è¿™æ ·åšçš„åŸå› ï¼š ç©å…·é—®é¢˜å…·æœ‰å¾ˆå¤§çš„ä¼˜åŠ¿ï¼Œå³å¾ˆæ˜æ˜¾æŸäº›æ¨è®ºæ˜¯å¤šä¹ˆå›°éš¾ï¼Œä»è€Œä¸ºæˆ‘ä»¬æä¾›äº†å¯¹æ¨¡å‹èƒ½åŠ›çš„ç›¸å½“å¥½çš„å°è±¡ã€‚ åœ¨ç°å®ç¯å¢ƒä¸­è¯„ä¼°æ–¹æ³•å½“ç„¶æ˜¯æ€§èƒ½ä¸å¯æˆ–ç¼ºçš„è¡¡é‡æ ‡å‡† ä½œè€…ä¸ºäº†è¯„ä¼°çœŸå®ä¸–ç•Œæ•°æ®çš„RRNæ¨¡å‹ï¼Œè¿˜ä»ä»ä¸¤ä¸ªè‘—åçš„çŸ¥è¯†åº“DBpediaå’ŒClarosä¸­æå–äº†æ•°æ®é›†ã€‚ ç»“æœ RRNèƒ½å¤Ÿæœ‰æ•ˆåœ°ç¼–ç æä¾›çš„å…³äºç±»å’Œå…³ç³»çš„äº‹å® å¯¹äºå…³ç³»çš„æ¨ç†ï¼Œå¯ä»¥çœ‹åˆ°DBpediaçš„å‡†ç¡®åº¦ç•¥ä½äº98.9ï¼…ï¼Œè€Œå…¶ä»–æ•°æ®é›†ä¸­çš„å¯å¯¼å‡ºå…³ç³»åœ¨æ‰€æœ‰æƒ…å†µä¸­è‡³å°‘99.6ï¼…è¢«æ­£ç¡®é¢„æµ‹ã€‚ å¯ä»¥é¢„æµ‹è¯¥æ¨¡å‹åœ¨é¢„æµ‹å¯æ¨æ–­ç±»åˆ«æ–¹é¢æ¯”åœ¨å…³ç³»æ–¹é¢è¡¨ç°æ›´å¥½ï¼Œå› ä¸ºå¤§å¤šæ•°è¿™äº›éƒ½æ˜¯ä»…ä¾èµ–äºå•ä¸ªä¸‰å…ƒç»„çš„æ¨è®ºã€‚ ä¸ºäº†è¯„ä¼°ä½œè€…æå‡ºçš„KRRæ–¹æ³•å¸¸å¸¸é‡åˆ°çš„é—®é¢˜ï¼Œä½œè€…è¿›è¡Œäº†å¦‚ä¸‹å®éªŒï¼š å¯¹äºç¼ºå°‘ä¿¡æ¯çš„é—®é¢˜ï¼Œä½œè€…éšæœºåˆ é™¤äº†ä¸€ä¸ªæ— æ³•é€šè¿‡æ¯ä¸ªæ ·æœ¬çš„ç¬¦å·æ¨ç†æ¨æ–­å‡ºçš„äº‹å®ï¼Œå¹¶æ£€æŸ¥æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿæ­£ç¡®åœ°é‡å»ºå®ƒã€‚ç»“æœï¼šå¯¹äºDBpediaæ¥è¯´ï¼Œ33.8ï¼…çš„å¤±è¸ªä¸‰å…ƒç»„å°±æ˜¯è¿™ç§æƒ…å†µï¼Œè€Œå¯¹äºClarosæ¥è¯´ï¼Œ38.4ï¼…è¢«æ­£ç¡®é¢„æµ‹ å¯¹äºå†²çªçš„é—®é¢˜ï¼Œä½œè€…é€šè¿‡åœ¨æ¯ä¸ªæµ‹è¯•æ ·æœ¬ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªäº‹å®æ¥æµ‹è¯•æ¨¡å‹è§£å†³å†²çªçš„èƒ½åŠ›ï¼Œå¹¶æ·»åŠ ç›¸åŒçš„å¦å®šç‰ˆæœ¬ä½œä¸ºå¦ä¸€ä¸ªäº‹å®ã€‚å¯¹äºDBpediaï¼ŒRRNæ­£ç¡®è§£å†³äº†88.4ï¼…çš„å¼•å…¥å†²çªï¼Œè€Œå¯¹äºClarosï¼Œå®ƒç”šè‡³è¾¾åˆ°äº†96.2ï¼…ã€‚ç„¶è€Œï¼Œæœ€é‡è¦çš„æ˜¯ï¼Œå¯¹äºä»»ä½•ä¸€ä¸ªæŸåçš„æ•°æ®é›†ï¼Œä¹‹å‰æŠ¥å‘Šçš„æ€»ç²¾åº¦éƒ½æ²¡æœ‰ä¸‹é™è¶…è¿‡0.9ã€‚ æ‰€æœ‰RRNçš„æŸ¥è¯¢é¢„æµ‹éƒ½å®Œå…¨åŸºäºå®ƒä¸ºå„ä¸ªæ•°æ®é›†ä¸­çš„ä¸ªä½“ç”Ÿæˆçš„åµŒå…¥ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆä»”ç»†ç ”ç©¶è¿™æ ·ä¸€ç»„åµŒå…¥å‘é‡æ˜¯æœ‰ç›Šçš„ã€‚ æ€è€ƒæœ¬è®ºæ–‡å±äºçŸ¥è¯†å›¾è°±çš„ä¸‹æ¸¸ä»»åŠ¡ï¼Œä¹Ÿå°±æ˜¯çŸ¥è¯†å›¾è°±çš„åº”ç”¨çš„ä¸€ä¸ªä¾‹å­ã€‚è¿™ç¯‡è®ºæ–‡çš„æ–¹æ³•æ ¹æ®ä½œè€…æè¿°RRNæ˜¯ç¬¬ä¸€ä¸ªåŸºäºæ·±åº¦å­¦ä¹ çš„å…¨é¢æœ¬ä½“æ¨ç†æ–¹æ³•ã€‚ä½†æ˜¯å…·ä½“çš„æ“ä½œæ–¹æ³•è®ºæ–‡ä¸­å†™çš„æ¯”è¾ƒæ¸…æ™°ï¼Œæ„Ÿè§‰è‡ªå·±æ˜¯ç†è§£äº†ã€‚é‡ç‚¹å°±æ˜¯å¯¹äºä¸ªä½“çš„åµŒå…¥è¡¨ç¤ºï¼Œå¦‚æœç±»æ¯”çš„è¯å°±æ˜¯è¯å‘é‡ï¼Œä½œè€…é€šè¿‡ä¸æ–­çš„å¤„ç†æ›´æ–°è¿™ä¸ªè¯å‘é‡ï¼Œæœ€åé€šè¿‡æ‰€è·çš„è¯å‘é‡è¿›è¡Œæ¨ç†ã€‚å¹¶ä¸”ä»è¿™ç¯‡æ–‡ç« ä¸­å¯ä»¥çœ‹åˆ°ä½œè€…ä½¿ç”¨çš„çŸ¥è¯†å›¾è°±å’Œæˆ‘ä¹‹å‰åœ¨å¼„çš„å…³ç³»ä¸‰å…ƒç»„æœ‰æ‰€åŒºåˆ«ã€‚ è®ºæ–‡ä¸‹è½½é“¾æ¥]]></content>
      <categories>
        <category>è®ºæ–‡é˜…è¯»ç¬”è®°</category>
      </categories>
      <tags>
        <tag>çŸ¥è¯†å›¾è°±</tag>
        <tag>æ·±åº¦å­¦ä¹ </tag>
        <tag>Ontology</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[åˆæ¬¡è§é¢ï¼Œä½ å¥½NYSDYï¼]]></title>
    <url>%2Fpost%2Fessay%2F</url>
    <content type="text"></content>
  </entry>
</search>
