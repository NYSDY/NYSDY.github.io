---
title: 范数与距离的联系
copyright: true
top: false
cover: false
toc: true
mathjax: true
date: 2020-10-18 16:31:38
password:
english_title: the_relation_between_norm_and_distance
tags:
- 范数
- 度量距离
categories:
- 数学基础
---

# 1 范数

向量的范数可以简单形象的理解为向量的长度，或者向量到零点的距离，或者相应的两个点之间的距离。

常用的向量的范数：

**L0范数**:为x向量各个非零元素的个数。
**L2范数**:为x向量各个元素平方和的1/2次方。L2范数又称Euclidean范数或者Frobenius范数
**Lp范数**:为x向量各个元素绝对值p次方和的1/p次方

**L∞范数**:为x向量各个元素绝对值最大那个元素的绝对值，如下：$\|\mathrm{x}\| \infty=\max \left(\left|\mathrm{x}_{1}\right|,\left|\mathrm{x}_{2}\right|, \ldots,\left|\mathrm{x}_{\mathrm{n}}\right|\right)$

## 联系和区别

也就是如果我们使用L0范数，即希望w的大部分元素都是0. （w是稀疏的）所以可以用于ML中做稀疏编码，特征选择。通过最小化L0范数，来寻找最少最优的稀疏特征项。但不幸的是，L0范数的最优化问题是一个**NP hard**问题，而且理论上有证明，**L1范数是L0范数的最优凸近似**，因此通常使用L1范数来代替。

L1范数的解通常是**稀疏性**的，倾向于选择数目较少的一些非常大的值或者数目较多的insignificant的小值。

L2范数越小，可以使得w的每个元素都很小，接近于0，但与L1范数不同的是他不会让它等于0而是接近于0.
由于**L1范数并没有平滑的函数**表示，起初L1最优化问题解决起来非常困难，但随着计算机技术的到来，利用很多凸优化算法使得L1最优化成为可能。

# 2 距离

**欧式距离（对应L2范数）**：最常见的两点之间或多点之间的距离表示法，又称之为欧几里得度量，它定义于欧几里得空间中。n维空间中两个点x1(x11,x12,…,x1n)与 x2(x21,x22,…,x2n)间的欧氏距离：$d_{12}=\sqrt{\sum_{k=1}^{n}\left(x_{1 k}-x_{2 k}\right)^{2}}$， 也可以用表示成向量运算的形式：$d_{12}=\sqrt{(a-b)(a-b)^{T}}$

**曼哈顿距离(对应L1-范数)**：也就是在欧几里得空间的固定直角坐标系上两点所形成的线段对轴产生的投影的距离总和。例如在平面上，坐标（x1, y1）的点P1与坐标（x2, y2）的点P2的曼哈顿距离为：$\left|x_{1}-x_{2}\right|+\left|y_{1}-y_{2}\right|$, 要注意的是，曼哈顿距离依赖座标系统的转度，而非系统在座标轴上的平移或映射。

**切比雪夫距离(对应L∞范数)**:若二个向量或二个点x1和x2，其坐标分别为(x11, x12, x13, ... , x1n)和(x21, x22, x23, ... , x2n)，则二者的切比雪夫距离为：d = max(|x1i - x2i|)，i从1到n。

**闵可夫斯基距离(Minkowski Distance)**，闵氏距离不是一种距离，而是一组距离的定义。对应Lp范数，p为参数。

闵氏距离的定义：两个n维变量（或者两个n维空间点）x1(x11,x12,…,x1n)与 x2(x21,x22,…,x2n)间的闵可夫斯基距离定义为： $d_{12}=\sqrt[p]{\sum_{k=1}^{n}\left|x_{1 k}-x_{2 k}\right|^{p}}$

- 其中p是一个变参数。
- 当p=1时，就是曼哈顿距离，
- 当p=2时，就是欧氏距离，
- 当p→∞时，就是切比雪夫距离，    

根据变参数的不同，闵氏距离可以表示一类的距离。

下图是二维空间p取不同值时，与原点的$L_p$距离为1的图形。

![](http://image.nysdy.com/20201018161325.png)

[Mahalanobis距离](https://zhuanlan.zhihu.com/p/46626607)：也称作马氏距离。在近邻分类法中，常采用欧式距离和马氏距离。

一些参考资料供进一步理解：

- [从K近邻算法、距离度量谈到KD树、SIFT+BBF算法](https://blog.csdn.net/v_july_v/article/details/8203674)

# 3 在机器学习中的应用

L1范数和L2范数，用于机器学习的L1正则化、L2正则化。对于线性回归模型，使用L1正则化的模型建叫做Lasso回归，使用L2正则化的模型叫做Ridge回归（岭回归）。

其作用是：

L1正则化是指权值向量w中各个元素的绝对值之和，可以产生稀疏权值矩阵（稀疏矩阵指的是很多元素为0，只有少数元素是非零值的矩阵，即得到的线性回归模型的大部分系数都是0. ），即产生一个稀疏模型，可以用于特征选择；

L2正则化是指权值向量w中各个元素的平方和然后再求平方根，可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合。

至于为什么L1正则化能增加稀疏性，L2正则化能防止过拟合，原理可查看参考资料：

- [机器学习中正则化项L1和L2的直观理解](https://blog.csdn.net/jinping_shi/article/details/52433975)

# **参考链接：**

1. [范数与距离的关系以及在机器学习中的应用](https://blog.csdn.net/kingzone_2008/article/details/15073987)
2. https://blog.csdn.net/SanyHo/article/details/105803103
3. [曼哈顿距离（L1范数）& 欧式距离（L2范数）区别](https://blog.csdn.net/SanyHo/article/details/105803103)
4. [马氏距离(Mahalanobis Distance)](https://zhuanlan.zhihu.com/p/46626607)

# 